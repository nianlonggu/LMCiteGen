{"citing_id": "2304.05526v1", "cited_id": "1104.0654", "section_title": "I. Introduction", "citation": "For many applications in the noiseless setting, simple \u2113 1 -minimization has proven to be the approach of choice due to its relative conceptual ease, implementability as a linear program, and favorable performance even when compared with tailored regularizers #REFR .", "text_before_citation": ["A linear system is, in particular, a compact means of describing a linear relationship y = \u03a8u between a sequence of of inputs u and a sequence of observations y, from which even in the most optimistic circumstances generic u can only be reconstructed up to ker \u03a8.", "However, by assuming that u is sparse in an appropriate sense, established results in sparse recovery provide favorable guarantees on exact reconstruction for an appropriate choice of optimization algorithm.", "The most common such case considered is when u is assumed to have support of size not greater than s, and is termed regular sparsity.", "Other support-based notions of sparsity include block #OTHEREFR , group #OTHEREFR , and tree-based sparsity #OTHEREFR , and are each subsumed by the more general notion of model-based sparsity #OTHEREFR .", "For each of these sparsity patterns, the literature has provided tailored optimization problems and recovery guarantees, with varying levels of robustness to noise, ease of checking, and conceptual nuance."], "text_after_citation": ["For regular sparsity, the necessary and sufficient condition for successful unique recovery is the satisfaction of the so-called nullspace property (NUP), which requires vectors in the nullspace of \u03a8 to have smaller \u2113 1 norm on s-sparse supports than on the complement of s-sparse supports (see Def. 1).", "Recent results have even shown that for any support-based notion of sparsity, there exists a straightforward extension of the NUP, termed the generalized nullspace property, which provides necessary and sufficient guarantees #OTHEREFR .", "In light of the success of this approach to signal reconstruction, recent literature has provided tailored algorithms for sparse recovery in linear dynamical systems (LDSs), where the assumption of sparsity has been variously made on the initial conditions #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , dynamics #OTHEREFR , measurement noise #OTHEREFR , and inputs #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR .", "Even with all of this prior work, there are few existing guarantees on the performance of these algorithms, and the guarantees that have been produced are typically probabilistic in nature or make restrictive assumptions on the sparsity patterns, such as the state and inputs being simultaneously sparse with respect to an orthogonal dictionary.", "As a result, many results for the general, noiseless setting, including the establishment of necessary and sufficient guarantees, have not yet appeared in the literature."], "citing_paper_content": {"title": "Necessary And Sufficient Conditions For Simultaneous State And Input Recovery Of Linear Systems With Sparse Inputs By \u2113 1 -Minimization", "abstract": "The study of theoretical conditions for recovering sparse signals from compressive measurements has received a lot of attention in the research community. In parallel, there has been a great amount of work characterizing conditions for the recovery both the state and the input to a linear dynamical system (LDS), including a handful of results on recovering sparse inputs. However, existing sufficient conditions for recovering sparse inputs to an LDS are conservative and hard to interpret, while necessary and sufficient conditions have not yet appeared in the literature. In this work, we provide (1) the first characterization of necessary and sufficient conditions for the existence and uniqueness of sparse inputs to an LDS, (2) the first necessary and sufficient conditions for a linear program to recover both an unknown initial state and a sparse input, and (3) simple, interpretable recovery conditions in terms of the LDS parameters. We conclude with a numerical validation of these claims and discuss implications and future directions."}, "cited_paper_content": {"title": "Block-Sparse Recovery Via Convex Optimization", "abstract": "Given a dictionary that consists of multiple blocks and a signal that lives in the range space of only a few blocks, we study the problem of finding a block-sparse representation of the signal, i.e., a representation that uses the minimum number of blocks. Motivated by signal/image processing and computer vision applications, such as face recognition, we consider the block-sparse recovery problem in the case where the number of atoms in each block is arbitrary, possibly much larger than the dimension of the underlying subspace. To find a block-sparse representation of a signal, we propose two classes of non-convex optimization programs, which aim to minimize the number of nonzero coefficient blocks and the number of nonzero reconstructed vectors from the blocks, respectively. Since both classes of problems are NP-hard, we propose convex relaxations and derive conditions under which each class of the convex programs is equivalent to the original non-convex formulation. Our conditions depend on the notions of mutual and cumulative subspace coherence of a dictionary, which are natural generalizations of existing notions of mutual and cumulative coherence. We evaluate the performance of the proposed convex programs through simulations as well as real experiments on face recognition. We show that treating the face recognition problem as a block-sparse recovery problem improves the state-of-the-art results by 10% with only 25% of the training data."}, "keywords": ["tailored regularizers"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For block-sparse recovery problems, non-convex optimization programs have been proposed to minimize the number of nonzero coefficient blocks and the number of nonzero reconstructed vectors from the blocks, respectively #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To recover a block-sparse representation of a signal, the authors propose two classes of non-convex optimization programs, which aim to minimize the number of nonzero coefficient blocks and the number of nonzero reconstructed vectors from the blocks, respectively, and provide convex relaxations and conditions under which each class of the convex programs is equivalent to the original non-convex formulation, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Tailored optimization problems and recovery guarantees have been proposed in the literature for various sparsity patterns, including regular sparsity, block, group, and tree-based sparsity, each with varying levels of robustness to noise, ease of checking, and conceptual nuance #REFR, which can be used as a method to address the problem of finding a block-sparse representation of a signal.", "given_citation_intent": "method", "given_keywords": ["tailored regularizers"]}]}
{"citing_id": "2304.08929v1", "cited_id": "1903.05711", "section_title": "Modelnet40:", "citation": "We follow the experimental setup of PointNetLK #REFR by dividing the dataset into two parts: 20 classes for training, and another 20 classes for cross-class testing.", "text_before_citation": ["We train and test our method on the Model-Net40 #OTHEREFR dataset, which consists of 12311 CAD models from 40 object classes, containing 3D CAD models from 40 classes."], "text_after_citation": ["The source point cloud is the vertices from ModelNet40, which we uniformly sample 1024 points and normalize to a unit box at the origin [0, 1] 3 .", "Following #OTHEREFR , the rotation along each axis is uniformly sampled in [0, 45 \u2022 ], and the translation is in [\u22120.5, 0.5].", "The source point cloud is rigidly transformed to generate the target point cloud, which is then used to train and evaluate the registration model.", "3DMatch: 3DMatch #OTHEREFR is a collection of several real-world indoor datasets and a widely used registration benchmark that captures data using Kinect cameras, including complex 3D RGB-D scans obtained from multiple scenes such as offices, hotels, and kitchens.", "Following PointNetLK-Revisited #OTHEREFR , we chose 8 categories of scenes from 7-Scenes #OTHEREFR and SUN3D #OTHEREFR for testing, and the rest are used for training."], "citing_paper_content": {"title": "Sdfreg: Learning Signed Distance Functions For Point Cloud Registration", "abstract": "Learning-based point cloud registration methods can handle clean point clouds well, while it is still challenging to generalize to noisy and partial point clouds. To this end, we propose a novel framework for noisy and partial point cloud registration. By introducing a neural implicit function representation, we replace the problem of rigid registration between point clouds with a registration problem between the point cloud and the neural implicit function. We then alternately optimize the implicit function representation and the registration between the implicit function and point cloud. In this way, point cloud registration can be performed in a coarse-to-fine manner. Since our method avoids computing point correspondences, it is robust to the noise and incompleteness of point clouds. Compared with the registration methods based on global features, our method can deal with surfaces with large density variations and achieve higher registration accuracy. Experimental results and comparisons demonstrate the effectiveness of the proposed framework."}, "cited_paper_content": {"title": "Pointnetlk: Robust & Efficient Point Cloud Registration Using Pointnet", "abstract": "PointNet has revolutionized how we think about representing point clouds. For classification and segmentation tasks, the approach and its subsequent extensions are state-of-the-art. To date, the successful application of PointNet to point cloud registration has remained elusive. In this paper we argue that PointNet itself can be thought of as a learnable \"imaging\" function. As a consequence, classical vision algorithms for image alignment can be applied on the problem - namely the Lucas & Kanade (LK) algorithm. Our central innovations stem from: (i) how to modify the LK algorithm to accommodate the PointNet imaging function, and (ii) unrolling PointNet and the LK algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efficiency - opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at this https URL."}, "keywords": ["PointNetLK"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We train and test our method on the Model-Net40 dataset, which consists of 12311 CAD models from 40 object classes, containing 3D CAD models from 40 classes. Our proposed method achieves higher registration accuracy compared with the registration methods based on global features #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of noisy and partial point cloud registration, the authors propose a novel framework that replaces the problem of rigid registration between point clouds with a registration problem between the point cloud and the neural implicit function, achieving robustness to the noise and incompleteness of point clouds #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement our proposed framework for point cloud registration, we use a learnable \"imaging\" function, which is modified from the Lucas & Kanade algorithm to accommodate the PointNet imaging function, as described in Pointnetlk: Robust & Efficient Point Cloud Registration Using Pointnet #REFR.", "given_citation_intent": "method", "given_keywords": ["PointNetLK"]}]}
{"citing_id": "2304.01409v1", "cited_id": "1609.02194", "section_title": "B. Network Design Of \u03a6 R", "citation": "Theorem 4.2 shows that the gauge map is differentiable with respect to the output of the neural layers, and hence enables the computation of backpropagation gradients in #REFR and the training of the architecture in Fig. 1 .", "text_before_citation": ["Here, we show that the non-differentiability introduced by the gauge map is no more severe than the non-differentiability that is already present in the neural network activation functions, and the end-to-end policy is differentiable almost everywhere: Theorem 4.2. Let P and Q be polyhedral C-sets.", "Standard automatic differentiation procedures, when applied to the gauge map Gp\u00a8| P, Qq, will return the gradient of Gp\u00a8| P, Qq for almost all z P P.", "Proof.", "The set P can be partitioned such that the gauge map is a different analytic function on each region of the partition (excluding the origin).", "By setting Gp0 | P, Qq :\" 0, we obtain a function for which standard automatic differentiation procedures will compute the gradient of Gp\u00a8| P, Qq at all z P P except possibly on a set of measure zero #OTHEREFR . Details are in Appendix E."], "text_after_citation": ["The effectiveness of the proposed learning architecture is validated on a modified IEEE 118-bus system as shown in the next section."], "citing_paper_content": {"title": "An Efficient Learning-Based Solver For Two-Stage Dc Optimal Power Flow With Feasibility Guarantees", "abstract": "In this paper, we consider the scenario-based twostage stochastic DC optimal power flow (OPF) problem for optimal and reliable dispatch when the load is facing uncertainty. Although this problem is a linear program, it remains computationally challenging to solve due to the large number of scenarios needed to accurately represent the uncertainties. To mitigate the computational issues, many techniques have been proposed to approximate the second-stage decisions so they can dealt more efficiently. The challenge of finding good policies to approximate the second-stage decisions is that these solutions need to be feasible, which has been difficult to achieve with existing policies. To address these challenges, this paper proposes a learning method to solve the two-stage problem in a more efficient and optimal way. A technique called the gauge map is incorporated into the learning architecture design to guarantee the learned solutions' feasibility to the network constraints. Namely, we can design policies that are feed forward functions that only output feasible solutions. Simulation results on standard IEEE systems show that, compared to iterative solvers and the widely used affine policy, our proposed method not only learns solutions of good quality but also accelerates the computation by orders of magnitude."}, "cited_paper_content": {"title": "Corrective Control To Handle Forecast Uncertainty: A Chance Constrained Optimal Power Flow", "abstract": "Higher shares of electricity generation from renewable energy sources and market liberalization is increasing uncertainty in power systems operation. At the same time, operation is becoming more flexible with improved control systems and new technology such as phase shifting transformers (PSTs) and high voltage direct current connections (HVDC). Previous studies have shown that the use of corrective control in response to outages contributes to a reduction in operating cost, while maintaining N-1 security. In this work, we propose a method to extend the use of corrective control of PSTs and HVDCs to react to uncertainty. We characterize the uncertainty as continuous random variables, and define the corrective control actions through affine control policies. This allows us to efficiently model control reactions to a large number of uncertainty sources. The control policies are then included in a chance constrained optimal power flow formulation, which guarantees that the system constraints are enforced with a desired probability. By applying an analytical reformulation of the chance constraints, we obtain a second-order cone problem for which we develop an efficient solution algorithm. In a case study for the IEEE 118 bus system, we show that corrective control for uncertainty leads to a decrease in operational cost, while maintaining system security. Further, we demonstrate the scalability of the method by solving the problem for the IEEE 300 bus and the Polish system test cases."}, "keywords": ["backpropagation gradients"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In a case study for the IEEE 118 bus system, the authors of the proposed method for corrective control to handle forecast uncertainty showed that it leads to a decrease in operational cost while maintaining system security #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently model control reactions to a large number of uncertainty sources, the authors proposed a method to extend the use of corrective control of PSTs and HVDCs to react to uncertainty, which is characterized as continuous random variables and defined through affine control policies, as described in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a method to extend the use of corrective control of PSTs and HVDCs to react to uncertainty, and the analytical reformulation of the chance constraints allows for the computation of backpropagation gradients #REFR.", "given_citation_intent": "background", "given_keywords": ["backpropagation gradients"]}]}
{"citing_id": "2304.02478v1", "cited_id": "1904.09751", "section_title": "Limitations And Future Research", "citation": "Another limitation is such tools' propensity to degenerate, that is, to generate bland and repetitive text #REFR .", "text_before_citation": ["Subsequent research could fine-tune such models to improve their capacity to generate appropriate language for the target text type and explore such models' contributions to student writing.", "Besides, study should expand to the largest, proprietary language language models like ChatGPT and to AI-NLG tools that can generate several paragraphs of text.", "Although AI-NLG tools hold promise to improve students' writing, SOTA AI-NLG tools that predict text suffer limitations and their use raises risk in the writing classroom.", "One limitation is such tools' propensity to hallucinate, that is, to generate text that deviates from its source input and fails to meet user expectations #OTHEREFR .", "Such text can comprise offensive or biased material #OTHEREFR , or factually incorrect and nonsensical answers, although plausible sounding #OTHEREFR ."], "text_after_citation": ["Therefore, research will be necessary into prompt programming #OTHEREFR of SOTA AI-NLG tools in the writing classroom, that is, the careful selection of input by which a student interacts with and controls an AI-NLG tool to perform a desired task.", "Since rewriting a prompt can significantly change a tool's performance, future studies should investigate the conditions under which a student would prompt a SOTA AI-NLG tool, what prompts students have used to generate text, and whether students perceive any AI-generated text as satisfactory for integration into a composition.", "Future research may explore students' interactions with AI-NLG tools in the writing process, using methods such as screen recordings, think-aloud protocols, interviews, and stimulated recalls, to provide a complete understanding of how the tools contribute to students'", "writing and how different students use AI for writing.", "16 3 3 4 10 3 3 3 9 3 3 3.5 9.5 0 10"], "citing_paper_content": {"title": "Exploring Ai-Generated Text In Student Writing: How Does Ai Help?", "abstract": "English as a foreign language (EFL) students' use of text generated from artificial intelligence (AI) natural language generation (NLG) tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higherquality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language, and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of the clusters reveal some benefit of AI-generated text in improving the quality of both high-scoring students' and low-scoring students' writing. The findings can inform pedagogical strategies to use AI-generated text for EFL students' writing and to address digital divides. This study contributes designs of NLG tools and writing activities to implement AI-generated text in schools."}, "cited_paper_content": {"title": "The Curious Case Of Neural Text Degeneration", "abstract": "Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration \u2014 output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is the best decoding strategy for generating long-form text that is both high-quality \u2014 as measured by human evaluation \u2014 and as diverse as human-written text."}, "keywords": ["repetitive text"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of AI-NLG tools in generating appropriate language for the target text type, subsequent research could fine-tune such models and explore their contributions to student writing. As noted in a recent study, the use of Nucleus Sampling, a method proposed in #REFR, can draw considerably higher quality text out of neural language models and avoid text degeneration by truncating the unreliable tail of the probability distribution.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of AI-NLG tools in generating appropriate language for the target text type, subsequent research could explore Nucleus Sampling, a method proposed by Holtzman et al. that draws considerably higher quality text out of neural language models by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of maximization-based decoding methods such as beam search can lead to degeneration, resulting in repetitive and incoherent text, as shown in the study by Holtzman et al. (2019) on neural text degeneration #REFR.", "given_citation_intent": "background", "given_keywords": ["repetitive text"]}]}
{"citing_id": "2305.00508v1", "cited_id": "1802.01561", "section_title": "Experiment Setup", "citation": "We train a vanilla IMPALA #REFR agent for the initial data gathering in each environment.", "text_before_citation": ["In TreeMaze, every interaction, including opening a door, picking up a key, and interacting with other objects, is seen as an achievement.", "The rooms are generated with the guarantee that it's possible to unlock all achievements.", "The locations of the rooms are randomized but the dependency graph is fixed.", "We use three versions of TreeMaze in our experiments, the TreeMaze-Easy which contains 4 rooms and 9 achievements, the TreeMaze-Normal which contains 10 rooms and 19 achievements and the TreeMaze-Hard that contains 18 rooms and 30 achievements.", "For the Crafter environment, we also include a limited version (Crafter-Limited) which only contains 7 of the 21 achievements in the full environment."], "text_after_citation": ["In the full Crafter environment, the IMPALA agent discovers 17 achievements (with > 1% reaching rate); In all the remaining environments, the IMPALA agents discover all of the achievements.", "We consider the 17 achievements reachable by a vanilla IM-PALA agent as the easy achievement set, and the remaining four achievements (make iron pickaxe, make iron sword, defeat zombie, collect diamond) as the hard achievement set.", "The reason why these achievements are particularly hard to complete is that crafting iron tools in Crafter has three prerequisites: the character near a crafting table, the character near a furnace, the character has at least one wood, one coal, and one iron in the inventory.", "Of the 4 hard achievements, collect diamond is objectively the hardest achievement to unlock as not only does it depends on the make iron pickaxe, but also that the diamond resource is extremely scarce in the game. The complete achievement list can be found in Sec. B.1."], "citing_paper_content": {"title": "Learning Achievement Structure For Struc-Tured Exploration In Domains With Sparse Re-Ward", "abstract": "We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with highdimensional observations like images."}, "cited_paper_content": {"title": "Impala: Scalable Distributed Deep-Rl With Importance Weighted Actor-Learner Architectures", "abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time, which is already a problem in single task learning. We have developed a new distributed agent IMPALA (Importance-Weighted Actor Learner Architecture) that can scale to thousands of machines and achieve a throughput rate of 250,000 frames per second. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace, which was critical for achieving learning stability. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents, use less data and crucially exhibits positive transfer between tasks as a result of its multi-task approach."}, "keywords": ["initial data gathering", "vanilla IMPALA agent"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For achieving stable learning at high throughput, the authors of SEA could benefit from using the distributed agent IMPALA (Importance-Weighted Actor Learner Architecture) proposed by Espeholt et al. in their work on scalable distributed deep-RL with importance weighted actor-learner architectures #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To handle the increased amount of data and extended training time in achievement-based environments, a new distributed agent called IMPALA (Importance-Weighted Actor Learner Architecture) has been developed that can scale to thousands of machines and achieve a throughput rate of 250,000 frames per second, which was critical for achieving learning stability. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To handle the increased amount of data and extended training time in multi-task learning, the authors of this manuscript propose a new distributed agent called IMPALA (Importance-Weighted Actor Learner Architecture) that can scale to thousands of machines and achieve a throughput rate of 250,000 frames per second, which could be a potential solution for initial data gathering and outperform the vanilla IMPALA agent. #REFR", "given_citation_intent": "method", "given_keywords": ["initial data gathering", "vanilla IMPALA agent"]}]}
{"citing_id": "2304.09620v1", "cited_id": "1904.09730", "section_title": "Ablation Experiments", "citation": "As compared to ELAN, the results of the studies show that using DCELAN results in improvements in mDice and mIOU of 4.82% and 2.78%, respectively, on the CVC-ClinicDB #REFR dataset.", "text_before_citation": ["The following ablation experiments were carried out by us in order to evaluate the effectiveness of the DCELANM-Net framework in a variety of diverse environments. The results are shown in Table 4 .", "In our studies, we compared the performances of ELAN, DCELAN, and DCELAN + Micro-MAE on two different datasets.", "These datasets were CVC-ClinicDB and Kvasir-SEG #OTHEREFR , respectively."], "text_after_citation": ["When compared to ELAN, the performance of mDice and mIOU under DCELAN+Micro-MAE was enhanced by 6.77% and 5.37%, respectively.", "DCELAN performed significantly better than ELAN, mDice, and mIOU on the Kvasir-SEG #OTHEREFR dataset, with a 6.03% and 5.43% improvement, respectively.", "When compared to ELAN, mDice, and mIOU, the performance of DCELAN+Micro-MAE was enhanced by 7.90% and 5.78%, respectively.", "As a result, it is possible to demonstrate that the performance of the combined DCELANM-Net framework is better when compared."], "citing_paper_content": {"title": "Dcelanm-Net:Medical Image Segmentation Based On Dual Channel Efficient Layer Aggregation Network With Learner", "abstract": "Throughout the writing of this dissertation I have received a great deal of support and assistance. I would first like to thank my supervisor, Jun Su, whose expertise was invaluable in formulating the research questions and methodology. Your insightful feedback pushed me to sharpen my thinking and brought my work to a higher level. I would particularly like to acknowledge my teammate/group mate/team members, Zhangrun Xia, Krzysztof Przystupa and Orest Kochan for their wonderful collaboration and patient support Abstract. Segmenting medical images is a principal component of computer vision. The UNet model framework has taken over as the standard framework for this activity across a wide range of medical picture segmentation applications. Due to CNN's convolution operation limitations, the model's global modeling ability isn't absolutely perfect. Moreover, a single convolution operation cannot gather feature information at various scales, which will have an impact on the quality of the global feature extraction as well as the localization of local details. The DCELANM-Net structure, which this article offers, is a model that ingeniously combines a Dual Channel Efficient Layer Aggregation Network (DCELAN) and a Micro Masked Autoencoder (Micro-MAE). On the one hand, for the DCELAN, the features are more effectively fitted by deepening the network structure; the deeper network can successfully learn and fuse the features, which can more accurately locate the local feature information; and the utilization of each layer of channels is more effectively improved by widening"}, "cited_paper_content": {"title": "An Energy And Gpu-Computation Efficient Backbone Network For Real-Time Object Detection", "abstract": "As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet."}, "keywords": ["CVC-ClinicDB dataset"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In our studies, we compared the performances of ELAN, DCELAN, and DCELAN + Micro-MAE on two different datasets. These datasets were CVC-ClinicDB and Kvasir-SEG #REFR, respectively, citing the reference paper on an energy and GPU-computation efficient backbone network for real-time object detection.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the results shown in Table 4, our DCELANM-Net outperforms ELAN, DCELAN, and DCELAN + Micro-MAE on both CVC-ClinicDB and Kvasir-SEG datasets, achieving faster speed and better energy efficiency #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the results shown in Table 4, our proposed DCELANM-Net outperforms ELAN, DCELAN, and DCELAN + Micro-MAE on the CVC-ClinicDB dataset, achieving state-of-the-art performance with faster speed and better energy efficiency #REFR.", "given_citation_intent": "result", "given_keywords": ["CVC-ClinicDB dataset"]}]}
{"citing_id": "2303.09515v1", "cited_id": "1908.01503", "section_title": "I. Introduction", "citation": "In the context of networked feedback systems, the AoI-based policies have been proposed for solving resource allocation and enduser uncertainty reduction problems as in #REFR .", "text_before_citation": ["Further, we improve upon the convergence guarantees in #OTHEREFR for the case where the network is erasure free by proposing a novel scheduling policy.", "Finally, we employ this policy to construct an approximate Nash solution for the finite-agent consensus problem.", "In literature, the early works #OTHEREFR , #OTHEREFR have dealt with an optimal control problem with unreliable communication, albeit, for a single agent system and an unconstrained network under the TCP and the UDP communication protocols.", "The work #OTHEREFR extends the setting to multi-agent games; however, the considered network is unconstrained.", "In order to measure timeliness in communication networks, age of information (AoI) has been introduced as a potential metric."], "text_after_citation": ["Recently, age of incorrect information (AoII) is proposed for solving multiagent remote state estimation problems #OTHEREFR .", "Age-optimal scheduling policies have been considered with Markovian error-prone channel state in #OTHEREFR , #OTHEREFR , with unknown erasure probabilities in #OTHEREFR , and over erroneous broadcast channels in #OTHEREFR .", "A more detailed literature review on age-optimal scheduling policies can be found in #OTHEREFR .", "To appropriately handle the concerns of increasing network interactions, one of the most relevant framework is that of mean-field games (MFGs) #OTHEREFR , #OTHEREFR .", "It leads one to circumvent the issues posed by scalability, by allowing for a representative agent to play against the population, although, at the cost of entailing an approximate equilibrium solution to the finite-agent consensus problem."], "citing_paper_content": {"title": "Large Population Games On Constrained Unreliable Networks", "abstract": "This paper studies an N-agent cost-coupled game where the agents are connected via an unreliable capacity constrained network. Each agent receives state information over that network which loses packets with probability p. A Base station (BS) actively schedules agent communications over the network by minimizing a weighted Age of Information (WAoI) based cost function under a capacity limit C < N on the number of transmission attempts at each instant. Under a standard information structure, we show that the problem can be decoupled into a scheduling problem for the BS and a game problem for the N agents. Since the scheduling problem is an NP hard combinatorics problem, we propose an approximately optimal solution which approaches the optimal solution as N \u2192 \u221e. In the process, we also provide some insights on the case without channel erasure. Next, to solve the large population game problem, we use the mean-field game framework to compute an approximate decentralized Nash equilibrium. Finally, we validate the theoretical results using a numerical example."}, "cited_paper_content": {"title": "Optimal Scheduling For Discounted Age Penalty Minimization In Multi-Loop Networked Control", "abstract": "Age-of-information (AoI) is a metric quantifying information freshness at the receiver. Since AoI combines packet generation frequency, packet loss, and delay into a single metric, it has received a lot of research attention as an interface between communication network and application. In this work, we apply AoI to the problem of wireless scheduling for multi-loop networked control systems (NCS), i.e., feedback control loops closed over a shared wireless network. We model the scheduling problem as a Markov decision process (MDP) with AoI as its observable states and derive a relation of control system error and AoI. We further derive a stationary scheduling policy to minimize control error over an infinite horizon. We show that our scheduler outperforms the state-of-the-art scheduling policies for NCS. To the best of our knowledge, this is the first work proposing an AoI-based wireless scheduling policy that minimizes the control error over an infinite horizon for multi-loop NCS."}, "keywords": ["networked feedback systems"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In order to measure timeliness in communication networks, age of information (AoI) has been introduced as a potential metric. The authors need to cite the reference paper #REFR, which proposes an AoI-based wireless scheduling policy that minimizes the control error over an infinite horizon for multi-loop networked control systems.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In order to measure timeliness in communication networks, the authors propose a novel scheduling policy based on age of information (AoI) in #REFR, which can be applied to multi-loop networked control systems to minimize control error over an infinite horizon.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In order to measure timeliness in communication networks for networked feedback systems, age of information (AoI) has been introduced as a potential metric, and the authors of this manuscript propose a novel scheduling policy based on AoI for multi-loop networked control systems, which outperforms the state-of-the-art scheduling policies for NCS, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": ["networked feedback systems"]}]}
{"citing_id": "2304.01666v1", "cited_id": "1912.01072", "section_title": "Approaches To Cssdetection", "citation": "Typically, the prototypes \u00b5 1 w and \u00b5 2 w are determined through aggregation by averaging over \u03a6 1 w and \u03a6 2 w , respectively (e.g., #REFR ).", "text_before_citation": ["Cosine distance (CD).", "The shift s w is measured as the cosine distance (CD) between the word prototypes \u00b5 1 w , \u00b5 2 w as follows:", "EQUATION", "where CS is the cosine similarity between the prototypes.", "Intuitively, the greater the CD(\u00b5 1 w , \u00b5 2 w ), the greater the shift in the dominant sense of w."], "text_after_citation": ["As a difference, in #OTHEREFR , the prototype embedding \u00b5 2 w at time step t = 2 is computed by updating the prototype embedding \u00b5 1 w at time step t = 1 through a weighted running average (e.g., #OTHEREFR ).", "In #OTHEREFR", "[2020b] , the CD metric is employed in a multilingual experiment where the shift is measured across a diachronic corpus with texts of different languages. This is the only example of cross-language shift detection.", "CD is also used in time-aware approaches.", "The integration of extra-linguistic information into word embeddings, such as time and social space, has been proposed in previous work based on static models #OTHEREFR Blei, 2018, Zeng et al., 2018] ."], "citing_paper_content": {"title": "A Survey On Contextualised Semantic Shift Detection", "abstract": "Semantic Shift Detection (SSD) is the task of identifying, interpreting, and assessing the possible change over time in the meanings of a target word. Traditionally, SSD has been addressed by linguists and social scientists through manual and time-consuming activities. In the recent years, computational approaches based on Natural Language Processing and word embeddings gained increasing attention to automate SSD as much as possible. In particular, over the past three years, significant advancements have been made almost exclusively based on word contextualised embedding models, which can handle the multiple usages/meanings of the words and better capture the related semantic shifts. In this paper, we survey the approaches based on contextualised embeddings for SSD (i.e., CSSDetection) and we propose a classification framework characterised by meaning representation, time-awareness, and learning modality dimensions. The framework is exploited i) to review the measures for shift assessment, ii) to compare the approaches on performance, and iii) to discuss the current issues in terms of scalability, interpretability, and robustness. Open challenges and future research directions about CSSDetection are finally outlined."}, "cited_paper_content": {"title": "Leveraging Contextual Embeddings For Detecting Diachronic Semantic Shift", "abstract": "We propose a new method that leverages contextual embeddings for the task of diachronic semantic shift detection by generating time specific word representations from BERT embeddings. The results of the experiments in the domain specific LiverpoolFC corpus suggest that the proposed method has performance comparable to the current state-of-the-art without requiring any time consuming domain adaptation on a large corpora. The results on a newly created Brexit news corpus suggest that the method can be successfully used for the detection of a short-term yearly semantic shift. And lastly, the model also shows promising results in a multilingual settings, where the task was to detect differences and similarities between diachronic semantic shifts in different languages."}, "keywords": ["w", "aggregation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose a new method for diachronic semantic shift detection that leverages contextual embeddings to generate time-specific word representations from BERT embeddings #REFR. The method shows promising results in domain-specific and multilingual settings, and does not require time-consuming domain adaptation on large corpora.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method for diachronic semantic shift detection leverages contextual embeddings to generate time-specific word representations from BERT embeddings, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The method proposed by the authors leverages contextual embeddings to generate time-specific word representations from BERT embeddings, which can be used for detecting diachronic semantic shift in a short-term yearly semantic shift and multilingual settings, and has performance comparable to the current state-of-the-art without requiring any time-consuming domain adaptation on a large corpora. #REFR", "given_citation_intent": "background", "given_keywords": ["w", "aggregation"]}]}
{"citing_id": "2305.02461v1", "cited_id": "1901.10696", "section_title": "Background And Related Work", "citation": "There is some debate about these conflicting results #REFR , whether they arise from Urbano et al.'s use of parametric distributions, but Urbano et al.", "text_before_citation": ["Simulation allowed them to control whether the null hypothesis was true or not, and compute actual error rates and power. Parapar et al.", "#OTHEREFR 's method simulates new runs for the same topic while Urbano et al.", "#OTHEREFR 's approach simulates new topics for the same system. Based on their findings, Urbano et al.", "#OTHEREFR recommended the use of the -test while Parapar et al.", "#OTHEREFR recommended the use of the sign and Wilcoxon tests."], "text_after_citation": ["#OTHEREFR argue that their methods would favor the Wilcoxon test if it was appropriate.", "We treat the topic evaluation scores of a system as observations of a random variable following Sakai #OTHEREFR .", "Therefore, we adopt the method proposed by Urbano and Nagler #OTHEREFR for simulation."], "citing_paper_content": {"title": "Significance Testing For Large Search And Recommendation Experiments", "abstract": "A number of information retrieval studies have been done to assess which statistical techniques are appropriate for comparing systems. However, these studies are focused on TREC-style experiments, which typically have fewer than 100 topics. There is no similar line of work for large search and recommendation experiments; such studies typically have thousands of topics or users and much sparser relevance judgements, so it is not clear if recommendations for analyzing traditional TREC experiments apply to these settings. In this paper, we empirically study the behavior of significance tests with large search and recommendation evaluation data. Our results show that the Wilcoxon and Sign tests show significantly higher Type-1 error rates for large sample sizes than the bootstrap, randomization and t-tests, which were more consistent with the expected error rate. While the statistical tests displayed differences in their power for smaller sample sizes, they showed no difference in their power for large sample sizes. We recommend the sign and Wilcoxon tests should not be used to analyze large scale evaluation results. Our result demonstrate that with Top-recommendation and large search evaluation data, most tests would have a 100% chance of finding statistically significant results. Therefore, the effect size should be used to determine practical or scientific significance."}, "cited_paper_content": {"title": "Using Score Distributions To Compare Statistical Significance Tests For Information Retrieval Evaluation", "abstract": "Preprint of our Journal of the Association for Information Science and Technology (JASIST) paper"}, "keywords": ["conflicting results", "parametric distributions"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Simulation allowed them to control whether the null hypothesis was true or not, and compute actual error rates and power. Parapar et al.'s method simulates new runs for the same topic while Urbano et al.'s approach simulates new topics for the same system. Based on their findings, Urbano et al. recommended the use of the -test while Parapar et al. recommended the use of the sign and Wilcoxon tests. In addition, the Wilcoxon and Sign tests were found to have significantly higher Type-1 error rates for large sample sizes than the bootstrap, randomization and t-tests, which were more consistent with the expected error rate, according to the study by Voorhees and Harman #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Parapar et al. recommended the use of the sign and Wilcoxon tests for comparing statistical significance tests for information retrieval evaluation, but according to the results of the study presented in this manuscript, these tests should not be used to analyze large scale evaluation results (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Parapar et al. #REFR recommended the use of the sign and Wilcoxon tests, while the results of this study show that these tests have significantly higher Type-1 error rates for large sample sizes than the bootstrap, randomization, and t-tests, which are more consistent with the expected error rate, leading to conflicting results in the evaluation of parametric distributions.", "given_citation_intent": "result", "given_keywords": ["conflicting results", "parametric distributions"]}]}
{"citing_id": "2303.01549v1", "cited_id": "1812.06860", "section_title": "Ii. Problem Statement", "citation": "To this end, we formally introduce the concept of Probabilistic Reachable Set as follows #REFR .", "text_before_citation": ["It contains all possible reachable states x k at time k, for any possible states x k\u22121 at time k \u2212 1, any possible control inputs u k\u22121 without violating actuator constraints, any model parameters \u03b8 k\u22121 , and any disturbances w k\u22121 .", "The goal of performing reachability analysis is to figure out a reachable set X k .", "However, since the sets U k\u22121 , \u0398, W, and X 0 are not limited to bounded sets, the reachable set X k may be unbounded.", "Hence, it's impossible to find a bounded set in which the state x k is guaranteed to lie.", "Instead, we expect to find a bounded set such that the probability of the state x k lying in the bounded set is greater than a confidence level."], "text_after_citation": ["Definition 2 (Probabilistic Reachable Set).", "At time k, a bounded set is defined to be a probabilistic reachable setX k of the dynamic system eq. (1) at confidence level \u03b1 if and only if", "Pr", "EQUATION", "A probabilistic reachable set is a set of possible states that a system can reach with a certain probability. Note thatX k may not exist."], "citing_paper_content": {"title": "Convex Approximation For Probabilistic Reachable Set Under Data-Driven Uncertainties", "abstract": "This paper is proposed to efficiently provide a convex approximation for the probabilistic reachable set of a dynamic system in the face of uncertainties. When the uncertainties are not limited to bounded ones, it may be impossible to find a bounded reachable set of the system. Instead, we turn to find a probabilistic reachable set that bounds system states with confidence. A data-driven approach of Kernel Density Estimator (KDE) accelerated by Fast Fourier Transform (FFT) is customized to model the uncertainties and obtain the probabilistic reachable set efficiently. However, the irregular or non-convex shape of the probabilistic reachable set refrains it from practice. For the sake of real applications, we formulate an optimization problem as Mixed Integer Nonlinear Programming (MINLP) whose solution accounts for an optimal n-sided convex polygon to approximate the probabilistic reachable set. A heuristic algorithm is then developed to solve the MINLP efficiently while ensuring accuracy. The results of comprehensive case studies demonstrate the near-optimality, accuracy, efficiency, and robustness enjoyed by the proposed algorithm. The benefits of this work pave the way for its promising applications to safety-critical real-time motion planning of uncertain systems."}, "cited_paper_content": {"title": "Recursively Feasible Stochastic Model Predictive Control Using Indirect Feedback", "abstract": "We present a stochastic model predictive control (MPC) method for linear discrete-time systems subject to possibly unbounded and correlated additive stochastic disturbance sequences. Chance constraints are treated in analogy to robust MPC using the concept of probabilistic reachable sets for constraint tightening. We introduce an initialization of each MPC iteration which is always recursively feasibility and thereby allows that chance constraint satisfaction for the closed-loop system can readily be shown. Under an i.i.d. zero mean assumption on the additive disturbance, we furthermore provide an average asymptotic performance bound. Two examples illustrate the approach, highlighting feedback properties of the novel initialization scheme, as well as the inclusion of time-varying, correlated disturbances in a building control setting."}, "keywords": ["Probabilistic Reachable Set"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of unbounded reachable sets, a probabilistic reachable set can be used to bound system states with confidence. This approach has been successfully applied in various fields, such as building control #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The concept of probabilistic reachable sets for constraint tightening is used in stochastic model predictive control (MPC) for linear discrete-time systems subject to possibly unbounded and correlated additive stochastic disturbance sequences, as presented in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The concept of probabilistic reachable sets for constraint tightening is used to treat chance constraints in analogy to robust MPC in the face of uncertainties, as presented in the reference paper #REFR. This approach is particularly useful when dealing with unbounded and correlated additive stochastic disturbance sequences, and it allows finding a bounded set such that the probability of the state x_k lying in the bounded set is greater than a confidence level, which is the essence of the Probabilistic Reachable Set.", "given_citation_intent": "background", "given_keywords": ["Probabilistic Reachable Set"]}]}
{"citing_id": "2303.05186v1", "cited_id": "1906.11527", "section_title": "Introduction", "citation": "RL algorithms have multiple hyperparameters that require careful tuning as it is a core aspect of obtaining the state-of-the-art performance #REFR .", "text_before_citation": ["Reinforcement Learning (RL) is a sub-field of Machine Learning with a great success in applications such as self-driving cars, industry automation, among many others #OTHEREFR .", "In RL, autonomous agents learn through trial-and-error how to find optimal solutions to a problem #OTHEREFR ."], "text_after_citation": ["The search for the best hyperparameter configuration is a sequential decision process in which initial values are set, and later adjusted, through a mixture of intuition and trial-and-error, to optimise an observed performance to maximise the accuracy or minimise the loss #OTHEREFR .", "Hyperparameter Optimisation (HPO) often requires expensive manual or automated hyperparameter searches in order to perform properly on an application domain #OTHEREFR .", "However, a noticeable limitation is the high cost related to algorithm evaluation, which makes the tuning process highly inefficient, computational expensive, and commonly adds extra algorithm developing overheads to the RL agent decision-making processes #OTHEREFR .", "The full behaviour of complex RL systems often only emerges during operation.", "They thus need to be monitored at runtime to check that they adhere to their requirements #OTHEREFR ."], "citing_paper_content": {"title": "A Framework For History-Aware Hyperparameter Optimisation In Reinforcement Learning", "abstract": "A Reinforcement Learning (RL) system depends on a set of initial conditions (hyperparameters) that affect the system's performance. However, defining a good choice of hyperparameters is a challenging problem. Hyperparameter tuning often requires manual or automated searches to find optimal values. Nonetheless, a noticeable limitation is the high cost of algorithm evaluation for complex models, making the tuning process computationally expensive and time-consuming. In this paper, we propose a framework based on integrating complex event processing and temporal models, to alleviate these trade-offs. Through this combination, it is possible to gain insights about a running RL system efficiently and unobtrusively based on data stream monitoring and to create abstract representations that allow reasoning about the historical behaviour of the RL system. The obtained knowledge is exploited to provide feedback to the RL system for optimising its hyperparameters while making effective use of parallel resources. We introduce a novel history-aware epsilon-greedy logic for hyperparameter optimisation that instead of using static hyperparameters that are kept fixed for the whole training, adjusts the hyperparameters at runtime based on the analysis of the agent's performance over time windows in a single agent's lifetime. We tested the proposed approach in a 5G mobile communications case study that uses DQN, a variant of RL, for its decision-making. Our experiments demonstrated the effects of hyperparameter tuning using history on training stability and reward values. The encouraging results show that the proposed history-aware framework significantly improved performance compared to traditional hyperparameter tuning approaches."}, "cited_paper_content": {"title": "Hyp-Rl : Hyperparameter Optimization By Reinforcement Learning", "abstract": "Hyperparameter tuning is an omnipresent problem in machine learning as it is an integral aspect of obtaining the state-of-the-art performance for any model. Most often, hyperparameters are optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample (grid search). More recently, methods have been introduced that build a so-called surrogate model that predicts the validation loss for a specific hyperparameter setting, model and dataset and then sequentially select the next hyperparameter to test, based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function (sequential model-based Bayesian optimization, SMBO). ::: In this paper we model the hyperparameter optimization problem as a sequential decision problem, which hyperparameter to test next, and address it with reinforcement learning. This way our model does not have to rely on a heuristic acquisition function like SMBO, but can learn which hyperparameters to test next based on the subsequent reduction in validation loss they will eventually lead to, either because they yield good models themselves or because they allow the hyperparameter selection policy to build a better surrogate model that is able to choose better hyperparameters later on. Experiments on a large battery of 50 data sets demonstrate that our method outperforms the state-of-the-art approaches for hyperparameter learning."}, "keywords": ["multiple hyperparameters"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Reinforcement Learning (RL) is an integral aspect of obtaining the state-of-the-art performance for any model, and hyperparameter tuning is an omnipresent problem in machine learning #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Hyperparameter tuning is a crucial aspect of obtaining the state-of-the-art performance for any model in machine learning, and recent methods have been introduced to address this issue, such as sequential model-based Bayesian optimization (SMBO) and Hyp-Rl, a reinforcement learning-based approach proposed by #REFR. #OTHEREFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Multiple hyperparameters are often optimized just by training a model on a grid of possible hyperparameter values and taking the one that performs best on a validation sample, but more recently, methods have been introduced that build a surrogate model that predicts the validation loss for a specific hyperparameter setting, model, and dataset, and then sequentially select the next hyperparameter to test based on a heuristic function of the expected value and the uncertainty of the surrogate model called acquisition function #REFR.", "given_citation_intent": "background", "given_keywords": ["multiple hyperparameters"]}]}
{"citing_id": "2304.13148v1", "cited_id": "1705.00648", "section_title": "Preprocessing", "citation": "The Liar dataset #REFR provides even more labels: 'true', 'mostly-true', 'half-true', 'barely-true', 'false', and 'pants-fire'.", "text_before_citation": ["If possible, the authors' recommendation for a threshold is followed.", "The original format of every dataset is shown in Table 3 .", "For instance, the Kaggle Jigsaw data #OTHEREFR is labeled on a scale from 0 to 1.", "The authors recommend using a 0.5 threshold to binarize the label, which we follow for MBIB. Also, we collapse multi-categorical labels into two categories.", "For instance, for the political bias task, 'right' and 'left' are combined into 'biased' #OTHEREFR ."], "text_after_citation": ["The first four labels are combined into a single 'true' label and the last two into one 'false' label.", "Even though social media-specific elements such as hashtags or emoticons are used in related areas such as sentiment analysis #OTHEREFR , we remove them to not deviate further from a news format.", "As an additional step, we enrich the FakeNewsNet dataset by scraping the tweets or articles referred to by the Tweet IDs given in the original resource #OTHEREFR .", "Every decision with regards to the unified format is furthermore detailed in the MBIB repository."], "citing_paper_content": {"title": "Introducing Mbib -The First Media Bias Identification Benchmark Task And Dataset Collection", "abstract": "Although media bias detection is a complex multi-task problem, there is, to date, no unified benchmark grouping these evaluation tasks. We introduce the Media Bias Identification Benchmark (MBIB), a comprehensive benchmark that groups different types of media bias (e.g., linguistic, cognitive, political) under a common framework to test how prospective detection techniques generalize. After reviewing 115 datasets, we select nine tasks and carefully propose 22 associated datasets for evaluating media bias detection techniques. We evaluate MBIB using state-of-the-art Transformer techniques (e.g., T5, BART). Our results suggest that while hate speech, racial bias, and gender bias are easier to detect, models struggle to handle certain bias types, e.g., cognitive and political bias. However, our results show that no single technique can outperform all the others significantly. We also find an uneven distribution of research interest and resource allocation to the individual tasks in media bias. A unified benchmark encourages the development of more robust systems and shifts the current paradigm in media bias detection evaluation towards solutions that tackle not one but multiple media bias types simultaneously."}, "cited_paper_content": {"title": "\"Liar, Liar Pants On Fire\": A New Benchmark Dataset For Fake News Detection", "abstract": "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model."}, "keywords": ["Liar dataset"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors used a new benchmark dataset for fake news detection called liar, which contains 12.8K manually labeled short statements in various contexts, to evaluate their approach. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Automatic fake news detection has been a challenging problem due to the lack of labeled benchmark datasets, which has limited statistical approaches to combating fake news; however, the authors of this manuscript have addressed this issue by introducing the Media Bias Identification Benchmark (MBIB) dataset, which groups different types of media bias under a common framework to test how prospective detection techniques generalize, and they recommend using a threshold of 0.5 to binarize the label, which is followed for MBIB #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To combat the lack of labeled benchmark datasets for fake news detection, the authors of this manuscript refer to the Liar dataset, a publicly available dataset consisting of 12.8K manually labeled short statements in various contexts, which can be used for fact-checking research as well as automatic fake news detection based on surface-level linguistic patterns. #REFR", "given_citation_intent": "background", "given_keywords": ["Liar dataset"]}]}
{"citing_id": "2305.00635v1", "cited_id": "1606.09375", "section_title": "Network Architectures", "citation": "Each graph convolution block applies the Chebyshev spectral graph convolution (ChebConv) #REFR followed by batch normalization and a leaky rectified linear unit (LeakyReLU).", "text_before_citation": ["In this study, we consider two network architectures, SGCN and MGCN, to investigate the effect of network architecture on the mesh inpainting performance using a self-prior.", "As its name indicates, SGCN performs the graph convolution defined on the mesh in the original resolution.", "SGCN consists of 13 graph convolution blocks and one vertex-wise fully connected layer."], "text_after_citation": ["In contrast, MGCN employs an hourglass-type encoderdecoder network architecture. The encoder and decoder consist of several computation blocks.", "Each encoder block consists of five ChebConv layers with average mesh pooling, while each decoder block consists of five ChebConv layers with mesh unpooling.", "The last decoder block is followed by a single fully connected layer.", "The filter size of each ChebConv is determined by the order K of the Chebyshev polynomial.", "We use K = 3, corresponding to the convolution over vertices in 2-ring neighbors."], "citing_paper_content": {"title": "Learning Self-Prior For Mesh Inpainting Using Self-Supervised Graph Convolutional Networks", "abstract": "This study presents a self-prior-based mesh inpainting framework that requires only an incomplete mesh as input, without the need for any training datasets. Additionally, our method maintains the polygonal mesh format throughout the inpainting process without converting the shape format to an intermediate, such as a voxel grid, a point cloud, or an implicit function, which are typically considered easier for deep neural networks to process. To achieve this goal, we introduce two graph convolutional networks (GCNs): single-resolution GCN (SGCN) and multi-resolution GCN (MGCN), both trained in a self-supervised manner. Our approach refines a watertight mesh obtained from the initial hole filling to generate a completed output mesh. Specifically, we train the GCNs to deform an oversmoothed version of the input mesh into the expected completed shape. To supervise the GCNs for accurate vertex displacements, despite the unknown correct displacements at real holes, we utilize multiple sets of meshes with several connected regions marked as fake holes. The correct displacements are known for vertices in these fake holes, enabling network training with loss functions that assess the accuracy of displacement vectors estimated by the GCNs. We demonstrate that our method outperforms traditional dataset-independent approaches and exhibits greater robustness compared to other deep-learning-based methods for shapes that less frequently appear in shape datasets."}, "cited_paper_content": {"title": "Convolutional Neural Networks On Graphs With Fast Localized Spectral Filtering", "abstract": "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}, "keywords": ["graph convolution block"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To perform graph convolution on the mesh in the original resolution, SGCN is used, which consists of 13 graph convolution blocks and one vertex-wise fully connected layer #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To investigate the effect of network architecture on mesh inpainting performance using a self-prior, the authors consider two network architectures, SGCN and MGCN, where SGCN performs the graph convolution defined on the mesh in the original resolution, and consists of 13 graph convolution blocks and one vertex-wise fully connected layer (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To perform graph convolution on the mesh, the authors utilized 13 graph convolution blocks and one vertex-wise fully connected layer, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["graph convolution block"]}]}
{"citing_id": "2304.13636v1", "cited_id": "1711.01299", "section_title": "Vi. Related Work", "citation": "For instance, BoostClean #REFR deals with the error repair task as a statistical boosting problem.", "text_before_citation": ["In fact, the above presented techniques, such as HoloClean, OpenRefine, and BARAN, do not consider the requirements imposed by the downstream ML applications.", "They tend to improve the data quality regardless of where and how the data comes from or how the data will be consumed.", "Therefore, a new set of techniques and tools has been emerged which strives to jointly optimize the cleaning and modeling tasks.", "In other words, these ML-oriented methods focus on selecting the optimal repair candidates with the objective of improving the performance of specific predictive models.", "Accordingly, these methods assume the availability of repair candidates from other ML-agnostic methods."], "text_after_citation": ["Specifically, it composes a set of weak learners into a strong learner.", "To generate the weak learners, BoostClean iteratively selects a pair of detection and repair methods, before applying them to the training set to derive a new model.", "ActiveClean #OTHEREFR is another ML-oriented method, principally employed for models with convex loss functions.", "It formulates the data cleaning task as a stochastic gradient descent problem.", "Initially, it trains a model on a dirty training set, where such a model is to be iteratively updated until reaching a global minimum."], "citing_paper_content": {"title": "Autocure: Automated Tabular Data Curation Technique For Ml Pipelines", "abstract": "Machine learning algorithms have become increasingly prevalent in multiple domains, such as autonomous driving, healthcare, and finance. In such domains, data preparation remains a significant challenge in developing accurate models, requiring significant expertise and time investment to search the huge search space of well-suited data curation and transformation tools. To address this challenge, we present AutoCure, a novel and configuration-free data curation pipeline that improves the quality of tabular data. Unlike traditional data curation methods, AutoCure synthetically enhances the density of the clean data fraction through an adaptive ensemble-based error detection method and a data augmentation module. In practice, AutoCure can be integrated with open source tools, e.g., Auto-sklearn, H2O, and TPOT, to promote the democratization of machine learning. As a proof of concept, we provide a comparative evaluation of AutoCure against 28 combinations of traditional data curation tools, demonstrating superior performance and predictive accuracy without user intervention. Our evaluation shows that AutoCure is an effective approach to automating data preparation and improving the accuracy of machine learning models."}, "cited_paper_content": {"title": "Boostclean: Automated Error Detection And Repair For Machine Learning", "abstract": "Predictive models based on machine learning can be highly sensitive to data error. Training data are often combined with a variety of different sources, each susceptible to different types of inconsistencies, and new data streams during prediction time, the model may encounter previously unseen inconsistencies. An important class of such inconsistencies is domain value violations that occur when an attribute value is outside of an allowed domain. We explore automatically detecting and repairing such violations by leveraging the often available clean test labels to determine whether a given detection and repair combination will improve model accuracy. We present BoostClean which automatically selects an ensemble of error detection and repair combinations using statistical boosting. BoostClean selects this ensemble from an extensible library that is pre-populated general detection functions, including a novel detector based on the Word2Vec deep learning model, which detects errors across a diverse set of domains. Our evaluation on a collection of 12 datasets from Kaggle, the UCI repository, real-world data analyses, and production datasets that show that Boost- Clean can increase absolute prediction accuracy by up to 9% over the best non-ensembled alternatives. Our optimizations including parallelism, materialization, and indexing techniques show a 22.2x end-to-end speedup on a 16-core machine."}, "keywords": ["BoostClean", "error repair task"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A new set of techniques and tools has emerged which strives to jointly optimize the cleaning and modeling tasks with the objective of improving the performance of specific predictive models, assuming the availability of repair candidates from other ML-agnostic methods. BoostClean, presented in #REFR, is an automated error detection and repair tool that selects an ensemble of error detection and repair combinations using statistical boosting, and can increase absolute prediction accuracy by up to 9% over the best non-ensembled alternatives.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "ML-oriented methods focus on selecting the optimal repair candidates with the objective of improving the performance of specific predictive models, assuming the availability of repair candidates from other ML-agnostic methods. BoostClean, presented in #REFR, automatically selects an ensemble of error detection and repair combinations using statistical boosting, which can increase absolute prediction accuracy by up to 9% over the best non-ensembled alternatives.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "ML-oriented methods focus on selecting the optimal repair candidates to improve the performance of specific predictive models, assuming the availability of repair candidates from other ML-agnostic methods. BoostClean, a statistical boosting-based ensemble method, automatically selects an ensemble of error detection and repair combinations to improve model accuracy, as demonstrated in their evaluation on various datasets. #REFR", "given_citation_intent": "background", "given_keywords": ["BoostClean", "error repair task"]}]}
{"citing_id": "2303.15422v1", "cited_id": "1904.09675", "section_title": "Saliency And Coverage", "citation": "Conversely, BertScore #REFR ) calculated with concatenated predictions and references captures semantic similarity, but its token-wise matching with contextual representations built in one encoding pass prevents evaluating the semantic similarity among individual phrases.", "text_before_citation": ["Desiderata: semantically similar phrases should be credited; matching should be at phrase level.", "Precision and recall with exact matching can capture saliency and coverage, while they struggle at dealing with semantic similarity."], "text_after_citation": ["To enjoy the advantage of both, we introduce phrase-level semantic-based matching and define semantic precision (SemP ), recall (SemR), and F1 (SemF 1) as follows:", "SemP (P, Y) = p\u2208P max y\u2208Y (1(sim(p,y)>\u03b1)\u2022sim(p,y))"], "citing_paper_content": {"title": "Kpeval: Towards Fine-Grained Semantic-Based Evaluation Of Keyphrase Extraction And Generation Systems", "abstract": "Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation only relies on exact matching with human references and disregards referencefree attributes. This scheme fails to recognize systems that generate keyphrases that are semantically equivalent to the references or keyphrases that have practical utility. To better understand the strengths and weaknesses of different keyphrase systems, we propose a comprehensive evaluation framework consisting of six critical dimensions: naturalness, faithfulness, saliency, coverage, diversity, and utility. For each dimension, we discuss the desiderata and design semantic-based metrics that align with the evaluation objectives. Rigorous meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using this framework, we re-evaluate 18 keyphrase systems and further discover that (1) the best model differs in different dimensions, with pre-trained language models achieving the best in most dimensions; (2) the utility in downstream tasks does not always correlate well with reference-based metrics; and (3) large language models exhibit a strong performance in reference-free evaluation."}, "cited_paper_content": {"title": "Bertscore: Evaluating Text Generation With Bert", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics."}, "keywords": ["semantic similarity"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To credit semantically similar phrases, it is important to move beyond exact matching at the phrase level. Precision and recall can capture saliency and coverage, but they struggle with semantic similarity. To address this issue, BERTScore has been proposed as an automatic evaluation metric for text generation that computes token similarity using contextual embeddings. BERTScore has been shown to correlate better with human judgments and provide stronger model selection performance than existing metrics, making it a valuable tool for evaluating keyphrase extraction and generation systems. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To credit semantically similar phrases, the proposed BERTScore metric computes token similarity using contextual embeddings instead of exact matches, which aligns with the desiderata discussed in this study (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To credit semantically similar phrases, the proposed BERTScore metric computes token similarity using contextual embeddings instead of exact matches, which aligns with the evaluation objectives of the proposed framework. #REFR", "given_citation_intent": "background", "given_keywords": ["semantic similarity"]}]}
{"citing_id": "2303.07354v1", "cited_id": "1911.00172", "section_title": "Continual Learning Performance", "citation": "One challenge with MetaTroll under this continual learning evaluation is that at test time it needs to know which adapter to use -information that most other systems do not require as they don't have campaign-specific parameters (exception: AdBERT and GPT-3 #REFR ).", "text_before_citation": ["This is a more realistic setting for a troll detection system, as it should continually adapt to new campaigns that appear over time.", "But in this setting it will suffer from catastrophic forgetting #OTHEREFR , where after adapting to newer campaigns its performance to classify older campaigns will degrade.", "To simulate this continual learning setting, we next evaluate the troll detection models on a past campaign after it has been adapted for a number of campaigns in sequence.", "For example, a system is first adapted to GRU-2020-NATO (G), and then to IRA-2020-Russia (I), Uganda-2021-NRM (U) and China-2021-Xinjiang (C) in sequence (denoted as G\u2192I\u2192U\u2192C).", "We then test the system using trolls using the past campaigns, i.e. G, I and U."], "text_after_citation": ["We experiment a simple approach to solve this: have MetaTroll classify a user using all its adapters, and select the outcome that has the highest probability.", "#OTHEREFR We present (5-shot) troll classification results under this continual learning setting in Table 3 .", "AdBERT and GPT-3 suffers little catastrophic forgetting, as they have campaign-specific parameters or prompts (they are unaffected by continual learning as their base model is unchanged), although their performance is only marginally above random chance in the first place.", "MetaTroll is the clear winner here, with only <5% accuracy degradation over time.", "#OTHEREFR In contrast, the meta-learning methods and few-shot classifiers suffer from catastrophic forgetting and their performance on older campaigns drops substantially (e.g."], "citing_paper_content": {"title": "Metatroll: Few-Shot Detection Of State-Sponsored Trolls With Transformer Adapters", "abstract": "State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g. the influence campaign by Russia's Internet Research Agency on the 2016 US Election), and they fall short when dealing with novel campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce campaign-specific transformer adapters to MetaTroll to \"memorise\" campaign-specific knowledge so as to tackle catastrophic forgetting, where a model \"forgets\" how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to multilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git CCS CONCEPTS \u2022 Computing methodologies \u2192 Natural language processing; Information extraction."}, "cited_paper_content": {"title": "Generalization Through Memorization: Nearest Neighbor Language Models", "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this transformation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 -- a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."}, "keywords": ["MetaTroll", "continual learning evaluation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To tackle catastrophic forgetting in few-shot transfer learning, MetaTroll uses campaign-specific transformer adapters to \"memorise\" campaign-specific knowledge #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prevent catastrophic forgetting in troll detection models, MetaTroll uses campaign-specific transformer adapters to \"memorise\" campaign-specific knowledge, as proposed in the #REFR paper on nearest neighbor language models.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To tackle catastrophic forgetting in continual learning evaluation, MetaTroll uses campaign-specific transformer adapters to \"memorise\" campaign-specific knowledge #REFR.", "given_citation_intent": "background", "given_keywords": ["MetaTroll", "continual learning evaluation"]}]}
{"citing_id": "2303.11327v1", "cited_id": "2003.08934", "section_title": "Learning 3D Compact Scene Representations", "citation": "Neural radiance fields #REFR are capable of learning a 3D representation that can reconstruct a volumetric 3D scene representation from a set of images.", "text_before_citation": [], "text_after_citation": ["Voxel-based methods #OTHEREFR speed up the learning process by explicitly storing the scene properties (e.g., density, color and feature) in its voxel grids.", "We leverage Direct Voxel Grid Optimization (DVGO) #OTHEREFR as our backbone for 3D compact representation for its fast speed.", "DVGO stores the learned density and color properties in its grid cells.", "The rendering of multi-view images is by interpolating through the voxel grids to get the density and color for each sampled point along each sampled ray, and integrating the colors based on the rendering alpha weights calculated from densities according to quadrature rule #OTHEREFR .", "The model is trained by minimizing the L2 loss between the rendered multi-view images and the ground-truth multi-view images."], "citing_paper_content": {"title": "3D Concept Learning And Reasoning From Multi-View Images", "abstract": "Figure 1. An exemplar scene with multi-view images and question-answer pairs of our 3DMV-VQA dataset. 3DMV-VQA contains four question types: concept, counting, relation, comparison. Orange words denote semantic concepts; blue words denote the relations."}, "cited_paper_content": {"title": "Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis", "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."}, "keywords": ["3D representation", "Neural radiance fields"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors present a method for synthesizing novel views of complex scenes using a fully-connected deep network, which achieves state-of-the-art results by optimizing an underlying continuous volumetric scene function with a sparse set of input views #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a method for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views, which achieves state-of-the-art results and is based on a fully-connected deep network that represents a scene using a single continuous 5D coordinate. #REFR provides a novel approach to view synthesis that outperforms prior work on neural rendering and view synthesis.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a method for synthesizing novel views of complex scenes by optimizing a 3D representation using neural radiance fields, which has achieved state-of-the-art results in view synthesis #REFR.", "given_citation_intent": "background", "given_keywords": ["3D representation", "Neural radiance fields"]}]}
{"citing_id": "2303.11590v1", "cited_id": "1709.07523", "section_title": "I. Introduction", "citation": "HJR is also, however, infamous for being a burdensome computational approach because of its dependency on spatial gradient approximations, making it sensitive to the curse of dimensionality #REFR .", "text_before_citation": ["The technical rigor involved in planning for success while simultaneously avoiding failure tends to force methods to sacrifice guarantees (e.g.", "datadriven methods where success is higher but failure is too) or feasibility (e.g.", "differential inclusion where solutions are often over-conservative or burdensome to solve).", "Among these approaches, Hamilton Jacobi reachability (HJR) is well known for being a robust approach to optimal control and safe path planning #OTHEREFR .", "It has taken seat as the golden standard in robotics, autonomous driving and other stochastic control because of its connection to the fundamental theory underlying differential games #OTHEREFR , #OTHEREFR which describe how to optimally posture a system to withstand antagonistic or stochastic disturbances #OTHEREFR ."], "text_after_citation": ["If this theory could be extended to higher dimensional systems, engineering efforts in diverse domains, particularly in medicine, finance and other large systems, could make strides where simpler (dimension-robust) controllers are unable to overcome stochasticity.", "Toward this end, several works have been put forth, including set-based propagation, often called the method of zonotopes #OTHEREFR , to over-approximate the Hamilton Jacobi reachable states with linear systems #OTHEREFR , #OTHEREFR and in some special classes of nonlinear systems #OTHEREFR .", "The only shortcomings with these methods are that they don't inherently provide an optimal controller and also tend to be overly-conservative.", "Another direction has been pursued in the face of this dimensionality obstacle: the method of decomposition and Sharpless, Shinde, Kim, and Herbert are with University of California, San Diego. Chow is with the University of California, Riverside.", "{wsharpless, nshinde, mak009, sherbert}@ucsd.edu, yattinc@ucr.edu This work is supported by ONR YIP N00014-22-1-2292. system reduction for HJR."], "citing_paper_content": {"title": "Koopman-Hopf Hamilton-Jacobi Reachability And Control", "abstract": "The Hopf formula for Hamilton-Jacobi Reachability analysis has been proposed for solving viscosity solutions of high-dimensional differential games as a space-parallelizeable method. In exchange, however, a complex, potentially nonconvex optimization problem must be solved, limiting its application to linear time-varying systems. With the intent of solving Hamilton-Jacobi backwards reachable sets (BRS) and their corresponding online controllers, we pair the Hopf solution with Koopman theory, which can linearize high-dimensional nonlinear systems. We find that this is a viable method for approximating the BRS and performs better than local linearizations. Furthermore, we construct a Koopman-Hopf controller for robustly driving a 10-dimensional, nonlinear, stochastic, glycolysis model and find that it significantly outcompetes both stochastic and game-theoretic Koopman-based model predictive controllers against stochastic disturbance."}, "cited_paper_content": {"title": "Hamilton-Jacobi Reachability: A Brief Overview And Recent Advances", "abstract": "Hamilton-Jacobi (HJ) reachability analysis is an important formal verification method for guaranteeing performance and safety properties of dynamical systems; it has been applied to many small-scale systems in the past decade. Its advantages include compatibility with general nonlinear system dynamics, formal treatment of bounded disturbances, and the availability of well-developed numerical tools. The main challenge is addressing its exponential computational complexity with respect to the number of state variables. In this tutorial, we present an overview of basic HJ reachability theory and provide instructions for using the most recent numerical tools, including an efficient GPU-parallelized implementation of a Level Set Toolbox for computing reachable sets. In addition, we review some of the current work in high-dimensional HJ reachability to show how the dimensionality challenge can be alleviated via various general theoretical and application-specific insights."}, "keywords": ["spatial gradient approximations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Hamilton-Jacobi reachability (HJR) is a well-known approach to optimal control and safe path planning, which has taken seat as the golden standard in robotics, autonomous driving, and other stochastic control due to its connection to the fundamental theory underlying differential games #OTHEREFR, #OTHEREFR. The authors could benefit from the recent advances in HJR theory and numerical tools presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Hamilton-Jacobi reachability analysis is a well-known approach for optimal control and safe path planning in robotics and autonomous driving, and has been applied to many small-scale systems in the past decade due to its compatibility with general nonlinear system dynamics and formal treatment of bounded disturbances #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Hamilton-Jacobi reachability analysis is a robust approach to optimal control and safe path planning, which has taken seat as the golden standard in robotics, autonomous driving, and other stochastic control, owing to its compatibility with general nonlinear system dynamics and formal treatment of bounded disturbances #OTHEREFR. The recent advances in the numerical tools, including an efficient GPU-parallelized implementation of a Level Set Toolbox for computing reachable sets, have made it possible to address the exponential computational complexity of HJR with respect to the number of state variables, and to alleviate the dimensionality challenge via various general theoretical and application-specific insights #REFR. Moreover, the pairing of the Hopf solution with Koopman theory has been found to be a viable method for approximating the backward reachable sets and performing better than local linearizations, which can linearize high-dimensional nonlinear systems #REFR. The spatial gradient approximations are used to solve the complex, potentially nonconvex optimization problem, limiting its application to linear time-varying systems.", "given_citation_intent": "background", "given_keywords": ["spatial gradient approximations"]}]}
{"citing_id": "2303.11589v1", "cited_id": "1810.04805", "section_title": "A. Additional Implementation Details", "citation": "For the transformer encoder, we simply adopt the same hyperparameters of BERT-base #REFR encoder, i.e., 12 layers, 12 attention heads, 768 hidden size, and 3,072 dimensions for the feed forward layer. Hyperparameters.", "text_before_citation": ["Thus, we attempt to analyse the noise process by observing the standard derivation of the cumulative matrix. A higher std.", "indicates a more sparse matrix and hence a lower transition probability to other coordinate tokens. As shown in Fig.", "6 , our schedule presents a gentler noising process and a more stable convergence state compared to the original linear schedule (as suggested by a higher std.", "at the beginning of the forward process, and a lower std. at the very end of the process). Denoising Model. We present the model architecture as in Fig. 7 .", "We embed the input sequence, input timestep, and positions with 768, 128, and 768 dimensions respectively. The dropout rate is set as 0.1."], "text_after_citation": ["We train our model using AdamW optimizer [30] with lr = 0.00004, betas = (0.9, 0.999), and zero weight decay.", "We also apply an exponential moving average (EMA) over model parameters with a rate of 0.9999. We set the batch size as 64.", "For RICO #OTHEREFR , we train the model with 2 V100 GPUs for 175,000 steps to achieve the best results; and for PublayNet #OTHEREFR , we train the model with 4 V100 GPUs for 350,000 steps.", "For the schedule of type tokens, we set T = 160 in Eq. #OTHEREFR .", "For the schedule of coordinate tokens, we set T =T = 160, g = 12.4, h = 2.48, and = 0.0001 in \u03b2 t = g/(T \u2212 t + ) h ."], "citing_paper_content": {"title": "Layoutdiffusion: Improving Graphic Layout Generation By Discrete Diffusion Probabilistic Models", "abstract": "Creating graphic layouts is a fundamental step in graphic designs. In this work, we present a novel generative model named LayoutDiffusion for automatic layout generation. As layout is typically represented as a sequence of discrete tokens, LayoutDiffusion models layout generation as a discrete denoising diffusion process. It learns to reverse a mild forward process, in which layouts become increasingly chaotic with the growth of forward steps and layouts in the neighboring steps do not differ too much. Designing such a mild forward process is however very challenging as layout has both categorical attributes and ordinal attributes. To tackle the challenge, we summarize three critical factors for achieving a mild forward process for the layout, i.e., legality, coordinate proximity and type disruption. Based on the factors, we propose a block-wise transition matrix coupled with a piece-wise linear noise schedule. Experiments on RICO and PubLayNet datasets show that Layout-Diffusion outperforms state-of-the-art approaches significantly. Moreover, it enables two conditional layout generation tasks in a plug-and-play manner without retraining and achieves better performance than existing methods."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["feed forward layer", "BERT-base encoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To embed the input sequence, input timestep, and positions, the authors set the dimensions as 768, 128, and 768 respectively, with a dropout rate of 0.1 #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To tackle the challenge of designing a mild forward process for layout generation, the authors of LayoutDiffusion proposed a block-wise transition matrix coupled with a piece-wise linear noise schedule, which was inspired by the Bidirectional Encoder Representations from Transformers (#REFR) model.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement the proposed denoising diffusion process, the authors embed the input sequence, input timestep, and positions with 768, 128, and 768 dimensions respectively, and set the dropout rate as 0.1, as described in their method. #REFR", "given_citation_intent": "method", "given_keywords": ["feed forward layer", "BERT-base encoder"]}]}
{"citing_id": "2304.07743v1", "cited_id": "2003.08934", "section_title": "Introduction", "citation": "Yet, the formulations of the original NeRF #REFR and its followup variants assume that images were acquired in clear air, i.e., in a medium that does not scatter or absorb light in a significant manner and that the acquired image is composed solely of the object radiance.", "text_before_citation": ["The pioneering work of Mildenhall et al.", "#OTHEREFR on Neural Radiance Fields (NeRFs) has tremendously advanced the field of Neural Rendering, due to its flexibility and unprecedented quality of synthesized images."], "text_after_citation": ["The NeRF formulation is based on volumetric rendering equations that take into account sampled points along 3D rays.", "Assuming a clear air environment, an implicit assumption, which is often enforced explicitly with dedicated loss components #OTHEREFR , is that a single opaque (high density) object is encountered per ray, with zero density between the camera and the object.", "In stark contrast to clear air case, when the medium is absorbing and / or scattering (e.g., haze, fog, smog, and all aquatic habitats), the volume rendering equation has a true volumetric meaning, as the entire volume, and not only the object, contributes to image intensity.", "As the NeRF model estimates color and density at every point of a scene, it lends itself perfectly to general volumetric rendering, given that the appropriate rendering model is used.", "Here, we bridge this gap with SeaThru-NeRF, a framework that incorporates a rendering model that takes into account scattering media."], "citing_paper_content": {"title": "Seathru-Nerf: Neural Radiance Fields In Scattering Media", "abstract": "for Marine Sciences in Eilat sea-thru-nerf.github.io Figure 1. NeRFs have not yet tackled scenes in which the medium strongly influences the appearances of objects, as in the case of underwater imagery. By incorporating a scattering image formation model into the NeRF rendering equations, we are able to separate the scene into 'clean' and backscatter components. Consequently, we can render photorealistic novel-views with or without the participating medium, in the latter case recovering colors as if the image was taken in clear air. Results on the Cura\u00e7ao scene: A RAW image (left) is brightened and white balanced (WB) for visualization, showing more detail, while areas further from the camera (top-right corner) are occluded and attenuated by severe backscatter-which is effectively removed in our restored image. Please zoom-in to observe the details."}, "cited_paper_content": {"title": "Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis", "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."}, "keywords": ["object radiance"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The pioneering work of Mildenhall et al. on Neural Radiance Fields (NeRFs) has tremendously advanced the field of Neural Rendering, due to its flexibility and unprecedented quality of synthesized images. The authors need to cite the reference paper, Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis, to provide a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Neural Radiance Fields (NeRFs) have recently achieved state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views #REFR, which has tremendously advanced the field of Neural Rendering.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Neural Radiance Fields (NeRFs) have been widely used in the field of Neural Rendering due to their flexibility and unprecedented quality of synthesized images, and the method proposed by Mildenhall et al. #REFR, which represents scenes using a fully-connected deep network and synthesizes views by querying 5D coordinates along camera rays, has achieved state-of-the-art results for rendering photorealistic novel-views of scenes with complicated geometry and appearance, by modeling object radiance as a continuous volumetric scene function.", "given_citation_intent": "background", "given_keywords": ["object radiance"]}]}
{"citing_id": "2303.10753v1", "cited_id": "1801.00718", "section_title": "Introduction", "citation": "Parametric methods, such as autoregres-sive models and Markov models #REFR , assume a specific distribution for the data and can achieve higher accuracy but require more computational resources.", "text_before_citation": ["Change point detection is a fundamental technique for community detection and identifying the formation of echo chambers #OTHEREFR , which can amplify bias and increase misinformation in online social networks #OTHEREFR .", "By integrating changes in temporal patterns into segregation models, we can better understand and ultimately mitigate the effects of echo chambers on online social networks.", "Change point information is also useful in social learning setups #OTHEREFR , where risk-averse agents adjust their estimation of a varying network state.", "Existing change point detection approaches can be categorized into three main groups: non-parametric methods, parametric methods, and Bayesian methods.", "Nonparametric methods, such as cumulative sum (CUSUM) #OTHEREFR and sliding window methods #OTHEREFR , do not assume any specific distribution for the data and can handle both abrupt and gradual changes."], "text_after_citation": ["Bayesian methods, such as Bayesian change point analysis #OTHEREFR , use probabilistic models for the prior of the change point and the observation likelihood to estimate the change points and can handle uncertainty and missing data.", "However, detecting changes in social networks is a challenging problem due to the large size of social networks and their complex dynamics.", "Additionally, the non-Euclidean nature of networks presents a challenge because traditional statistical tools developed for scalar and vector data are inadequate.", "Therefore, there is a need for efficient and effective methods to detect changes in social networks.", "Fr\u00e9chet mean and variance provide a method for calculating mean and variance for metric space-valued random variables, which allows us to examine statistical data for data items located in abstract spaces without algebraic structure and operations such as networks."], "citing_paper_content": {"title": "Fr\u00e9chet Statistics Based Change Point Detection In Dynamic Social Networks", "abstract": "This paper proposes a method to detect change points in dynamic social networks using Fr\u00e9chet statistics. We address two main questions: (1) what metric can quantify the distances between graph Laplacians in a dynamic network and enable efficient computation, and (2) how can the Fr\u00e9chet statistics be extended to detect multiple change points while maintaining the significance level of the hypothesis test? Our solution defines a metric space for graph Laplacians using the Log-Euclidean metric, enabling a closed-form formula for Fr\u00e9chet mean and variance. We present a framework for change point detection using Fr\u00e9chet statistics and extend it to multiple change points with binary segmentation. The proposed algorithm uses incremental computation for Fr\u00e9chet mean and variance to improve efficiency and is validated on simulated and two real-world datasets, namely the UCI message dataset and the Enron email dataset."}, "cited_paper_content": {"title": "Selective Review Of Offline Change Point Detection Methods", "abstract": "This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures."}, "keywords": ["Markov models"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Existing change point detection approaches can be categorized into three main groups: non-parametric methods, parametric methods, and Bayesian methods. Nonparametric methods, such as cumulative sum (CUSUM) #OTHEREFR and sliding window methods #OTHEREFR, have been extensively studied. For instance, the authors of #REFR provide a selective survey of algorithms for the offline detection of multiple change points in multivariate time series, which can be useful in social learning setups where risk-averse agents adjust their estimation of a varying network state.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "One of the methods for change point detection in multivariate time series is characterized by a cost function, a search method, and a constraint on the number of changes, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect multiple change points in multivariate time series, a selective review of offline change point detection methods proposed a general yet structuring methodological strategy that characterizes detection algorithms by a cost function, a search method, and a constraint on the number of changes #REFR.", "given_citation_intent": "method", "given_keywords": ["Markov models"]}]}
{"citing_id": "2303.01507v1", "cited_id": "1802.04208", "section_title": "G Comparison With Other Denoiser-Based Defense", "citation": "For DefenseGAN, which is originally designed to defend against adversarial images by finding the optimal noise that generates the most similar image to the adversarial counterpart, we adopt it to the audio domain, choosing WaveGAN #REFR as the GAN model in this pipeline.", "text_before_citation": ["We compare AudioPure with DefenseGAN #OTHEREFR and Joint Adversarial Finetuning #OTHEREFR ."], "text_after_citation": ["We train a WaveGAN on the SC09 dataset for 100 epochs, using the Adam optimizer with lr = 1e \u2212 3, \u03b2 1 = 0.5, and \u03b2 2 = 0.9.", "For Joint Adversarial Fine-tuning, we follow the setting of #OTHEREFR", "(2022) , using a Conv-TasNet #OTHEREFR as the denoiser. And like Joshi et al.", "(2022), we craft an offline adversarial SC09 dataset against the pretrained classifier by using L-inf PGD-100 attacks with = 0.002 (denoted as OffAdv-SC09).", "Then we train a Conv-TasNet model on OffAdv-SC09 for 30 epochs to get the pretrained denoiser."], "citing_paper_content": {"title": "Defending Against Adversarial Audio Via Diffusion Model", "abstract": "Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via offthe-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or retraining. We conduct extensive experiments on speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. L 2 or L \u221e-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by L 2 or L \u221enorm (up to +20% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by L 2-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines. Code is available at https://github.com/cychomatica/AudioPure. * work done during the internship at ASU."}, "cited_paper_content": {"title": "Adversarial Audio Synthesis", "abstract": "Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising."}, "keywords": ["adversarial images", "GAN model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We compare AudioPure with DefenseGAN (Kumar et al., 2019) and Joint Adversarial Finetuning (Xie et al., 2018), and propose an adversarial purification-based defense pipeline that takes advantage of the strong generation ability of diffusion models to purify the noisy audio and recover clean audio (Donahue et al., 2019) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compare AudioPure with DefenseGAN and Joint Adversarial Finetuning, both of which apply GANs designed for image generation on image-like audio feature representations, whereas AudioPure uses an adversarial purification-based defense pipeline via off-the-shelf diffusion models to add a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate adversarial audio examples, AudioPure employs an adversarial purification-based defense pipeline that adds a small amount of noise to the audio and then purifies it using a GAN model, as introduced in WaveGAN #REFR.", "given_citation_intent": "method", "given_keywords": ["adversarial images", "GAN model"]}]}
{"citing_id": "2303.07972v1", "cited_id": "1909.12908", "section_title": "A. 4-Dof Grasping", "citation": "If, additionally, grasps are constrained to a top-down direction, the surface the objects rest on hinders the gripper from tilting the object while approaching it, which generally improves grasp success rates substantially #REFR .", "text_before_citation": ["4-DoF grasp samplers restrict the approach direction of the grasps to be perpendicular to a specific grasping plane #OTHEREFR - #OTHEREFR . As such, these methods are constrained by construction.", "One of the obvious benefits of constraining grasps to 4-DoF instead of 6 is that it restricts the space of possible grasps, which facilitates learning."], "text_after_citation": ["However, if grasps are generated from a camera plane that is not top-down, which is the case in this work, grasp success deteriorates drastically #OTHEREFR .", "Therefore, we do not restrict the approach direction by construction and instead propose constraining it to any subset of SO(3)."], "citing_paper_content": {"title": "Gonet: An Approach-Constrained Generative Grasp Sampling Network", "abstract": "Constraining the approach direction of grasps is important when picking objects in confined spaces, such as when emptying a shelf. Yet, such capabilities are not available in state-of-the-art data-driven grasp sampling methods that sample grasps all around the object. In this work, we address the specific problem of training approach-constrained datadriven grasp samplers and how to generate good grasping directions automatically. Our solution is GoNet: a generative grasp sampler that can constrain the grasp approach direction to lie close to a specified direction. This is achieved by discretizing SO(3) into bins and training GoNet to generate grasps from those bins. At run-time, the bin aligning with the second largest principal component of the observed point cloud is selected. GoNet is benchmarked against GraspNet, a state-ofthe-art unconstrained grasp sampler, in an unconfined grasping experiment in simulation and on an unconfined and confined grasping experiment in the real world. The results demonstrate that GoNet achieves higher success-over-coverage in simulation and a 12%-18% higher success rate in real-world table-picking and shelf-picking tasks than the baseline."}, "cited_paper_content": {"title": "Beyond Top-Grasps Through Scene Completion", "abstract": "Current end-to-end grasp planning methods propose grasps in the order of (milli)seconds that attain high grasp success rates on a diverse set of objects, but often by constraining the workspace to top-grasps. In this work, we present a method that allows end-to-end top grasp planning methods to generate full six-degree-of-freedom grasps using a single RGB-D view as input. This is achieved by estimating the complete shape of the object to be grasped, then simulating different viewpoints of the object, passing the simulated viewpoints to an end-to-end grasp generation method, and finally executing the overall best grasp. The method was experimentally validated on a Franka Emika Panda by comparing 429 grasps generated by the state-of-the-art Fully Convolutional Grasp Quality CNN, both on simulated and real camera viewpoints. The results show statistically significant improvements in terms of grasp success rate when using simulated viewpoints over real camera viewpoints, especially when the real camera viewpoint is angled."}, "keywords": ["grasps", "grasp success rates"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One way to expand the capabilities of end-to-end grasp planning methods is to allow them to generate full six-degree-of-freedom grasps using a single RGB-D view as input #REFR. This approach has been experimentally validated and shown to improve grasp success rates, especially when the real camera viewpoint is angled.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "One way to overcome the limitations of 4-DoF grasp samplers is to estimate the complete shape of the object to be grasped and simulate different viewpoints of the object, as proposed in the reference paper #REFR, allowing end-to-end top grasp planning methods to generate full six-degree-of-freedom grasps using a single RGB-D view as input.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "4-DoF grasp samplers restrict the approach direction of the grasps to be perpendicular to a specific grasping plane, which constrains the space of possible grasps and facilitates learning. However, end-to-end grasp planning methods can generate full six-degree-of-freedom grasps using a single RGB-D view as input, achieving high grasp success rates on a diverse set of objects #REFR.", "given_citation_intent": "background", "given_keywords": ["grasps", "grasp success rates"]}]}
{"citing_id": "2304.13536v1", "cited_id": "1711.11279", "section_title": "Introduction & Related Work", "citation": "Moreover, internal representations of neural networks can be harnessed to generate concept activation vectors #REFR .", "text_before_citation": ["However, in the context of end-to-end trained neural networks without explicit feature engineering we also have an input space that is much less interpretable than engineered features.", "That leads to feature attribution methods coming short in interpretability as saliency maps lack semantic concepts in complex input spaces.", "Moreover, global insights across whole datasets are desired to explain the intertwined system of data and trained model.", "To overcome this limitation, we can evaluate the impact of the occurence of particular semantic concepts in the input signal, a method named concept influence #OTHEREFR .", "Alternative methods to evaluate the impact of semantic concepts on the model output include the inpainting or masking of specific concepts and quantifying the change of output of the neural network [Williford et al. 2020] ."], "text_after_citation": ["Gaze events like fixations and saccades are main descriptive concepts of eye tracking research #OTHEREFR", "2011] and thus are natural candidates for semantic concepts in gaze signals.", "By evaluating the concept influence of these gaze concepts across entire datasets we generate insights on both model and data.", "Prior eye gaze related research either uses descriptive input features on machine learning models #OTHEREFR", "2016] or is limited to statistical analysis #OTHEREFR"], "citing_paper_content": {"title": "Bridging The Gap: Gaze Events As Interpretable Concepts To Explain Deep Neural Sequence Models", "abstract": "Recent work in XAI for eye tracking data has evaluated the suitability of feature attribution methods to explain the output of deep neural sequence models for the task of oculomotric biometric identification. These methods provide saliency maps to highlight important input features of a specific eye gaze sequence. However, to date, its localization analysis has been lacking a quantitative approach across entire datasets. In this work, we employ established gaze event detection algorithms for fixations and saccades and quantitatively evaluate the impact of these events by determining their concept influence. Input features that belong to saccades are shown to be substantially more important than features that belong to fixations. By dissecting saccade events into sub-events, we are able to show that gaze samples that are close to the saccadic peak velocity are most influential. We further investigate the effect of event properties like saccadic amplitude or fixational dispersion on the resulting concept influence."}, "cited_paper_content": {"title": "Interpretability Beyond Feature Attribution: Quantitative Testing With Concept Activation Vectors (Tcav)", "abstract": "The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of \"zebra\" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application."}, "keywords": ["concept activation vectors"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitation of feature attribution methods in interpretability, we can evaluate the impact of the occurrence of particular semantic concepts in the input signal, a method named concept influence #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of interpreting deep learning models, the authors of this manuscript propose the use of Concept Activation Vectors (CAVs) as part of a technique called Testing with CAVs (TCAV) to quantify the importance of user-defined concepts to a classification result, which could provide insights for a standard image classification network as well as a medical application #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of interpreting deep learning models, Concept Activation Vectors (CAVs) have been introduced as a means of providing an interpretation of a neural net's internal state in terms of human-friendly concepts, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["concept activation vectors"]}]}
{"citing_id": "2303.04533v1", "cited_id": "1905.00967", "section_title": "Introduction", "citation": "Building on the idea, a higher order ALE Discontinuous Galerkin (DG) method was designed by #REFR .", "text_before_citation": ["This approach can be efficient if the number of remeshing required are very low, since the simulation is fully ALE and free from interpolation errors.", "The approach described above is sometimes also known as remap ALE methods or indirect ALE methods.", "Remap ALE schemes have been used extensively in some problems and some of the recent work on the topic can be found in #OTHEREFR .", "For large deformations, the number of required remeshing increase and this can be both costly and result in poor accuracy due to the solution transfer step.", "Another approach to get around this problem was presented in #OTHEREFR , where the connectivity of the moving mesh is dynamically regenerated via a moving unstructured but conforming Voronoi tessellation of the domain."], "text_after_citation": ["In these methods, a new mesh is generated at every time step, a correspondence is generated between the elements in the two time steps, and finally a ALE DG method is solved on the mesh, where the space-time information is gleaned from the correspondence information.", "In the above method, there is a possibility of non-regular (\"crazy\") finite elements appearing in the mesh which is taken care of using the techniques in #OTHEREFR .", "Due to no inherent need to remap the solutions in these methods, these methods are often referred to as direct ALE DG methods.", "Since there is no remapping of solutions needed, the methods do not suffer from the interpolation error, however, it requires a mesh to be generated at every time step and corresponding connection information to be determined which can be costly.", "There is a third approach which was first outlined in #OTHEREFR ."], "citing_paper_content": {"title": "Arbitrary Lagrangian-Eulerian Discontinuous Galerkin Methods With Application To Compressible Flows", "abstract": ""}, "cited_paper_content": {"title": "High Order Direct Arbitrary-Lagrangian-Eulerian Schemes On Moving Voronoi Meshes With Topology Changes", "abstract": "We present a new family of very high order accurate direct Arbitrary-Lagrangian-Eulerian (ALE) Finite Volume (FV) and Discontinuous Galerkin (DG) schemes for the solution of nonlinear hyperbolic PDE systems on moving Voronoi meshes that are regenerated at each time step and which explicitly allow topology changes in time. The Voronoi tessellations are obtained from a set of generator points that move with the local fluid velocity. We employ an AREPO-type approach, which rapidly rebuilds a new high quality mesh exploiting the previous one, but \\textit{rearranging} the element shapes and neighbors in order to guarantee that the mesh evolution is robust even for vortex flows and for very long computational times. The old and new Voronoi elements associated to the same generator point are connected in space--time to construct closed space-time control volumes, whose bottom and top faces may be polygons with a different number of sides. We also need to incorporate some degenerate space--time sliver elements, which are needed in order to fill the space-time holes that arise because of the topology changes. The final ALE FV-DG scheme is obtained by a novel redesign of the high order accurate fully discrete direct ALE schemes of Boscheri and Dumbser, which have been extended here to general moving Voronoi meshes and space--time sliver elements, and it is based on the integration over arbitrary shaped closed space-time control volumes. Numerical convergence studies as well as a large set of benchmark problems for hydrodynamics and magnetohydrodynamics (MHD) demonstrate the accuracy and robustness of the method. Our numerical results clearly show that the new combination of very high order schemes with regenerated meshes that allow topology changes in each time step lead to substantial improvements over the existing state of the art in direct ALE methods."}, "keywords": ["ALE Discontinuous Galerkin"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Another approach to avoid the poor accuracy due to the solution transfer step and the high cost of remeshing for large deformations is to use a moving unstructured but conforming Voronoi tessellation of the domain, as presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors presented a novel redesign of high order accurate direct Arbitrary-Lagrangian-Eulerian (ALE) schemes for the solution of nonlinear hyperbolic PDE systems on moving Voronoi meshes that allow topology changes in time, which can be found in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitations of remap ALE schemes, some recent work has proposed dynamically regenerated meshes using Voronoi tessellations, which allow topology changes and have been shown to be accurate and robust for nonlinear hyperbolic PDE systems on moving meshes, as demonstrated in #REFR, which presents a new family of very high order accurate direct ALE Finite Volume and Discontinuous Galerkin schemes.", "given_citation_intent": "method", "given_keywords": ["ALE Discontinuous Galerkin"]}]}
{"citing_id": "2304.09061v1", "cited_id": "1810.01520", "section_title": "Towards Large-Scale Apc.", "citation": "Overall, the modern sequence modeling neural networks from Section 2.2.2 were seldom used for candidate retrieval in these systems, either because they were used in a way that did not scale to millions of songs, or led to underperforming results with respect to alternatives #REFR .", "text_before_citation": ["#OTHEREFR analyzed key insights from the challenge, explaining that many teams leveraged:", "\u2022 Ensemble architectures: several teams combined several APC models of the form : P \u00d7 S \u2192 R, each of them providing different \"candidate\" songs to extend playlists.", "\u2022 Or, two-stage architectures: a first simple model rapidly scored all songs from S, retrieving a subset of \"candidate\" songs, several orders of magnitude smaller than the original song set.", "Then, some more sophisticated model(s) re-ranked candidate songs to improve APC recommendations.", "The performance of such two-stage strategies was highly dependent on the quality of candidate retrieval models #OTHEREFR ."], "text_after_citation": ["When considering both candidate selection models of ensemble/two-stage architectures, as well as single-stage architectures directly scoring all songs, most approaches leveraged simpler and faster non-parametric nearest neighbor models, applied to various CF-based song/playlist embedding representations learned using models from Section 2.2.1.", "In summary, most teams favored simpler and more scalable methods than the ones presented in Section 2.2.2 despite their promising performances on smaller datasets.", "We believe this challenge highlighted the current discrepancy between the complexity and evaluation procedure of APC models proposed in the recent scientific literature, and the simpler ones successfully used for large-scale applications.", "In particular, in the context of a global questioning of the ability of deep learning to actually improve recommender systems #OTHEREFR , we believe that research on APC would benefit from a formal characterization of the requirements expected from suitable solutions for large-scale APC applications, which we provide in the remainder of this paper."], "citing_paper_content": {"title": "A Scalable Framework For Automatic Playlist Continuation On Music Streaming Services", "abstract": "Music streaming services often aim to recommend songs for users to extend the playlists they have created on these services. However, extending playlists while preserving their musical characteristics and matching user preferences remains a challenging task, commonly referred to as Automatic Playlist Continuation (APC). Besides, while these services often need to select the best songs to recommend in real-time and among large catalogs with millions of candidates, recent research on APC mainly focused on models with few scalability guarantees and evaluated on relatively small datasets. In this paper, we introduce a general framework to build scalable yet effective APC models for large-scale applications. Based on a represent-then-aggregate strategy, it ensures scalability by design while remaining flexible enough to incorporate a wide range of representation learning and sequence modeling techniques, e.g., based on Transformers. We demonstrate the relevance of this framework through in-depth experimental validation on Spotify's Million Playlist Dataset (MPD), the largest public dataset for APC. We also describe how, in 2022, we successfully leveraged this framework to improve APC in production on Deezer. We report results from a large-scale online A/B test on this service, emphasizing the practical impact of our approach in such a real-world application."}, "cited_paper_content": {"title": "An Analysis Of Approaches Taken In The Acm Recsys Challenge 2018 For Automatic Music Playlist Continuation", "abstract": "The ACM Recommender Systems Challenge 2018 focused on the task of automatic music playlist continuation, which is a form of the more general task of sequential recommendation. Given a playlist of arbitrary length with some additional meta-data, the task was to recommend up to 500 tracks that fit the target characteristics of the original playlist. For the RecSys Challenge, Spotify released a dataset of one million user-generated playlists. Participants could compete in two tracks, i.e., main and creative tracks. Participants in the main track were only allowed to use the provided training set, however, in the creative track, the use of external public sources was permitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams submitted 239 runs to the creative track. The highest performing team in the main track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average number of recommended songs clicks of 1.784. In the creative track, an R-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was obtained by the best team. This article provides an overview of the challenge, including motivation, task definition, dataset description, and evaluation. We further report and analyze the results obtained by the top-performing teams in each track and explore the approaches taken by the winners. We finally summarize our key findings, discuss generalizability of approaches and results to domains other than music, and list the open avenues and possible future directions in the area of automatic playlist continuation."}, "keywords": ["songs"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Many teams leveraged ensemble architectures and two-stage architectures to improve Automatic Playlist Continuation (APC) recommendations, as analyzed in the reference paper (An Analysis Of Approaches Taken In The Acm Recsys Challenge 2018 For Automatic Music Playlist Continuation) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Several teams in the APC challenge leveraged ensemble architectures and two-stage architectures to improve their recommendations, as explained in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Several teams in the APC challenge leveraged ensemble architectures and two-stage architectures to extend playlists with different \"candidate\" #KEYWORD(songs) to improve APC recommendations, as analyzed in #REFR.", "given_citation_intent": "method", "given_keywords": ["songs"]}]}
{"citing_id": "2304.13653v1", "cited_id": "1511.06342", "section_title": "Stage 2: Distillation And Self-Play", "citation": "Since the teacher policies are trained in the same environment for the same embodiment, we can use policy distillation #REFR , by adding a regularization term to encourage the output of the agent's policy to be similar to that of the teachers'.", "text_before_citation": ["The setup is the same as for training the soccer teacher, except now episodes terminate only when either the agent or the opponent scores.", "When the agent is on the ground, out of bounds, or in the goal penalty area, it receives a fixed penalty per timestep and all positive reward components are ignored.", "For instance, if the agent is on the ground when a goal is scored, then it receives a zero for the scoring reward component.", "At the beginning of an episode, the agent is initialized either laying on the ground on its front, on its back, or in a default standing pose, with equal probability.", "Distillation: Given the soccer and get-up teachers, the agent must learn to transition smoothly between these two skills and improve upon their robustness."], "text_after_citation": ["This approach is related to prior work that regularizes a student policy to a common shared policy across tasks #OTHEREFR or a default policy that receives limited state information #OTHEREFR , as well as work on kickstarting #OTHEREFR and reusing learned skills for humanoid soccer in simulation (Liu et al., 2022) .", "Unlike most prior work, in our setting the teacher policies are useful in mutually exclusive sets of states: the get-up teacher is only useful in states where the agent is on the ground, and the soccer teacher is only useful in states where the agent is upright.", "Thus the agent should only be regularized to one teacher at a time, depending on which state it is in.", "Based on this domain-specific knowledge, we split the training objective into two components-one that combines the soccer training objective J ( ) with KL regularization to the soccer teacher and one that combines it with KL regularization to the get-up teacher :", "EQUATION"], "citing_paper_content": {"title": "Learning Agile Soccer Skills For A Bipedal Robot With Deep Reinforcement Learning", "abstract": "We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner-well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156 % faster, took 63 % less time to get up, and kicked 24 % faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website: https://sites.google.com/view/op3-soccer."}, "cited_paper_content": {"title": "Actor-Mimic: Deep Multitask And Transfer Reinforcement Learning", "abstract": "The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed \"Actor-Mimic\", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods."}, "keywords": ["policy distillation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To enable an autonomous agent to learn how to behave in multiple tasks simultaneously and generalize its knowledge to new domains, the authors proposed a novel method of multitask and transfer learning called \"Actor-Mimic\" that exploits the use of deep reinforcement learning and model compression techniques to train a single policy network with the guidance of several expert teachers #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable the agent to learn how to behave in multiple tasks simultaneously and generalize its knowledge to new domains, the authors propose the \"Actor-Mimic\" method that exploits the use of deep reinforcement learning and model compression techniques, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable the agent to learn how to transition smoothly between skills and improve their robustness, the authors used policy distillation, a method that trains a single policy network to learn how to act in a set of distinct tasks by using the guidance of several expert teachers, as described in the Actor-Mimic paper. #REFR", "given_citation_intent": "method", "given_keywords": ["policy distillation"]}]}
{"citing_id": "2303.10598v2", "cited_id": "1409.1556", "section_title": "Two-Stage Model Training", "citation": "We train the feature grid and the 2D CNN decoder simultaneously, with the supervision of both RGB images and their bilinearly up-sampled feature maps extracted from ReLU3_1 layer of pre-trained VGG #REFR .", "text_before_citation": ["The training of our model is divided into the feature grid training stage and the stylization training stage, the former is trained with the target of novel view synthesis, and the latter is trained with the target of style transfer. Feature grid training stage (First stage).", "We first learn the feature grid 3D representation for the novel view synthesis task, in preparation for performing feature transformation for style transfer."], "text_after_citation": ["By aligning the VGG features with the feature grid, the reconstructed features acquire semantic information.", "We use density grid pre-trained solely on RGB images since the supervising feature maps are not strictly multi-view consistent.", "The training objective is the mean square error (MSE) between the predicted and ground truth feature maps and RGB images.", "Following #OTHEREFR , we use perceptual loss #OTHEREFR as additional supervision to increase reconstructed image quality. The overall loss function is:", "L grid = r\u2208R F (r) \u2212 F (r) 2 2 + \u00ce R \u2212 I R 2 2 + l\u2208lp F l (\u00ce R ) \u2212 F l (I R ) 2 2 , (10)"], "citing_paper_content": {"title": "Stylerf: Zero-Shot 3D Style Transfer Of Neural Radiance Fields", "abstract": "3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to highquality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhaoliu.github.io/StyleRF/"}, "cited_paper_content": {"title": "Very Deep Convolutional Networks For Large-Scale Image Recognition", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."}, "keywords": ["RGB images", "2D CNN decoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We first learn the feature grid 3D representation for the novel view synthesis task, in preparation for performing feature transformation for style transfer, following the approach proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To perform feature transformation for style transfer, the authors of Stylerf utilized a feature grid 3D representation, which was learned in the feature grid training stage, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To perform feature transformation for style transfer, the authors of this work learned the feature grid 3D representation for the novel view synthesis task, which was inspired by the significant improvement achieved by pushing the depth of convolutional networks to 16-19 weight layers using small 3x3 convolution filters in the #REFR paper.", "given_citation_intent": "method", "given_keywords": ["RGB images", "2D CNN decoder"]}]}
{"citing_id": "2304.02205v1", "cited_id": "1905.12470", "section_title": "Data Collection.", "citation": "We crawl all this information for future exploitation in improving student modeling as well as other relevant topics such as adaptive learning recommendation #REFR .", "text_before_citation": ["For each student, we sort their exercise records in time order and collect the timestamp to build an interaction sequence.", "Specifically, thanks to the collection of the standard answer, we can automatically assign the performance score of each behavior.", "We further consider the Half Right scene that a student gives a partially correct answer in a multiple choice question and classify the scores into three levels: 0-Wrong, 1-Half Right, 2-Right.", "Researchers can decide whether to reduce it to a dichotomy when employing our dataset.", "(4) Associated Resources and Behaviors: Except for the exercising information and behaviors, there are other types of relevant features, such as the course videos that are in the same chapters, video-watching behaviors, and the course descriptions."], "text_after_citation": ["Furthermore, as MOOCCubeX provides the concept set of these resources, we preserve them as a distant supervision dictionary for subsequent annotation."], "citing_paper_content": {"title": "Moocradar: A Fine-Grained And Multi-Aspect Knowledge Repository For Improving Cognitive Student Modeling In Moocs", "abstract": "Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2, 513 exercise questions, 5, 600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods. Moreover, to support the convenient usage for researchers, we release a set of tools for data querying,"}, "cited_paper_content": {"title": "Exploiting Cognitive Structure For Adaptive Learning", "abstract": "Adaptive learning, also known as adaptive teaching, relies on learning path recommendation, which sequentially recommends personalized learning items (e.g., lectures, exercises) to satisfy the unique needs of each learner. Although it is well known that modeling the cognitive structure including knowledge level of learners and knowledge structure (e.g., the prerequisite relations) of learning items is important for learning path recommendation, existing methods for adaptive learning often separately focus on either knowledge levels of learners or knowledge structure of learning items. To fully exploit the multifaceted cognitive structure for learning path recommendation, we propose a Cognitive Structure Enhanced framework for Adaptive Learning, named CSEAL. By viewing path recommendation as a Markov Decision Process and applying an actor-critic algorithm, CSEAL can sequentially identify the right learning items to different learners. Specifically, we first utilize a recurrent neural network to trace the evolving knowledge levels of learners at each learning step. Then, we design a navigation algorithm on the knowledge structure to ensure the logicality of learning paths, which reduces the search space in the decision process. Finally, the actor-critic algorithm is used to determine what to learn next and whose parameters are dynamically updated along the learning path. Extensive experiments on real-world data demonstrate the effectiveness and robustness of CSEAL."}, "keywords": ["student modeling", "adaptive learning recommendation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To ensure a comprehensive annotation of fine-grained concepts and cognitive labels, MoocRadar presents a multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To fully exploit the multifaceted cognitive structure for learning path recommendation, the authors proposed a Cognitive Structure Enhanced framework for Adaptive Learning, named CSEAL, which can sequentially identify the right learning items to different learners by viewing path recommendation as a Markov Decision Process and applying an actor-critic algorithm. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adaptive learning recommendation is a crucial task in student modeling, and it is well known that modeling the cognitive structure is important for personalized learning. To fully exploit the multifaceted cognitive structure for learning path recommendation, CSEAL, a Cognitive Structure Enhanced framework for Adaptive Learning, is proposed in the reference paper #REFR, which sequentially recommends personalized learning items to satisfy the unique needs of each learner by viewing path recommendation as a Markov Decision Process and applying an actor-critic algorithm.", "given_citation_intent": "background", "given_keywords": ["student modeling", "adaptive learning recommendation"]}]}
{"citing_id": "2303.08525v1", "cited_id": "1612.04335", "section_title": "E. Comparison With State-Of-The-Art Models", "citation": "The first five columns are ODIs from the Salient360! dataset, and the last three columns are ODIs from the Saliency in VR dataset #REFR .", "text_before_citation": ["At each prediction stage, the dilated convolutional neural network was used to acquire a larger receptive field, and the SE blocks were used to assign different alpha-values to different layers according to their channel attentions, to help make more precise saliency prediction.", "Besides, we shared weights among all the layers to make our model lightweight.", "Through all the aforementioned operations, the proposed MRGAN360 model is both more accurate and lightweight than the other state-of-the-art methods.", "In our future work, we plan to improve the model on detecting finer yet more accurate features in high-resolution 360 o images.", "The implementation codes and pre-trained models will be made publicly available. dataset #OTHEREFR and the Saliency in VR dataset."], "text_after_citation": [], "citing_paper_content": {"title": "Mrgan360: Multi-Stage Recurrent Generative Adversarial Network For 360 Degree Image Saliency Prediction", "abstract": "Thanks to the ability of providing an immersive and interactive experience, the uptake of 360 degree image content has been rapidly growing in consumer and industrial applications. Compared to planar 2D images, saliency prediction for 360 degree images is more challenging due to their high resolutions and spherical viewing ranges. Currently, most high-performance saliency prediction models for omnidirectional images (ODIs) rely on deeper or broader convolutional neural networks (CNNs), which benefit from CNNs' superior feature representation capabilities while suffering from their high computational costs. In this paper, inspired by the human visual cognitive process, i.e., human being's perception of a visual scene is always accomplished by multiple stages of analysis, we propose a novel multi-stage recurrent generative adversarial networks for ODIs dubbed MRGAN360, to predict the saliency maps stage by stage. At each stage, the prediction model takes as input the original image and the output of the previous stage and outputs a more accurate saliency map. We employ a recurrent neural network among adjacent prediction stages to model their correlations, and exploit a discriminator at the end of each stage to supervise the output saliency map. In addition, we share the weights among all the stages to obtain a lightweight architecture that is computationally cheap. Extensive experiments are conducted to demonstrate that our proposed model outperforms the stateof-the-art model in terms of both prediction accuracy and model size."}, "cited_paper_content": {"title": "Saliency In Vr: How Do People Explore Virtual Environments?", "abstract": "Understanding how people explore immersive virtual environments is crucial for many applications, such as designing virtual reality (VR) content, developing new compression algorithms, or learning computational models of saliency or visual attention. Whereas a body of recent work has focused on modeling saliency in desktop viewing conditions, VR is very different from these conditions in that viewing behavior is governed by stereoscopic vision and by the complex interaction of head orientation, gaze, and other kinematic constraints. To further our understanding of viewing behavior and saliency in VR, we capture and analyze gaze and head orientation data of 169 users exploring stereoscopic, static omni-directional panoramas, for a total of 1980 head and gaze trajectories for three different viewing conditions. We provide a thorough analysis of our data, which leads to several important insights, such as the existence of a particular fixation bias, which we then use to adapt existing saliency predictors to immersive VR conditions. In addition, we explore other applications of our data and analysis, including automatic alignment of VR video cuts, panorama thumbnails, panorama video synopsis, and saliency-basedcompression."}, "keywords": ["Saliency"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To further improve the accuracy of saliency prediction in high-resolution 360 o images, the authors plan to incorporate finer features in their MRGAN360 model. The implementation codes and pre-trained models will be made publicly available. Additionally, the authors cite the Saliency in VR dataset#REFR, which provides a thorough analysis of gaze and head orientation data of users exploring stereoscopic, static omni-directional panoramas, and adapts existing saliency predictors to immersive VR conditions.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To further our understanding of viewing behavior and saliency in VR, the authors of this manuscript used the Saliency in VR dataset #REFR to capture and analyze gaze and head orientation data of users exploring stereoscopic, static omni-directional panoramas, leading to several important insights.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand saliency in VR, researchers have explored how people explore immersive virtual environments, which has led to important insights such as the existence of a particular fixation bias and the adaptation of existing saliency predictors to immersive VR conditions (#REFR).", "given_citation_intent": "background", "given_keywords": ["Saliency"]}]}
{"citing_id": "2303.00566v1", "cited_id": "1810.00859", "section_title": "Dynamic During Training", "citation": "Dynamic Sparse Graph (DSG) #REFR activates a small number of critical neurons with the constructed sparse graph at every iteration dynamically.", "text_before_citation": ["CHannel EXploration (CHEX) #OTHEREFR uses two processes to dynamically adjust filter importance.", "The first process is the channel pruning process with Column Subset Selection criterion #OTHEREFR .", "The second process is the regrowing process, which is based on orthogonal projection #OTHEREFR to avoid regrowing redundant channels and to explore channel diversity.", "Regrowing channels are restored to the most recently used (MRU) parameters rather than zeros.", "To better preserve model accuracy, the remaining channels are re-distributed dynamically among all the layers."], "text_after_citation": ["DSG is developed from the argument that direct computation according to output activations is very costly for finding critical neurons.", "The dimension reduction search is formulated to forecast the activation output by computing input and filters in a lower dimension.", "Furthermore, to prevent BN layers from damaging sparsity, Liu et al.", "introduce the double-mask selection that uses the same selection mask before and after BN layers #OTHEREFR .", "Other methods such as SCP #OTHEREFR and DMC #OTHEREFR also maintain the mask dynamically. These are discussed in Section 2.3."], "citing_paper_content": {"title": "Structured Pruning For Deep Convolutional Neural Networks: A Survey", "abstract": "The remarkable performance of deep Convolutional neural networks (CNNs) is generally attributed to their deeper and wider architectures, which can come with significant computational costs. Pruning neural networks has thus gained interest since it effectively lowers storage and computational costs. In contrast to weight pruning, which results in unstructured models, structured pruning provides the benefit of realistic acceleration by producing models that are friendly to hardware implementation. The special requirements of structured pruning have led to the discovery of numerous new challenges and the development of innovative solutions. This article surveys the recent progress towards structured pruning of deep CNNs. We summarize and compare the state-of-the-art structured pruning techniques with respect to filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. While discussing structured pruning algorithms, we briefly introduce the unstructured pruning counterpart to emphasize their differences. Furthermore, we provide insights into potential research opportunities in the field of structured pruning. A curated list of neural network pruning papers can be found at https://github.com/he-y/Awesome-Pruning."}, "cited_paper_content": {"title": "Dynamic Sparse Graph For Efficient Deep Learning", "abstract": "We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices. However, most of the previous studies optimize for inference while neglect training or even complicate it. Training is far more intractable, since (i) the neurons dominate the memory cost rather than the weights in inference; (ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid; (iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity. To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimension-reduction search (DRS) and obtains the BN compatibility via a double-mask selection (DMS). Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks."}, "keywords": ["critical neurons", "Dynamic Sparse"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of dynamic activation and batch normalization in training deep neural networks, the authors propose a dynamic and sparse graph (DSG) structure that activates only a small amount of neurons with high selectivity at each iteration via a dimension-reduction search (DRS) and obtains the batch normalization compatibility via a double-mask selection (DMS) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of dynamic activation and batch normalization in training deep neural networks, the authors of the manuscript propose a dynamic and sparse graph (DSG) structure that activates only a small amount of neurons with high selectivity at each iteration via a dimension-reduction search (DRS) and obtains the BN compatibility via a double-mask selection (DMS) #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of training deep neural networks with critical neurons dominating the memory cost, the authors propose a Dynamic Sparse Graph (DSG) structure that activates only a small amount of neurons with high selectivity at each iteration, as described in #REFR.", "given_citation_intent": "background", "given_keywords": ["critical neurons", "Dynamic Sparse"]}]}
{"citing_id": "2304.05464v1", "cited_id": "1709.01507", "section_title": "Network Architecture", "citation": "All MBConv blocks map to d m \u2192 2 \u00d7 d m \u2192 d m channels and contain Squeeze-Excitation layers #REFR .", "text_before_citation": ["They combine depthwhise convolution and regular pointwise convolutions for computationally efficient spatial encoding.", "We perform temporal encoding on downsampled feature maps via the attention-based L-TAE #OTHEREFR , which is designed for satellite image time series and computationally more efficient than transformers. The network architecture is illustrated in Fig. 2 and further described in the following paragraphs.", "Pre-aggregation shared encoder The T different input images are processed in parallel by a shared spatial encoding branch.", "This encoder is composed of a pointwise convolution C in \u2192 d m , followed by a specifiable number n e of MBConv blocks.", "Following #OTHEREFR we use group normal-isation in the encoding branch."], "text_after_citation": ["Ultimately, each input image X t is mapped to a feature map f t of the same resolution.", "Attention-based temporal aggregation Following recent literature, we employ self-attention to aggregate a sequence of feature maps", "[f 1 , \u2022 \u2022 \u2022 , f T ]", "into a single one.", "We first down-sample features f t with a single maxpooling operation to low resolution feature maps f t of size d m \u00d7 H \u00d7 W ."], "citing_paper_content": {"title": "Uncrtaints: Uncertainty Quantification For Cloud Removal In Optical Satellite Time Series", "abstract": "Clouds and haze often occlude optical satellite images, hindering continuous, dense monitoring of the Earth's surface. Although modern deep learning methods can implicitly learn to ignore such occlusions, explicit cloud removal as pre-processing enables manual interpretation and allows training models when only few annotations are available. Cloud removal is challenging due to the wide range of occlusion scenarios-from scenes partially visible through haze, to completely opaque cloud coverage. Furthermore, integrating reconstructed images in downstream applications would greatly benefit from trustworthy quality assessment. In this paper, we introduce UnCRtainTS, a method for multi-temporal cloud removal combining a novel attentionbased architecture, and a formulation for multivariate uncertainty prediction. These two components combined set a new state-of-the-art performance in terms of image reconstruction on two public cloud removal datasets. Additionally, we show how the well-calibrated predicted uncertainties enable a precise control of the reconstruction quality."}, "cited_paper_content": {"title": "Squeeze-And-Excitation Networks", "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, much existing work has shown the benefits of enhancing spatial encoding. In this work, we focus on channels and propose a novel architectural unit, which we term the\"Squeeze-and-Excitation\"(SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at slight computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a 25% relative improvement over the winning entry of 2016."}, "keywords": ["MBConv blocks", "Squeeze-Excitation layers"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "They combine depthwhise convolution and regular pointwise convolutions for computationally efficient spatial encoding. We perform temporal encoding on downsampled feature maps via the attention-based L-TAE #OTHEREFR , which is designed for satellite image time series and computationally more efficient than transformers. The network architecture is illustrated in Fig. 2 and further described in the following paragraphs. Pre-aggregation shared encoder The T different input images are processed in parallel by a shared spatial encoding branch. This encoder is composed of a pointwise convolution C in \u2192 d m , followed by a specifiable number n e of MBConv blocks. Following #REFR, we propose a novel architectural unit, the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the representational power of the network, the authors propose a novel architectural unit called the \"Squeeze-and-Excitation\" (SE) block, which adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the representational power of the network, the authors propose a novel architectural unit called the \"Squeeze-and-Excitation\" (SE) block, which adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels, and can be stacked together to construct SENet architectures that generalise extremely well across challenging datasets, as shown in the reference paper #REFR. The encoding branch in the network architecture consists of a pointwise convolution followed by a specifiable number of MBConv blocks, and group normalization is used following #OTHEREFR.", "given_citation_intent": "background", "given_keywords": ["MBConv blocks", "Squeeze-Excitation layers"]}]}
{"citing_id": "2304.00436v1", "cited_id": "1405.0312", "section_title": "Experiment Settings", "citation": "The images are from the COCO dataset #REFR with a size of 640\u00d7480. We report results on its validation split.", "text_before_citation": ["Dataset We evaluated our method on the VQA-v2 dataset #OTHEREFR .", "VQA-v2 covers 82.8k images and 443.8k questions for training and 40.5k images, and 214.4k questions for validation."], "text_after_citation": ["Model architecture and hyperparameters We mounted the attack on a commonly used visual question answering model called Modular Co-Attention Network (MCAN) #OTHEREFR that leverages several Modular Co-Attention (MCA) layers cascaded in depth.", "Each MCA layer employs both the selfattention and guided-attention of input channels.", "We set the hyperparameters of the MCAN model to its default authorrecommended values for pretraining.", "We mounted the attack after training the model for 20 epochs showing no further improvement in its validation accuracy.", "To mount the attack, we employed a batch size of 128, a learning rate of 0.0001 using Adam #OTHEREFR , and an epoch of two."], "citing_paper_content": {"title": "Instance-Level Trojan Attacks On Visual Question Answering Via Adversarial Learning In Neuron Activation Space", "abstract": "Malicious perturbations embedded in input data, known as Trojan attacks, can cause neural networks to misbehave. However, the impact of a Trojan attack is reduced during fine-tuning of the model, which involves transferring knowledge from a pretrained large-scale model like visual question answering (VQA) to the target model. To mitigate the effects of a Trojan attack, replacing and fine-tuning multiple layers of the pretrained model is possible. This research focuses on sample efficiency, stealthiness and variation, and robustness to model fine-tuning. To address these challenges, we propose an instance-level Trojan attack that generates diverse Trojans across input samples and modalities. Adversarial learning establishes a correlation between a specified perturbation layer and the misbehavior of the fine-tuned model. We conducted extensive experiments on the VQA-v2 dataset using a range of metrics. The results show that our proposed method can effectively adapt to a fine-tuned model with minimal samples. Specifically, we found that a model with a single fine-tuning layer can be compromised using a single shot of adversarial samples, while a model with more fine-tuning layers can be compromised using only a few shots."}, "cited_paper_content": {"title": "Microsoft Coco: Common Objects In Context", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."}, "keywords": ["COCO dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We evaluated our method on the VQA-v2 dataset, which contains images of complex everyday scenes containing common objects in their natural context #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate diverse Trojans across input samples and modalities, the authors proposed an instance-level Trojan attack that leverages the Microsoft Coco dataset for precise object localization #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate diverse Trojans across input samples and modalities, we utilized the COCO dataset, which contains photos of 91 object types that would be easily recognizable by a 4-year-old, and has a total of 2.5 million labeled instances in 328k images #REFR.", "given_citation_intent": "method", "given_keywords": ["COCO dataset"]}]}
{"citing_id": "2304.00212v1", "cited_id": "1908.03265", "section_title": "Implementation & Evaluation Metrics", "citation": "The network is trained with RAdam #REFR with the initial learning rate as 1 \u00d7 10 \u22124 and a polynomial learning rate decay.", "text_before_citation": ["Training and Testing.", "Each CT scan is resampled into the median spacing per tumor dataset (e.g., 3\u00d70.419\u00d70.419 mm for the pancreatic dataset) and normalized into zero mean and unit variance.", "Our model is trained using a batch size of 2 on one GPU (with 28 \u00d7 192 \u00d7 320 patch size for pancreatic, 40 \u00d7 192 \u00d7 224 for liver).", "We adopt the drop path #OTHEREFR strategy with a probability of 0.2 for regularization.", "During training, extensive data augmentation is utilized on-the-fly #OTHEREFR to improve the generalization, including random rotation and scaling, elastic deformation, additive brightness, and gamma scaling."], "text_after_citation": ["We first pre-train the nnUNet backbone for 1000 epochs and finetune the whole architecture jointly for another 200 epochs.", "During finetuning, we keep the backbone weights fixed for the first 50 epochs, and then set it with a learning rate multiplier of 0.1 for the next 150 epochs.", "The number of object queries (i.e., cluster centers) N is 32, and the query distri-", "bution (N 1 , N 2 , N 3 )", "is set as #OTHEREFR ."], "citing_paper_content": {"title": "Devil Is In The Queries: Advancing Mask Transformers For Real-World Medical Image Segmentation And Out-Of-Distribution Localization", "abstract": "Real-world medical image segmentation has tremendous long-tailed complexity of objects, among which tail conditions correlate with relatively rare diseases and are clinically significant. A trustworthy medical AI algorithm should demonstrate its effectiveness on tail conditions to avoid clinically dangerous damage in these out-ofdistribution (OOD) cases. In this paper, we adopt the concept of object queries in Mask Transformers to formulate semantic segmentation as a soft cluster assignment. The queries fit the feature-level cluster centers of inliers during training. Therefore, when performing inference on a medical image in real-world scenarios, the similarity between pixels and the queries detects and localizes OOD regions. We term this OOD localization as MaxQuery. Furthermore, the foregrounds of real-world medical images, whether OOD objects or inliers, are lesions. The difference between them is less than that between the foreground and background, possibly misleading the object queries to focus redundantly on the background. Thus, we propose a query-distribution (QD) loss to enforce clear boundaries between segmentation targets and other regions at the query level, improving the inlier segmentation and OOD indication. Our proposed framework is tested on two realworld segmentation tasks, i.e., segmentation of pancreatic and liver tumors, outperforming previous state-of-the-art algorithms by an average of 7.39% on AUROC, 14.69% on AUPR, and 13.79% on FPR95 for OOD localization. On the other hand, our framework improves the performance of inlier segmentation by an average of 5.27% DSC when compared with the leading baseline nnUNet."}, "cited_paper_content": {"title": "On The Variance Of The Adaptive Learning Rate And Beyond", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam."}, "keywords": ["initial learning rate"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "During training, extensive data augmentation is utilized on-the-fly #REFR to improve the generalization, including random rotation and scaling, elastic deformation, additive brightness, and gamma scaling. We adopt the learning rate warmup heuristic to stabilize training and accelerate convergence, as proposed in the reference paper \"On The Variance Of The Adaptive Learning Rate And Beyond\".", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To rectify the variance of the adaptive learning rate, the authors of this paper propose a novel variant of Adam called Rectified Adam (RAdam) #REFR, which has been shown to be effective and robust in various tasks including image classification and language modeling.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of the large variance of the initial learning rate in the early stage of training, the authors propose a novel variant of Adam, called Rectified Adam (RAdam), which rectifies the variance of the adaptive learning rate, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["initial learning rate"]}]}
{"citing_id": "2303.00459v2", "cited_id": "2002.04960", "section_title": "A. Snr Bounds And Asymptotic Analysis", "citation": "In summary, the conventional UPW model is valid for most practical cases in terms of the received power, but the accurate near-field modelling needs to be considered when the IRS size becomes significantly large, especially for the asymptotic analysis, which is consistent with the conclusions in #REFR . Fig.", "text_before_citation": ["The transmit SNR isP = 90 dB, and the BS and user are assumed to be located at q = [10, 0, 0] T m and p = [100, 0, 0] T m, respectively.", "It is firstly observed that for UPA-based XL-IRS, the derived bounds in Lemma 1 are sufficiently tight for the SNR estimation. In particular, Figs.", "7(b) and 7(c) validate our closed-form bounds in #OTHEREFR and #OTHEREFR .", "Furthermore, as the IRS size L becomes large, the SNR in model in #OTHEREFR is approximately consistent with our developed near-field model when the IRS size is not very large.", "However, as the IRS size exceeds a certain threshold, such two models exhibit drastically different scaling laws, i.e., converging to a constant value versus increasing unboundedly."], "text_after_citation": ["8 plots the SNR versus the link distance r q between the BS and the IRS center, where the square IRS size is set as L y = L z 8 m.", "We compare the resulting SNR with the following expressions, i.e., the summation form in #OTHEREFR , generic lower-and upper-bounds in #OTHEREFR , and the square power scaling law under the conventional UPW model in #OTHEREFR .", "The BS direction is set as (\u03b8 q , \u03c6 q ) = \u03c0 3 , \u03c0 6 , and the user is located at (r p , \u03b8 p , \u03c6 p ) = 200 m, 3\u03c0 4 , \u2212 \u03c0 5 .", "It is observed that the SNR bounds given in Theorem 1 are rather accurate, and the conventional UPW model in general over-estimates the SNR values when taking into account the directional gain patterns of IRS's reflecting elements, as illustrated in Fig. 8(b) and 8(c). In particular, Fig.", "8(c) reveals that the SNR does not necessarily monotonically decrease with the distance from the BS to the XL-IRS, because it also depends on the gain pattern of each element."], "citing_paper_content": {"title": "Near-Field Modelling And Performance Analysis For Extremely Large-Scale Irs Communications", "abstract": "Intelligent reflecting surface (IRS) is an emerging technology for wireless communications, thanks to its powerful capability to engineer the radio environment. However, in practice, this benefit is attainable only when the passive IRS is of sufficiently large size, for which the conventional uniform plane wave (UPW)-based far-field model may become invalid. In this paper, we pursue a near-field modelling and performance analysis for wireless communications with extremely large-scale IRS (XL-IRS). By taking into account the directional gain pattern of IRS's reflecting elements and the variations in signal amplitude across them, we derive both the lower-and upper-bounds of the resulting signal-to-noise ratio (SNR) for the generic uniform planar array (UPA)-based XL-IRS. Our results reveal that, instead of scaling quadratically and unboundedly with the number of reflecting elements M as in the conventional UPW-based model, the SNR under the new non-uniform spherical wave (NUSW)-based model increases with M with a diminishing return and eventually converges to a certain limit. To gain more insights, we"}, "cited_paper_content": {"title": "Power Scaling Laws And Near-Field Behaviors Of Massive Mimo And Intelligent Reflecting Surfaces", "abstract": "Large arrays might be the solution to the capacity problems in wireless communications. The signal-to-noise ratio (SNR) grows linearly with the number of array elements $N$ when using Massive MIMO receivers/relays. Moreover, intelligent reflecting surfaces (IRSs) have recently attracted attention since their SNR grows as $N^2$, which seems like a major benefit. In this paper, we use a deterministic propagation model for a planar array of arbitrary size, to demonstrate that the mentioned SNR behaviors, and associated power scaling laws, only apply in the far-field. They cannot be used to study the regime where $N\\to\\infty$. We derive an exact channel gain expression that captures the near-field behavior and use it to revisit the power scaling laws. We derive new finite asymptotic SNR limits but also conclude that these are unlikely to be approached in practice. We further prove that an IRS setup cannot achieve a higher SNR than the corresponding Massive MIMO setups, despite its faster SNR growth. The IRS typically must have a much larger array size to achieve the same SNR. Finally, we show that an optimized IRS can be interpreted as a reconfigurable lens and that it is generally suboptimal to operate it as an \"anomalous\" mirror."}, "keywords": ["accurate near-field modelling"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Furthermore, as the IRS size L becomes large, the SNR in model in #REFR is approximately consistent with our developed near-field model when the IRS size is not very large. However, as the IRS size exceeds a certain threshold, such two models exhibit drastically different scaling laws, i.e., converging to a constant value versus increasing unboundedly.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "As observed in the near-field model developed in this paper, the scaling behavior of the signal-to-noise ratio (SNR) for uniform planar array (UPA)-based XL-IRS is different from that in the conventional UPW-based model, and it converges to a certain limit instead of increasing unboundedly as in the latter model, which is consistent with the findings in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The accurate near-field modelling of extremely large-scale IRSs is crucial for estimating the signal-to-noise ratio, and the developed model in this paper shows that the SNR converges to a certain limit, which is in contrast to the unbounded increase predicted by the far-field model in #REFR.", "given_citation_intent": "result", "given_keywords": ["accurate near-field modelling"]}]}
{"citing_id": "2304.02101v1", "cited_id": "1708.02072", "section_title": "Continually Training Approximation Models", "citation": "This can result in overfitting to certain orientations, and catastrophic forgetting #REFR for others that may soon be ranked highly.", "text_before_citation": ["To cope with such data drift, MadEye employs continual learning (every 400-500 ms) to update the model's weights using the latest query results on orientations sent to the server for full workload inference.", "While continual learning has been applied to edge video analytics #OTHEREFR , MadEye requires several alterations from prior efforts.", "The main challenge is that within each retraining window, samples are only available for the orientations that MadEye's camera-side component recently visited and deemed worthy of backend inference.", "Since orientations are typically best for short total times ( \u00a72.3), there is often severe imbalance in the orientations covered by new training samples.", "For instance, with perfect rankings, the average 2-minute window sees only 9.3% of orientations get sent to the backend."], "text_after_citation": ["To deal with this, MadEye retrieves the most recent historical training samples from each orientation and uses this to balance the dataset.", "As we will discuss in \u00a73.3, we find that orientation shifts are often spatially localized, with changes to distant orientations happening over longer timescales.", "Thus, MadEye pads the data samples for neighboring orientations (up to 3 away from the latest one) to match the count for the most popular orientation in the retraining window.", "The remaining orientations use an exponentially declining number of samples based on their distance from the latest orientation."], "citing_paper_content": {"title": "Madeye: Boosting Live Video Analytics Accuracy With Adaptive Camera Configurations", "abstract": "Camera orientations (i.e., rotation and zoom) govern the content that a camera captures in a given scene, which in turn heavily influences the accuracy of live video analytics pipelines. However, existing analytics approaches leave this crucial adaptation knob untouched, instead opting to only alter the way that captured images from fixed orientations are encoded, streamed, and analyzed. We present MadEye, a camera-server system that automatically and continually adapts orientations to maximize accuracy for the workload and resource constraints at hand. To realize this using commodity pan-tilt-zoom (PTZ) cameras, MadEye embeds (1) a search algorithm that rapidly explores the massive space of orientations to identify a fruitful subset at each time, and (2) a novel knowledge distillation strategy to efficiently (with only camera resources) select the ones that maximize workload accuracy. Experiments on diverse workloads show that MadEye boosts accuracy by 2.9-25.7% for the same resource usage, or achieves the same accuracy with 2-3.7\u00d7 lower resource costs."}, "cited_paper_content": {"title": "Measuring Catastrophic Forgetting In Neural Networks", "abstract": "Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved."}, "keywords": ["certain orientations", "catastrophic forgetting"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of catastrophic forgetting in neural networks, various mechanisms have been proposed, including regularization, ensembling, rehearsal, dual-memory, and sparse-coding #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of catastrophic forgetting in neural networks, which occurs when a network trained to do a specific task cannot easily be trained to do new tasks, the authors of MadEye could refer to the work of Kirkpatrick et al. who introduced new metrics and benchmarks for comparing different mechanisms designed to mitigate catastrophic forgetting in neural networks #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of catastrophic forgetting in neural networks, various mechanisms have been proposed, including regularization, ensembling, rehearsal, dual-memory, and sparse-coding #REFR.", "given_citation_intent": "background", "given_keywords": ["certain orientations", "catastrophic forgetting"]}]}
{"citing_id": "2303.05744v1", "cited_id": "2001.01568", "section_title": "Bit Consumption Analysis", "citation": "Based on the results of the bitrate consumption and compression performance presented in Table R1 , it can be found that QVRF only has an impact on the compression of the latent representation, which requires a fixed number of bits for the side information. #REFR as the benchmark. Table R1 .", "text_before_citation": [], "text_after_citation": ["Results of bitrate consumption and compression performance on compressing kodim01 using the single model of QVRF with Cheng et al. #OTHEREFR for discrete bitrates."], "citing_paper_content": {"title": "Qvrf: A Quantization-Error-Aware Variable Rate Framework For Learned Image Compression", "abstract": "Learned image compression has exhibited promising compression performance, but variable bitrates over a wide range remain a challenge. State-of-the-art variable rate methods compromise the loss of model performance and require numerous additional parameters. In this paper, we present a Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a univariate quantization regulator a to achieve wide-range variable rates within a single model. Specifically, QVRF defines a quantization regulator vector coupled with predefined Lagrange multipliers to control quantization error of all latent representation for discrete variable rates. Additionally, the reparameterization method makes QVRF compatible with a round quantizer. Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods equipped with QVRF can achieve wide-range continuous variable rates within a single model without significant performance degradation. Furthermore, QVRF outperforms contemporary variable-rate methods in rate-distortion performance with minimal additional parameters."}, "cited_paper_content": {"title": "Learned Image Compression With Discretized Gaussian Mixture Likelihoods And Attention Modules", "abstract": "Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM. This project page is at this https URL https://github.com/ZhengxueCheng/Learned-Image-Compression-with-GMM-and-Attention"}, "keywords": ["compression performance"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In order to achieve a more accurate and flexible entropy model for learned image compression, the authors propose to use discretized Gaussian Mixture Likelihoods and attention modules, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in this paper achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets, and even generates more visually pleasant results when optimized by MS-SSIM, as demonstrated by the experimental results. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets, demonstrating significant improvements in compression performance. #REFR", "given_citation_intent": "result", "given_keywords": ["compression performance"]}]}
{"citing_id": "2304.05064v1", "cited_id": "1610.01722", "section_title": "Discussion", "citation": "Our results however do not directly contradict the results of #REFR itself, since it does not compare with the fast tools identified here and stands to a large degree on parametric and random benchmarks.", "text_before_citation": ["Although the mediocre performance of the other tools can be partially explained by their focus on a different kind of a problem or a dated underlying technology, and each of them is respectable in its own right, a point can be made against relying on them as a baseline in comparisons of tools for solving our kind of problem.", "MONA, opti-mised for a different settings (complex alphabets of bit-vectors with many minterms), is held back by the implicit determinisation, and, in our case, probably by the overhead of the symbolic representation.", "It also frequently runs out of the 32-bit address space for BDD nodes. Similarly for BRICS, which also always determinises.", "The low performance of BISIM is surprising relative to the good results of the up-to algorithms reported in #OTHEREFR .", "It is more consistent with #OTHEREFR where up-to algorithms were not wining against antichains on the more practical examples."], "text_after_citation": ["There is also always the possibility that we have prepared the input in a way not ideal for the tool.", "For instance, transformation to the separated AFA, required by BISIM, is not entirely trivial.", "Further investigation of this and a comparison with some other implementation of the up-to techniques seems to be needed.", "The lack of a raw speed of JALTIMPACT on BRE and AFA emptiness is expectable considering that it is meant for a different kind of systems, AFA over data words.", "The stable trends shown in the graphs suggest that an implementation of an interpolation-based abstraction refinement optimised for BRE and AFA emptiness might have a potential."], "citing_paper_content": {"title": "Reasoning About Regular Properties: A Comparative Study", "abstract": "Several new algorithms for deciding emptiness of Boolean combinations of regular languages and of languages of alternating automata (AFA) have been proposed recently, especially in the context of analysing regular expressions and in string constraint solving. The new algorithms demonstrated a significant potential, but they have never been systematically compared, neither among each other nor with the state-of-the art implementations of existing (non)deterministic automata-based methods. In this paper, we provide the first such comparison as well as an overview of the existing algorithms and their implementations. We collect a diverse benchmark mostly originating in or related to practical problems from string constraint solving, analysing LTL properties, and regular model checking, and evaluate collected implementations on it. The results reveal the best tools and hint on what the best algorithms and implementation techniques are. Roughly, although some advanced algorithms are fast, such as antichain algorithms and reductions to IC3/PDR, they are not as overwhelmingly dominant as sometimes presented and there is no clear winner. The simplest NFA-based technology may be actually the best choice, depending on the problem source and implementation style. Our findings should be highly relevant for development of these techniques as well as for related fields such as string constraint solving."}, "cited_paper_content": {"title": "A Symbolic Decision Procedure For Symbolic Alternating Finite Automata", "abstract": "We introduce Symbolic Alternating Finite Automata (s-AFA) as an expressive, succinct, and decidable model for describing sets of finite sequences over arbitrary alphabets. Boolean operations over s-AFAs have linear complexity, which is in sharp contrast with the quadratic cost of intersection and union for non-alternating symbolic automata. Due to this succinctness, emptiness and equivalence checking are PSpace-hard. We introduce an algorithm for checking the equivalence of two s-AFAs based on bisimulation up to congruence. This algorithm allows us to exploit the power of SAT and SMT solvers to efficiently search the state space of the s-AFAs. We evaluate our decision procedure on two verification and security applications: 1) checking satisfiability of linear temporal logic formulas over finite traces, and 2) checking equivalence of Boolean combinations of regular expressions. Our experiments show that our technique often outperforms existing techniques and it can be beneficial in both such applications."}, "keywords": ["random benchmarks"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The low performance of BISIM is surprising relative to the good results of the up-to algorithms reported in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The up-to algorithms reported in #OTHEREFR were found to be performing better than BISIM, which is consistent with the low performance of BISIM reported in the manuscript.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The up-to algorithms reported in #REFR outperformed BISIM on the more practical examples, which is consistent with the low performance of BISIM reported in #OTHEREFR. Therefore, it is important to carefully choose the right tool for solving regular expression problems, especially when dealing with random benchmarks.", "given_citation_intent": "result", "given_keywords": ["random benchmarks"]}]}
{"citing_id": "2303.11026v1", "cited_id": "1504.05811", "section_title": "D. Related Work", "citation": "In #REFR , GP was combined with a local search but this is harder to re-implement in a nondeterministic robotic scenario.", "text_before_citation": ["Evolutionary approaches to learn BTs were first applied to computer games #OTHEREFR , #OTHEREFR , #OTHEREFR - #OTHEREFR but later years have also seen it applied to robotic applications #OTHEREFR , #OTHEREFR , #OTHEREFR - #OTHEREFR .", "Authors in #OTHEREFR , #OTHEREFR successfully use Grammatical Evolution (GE).", "With GE, the design of the grammar requires domain knowledge and the engineering effort grows with the complexity of the task.", "Further, the disconnect between genotype and phenotype can make analysis and implementing heuristics difficult and hinders locality #OTHEREFR , #OTHEREFR .", "In #OTHEREFR , #OTHEREFR , #OTHEREFR , structural and dynamic constraints were implemented in the GP, speeding up the learning by preventing the generation of undesirable trees. Similar constraints are also used in our implementation."], "text_after_citation": ["Most notably, #OTHEREFR presented a method to combine the results from a planner with a GP algorithm.", "An adaptation of that method is what we use in the proposed approach of this paper to seed the GP with the information gathered in the demonstrations.", "There has been some previous work on combining LfD and BTs #OTHEREFR , #OTHEREFR , #OTHEREFR - #OTHEREFR .", "The method proposed in #OTHEREFR , #OTHEREFR , and #OTHEREFR learns a Decision Tree (DT) to map from state space to action space.", "The DT is then converted into a BT using the fact that BTs generalize DTs #OTHEREFR ."], "citing_paper_content": {"title": "A Framework For Learning Behavior Trees In Collaborative Robotic Applications", "abstract": "In modern industrial collaborative robotic applications, it is desirable to create robot programs automatically, intuitively, and time-efficiently. Moreover, robots need to be controlled by reactive policies to face the unpredictability of the environment they operate in. In this paper we propose a framework that combines a method that learns Behavior Trees (BTs) from demonstration with a method that evolves them with Genetic Programming (GP) for collaborative robotic applications. The main contribution of this paper is to show that by combining the two learning methods we obtain a method that allows non-expert users to semi-automatically, time-efficiently, and interactively generate BTs. We validate the framework with a series of manipulation experiments. The BT is fully learnt in simulation and then transferred to a real collaborative robot."}, "cited_paper_content": {"title": "Learning Of Behavior Trees For Autonomous Agents", "abstract": "Definition of an accurate system model for Automated Planner (AP) is often impractical, especially for real-world problems. Conversely, off-the-shelf planners fail to scale up and are domain dependent. These drawbacks are inherited from conventional transition systems such as Finite State Machines (FSMs) that describes the action-plan execution generated by the AP. On the other hand, Behavior Trees (BTs) represent a valid alternative to FSMs presenting many advantages in terms of modularity, reactiveness, scalability and domain-independence. In this paper, we propose a model-free AP framework using Genetic Programming (GP) to derive an optimal BT for an autonomous agent to achieve a given goal in unknown (but fully observable) environments. We illustrate the proposed framework using experiments conducted with an open source benchmark Mario AI for automated generation of BTs that can play the game character Mario to complete a certain level at various levels of difficulty to include enemies and obstacles."}, "keywords": ["nondeterministic robotic scenario"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Evolutionary approaches to learn BTs were first applied to computer games #OTHEREFR , #OTHEREFR , #OTHEREFR - #OTHEREFR but later years have also seen it applied to robotic applications #OTHEREFR , #OTHEREFR , #OTHEREFR - #OTHEREFR . Authors in #OTHEREFR , #OTHEREFR successfully use Grammatical Evolution (GE). In contrast, the proposed framework in this paper combines a method that learns Behavior Trees (BTs) from demonstration with a method that evolves them with Genetic Programming (GP) for collaborative robotic applications. #REFR presents a model-free AP framework using GP to derive an optimal BT for an autonomous agent to achieve a given goal in unknown environments.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn Behavior Trees (BTs) for autonomous agents, the authors propose a model-free Automated Planner (AP) framework using Genetic Programming (GP) in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn Behavior Trees (BTs) for autonomous agents in a nondeterministic robotic scenario, the authors propose a model-free AP framework using Genetic Programming (GP) that derives an optimal BT to achieve a given goal in unknown environments, as demonstrated in their experiments with an open source benchmark Mario AI #REFR.", "given_citation_intent": "method", "given_keywords": ["nondeterministic robotic scenario"]}]}
{"citing_id": "2304.09116v2", "cited_id": "1709.07871", "section_title": "Model Configuration And Comparison", "citation": "Specifically, we use a FiLM layer #REFR at every 3 WaveNet layers to fuse the condition information processed by the second Q-K-V attention in the prompting mechanism in the diffusion model.", "text_before_citation": ["The pitch and duration predictor share the same architecture of 30-layer 1D convolution with ReLU activation and layer normalization, 10 Q-K-V attention layers for in-context learning, which have 512 hidden dimensions and 8 attention heads and are placed every 3 1D convolution layers.", "Specifically, we set the dropout to 0.5 in the duration predictor and 0.2 in the pitch predictor.", "For the speech prompt encoder, we use a 6-layer Transformer with 512 hidden size, which has the same architecture as the phoneme encoder.", "As for the m query tokens in the first Q-K-V attention in the prompting mechanism in the diffusion model (as shown in Figure 3 ), we set the token number m to 32 and the hidden dimension to 512.", "The diffusion model contains 40 WaveNet layers #OTHEREFR , which consist of 1D dilated convolution layers with 3 kernel size, 1024 filter size, and 2 dilation size."], "text_after_citation": ["The hidden size in WaveNet is 512, and the dropout rate is 0.2.", "More details of the model configurations are shown in Appendix A."], "citing_paper_content": {"title": "Naturalspeech 2: Latent Diffusion Models Are Natural And Zero-Shot Speech And Singing Synthesizers", "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2."}, "cited_paper_content": {"title": "Film: Visual Reasoning With A General Conditioning Layer", "abstract": "We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot."}, "keywords": ["second Q-K-V attention", "layers"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To generate latent vectors conditioned on text input, the authors of NaturalSpeech 2 used a diffusion model, which has been shown to be highly effective for visual reasoning tasks in the FiLM paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate the latent vectors, the authors used a diffusion model conditioned on text input and leveraged a general-purpose conditioning method called FiLM, which modulates features in a coherent manner based on conditioning information, as introduced in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The prompting mechanism in the diffusion model utilizes the second Q-K-V attention with 32 tokens to facilitate in-context learning, as proposed in the FiLM method #REFR.", "given_citation_intent": "method", "given_keywords": ["second Q-K-V attention", "layers"]}]}
{"citing_id": "2305.00385v1", "cited_id": "1808.08114", "section_title": "Performance On Pi-Cai", "citation": "The second model is Attention UNet #REFR , a U-Net model with attention gates introduced in the skipped connections.", "text_before_citation": ["We first evaluated our proposed methods on PI-CAI validation sets.", "For comparable methods, we report results from 3 other state-of-the-art CNN segmentation models and 1 state-of-the-art transformer model.", "The first one is the basic UNet structure #OTHEREFR without the attention mechanism."], "text_after_citation": ["Attention gates allow the network learning to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task.", "The third model is DynUNet implemented in MONAI, which followed nnUNet architecture proposed by #OTHEREFR .", "The transformer model we compare to is Swin UNETR #OTHEREFR which has a Swin transformer encoder and UNet-like architecture.", "Swin UNETR achieved state-of-the-art results in Brain Tumor Segmentation (BraTS) and Beyond the Cranial Vault (BTCV) datasets.", "We facilitated a fair comparison by maintaining an identical preprocessing, augmentation, tuning and train-validation pipeline for each candidate system in a given experiment."], "citing_paper_content": {"title": "Cross-Shaped Windows Transformer With Self-Supervised Pretraining For Clinically Significant Prostate Cancer Detection In Bi-Parametric Mri", "abstract": "Multiparametric magnetic resonance imaging (mpMRI) has demonstrated promising results in prostate cancer (PCa) detection using deep convolutional neural networks (CNNs). Recently, transformers have achieved competitive performance compared to CNNs in computer vision. Large-scale transformers need abundant annotated data for training, which are difficult to obtain in medical imaging. Self-supervised learning can effectively leverage unlabeled data to extract useful semantic representations without annotation and its associated costs. This can improve model performance on downstream tasks with limited labelled data and increase generalizability. We introduce a novel end-to-end Cross-Shaped windows (CSwin) transformer UNet model, CSwin UNet, to detect clinically significant prostate cancer (csPCa) in prostate bi-parametric MR imaging (bpMRI) and demonstrate the effectiveness of our proposed selfsupervised pre-training framework. Using a large prostate bpMRI dataset with 1500 patients, we first pretrain CSwin transformer using multi-task self-supervised learning to improve data-efficiency and network generalizability. We then finetuned using lesion annotations to perform csPCa detection. Five-fold cross validation shows that self-supervised CSwin UNet achieves 0.888\u00b10.010 AUC and 0.545\u00b10.060 Average Precision (AP), significantly outperforming four state-of-the-art models (Swin UNETR, DynUNet, Attention UNet, UNet). Using a separate bpMRI dataset with 158 patients, we evaluated our method's robustness to external hold-out data. Self-supervised CSwin UNet achieves 0.79 AUC and 0.45 AP, still outperforming all other comparable methods and demonstrating generalization to a dataset shift."}, "cited_paper_content": {"title": "Attention Gated Networks: Learning To Leverage Salient Regions In Medical Images", "abstract": "Graphical abstract Figure. No Caption available. ABSTRACT We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U\u2010Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention mechanism can provide efficient object localisation while improving the overall prediction performance by reducing false positives. For segmentation, the proposed architecture is evaluated on two large 3D CT abdominal datasets with manual annotations for multiple organs. Experimental results show that AG models consistently improve the prediction performance of the base architectures across different datasets and training sizes while preserving computational efficiency. Moreover, AGs guide the model activations to be focused around salient regions, which provides better insights into how model predictions are made. The source code for the proposed AG models is publicly available."}, "keywords": ["attention gates"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We first evaluated our proposed methods on PI-CAI validation sets. For comparable methods, we report results from 3 other state-of-the-art CNN segmentation models and 1 state-of-the-art transformer model. The first one is the basic UNet structure #REFR without the attention mechanism. \n\nBackground: The basic UNet structure without the attention mechanism was used as a comparable method in this study.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compared our proposed method with four state-of-the-art models, including basic UNet structure without the attention mechanism, and achieved superior results, demonstrating the effectiveness of our approach in clinically significant prostate cancer detection in bi-parametric MRI. #REFR was used as a reference for the attention mechanism.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compared our proposed CSwin UNet model with four state-of-the-art models, including Attention UNet, which uses attention gates to highlight salient features in medical images, as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["attention gates"]}]}
{"citing_id": "2304.07152v1", "cited_id": "2002.08599", "section_title": "Proposed Framework", "citation": "Finally, a new set learning module #REFR aggregates subgraph representations obtained after a global pooling operation into a single one used in downstream tasks.", "text_before_citation": ["We adapt the binarization threshold to obtain a set of explanations with K edges, where K ranges from 5% to 75% of the initial number of edges in the original graph.", "An example of these two strategies is presented in Figure 2 , on the synthetic dataset BA-2Motifs where the five nodes cycle is responsible for the prediction.", "In the third step, we insert the backbone GNN into the DSS-GNN model introduced in ESAN in #OTHEREFR .", "The backbone works as the encoder that processes each explanation subgraph independently.", "A new message-passing module preprocesses the aggregation of the explanations working as an information-sharing module for the subgraphs."], "text_after_citation": ["Besides being used by the information-sharing module, the subgraph obtained by aggregating all the explanations is used to explain the model's prediction.", "We would like to stress the fact that this explanation, rather than being computed just from a post-hoc explainer, is used to train the model."], "citing_paper_content": {"title": "Combining Stochastic Explainers And Subgraph Neural Networks Can Increase Expressivity And Interpretability", "abstract": "Subgraph-enhanced graph neural networks (SGNN) can increase the expressive power of the standard message-passing framework. This model family represents each graph as a collection of subgraphs, generally extracted by random sampling or with hand-crafted heuristics. Our key observation is that by selecting \"meaningful\" subgraphs, besides improving the expressivity of a GNN, it is also possible to obtain interpretable results. For this purpose, we introduce a novel framework that jointly predicts the class of the graph and a set of explanatory sparse subgraphs, which can be analyzed to understand the decision process of the classifier. We compare the performance of our framework against standard subgraph extraction policies, like random node/edge deletion strategies. The subgraphs produced by our framework allow to achieve comparable performance in terms of accuracy, with the additional benefit of providing explanations."}, "cited_paper_content": {"title": "On Learning Sets Of Symmetric Elements", "abstract": "Learning from unordered sets is a fundamental learning setup, which is attracting increasing attention. Research in this area has focused on the case where elements of the set are represented by feature vectors, and far less emphasis has been given to the common case where set elements themselves adhere to certain symmetries. That case is relevant to numerous applications, from deblurring image bursts to multi-view 3D shape recognition and reconstruction. ::: In this paper, we present a principled approach to learning sets of general symmetric elements. We first characterize the space of linear layers that are equivariant both to element reordering and to the inherent symmetries of elements, like translation in the case of images. We further show that networks that are composed of these layers, called Deep Sets for Symmetric elements layers (DSS), are universal approximators of both invariant and equivariant functions. DSS layers are also straightforward to implement. Finally, we show that they improve over existing set-learning architectures in a series of experiments with images, graphs, and point-clouds."}, "keywords": ["subgraph representations", "new set learning"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the third step, we insert the backbone GNN into the DSS-GNN model introduced in ESAN in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We incorporate the DSS-GNN model introduced in ESAN #REFR, which uses Deep Sets for Symmetric elements layers to learn sets of general symmetric elements, into our framework to enhance the expressive power of the standard message-passing framework.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We incorporate the DSS-GNN model introduced in ESAN in #OTHEREFR, which uses Deep Sets for Symmetric elements layers (DSS) to learn from subgraph representations, enabling new set learning capabilities. #REFR", "given_citation_intent": "method", "given_keywords": ["subgraph representations", "new set learning"]}]}
{"citing_id": "2304.14802v1", "cited_id": "1808.06226", "section_title": "Experimental Settings", "citation": "We used the script from to tokenize the data and used SentencePiece #REFR to segment the tokens.", "text_before_citation": ["We followed the scripts in FairSeq to preprocess the data.", "The WMT DE\u2192EN dataset is larger, with 1.4M sentence pairs.", "We followed the preprocessing steps outlined in Takase and Kiyono (2021) by tokenizing the data with Moses tokenizer and then processing it with BPE #OTHEREFR .", "The model was trained on the WMT-14 training set and evaluated on the test set from years 2010 to 2016, following Takase and Kiyono (2021) .", "The OPUS-100 dataset is a large-scale multilingual dataset containing 100 languages and approximately 55M sentence pairs."], "text_after_citation": ["All data processing scripts are available in the Appendix G.", "Model Our model is implemented using the FairSeq framework.", "We follow the convention of using the same model size as previous works.", "Notably, our method does not introduce additional parameters to the vanilla Transformer network.", "We trained our models using the Adam (Kingma and Ba, 2014) optimizer with \u03b2 = (0.9, 0.98), = and used the invert_sqrt learning rate scheduler with warm up, unless specified otherwise For detailed training hyper-parameters, please refer to the Appendix G."], "citing_paper_content": {"title": "Residual: Transformer With Dual Residual Connections", "abstract": "Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer Normalization (Post-LN) and Pre-Layer Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together, and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empirical experiments to verify the effectiveness of ResiDual. Theoretically, we prove that ResiDual has a lower bound on the gradient to avoid the vanishing issue due to the residual connection from Pre-LN. Moreover, ResiDual also has diverse model representations to avoid the collapse issue due to the residual connection from Post-LN. Empirically, ResiDual outperforms both Post-LN and Pre-LN on several machine translation benchmarks across different network depths and data sizes. Thanks to the good theoretical and empirical performance, ResiDual Transformer can serve as a foundation architecture for different AI models (e.g., large language models). Our code is available at https://github.com/microsoft/ResiDual."}, "cited_paper_content": {"title": "Sentencepiece: A Simple And Language Independent Subword Tokenizer And Detokenizer For Neural Text Processing", "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece."}, "keywords": ["SentencePiece"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We followed the preprocessing steps outlined in Takase and Kiyono (2021) by tokenizing the data with Moses tokenizer and then processing it with BPE #OTHEREFR. The model was trained on the WMT-14 training set and evaluated on the test set from years 2010 to 2016, following Takase and Kiyono (2021). The SentencePiece tokenizer, which provides open-source C++ and Python implementations for subword units, was used to train subword models directly from raw sentences in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We tokenized the data with Moses tokenizer and then processed it with BPE, following the steps outlined in Takase and Kiyono (2021). SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, was used to train subword models directly from raw sentences, which allowed us to make a purely end-to-end and language independent system #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We tokenized the data with Moses tokenizer and then processed it with BPE #OTHEREFR, following the preprocessing steps outlined in Takase and Kiyono (2021). The method used for BPE tokenization is based on SentencePiece #REFR.", "given_citation_intent": "method", "given_keywords": ["SentencePiece"]}]}
{"citing_id": "2305.02457v1", "cited_id": "1510.00277", "section_title": "A. Dissimilarity Measures", "citation": "This means that D JS (p, q) is equal to the average amount of information in one randomly sampled word token about which of the two distributions it was sampled from #REFR .", "text_before_citation": ["p + q = M i=1 (p i + q i ).", "The JSD has a number of properties that are useful for its interpretation as a distance.", "It is symmetric, non-negative, and equal to 0 if and only if p = q.", "Furthermore, D JS (p, q) satisfies the triangle inequality and is thus a metric #OTHEREFR .", "Additionally, the JSD between distributions p and q is equivalent to the mutual information of variables sampled from p and q."], "text_after_citation": ["In this paper, we predominately considered a generalisation of the JSD whereby H in Equation (6) is replaced by the generalised entropy of order \u03b1 #OTHEREFR", "EQUATION", "This generalisation, first introduced in Ref.", "#OTHEREFR , yields a spectrum of divergence measures D \u03b1 parameterised by \u03b1.", "When \u03b1 = 1, we recover the usual Jensen-Shannon divergence D JS ."], "citing_paper_content": {"title": "Quantifying The Dissimilarity Of Texts", "abstract": "Quantifying the dissimilarity of two texts is an important aspect of a number of natural language processing tasks, including semantic information retrieval, topic classification, and document clustering. In this paper, we compared the properties and performance of different dissimilarity measures D using three different representations of texts-vocabularies, word frequency distributions, and vector embeddings-and three simple tasks-clustering texts by author, subject, and time period. Using the Project Gutenberg database, we found that the generalised Jensen-Shannon divergence applied to word frequencies performed strongly across all tasks, that D's based on vector embedding representations led to stronger performance for smaller texts, and that the optimal choice of approach was ultimately task-dependent. We also investigated, both analytically and numerically, the behaviour of the different D's when the two texts varied in length by a factor h. We demonstrated that the (natural) estimator of the Jaccard distance between vocabularies was inconsistent and computed explicitly the h-dependency of the bias of the estimator of the generalised Jensen-Shannon divergence applied to word frequencies. We also found numerically that the Jensen-Shannon divergence and embedding-based approaches were robust to changes in h, while the Jaccard distance was not."}, "cited_paper_content": {"title": "Similarity Of Symbol Frequency Distributions With Heavy Tails", "abstract": "Quantifying the similarity between symbolic sequences is a traditional problem in Information Theory which requires comparing the frequencies of symbols in different sequences. In numerous modern applications, ranging from DNA over music to texts, the distribution of symbol frequencies is characterized by heavy-tailed distributions (e.g., Zipf's law). The large number of low-frequency symbols in these distributions poses major difficulties to the estimation of the similarity between sequences, e.g., they hinder an accurate finite-size estimation of entropies. Here we show analytically how the systematic (bias) and statistical (fluctuations) errors in these estimations depend on the sample size~$N$ and on the exponent~$\\gamma$ of the heavy-tailed distribution. Our results are valid for the Shannon entropy $(\\alpha=1)$, its corresponding similarity measures (e.g., the Jensen-Shanon divergence), and also for measures based on the generalized entropy of order $\\alpha$. For small $\\alpha$'s, including $\\alpha=1$, the errors decay slower than the $1/N$-decay observed in short-tailed distributions. For $\\alpha$ larger than a critical value $\\alpha^* = 1+1/\\gamma \\leq 2$, the $1/N$-decay is recovered. We show the practical significance of our results by quantifying the evolution of the English language over the last two centuries using a complete $\\alpha$-spectrum of measures. We find that frequent words change more slowly than less frequent words and that $\\alpha=2$ provides the most robust measure to quantify language change."}, "keywords": ["information"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Here we show analytically how the systematic (bias) and statistical (fluctuations) errors in these estimations depend on the sample size~$N$ and on the exponent~$\\gamma$ of the heavy-tailed distribution #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Quantifying the similarity between symbolic sequences is a traditional problem in Information Theory which requires comparing the frequencies of symbols in different sequences, and the distribution of symbol frequencies is characterized by heavy-tailed distributions in numerous modern applications, ranging from DNA over music to texts, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The heavy-tailed distribution of symbol frequencies is a common characteristic in various modern applications, including DNA, music, and texts, which poses significant difficulties to the estimation of similarity between sequences, as it hinders an accurate finite-size estimation of entropies and leads to systematic and statistical errors. #REFR provides analytical results on the dependence of these errors on the sample size and the exponent of the heavy-tailed distribution, which are valid for the Shannon entropy and its corresponding similarity measures, including the Jensen-Shanon divergence, and measures based on the generalized entropy of order alpha.", "given_citation_intent": "background", "given_keywords": ["information"]}]}
{"citing_id": "2303.18223v7", "cited_id": "1706.03762", "section_title": "Technical Evolution Of Gpt-Series Models", "citation": "In 2017, the Transformer model #REFR was introduced by Google, and the OpenAI team quickly adapted their language modeling work to this new neural network architecture.", "text_before_citation": ["Overall, the research of OpenAI on LLMs can be roughly divided into the following stages 10 .", "Early Explorations.", "According to one interview with Ilya Sutskever #OTHEREFR (a co-founder and chief scientist of OpenAI), the idea of approaching intelligent systems with language models was already explored in the early days of Ope-nAI, while it was attempted with recurrent neural networks (RNN) #OTHEREFR .", "With the advent of Transformer, OpenAI developed two initial GPT models, namely GPT-1 #OTHEREFR and GPT-2 #OTHEREFR , which can considered as the foundation to more powerful models subsequently i.e., GPT-3 and GPT-4.", "\u2022 GPT-1."], "text_after_citation": ["They released the first GPT model in 2018, i.e., GPT-1 #OTHEREFR , and coined the abbreviation term GPT as the model name, standing for Generative Pre-Training.", "GPT-1 was developed based on a generative, decoder-only Transformer architecture, and adopted a hybrid approach of unsupervised pretraining and supervised fine-tuning.", "GPT-1 has set up the core architecture for the GPT-series models and established the underlying principle to model natural language text, i.e., predicting the next word.", "\u2022 GPT-2.", "Following a similar architecture of GPT-1, GPT-2 #OTHEREFR increased the parameter scale to 1.5B, which was trained with a large webpage dataset WebText."], "citing_paper_content": {"title": "A Survey Of Large Language Models", "abstract": "Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["language modeling work"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "With the advent of Transformer, OpenAI developed two initial GPT models, namely GPT-1 #REFR and GPT-2 #REFR, which can be considered as the foundation to more powerful models subsequently i.e., GPT-3 and GPT-4.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "OpenAI utilized the Transformer architecture proposed in \"Attention Is All You Need\" #REFR to develop their initial GPT models, which served as the foundation for subsequent models such as GPT-3 and GPT-4.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "OpenAI has been exploring the use of language modeling work for intelligent systems, and their early GPT models, including GPT-1 #REFR and GPT-2 #OTHEREFR, served as the foundation for subsequent more powerful models such as GPT-3 and GPT-4.", "given_citation_intent": "method", "given_keywords": ["language modeling work"]}]}
{"citing_id": "2304.05255v1", "cited_id": "1611.07725", "section_title": "Incremental Learning", "citation": "Rehearsal based methods #REFR save a few exemplars from the previous dataset and replay them or use them to constrain the model during the new training sessions.", "text_before_citation": ["Incremental learning aims to develop methods that can learn new knowledge from new data while not forgetting previous knowledge learned from the previous training stages.", "The existing methods can mainly be categorized as three types: distillation based, dynamic model based and rehearsal based #OTHEREFR .", "Distillation based methods focus on how to limit the change of the model by applying a loss on the weights directly #OTHEREFR , or on the output features #OTHEREFR and probabilities #OTHEREFR .", "Dynamic model based methods #OTHEREFR extend the architecture of the network to learn new knowledge from the new incoming data distribution."], "text_after_citation": ["Previously, incremental learning mainly focused on image classification problems.", "Recently, the community also developed incremental learning algorithms for other problems such as image generation #OTHEREFR , segmentation #OTHEREFR , object detection #OTHEREFR , video classification #OTHEREFR .", "But to the best of our knowledge, there is no work for incremental learning of counting problems yet."], "citing_paper_content": {"title": "Density Map Distillation For Incremental Object Counting", "abstract": "We investigate the problem of incremental learning for object counting, where a method must learn to count a variety of object classes from a sequence of datasets. A na\u00efve approach to incremental object counting would suffer from catastrophic forgetting, where it would suffer from a dramatic performance drop on previous tasks. In this paper, we propose a new exemplar-free functional regularization method, called Density Map Distillation (DMD). During training, we introduce a new counter head for each task and introduce a distillation loss to prevent forgetting of previous tasks. Additionally, we introduce a cross-task adaptor that projects the features of the current backbone to the previous backbone. This projector allows for the learning of new features while the backbone retains the relevant features for previous tasks. Finally, we set up experiments of incremental learning for counting new objects. Results confirm that our method greatly reduces catastrophic forgetting and outperforms existing methods."}, "cited_paper_content": {"title": "Icarl: Incremental Classifier And Representation Learning", "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail."}, "keywords": ["previous dataset", "new training sessions"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To prevent catastrophic forgetting in incremental object counting, Density Map Distillation (DMD) was proposed as a new exemplar-free functional regularization method that introduces a distillation loss and a cross-task adaptor, which projects the features of the current backbone to the previous backbone #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prevent forgetting of previous tasks in incremental object counting, Density Map Distillation (DMD) introduces a distillation loss and a cross-task adaptor that projects the features of the current backbone to the previous backbone #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prevent forgetting of previous tasks during incremental learning, distillation-based methods apply a loss on the output features or probabilities of the model #OTHEREFR. In this regard, the iCaRL method proposed in the reference paper allows learning in a class-incremental way, where only the training data for a small number of classes has to be present at the same time and new classes can be added progressively, making it suitable for learning from a sequence of previous datasets in new training sessions #REFR.", "given_citation_intent": "method", "given_keywords": ["previous dataset", "new training sessions"]}]}
{"citing_id": "2304.13614v1", "cited_id": "1912.06378", "section_title": "Method", "citation": "To investigate the effectiveness of our method, we mainly employ the cascade MVS- Net as the baseline and use two branches on the basis of Cas-MVSNet #REFR to predict the depth and signed distance, respectively.", "text_before_citation": ["In this section, we introduce the detailed structure of the proposed RA-MVSNet.", "As shown in Figure 2 , the overall framework mainly consists of three parts, including cost volume construction, the multi-scale depth map and signed distance prediction, and handling of ground truth.", "Since our proposed point-to-surface distance supervision employs an additional branch to calculate the signed distance of the sampling points around the surface through cost volume, it is easy to be added into the existing learning-based MVSNet scheme with slight changes."], "text_after_citation": [], "citing_paper_content": {"title": "Multi-View Stereo Representation Revist: Region-Aware Mvsnet", "abstract": "Deep learning-based multi-view stereo has emerged as a powerful paradigm for reconstructing the complete geometrically-detailed objects from multi-views. Most of the existing approaches only estimate the pixel-wise depth value by minimizing the gap between the predicted point and the intersection of ray and surface, which usually ignore the surface topology. It is essential to the textureless regions and surface boundary that cannot be properly reconstructed. To address this issue, we suggest to take advantage of point-to-surface distance so that the model is able to perceive a wider range of surfaces. To this end, we predict the distance volume from cost volume to estimate the signed distance of points around the surface. Our proposed RA-MVSNet is patch-awared, since the perception range is enhanced by associating hypothetical planes with a patch of surface. Therefore, it could increase the completion of textureless regions and reduce the outliers at the boundary. Moreover, the mesh topologies with fine details can be generated by the introduced distance volume. Comparing to the conventional deep learning-based multiview stereo methods, our proposed RA-MVSNet approach obtains more complete reconstruction results by taking advantage of signed distance supervision. The experiments on both the DTU and Tanks & Temples datasets demonstrate that our proposed approach achieves the state-of-the-art results."}, "cited_paper_content": {"title": "Cascade Cost Volume For High-Resolution Multi-View Stereo And Stereo Matching", "abstract": "The deep multi-view stereo (MVS) and stereo matching approaches generally construct 3D cost volumes to regularize and regress the output depth or disparity. These methods are limited when high-resolution outputs are needed since the memory and time costs grow cubically as the volume resolution increases. In this paper, we propose a both memory and time efficient cost volume formulation that is complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes. First, the proposed cost volume is built upon a standard feature pyramid encoding geometry and context at gradually finer scales. Then, we can narrow the depth (or disparity) range of each stage by the depth (or disparity) map from the previous stage. With gradually higher cost volume resolution and adaptive adjustment of depth (or disparity) intervals, the output is recovered in a coarser to fine manner. ::: We apply the cascade cost volume to the representative MVS-Net, and obtain a 23.1% improvement on DTU benchmark (1st place), with 50.6% and 74.2% reduction in GPU memory and run-time. It is also the state-of-the-art learning-based method on Tanks and Temples benchmark. The statistics of accuracy, run-time and GPU memory on other representative stereo CNNs also validate the effectiveness of our proposed method."}, "keywords": ["depth"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To construct the cost volume, we propose a memory and time efficient formulation that is complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes, as described in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct the cost volume, we adopt the cascade cost volume method proposed in #REFR, which is both memory and time efficient and complementary to existing multi-view stereo and stereo matching approaches based on 3D cost volumes.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct the cost volume, the proposed method in this paper encodes geometry and context at gradually finer scales, and narrows the depth range of each stage by the depth map from the previous stage, which leads to a more efficient and adaptive adjustment of depth intervals, as demonstrated by the 23.1% improvement on DTU benchmark with 50.6% and 74.2% reduction in GPU memory and run-time, respectively. #REFR", "given_citation_intent": "method", "given_keywords": ["depth"]}]}
{"citing_id": "2303.16132v1", "cited_id": "1810.00826", "section_title": "Baseline", "citation": "GIN is a well-known typical GNN that was expected to achieve the ability as the Weisfeiler-Lehman graph isomorphism test, but a graph-level readout function of GIN was specially designed to produce the embedding of the entire graph for graph classification tasks #REFR .", "text_before_citation": ["\u2022 GCN #OTHEREFR , GAT #OTHEREFR , GraphSAGE #OTHEREFR , and snowball GCN (SBGCN) #OTHEREFR : There are typical GNN models that were not specifically designed for graph classification.", "GCN is based on a variant of CNNs which operate directly on graphs. GAT is a GNN architecture using masked self-attentional layers.", "GraphSAGE is based on randomly edges sampling to enhance the ability of capturing global structure.", "SBGCN is a GCN architecture using a snowball structure to capture multi-scale information.", "\u2022 DiffPool #OTHEREFR , GIN #OTHEREFR , PGCN #OTHEREFR , and MRGNN #OTHEREFR : There are graph neural networks which have a special design for graph classification."], "text_after_citation": ["DiffPool is a differentiable graph pooling module that could generate hierarchical representations for graph classification.", "PGCN introduced the polynomial graph convolution layer to independently exploit neighbouring nodes at different topological distances, and used subsequent readout layers to generate graph representation.", "MRGNN used a shallow readout function to generate an unsupervised graph representation.", "\u2022 GraphTrans #OTHEREFR , Graphormer #OTHEREFR , and Spectral Attention Network (SAN) #OTHEREFR : They are three well-known graph-transformer based models.", "GraphTrans applied graph convolution operation to introduce local patterns and structure information of graphs."], "citing_paper_content": {"title": "Transformer And Snowball Graph Convolution Learning For Biomedical Graph Classification", "abstract": "Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical GNN models and the graph-transformer based GNN models. Keywords Graph neural networks \u2022 Transformer \u2022 Snowball graph convolutional networks 1 Introduction Graph or network is pervasive in describing and modeling complex systems in biomedicine, such as molecular graphs, multi-omics graphs, and pharmaceutical graphs. Recently, a growing number of deep learning methods, especially graph neural networks (GNNs) have been developed for learning and predicting on biomedical graph data, such as predicting the function of proteins in an interaction network, and predicting the toxicity or biological activity profile of a candidate drug [1, 2, 3]. Graph classification is one type of the most prominent learning tasks of GNNs, which typically learns a graph-level representation for graph classification in an end-to-end fashion. To learn an effective representation, GNNs were expected to capture the global patterns as well as the local patterns and their interaction [4]. There have been various attempts to enhance the expressive power of GNNs, for example, enlarging the receptive field [5], using effective graph pooling operations [6, 7], and relieving the over-smoothing caused by increased network depth [8, 9]. Compared with data from other fields, biomedical graphs are usually complex and noisy, and the data size is usually limited, since collecting biomedical graph data usually requires time consuming and laborious wet experiments [2]. In the view of biomedical graph data, it is difficult to develop targeted graph-level representation learning approaches to produce compact vector representations which can be optimized for the classification task. Recently, transformer [10] has been shown excel performance in nature language processing (NLP), such as Bert [11] and GPT [12], and this architecture was also introduced for graph structure data in biomedicine [13]. To date, transformer has been shown to be good at capturing the long-range dependency and global information in graph [14]. And, most researchers tried to properly incorporate structural information of graphs into GNN models with transformer, such"}, "cited_paper_content": {"title": "How Powerful Are Graph Neural Networks?", "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance."}, "keywords": ["graph classification tasks"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "TSEN, a novel graph neural network architecture for biomedical graph classification, combines graph snowball connection with graph transformer and outperforms state-of-the-art typical GNN models and graph-transformer based GNN models on four graph classification datasets #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To analyze the expressive power of graph neural networks (GNNs) and their limitations, the authors of this paper proposed a theoretical framework, which characterizes the discriminative power of popular GNN variants such as Graph Convolutional Networks and GraphSAGE, and showed that they cannot learn to distinguish certain simple graph structures #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Graph Convolutional Networks (GCN), GraphSAGE, and Graph Isomorphism Network (GIN) are some of the popular GNN variants that have achieved state-of-the-art results on graph classification tasks #REFR.", "given_citation_intent": "method", "given_keywords": ["graph classification tasks"]}]}
{"citing_id": "2303.06872v3", "cited_id": "1909.03557", "section_title": "A. Self-Attention", "citation": "In particular, it was shown in #REFR that a self-attention on CNN features was effective in camera pose regression.", "text_before_citation": ["Since Transformer #OTHEREFR made a great success in the literature on natural language process, its self-attention, one of the essential elements in Transformer, has been utilized in many studies for computer vision applications."], "text_after_citation": ["We use the same self-attention, but its input consists of image features computed from an input image and point cloud features computed from an input 2D point cloud.", "The input of the self-attention f i , which is a column vector, is first projected to generate query W q f i , key W k f i , and value W v f i by three learnable projections.", "Then, the value is weighted based on the normalized correlations between the query and the key.", "The correlations are calculated using the softmax function \u03c6, and these procedures can be represented as", "f Att = \u03c6 f T i W T q W k f i W v f i ."], "citing_paper_content": {"title": "Fusionloc: Camera-2D Lidar Fusion Using Multi-Head Self-Attention For End-To-End Serving Robot Relocalization", "abstract": "As technology advances in autonomous mobile robots, mobile service robots have been actively used more and more for various purposes. Especially, serving robots have been not surprising products anymore since the COVID-19 pandemic. One of the practical problems in operating serving a robot is that it often fails to estimate its pose on a map that it moves around. Whenever the failure happens, servers should bring the serving robot to its initial location and reboot it manually. In this paper, we focus on end-to-end relocalization of serving robots to address the problem. It is to predict robot pose directly from only the onboard sensor data using neural networks. In particular, we propose a deep neural network architecture for the relocalization based on camera-2D LiDAR sensor fusion. We call the proposed method FusionLoc. In the proposed method, the multi-head selfattention complements different types of information captured by the two sensors to regress the robot pose. Our experiments on a dataset collected by a commercial serving robot demonstrate that FusionLoc can provide better performances than previous endto-end relocalization methods taking only a single image or a 2D LiDAR point cloud as well as a straightforward fusion method concatenating their features."}, "cited_paper_content": {"title": "Atloc: Attention Guided Camera Localization", "abstract": "Deep learning has achieved impressive results in camera localization, but current single-image techniques typically suffer from a lack of robustness, leading to large outliers. To some extent, this has been tackled by sequential (multi-images) or geometry constraint approaches, which can learn to reject dynamic objects and illumination conditions to achieve better performance. In this work, we show that attention can be used to force the network to focus on more geometrically robust objects and features, achieving state-of-the-art performance in common benchmark, even if using only a single image as input. Extensive experimental evidence is provided through public indoor and outdoor datasets. Through visualization of the saliency maps, we demonstrate how the network learns to reject dynamic objects, yielding superior global camera pose regression performance. The source code is avaliable at this https URL."}, "keywords": ["CNN features"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Since Transformer made a great success in the literature on natural language process, its self-attention, one of the essential elements in Transformer, has been utilized in many studies for computer vision applications. In particular, the proposed method FusionLoc in this paper utilizes multi-head self-attention to complement different types of information captured by the camera and 2D LiDAR sensor to regress the robot pose, achieving better performance than previous end-to-end relocalization methods taking only a single image or a 2D LiDAR point cloud as well as a straightforward fusion method concatenating their features #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Since attention has been shown to improve the robustness of camera localization, the proposed method in this paper, FusionLoc, utilizes multi-head self-attention to complement different types of information captured by the camera and 2D LiDAR sensor, achieving better performance than previous end-to-end relocalization methods taking only a single image or a 2D LiDAR point cloud as well as a straightforward fusion method concatenating their features #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Since Transformer made a great success in the literature on natural language process, its self-attention, one of the essential elements in Transformer, has been utilized in many studies for computer vision applications, such as camera localization. In particular, the proposed method in #REFR utilizes multi-head self-attention to complement different types of information captured by camera and 2D LiDAR sensors to regress the robot pose. Moreover, the attention mechanism can force the network to focus on more geometrically robust objects and features, which is crucial for accurate camera localization using CNN features.", "given_citation_intent": "background", "given_keywords": ["CNN features"]}]}
{"citing_id": "2305.01191v1", "cited_id": "2003.08515", "section_title": "B. Differentiable Rendering-Based Camera Pose Estimation", "citation": "Specifically, we first train the PointRend model on synthetic data generated by the SAPIEN #REFR robot simulator and then fine-tune it on real data to enhance its generalization ability.", "text_before_citation": ["L(\u03be cb ) = (min(1, l \u03c0(exp(\u03be cb )T bl l)) \u2212 M ) 2 , (3)", "The above loss function is fully differentiable, we could use the gradient descent method with PyTorch #OTHEREFR Autograd to perform the optimization.", "Mask prediction.", "To obtain the robot arm mask M as the supervision for the differentiable rendering-based optimization, we leverage the off-the-shelf learning-based segmentation network PointRend #OTHEREFR to predict the mask from the RGB image.", "This obviates the need for additional manual annotation efforts, making the entire pipeline fully automated."], "text_after_citation": ["Camera pose initialization.", "There exist various methods for initializing the camera pose, such as traditional marker-based techniques #OTHEREFR , learning-based methods #OTHEREFR , or random initialization of the object pose within the frustum of the camera #OTHEREFR .", "In our work, we adopt the PVNet #OTHEREFR to perform the pose initialization.", "Given an RGB image of the robot arm with the initial joint pose, PVNet estimates the pre-defined 2D keypoint locations (obtained with farthest point sampling on the 3D object model) through pixelwise voting and subsequently solves the transformation using PnP #OTHEREFR .", "By leveraging pixel-wise voting, PVNet can tackle the challenges of occlusion and truncation, which are often encountered in real-world hand-eye calibration scenarios."], "citing_paper_content": {"title": "Easyhec: Accurate And Automatic Hand-Eye Calibration Via Differentiable Rendering And Space Exploration", "abstract": "Hand-eye calibration is a critical task in robotics, as it directly affects the efficacy of critical operations such as manipulation and grasping. Traditional methods for achieving this objective necessitate the careful design of joint poses and the use of specialized calibration markers, while most recent learning-based approaches using solely pose regression are limited in their abilities to diagnose inaccuracies. In this work, we introduce a new approach to hand-eye calibration called EasyHeC, which is markerless, white-box, and offers comprehensive coverage of positioning accuracy across the entire robot configuration space. We introduce two key technologies: differentiable rendering-based camera pose optimization and consistency-based joint space exploration, which enables accurate end-to-end optimization of the calibration process and eliminates the need for the laborious manual design of robot joint poses. Our evaluation demonstrates superior performance in synthetic and real-world datasets, enhancing downstream manipulation tasks by providing precise camera poses for locating and interacting with objects. The code is available at the project page: https://ootts.github.io/easyhec/."}, "cited_paper_content": {"title": "Sapien: A Simulated Part-Based Interactive Environment", "abstract": "Building home assistant robots has long been a pursuit for vision and robotics researchers. To achieve this task, a simulated environment with physically realistic simulation, sufficient articulated objects, and transferability to the real robot is indispensable. Existing environments achieve these requirements for robotics simulation with different levels of simplification and focus. We take one step further in constructing an environment that supports household tasks for training robot learning algorithm. Our work, SAPIEN, is a realistic and physics-rich simulated environment that hosts a large-scale set for articulated objects. Our SAPIEN enables various robotic vision and interaction tasks that require detailed part-level understanding.We evaluate state-of-the-art vision algorithms for part detection and motion attribute recognition as well as demonstrate robotic interaction tasks using heuristic approaches and reinforcement learning algorithms. We hope that our SAPIEN can open a lot of research directions yet to be explored, including learning cognition through interaction, part motion discovery, and construction of robotics-ready simulated game environment."}, "keywords": ["PointRend model", "SAPIEN robot simulator"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To obtain the robot arm mask M as the supervision for the differentiable rendering-based optimization, we leverage the off-the-shelf learning-based segmentation network PointRend, obviating the need for additional manual annotation efforts, making the entire pipeline fully automated #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain the robot arm mask M as the supervision for the differentiable rendering-based optimization, the authors leveraged an off-the-shelf learning-based segmentation network PointRend, obviating the need for additional manual annotation efforts, making the entire pipeline fully automated #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain the robot arm mask M as the supervision for the differentiable rendering-based optimization, the authors leveraged the PointRend model for mask prediction, eliminating the need for additional manual annotation efforts and making the entire pipeline fully automated #REFR. Additionally, the SAPIEN robot simulator could be used to evaluate the proposed approach in a realistic and physics-rich environment.", "given_citation_intent": "method", "given_keywords": ["PointRend model", "SAPIEN robot simulator"]}]}
{"citing_id": "2303.10500v1", "cited_id": "1801.10228", "section_title": "Ii. Blockchain-Based Process Orchestration", "citation": "Multichain is similar to Chorchain, but it is also capable of generating smart contracts for Hyperledger Fabric #REFR .", "text_before_citation": ["Lorikeet #OTHEREFR is a model-driven engineering approach that integrates assets into business processes.", "Lorikeet extends the BPMN 2.0 specification with support for asset registries and also transforms models into Solidity smart contracts.", "The smart contracts handle the orchestration of the process as well as interactions with the tokens.", "Chorchain #OTHEREFR is a tool that takes a BPMN choreography and generates an Ethereum smart contract that can be used to execute the model. ChorChain also includes a dedicated modelling tool.", "The same authors also released two additional but related tools: Multi-Chain #OTHEREFR and FlexChain #OTHEREFR ."], "text_after_citation": ["FlexChain can only produce Solidity smart contracts, but the user can also define a ruleset for each choreography.", "If a condition in the ruleset is met, then an off-chain processor will perform its underlying action.", "Our analysis showed that the process state, as well as the process trace, are easily recoverable from the process manager smart contracts for all the tools above.", "The Baseline protocol #OTHEREFR is a developing open standard that allows enterprises to synchronize complex, multi-party business processes on distributed ledger technologies.", "Business process workflows in Baseline are formed as state machines. The standard includes some essential and optional privacyrelated requirements."], "citing_paper_content": {"title": "Blockchain-Based, Confidentiality-Preserving Orchestration Of Collaborative Workflows", "abstract": "Business process collaboration between independent parties can be challenging, especially if the participants do not have complete trust in each other. Tracking actions and enforcing the activity authorizations of participants via blockchain-hosted smart contracts is an emerging solution to this lack of trust, with most state-of-the-art approaches generating the orchestrating smart contract logic from BPMN models. However, as a significant drawback in comparison to centralized business process orchestration, smart contract state typically leaks potentially sensitive information about the state of the collaboration. We describe a novel approach where the process manager smart contract only stores cryptographic commitments to the state and checks zero-knowledge proofs on update proposals. We cover a representative subset of BPMN, support message passing commitments between participants and provide an opensource end-to-end implementation. Under our approach, no party external to the collaboration can gain trustable knowledge of the current state of a process instance (barring collusion with a participant), even if it has full access to the blockchain history."}, "cited_paper_content": {"title": "Hyperledger Fabric: A Distributed Operating System For Permissioned Blockchains", "abstract": "Hyperledger Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains. Fabric is currently used in more than 400 prototypes and proofs-of-concept of distributed ledger technology, as well as several production systems, across different industries and use cases. Starting from the premise that there are no\"one-size-fits-all\"solutions, Fabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing blockchain platforms for running smart contracts that require code to be written in domain-specific languages or rely on a cryptocurrency. Furthermore, it uses a portable notion of membership for realizing the permissioned model, which may be integrated with industry-standard identity management. To support such flexibility, Fabric takes a novel approach to the design of a permissioned blockchain and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks. This paper describes Fabric, its architecture, the rationale behind various design decisions, its security model and guarantees, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency."}, "keywords": ["smart contracts", "Multichain"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors also developed Lorikeet #REFR, a model-driven engineering approach that extends BPMN 2.0 with support for asset registries and transforms models into Solidity smart contracts, and Chorchain #OTHEREFR, a tool that generates an Ethereum smart contract from a BPMN choreography, which can be used to execute the model and includes a dedicated modelling tool.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To integrate assets into business processes, Lorikeet #OTHEREFR extends the BPMN 2.0 specification with support for asset registries and transforms models into Solidity smart contracts, while Hyperledger Fabric #REFR is a modular and extensible open-source system for deploying and operating permissioned blockchains.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Lorikeet approach extends BPMN 2.0 with support for asset registries and transforms models into Solidity #REFR smart contracts, while ChorChain generates Ethereum smart contracts from BPMN choreography, including a dedicated modelling tool, and the authors of Multi-Chain and FlexChain also contributed to this area.", "given_citation_intent": "background", "given_keywords": ["smart contracts", "Multichain"]}]}
{"citing_id": "2304.05961v1", "cited_id": "1906.04379", "section_title": "A. Hyperspectral Image Classification", "citation": "However, the use of graph neural network for global relationship modeling incurs high computing and memory costs, and slow gradient decline #REFR .", "text_before_citation": ["Deep learning has emerged as a significant breakthrough in the field of machine learning, providing automatic feature learning from data #OTHEREFR .", "HSI classification, in particular, benefits from deep learning models due to their ability to handle complex and nonlinear relationships between input data and output classes #OTHEREFR , leading to improved classification accuracy compared to traditional machine learning methods #OTHEREFR , #OTHEREFR .", "To fully utilize the spectral-spatial features of HSI, joint spectral-spatial HSI feature extraction methods based on deep learning, such as recurrent neural network #OTHEREFR , deep residual network #OTHEREFR , #OTHEREFR , capsule networks #OTHEREFR , and transformer #OTHEREFR , have been proposed.", "While these methods have yielded satisfactory results, challenges remain, such as the inability to capture global relationships between samples.", "To address this issue, researchers have proposed using graph neural network to model the relationship between samples #OTHEREFR ."], "text_after_citation": ["In this paper, we propose using a diffusion model to model the global relationship of samples and verify the effectiveness of this method through extensive comparative experiments."], "citing_paper_content": {"title": "Spectraldiff: Hyperspectral Image Classification With Spectral-Spatial Diffusion Models", "abstract": "Hyperspectral image (HSI) classification is an important topic in the field of remote sensing, and has a wide range of applications in Earth science. HSIs contain hundreds of continuous bands, which are characterized by high dimension and high correlation between adjacent bands. The high dimension and redundancy of HSI data bring great difficulties to HSI classification. In recent years, a large number of HSI feature extraction and classification methods based on deep learning have been proposed. However, their ability to model the global relationships among samples in both spatial and spectral domains is still limited. In order to solve this problem, an HSI classification method with spectral-spatial diffusion models is proposed. The proposed method realizes the reconstruction of spectralspatial distribution of the training samples with the forward and reverse spectral-spatial diffusion process, thus modeling the global spatial-spectral relationship between samples. Then, we use the spectral-spatial denoising network of the reverse process to extract the unsupervised diffusion features. Features extracted by the spectral-spatial diffusion models can achieve cross-sample perception from the reconstructed distribution of the training samples, thus obtaining better classification performance. Experiments on three public HSI datasets show that the proposed method can achieve better performance than the state-of-the-art methods. The source code and the pre-trained spectral-spatial diffusion model will be publicly available at https://github.com/chenning0115/SpectralDiff. Index Terms-Deep neural network, hyperspectral image (HSI) classification, latent representation, feature extraction, diffusion model, deep generative model. I. INTRODUCTION H YPERSPECTRAL imaging is a cutting-edge technology that enables the acquisition of high-resolution spectral information of imaged objects. By integrating spatial and spectral reflectance information, each pixel in a hyperspectral"}, "cited_paper_content": {"title": "Band Attention Convolutional Networks For Hyperspectral Image Classification", "abstract": "Redundancy and noise exist in the bands of hyperspectral images (HSIs). Thus, it is a good property to be able to select suitable parts from hundreds of input bands for HSIs classification methods. In this letter, a band attention module (BAM) is proposed to implement the deep learning based HSIs classification with the capacity of band selection or weighting. The proposed BAM can be seen as a plug-and-play complementary component of the existing classification networks which fully considers the adverse effects caused by the redundancy of the bands when using convolutional neural networks (CNNs) for HSIs classification. Unlike most of deep learning methods used in HSIs, the band attention module which is customized according to the characteristics of hyperspectral images is embedded in the ordinary CNNs for better performance. At the same time, unlike classical band selection or weighting methods, the proposed method achieves the end-to-end training instead of the separated stages. Experiments are carried out on two HSI benchmark datasets. Compared to some classical and advanced deep learning methods, numerical simulations under different evaluation criteria show that the proposed method have good performance. Last but not least, some advanced CNNs are combined with the proposed BAM for better performance."}, "keywords": ["graph neural network"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To fully consider the adverse effects caused by the redundancy of the bands when using convolutional neural networks (CNNs) for HSIs classification, researchers have proposed a band attention module (BAM) as a plug-and-play complementary component of the existing classification networks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To fully consider the adverse effects caused by the redundancy of the bands when using convolutional neural networks (CNNs) for hyperspectral image classification, a band attention module (BAM) has been proposed as a plug-and-play complementary component of the existing classification networks, which achieves end-to-end training and better performance than classical band selection or weighting methods #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model the relationship between samples in hyperspectral image classification, researchers have proposed using graph neural network #REFR.", "given_citation_intent": "background", "given_keywords": ["graph neural network"]}]}
{"citing_id": "2305.00671v1", "cited_id": "1908.07678", "section_title": "I. Introduction", "citation": "ANN #REFR uses axial attention to capture remote subsurface context, etc. (3) Transformer-based decoder.", "text_before_citation": ["It mainly uses convolution to model contextual relationships, expand the perceptive field, extract local detail features, fuse multi-scale information, etc.", "For example, PSPNet #OTHEREFR utilizes Pyramid pooling techniques to model contextual features and expand the perceptive field.", "Some works #OTHEREFR , #OTHEREFR , #OTHEREFR use top-down connectivity to fuse multi-scale spatial and semantic information to enhance pixel representation and multi-scale object recognition. (2) Attention-based decoder.", "It means that the decoder uses the attention technique to obtain the similarity between pixels, and provides a focus on the region of interest.", "OCRNet #OTHEREFR uses attention to compute the similarity of pixel features to individual class features, and CCNet #OTHEREFR uses Criss-Cross Attention to capture longrange dependencies and model rich contextual features."], "text_after_citation": ["Some researchers have incorporated the transformer into decoder designs as a result of its ability to model global contextual features, capture long-range dependencies, and provide flexible prototype representations.", "For example, Segmenter #OTHEREFR uses transformer to build global contextual relations and embeds the prototypes of categories into transformer to learn together with feature maps.", "MaskFormer #OTHEREFR uses transformer's cross-attention to let the mask token learn the feature map's category representation. (4) MLP-based decoder.", "Recently, the MLP-based decoder has received more and more attention, mainly because of two aspects, the lightweight and efficient property of the MLP architecture, and the development of the transformer backbone, which offers global extraction capabilities for the encoder.", "SegFormer #OTHEREFR then proposed a lightweight multi-scale fusion MLP architecture that uses the transformer's backbone to arXiv:2305.00671v1 [cs.CV] 1 May 2023 achieve state-of-the-art performance on various datasets."], "citing_paper_content": {"title": "Prseg: A Lightweight Patch Rotate Mlp Decoder For Semantic Segmentation", "abstract": "The lightweight MLP-based decoder has become increasingly promising for semantic segmentation. However, the channel-wise MLP cannot expand the receptive fields, lacking the context modeling capacity, which is critical to semantic segmentation. In this paper, we propose a parametric-free patch rotate operation to reorganize the pixels spatially. It first divides the feature map into multiple groups and then rotates the patches within each group. Based on the proposed patch rotate operation, we design a novel segmentation network, named PRSeg, which includes an off-the-shelf backbone and a lightweight Patch Rotate MLP decoder containing multiple Dynamic Patch Rotate Blocks (DPR-Blocks). In each DPR-Block, the fully connected layer is performed following a Patch Rotate Module (PRM) to exchange spatial information between pixels. Specifically, in PRM, the feature map is first split into the reserved part and rotated part along the channel dimension according to the predicted probability of the Dynamic Channel Selection Module (DCSM), and our proposed patch rotate operation is only performed on the rotated part. Extensive experiments on ADE20K, Cityscapes and COCO-Stuff 10K datasets prove the effectiveness of our approach. We expect that our PRSeg can promote the development of MLPbased decoder in semantic segmentation."}, "cited_paper_content": {"title": "Asymmetric Non-Local Neural Networks For Semantic Segmentation", "abstract": "The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: this https URL."}, "keywords": ["(3) Transformer-based decoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Some works use top-down connectivity to fuse multi-scale spatial and semantic information to enhance pixel representation and multi-scale object recognition. For example, #REFR presents Asymmetric Non-local Neural Network to semantic segmentation, which leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce the computation and memory consumption of non-local module in semantic segmentation, Asymmetric Non-local Neural Network proposed by the authors leverages a pyramid sampling module into the non-local block and fuses the features of different levels under a sufficient consideration of long range dependencies, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of lacking context modeling capacity in MLP-based decoders, some recent works have proposed attention-based decoders #REFR, while others have utilized pyramid pooling techniques or top-down connectivity to model contextual features and fuse multi-scale information #OTHEREFR.", "given_citation_intent": "method", "given_keywords": ["(3) Transformer-based decoder"]}]}
{"citing_id": "2304.00320v1", "cited_id": "1804.06872", "section_title": "Introduction", "citation": "In our work, the random label noises can be either (1) drawn from probability distributions before training steps (but re-sampled by mini-batch sampling of SGD) or (2) realized by the random variables per training iteration #REFR .", "text_before_citation": ["In this work, we aim at investigating the influence of random label noises to the implicit regularization under mini-batch sampling of SGD.", "To simplify our research, we assume the training dataset as a set of vectors D = {x 1 , x 2 , x 3 , . . .", ", x N }.The label\u1ef9 i for every vector x i \u2208 D is the noisy response of the true neural network f * (x) such that y i = y i + \u03b5 i , y i = f * (x i ), and E[\u03b5 i ] = 0, var [", "EQUATION", "where the label noise \u03b5 i is assumed to be an independent zero-mean random variable."], "text_after_citation": ["Thus learning is to estimate \u03b8 in f (x, \u03b8) for approximating f * (x),"], "citing_paper_content": {"title": "Stochastic Gradient Descent With Random Label Noises: Doubly Stochastic Models And Inference Stabilizer", "abstract": "Random label noises (or observational noises) widely exist in practical machine learning settings. While previous studies primarily focus on the affects of label noises to the performance of learning, our work intends to investigate the implicit regularization effects of the label noises, under mini-batch sampling settings of stochastic gradient descent (SGD), with assumptions that label noises are unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noises, where we model the dynamics of SGD as a stochastic differentiable equation (SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients as many other works on SGD [1, 2], our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noises, as an implicit regularizer. Our theoretical analysis finds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters (namely inference stability). Though similar phenomenon have been investigated in [3], our work"}, "cited_paper_content": {"title": "Co-Teaching: Robust Training Of Deep Neural Networks With Extremely Noisy Labels", "abstract": "Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models."}, "keywords": ["random label noises"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To combat with noisy labels, the authors propose a new deep learning paradigm called Co-teaching, where two deep neural networks are trained simultaneously and let them teach each other given every mini-batch. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of noisy labels in deep learning, the authors propose a new paradigm called Co-teaching, which trains two deep neural networks simultaneously and lets them teach each other given every mini-batch, resulting in superior robustness of trained deep models compared to state-of-the-art methods (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To combat the influence of random label noises, the authors propose a new deep learning paradigm called Co-teaching, which trains two deep neural networks simultaneously and lets them teach each other given every mini-batch, resulting in superior robustness of trained deep models compared to the state-of-the-art methods on noisy versions of MNIST, CIFAR-10 and CIFAR-100 datasets. #REFR", "given_citation_intent": "method", "given_keywords": ["random label noises"]}]}
{"citing_id": "2305.00320v1", "cited_id": "1705.09406", "section_title": "Multimodal Fusion", "citation": "However, those attention models do not explicitly work at conserving the modalityspecific knowledge, missing the point raised by #REFR .", "text_before_citation": ["The module refactors the channels of each modality regarding how the intra-and inter-modality channels correlate. Based on the MMTM concept and inspired by H. #OTHEREFR", "(2020) that used the split operation to improve the dynamic channel selection, Su et al. (2020) presented the MSAF approach.", "The dynamic channel refactoring in such multimodal models may allow for fine-grained feature selection and limit corruption impact. Unlike previous approaches, modality attention #OTHEREFR , later updated by #OTHEREFR", "(2020) , provides soft attention weights for each modality to balance modality importance in the final embedding based on their discriminating capabilities.", "Again, such attention sounds to be a great approach to tackling punctually corrupted data."], "text_after_citation": ["Some transformers architectures tackle this aspect, conserving modality-specific knowledge through modality-specific streams and selfattention, and modality-shared knowledge thanks to modality-shared streams and cross-attention #OTHEREFR .", "However, transformer architectures are known to be complex and heavy Han et al.", "(2020) , which do not align with video-surveillance challenges, requiring close to real-time algorithms. Multimodal person ReID.", "Most approaches for person ReID #OTHEREFR focus on the unimodal (RGB) (H.", "#OTHEREFR and cross-modal #OTHEREFR settings. Few only focused on combining multimodal information. For example, J. #OTHEREFR used the contour information. used pose information."], "citing_paper_content": {"title": "Fusion For Visual-Infrared Person Reid In Real-World Surveillance Using Corrupted Multimodal Data -First Version Submitted To Ijcv", "abstract": "Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Despite their practical relevance, deep learning (DL) models for multimodal V-I ReID remain far less investigated than for single and cross-modal V to I settings. Moreover, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID-named Multimodal Middle Stream Fusion (MMSF)-that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing for dynamic balancing of the importance of each modality. The literature typically reports ReID performance using clean datasets, but more recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios, using data with realistic corruptions. However, these protocols are limited to unimodal V settings. For realistic evaluation of multimodal (and cross-modal) V-I person ReID models, we propose new challenging corrupted datasets for scenarios where V and I cameras are co-located (CL) and not co-located (NCL). Finally, the benefits of our Masking and Local Multimodal Data Augmentation (ML-MDA) strategy are explored to improve the robustness of ReID models to multimodal corruption. Our experiments on clean and corrupted versions of the SYSU-MM01, RegDB, and ThermalWORLD datasets indicate the multimodal V-I ReID models that are more likely to perform well in realworld operational conditions. In particular, our ML-MDA is an important strategy for a V-I person ReID system to sustain high accuracy and robustness when processing corrupted multimodal images. The multimodal ReID models provide the best accuracy and complexity trade-off under both CL and NCL settings and compared to state-of-art unimodal ReID systems, except for the Thermal-WORLD dataset due to its low-quality I. Our MMSF model outperforms every method under CL and NCL camera scenarios. GitHub code: https://github.com/art2611/MREiD-UCD-CCD.git."}, "cited_paper_content": {"title": "Multimodal Machine Learning: A Survey And Taxonomy", "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research."}, "keywords": ["attention models", "modalityspecific knowledge"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The dynamic channel refactoring in such multimodal models may allow for fine-grained feature selection and limit corruption impact. Unlike previous approaches, modality attention (MMT) #REFR provides soft attention weights for each modality to balance modality importance in the final embedding based on their discriminating capabilities.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Multimodal machine learning aims to build models that can process and relate information from multiple modalities, which is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Modality attention provides soft attention weights for each modality to balance modality importance in the final embedding based on their discriminating capabilities, making it a great approach to tackling punctually corrupted data (#REFR). Furthermore, multimodal models with dynamic channel refactoring may allow for fine-grained feature selection and limit corruption impact, as presented in the MSAF approach by Su et al. (2020), which is inspired by the MMTM concept and H. #OTHEREFR's (2020) use of the split operation to improve dynamic channel selection and modality-specific knowledge.", "given_citation_intent": "background", "given_keywords": ["attention models", "modalityspecific knowledge"]}]}
{"citing_id": "2304.04541v1", "cited_id": "1809.11096", "section_title": "Inference.", "citation": "Furthermore, we find out the recommendation performance can be improved by averaging the predictions yielded from different random seeds #REFR .", "text_before_citation": ["4) 7: end for 8: Predict h 0 via (h 1 , 1) (Eq.", "16) 9: Obtain probability distribution \u2190\u02c6( |h 0 ) (Eq.", "8) 10: return However, a major problem with Algorithm 2 is the lengthened diffusion steps.", "As we have to start from h until we reach h 0 , the time complexity of the inference procedure is increased by times.", "To address this issue, we notice that is trained to directly predict h 0 based on any h (1\u2264 \u2264 ), so it is able to directly predict h 0 from h without the need of the intermediate diffusion steps."], "text_after_citation": ["This is probably because different recommendation Obtain probability distribution \u2190\u02c6( |h 0 ) (Eq. 8)", "8: end for 9: return 1", "| | | | =1", "Algorithm 3 greatly saves computational cost by directly predicting h 0 from h .", "As we normally set the size of random seeds list as a small number (i.e., 10 by default), the additional computational cost incurred by using different random seeds can be minimal."], "citing_paper_content": {"title": "Sequential Recommendation With Diffusion Models", "abstract": "Generative models, such as Variational Auto-Encoder (VAE) and Generative Adversarial Network (GAN), have been successfully applied in sequential recommendation. These methods require sampling from probability distributions and adopt auxiliary loss functions to optimize the model, which can capture the uncertainty of user behaviors and alleviate exposure bias. However, existing generative models still suffer from the posterior collapse problem or the model collapse problem, thus limiting their applications in sequential recommendation. To tackle the challenges mentioned above, we leverage a new paradigm of the generative models, i.e., diffusion models, and present sequential recommendation with diffusion models (DiffRec), which can avoid the issues of VAE-and GAN-based models and show better performance. While diffusion models are originally proposed to process continuous image data, we design an additional transition in the forward process together with a transition in the reverse process to enable the processing of the discrete recommendation data. We also design a different noising strategy that only noises the target item instead of the whole sequence, which is more suitable for sequential recommendation. Based on the modified diffusion process, we derive the objective function of our framework using a simplification technique and design a denoise sequential recommender to fulfill the objective function. As the lengthened diffusion steps substantially increase the time complexity, we propose an efficient training strategy and an efficient inference strategy to reduce training and inference cost and improve recommendation diversity. Extensive experiment results on three public benchmark datasets verify the effectiveness of our approach and show that DiffRec outperforms the state-of-theart sequential recommendation models. 1 1 Our code will be available upon acceptance."}, "cited_paper_content": {"title": "Large Scale Gan Training For High Fidelity Natural Image Synthesis", "abstract": "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6."}, "keywords": ["recommendation performance", "different random seeds"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of lengthened diffusion steps in Algorithm 2, the authors propose a method that directly predicts h0 from h without the need for intermediate diffusion steps (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Diffusion models have been successfully applied in various fields, such as image processing, and have shown better performance than existing generative models such as VAE and GAN, as reported in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "DiffRec outperforms the state-of-the-art sequential recommendation models in terms of recommendation performance, as demonstrated in extensive experiments on three public benchmark datasets #REFR. Additionally, the use of different random seeds in the experiments further confirms the superiority of DiffRec.", "given_citation_intent": "background", "given_keywords": ["recommendation performance", "different random seeds"]}]}
{"citing_id": "2304.01621v1", "cited_id": "1508.06034", "section_title": "Wikipedia", "citation": "We infer that it is due to the impact of the computation method of ROUGE score as it is an n-gram-based metric #REFR .", "text_before_citation": ["We infer that it is due to the impact of the simplification module in KIS-mBART.", "Table 2 presents the results of all models on SPEK-TRUM.", "We find a similar pattern that SIMCSUM outperforms all baselines.", "We also compute the statistical significance of these results with the same procedure.", "The SPEKTRUM results are on the lower side compared to the WIKIPEDIA results due to zero-shot adaptability, especially for R\uf732."], "text_after_citation": ["The SPEKTRUM summaries have higher FRE scores compared to WIKIPEDIA.", "Interestingly, we find that all baselines perform lower than the GOLD summaries.", "However, the SIMCSUM score is similar to the GOLD summaries.", "Comparing the performance of mBART and KIS-mBART, KIS-mBART performs slightly lower than mBART for all scores except R\uf731because only WIKIPEDIA is used for fine-tuning of both models in KIS-mBART. Human Evaluation.", "We compare the SIMCSUM and mBART outputs for analyzing linguistic qualities because SIMCSUM's architecture is based on mBART."], "citing_paper_content": {"title": "Simcsum: Joint Learning Of Simplification And Cross-Lingual Summarization For Cross-Lingual Science Journalism", "abstract": "Cross-lingual science journalism generates popular science stories of scientific articles different from the source language for a nonexpert audience. Hence, a cross-lingual popular summary must contain the salient content of the input document, and the content should be coherent, comprehensible, and in a local language for the targeted audience. We improve these aspects of cross-lingual summary generation by joint training of two high-level NLP tasks, simplification and cross-lingual summarization. The former task reduces linguistic complexity, and the latter focuses on cross-lingual abstractive summarization. We propose a novel multi-task architecture-SIMC-SUM consisting of one shared encoder and two parallel decoders jointly learning simplification and cross-lingual summarization. We empirically investigate the performance of SIMC-SUM by comparing it with several strong baselines over several evaluation metrics and by human evaluation. Overall, SIMCSUM demonstrates statistically significant improvements over the state-of-the-art on two non-synthetic cross-lingual scientific datasets. Furthermore, we conduct an in-depth investigation into the linguistic properties of generated summaries and an error analysis."}, "cited_paper_content": {"title": "Better Summarization Evaluation With Word Embeddings For Rouge", "abstract": "ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefficients."}, "keywords": ["n-gram-based metric"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We find a similar pattern that SIMCSUM outperforms all baselines, including those that use ROUGE for evaluation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors utilized word embeddings to compute the semantic similarity of words used in summaries, which improved the evaluation of abstractive summarization compared to the lexical overlap-based ROUGE measure. #REFR (Better Summarization Evaluation With Word Embeddings For Rouge)", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in SIMC-SUM outperforms all baselines on the n-gram-based metric, as shown in Table 2, demonstrating the effectiveness of using word embeddings to compute semantic similarity instead of lexical overlaps #REFR.", "given_citation_intent": "method", "given_keywords": ["n-gram-based metric"]}]}
{"citing_id": "2304.10829v1", "cited_id": "1505.04597", "section_title": "Introduction", "citation": "In the field of remote sensing image segmentation, UNet has a very important position #REFR .", "text_before_citation": ["Many elements in remote sensing images have some connections, such as rivers and buildings often appearing together, and roads and vehicles often appearing together.", "Deep learning #OTHEREFR has shown amazing results in many fields."], "text_after_citation": ["UNet was mainly used for medical image segmentation when it was proposed.", "After several years of development, it has also been widely used in the field of remote sensing, and has many different variants, which have been applied to different types of tasks.", "The dataset I am using is the FloodNet Dataset #OTHEREFR , which is a multi category segmented remote sensing image dataset.", "The data is collected with a small UAS platform, DJI Mavic Pro quadcopters, after Hurricane Harvey. The whole dataset has 2343 images.", "The classic Unet network is divided into two parts: encoder and decoder, and the encoding and decoding processes are connected through residual connections to avoid information loss during the encoding process."], "citing_paper_content": {"title": "Deep Attention Unet: A Network Model With Global Feature Perception Ability", "abstract": "Remote sensing image segmentation is a specific task of remote sensing image interpretation. A good remote sensing image segmentation algorithm can provide guidance for environmental protection, agricultural production, and urban construction. This paper proposes a new type of UNet image segmentation algorithm based on channel self attention mechanism and residual connection called. In my experiment, the new network model improved mIOU by 2.48% compared to traditional UNet on the FoodNet dataset. The image segmentation algorithm proposed in this article enhances the internal connections between different items in the image, thus achieving better segmentation results for remote sensing images with occlusion."}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["remote sensing image", "segmentation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Many elements in remote sensing images have some connections, such as rivers and buildings often appearing together, and roads and vehicles often appearing together. Deep learning has shown amazing results in many fields (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Many elements in remote sensing images have some connections, such as rivers and buildings often appearing together, and roads and vehicles often appearing together. Deep learning has shown amazing results in many fields, and U-Net, a convolutional network for biomedical image segmentation, has been successfully trained end-to-end from very few images and outperforms the prior best method on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Deep learning has shown amazing results in many fields, including remote sensing image segmentation where the U-Net algorithm has been widely used due to its ability to capture context and enable precise localization #REFR.", "given_citation_intent": "background", "given_keywords": ["remote sensing image", "segmentation"]}]}
{"citing_id": "2304.03228v1", "cited_id": "1709.04696", "section_title": "Federated Machine Learning Module", "citation": "Attention layer: The attention layer is a key component of the transformer model used in natural language processing tasks #REFR , Parikh et al. [2016] .", "text_before_citation": ["Both the encoder (left-side) and decoder (right-side) are shown in Figure 2 .", "Both Encoder and Decoder are composed of layers and sub-layers that can be stacked on top of each other multiple times, which N x describes in Figure 2 .", "The first is a multi-head self-attention mechanism, and the second is a simple, position-wise, fully connected feed-forward network.", "The inputs and outputs are first embedded into an n-dimensional space.", "foreach client k \u2208 S t in parallel do 7 W k t+1 \u2190 ClientU pdate(k, W t , N l ) 8 W t+1 \u2190 k k=1 n k n W k t+1 end W t \u2190 (W t\u22121 + (W t \u2212 W t\u22121 )/t) end return M (W t )"], "text_after_citation": ["The attention mechanism enables the model to selectively focus on the relevant parts of the input sequence, rather than treating all inputs equally.", "The attention layer consists of three sets of parameters -query (Q), key (K), and value (V) -that are learned during the training process.", "In this paper, by applying the self-attention mechanism, we aim at capturing the long dependencies in the input sentence following these steps: \u2022 Obtain each word's weight by calculating the similitude between the query matrix and every key matrix (Q, K, V ).", "The dot product, concatenation, and perceptron are typically applied as similarity functions.", "\u2022 Normalize the similitude score of the preceding step and compute the weights with the SoftMax function."], "citing_paper_content": {"title": "Fedbot: Enhancing Privacy In Chatbots With Federated Learning", "abstract": "Chatbots are mainly data-driven and usually based on utterances that might be sensitive. However, training deep learning models on shared data can violate user privacy. Such issues have commonly existed in chatbots since their inception. In the literature, there have been many approaches to deal with privacy, such as differential privacy and secure multi-party computation, but most of them need to have access to users' data. In this context, Federated Learning (FL) aims to protect data privacy through distributed learning methods that keep the data in its location. This paper presents Fedbot, a proof-of-concept (POC) privacy-preserving chatbot that leverages large-scale customer support data. The POC combines Deep Bidirectional Transformer models and federated learning algorithms to protect customer data privacy during collaborative model training. The results of the proof-of-concept showcase the potential for privacy-preserving chatbots to transform the customer support industry by delivering personalized and efficient customer service that meets data privacy regulations and legal requirements. Furthermore, the system is specifically designed to improve its performance and accuracy over time by leveraging its ability to learn from previous interactions."}, "cited_paper_content": {"title": "Disan: Directional Self-Attention Network For Rnn/Cnn-Free Language Understanding", "abstract": "Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence(s) is directional and multi-dimensional (i.e., feature-wise). A light-weight neural net, \"Directional Self-Attention Network (DiSAN)\", is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02% on the Stanford Natural Language Inference (SNLI) dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets."}, "keywords": ["attention layer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Both the encoder (left-side) and decoder (right-side) are shown in Figure 2. Both Encoder and Decoder are composed of layers and sub-layers that can be stacked on top of each other multiple times, which N x describes in Figure 2. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise, fully connected feed-forward network. The inputs and outputs are first embedded into an n-dimensional space (DiSAN). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The encoder in Fedbot uses a multi-head self-attention mechanism, which is similar to the directional self-attention network proposed in DiSAN #REFR, to capture long-term dependencies between input elements.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The multi-head self-attention mechanism used in the encoder of the proposed model is similar to the one proposed in DiSAN, a directional self-attention network that achieves state-of-the-art results on several NLP tasks. #REFR", "given_citation_intent": "method", "given_keywords": ["attention layer"]}]}
{"citing_id": "2304.11335v1", "cited_id": "1703.06868", "section_title": "Image Style Transfer", "citation": "As illustrated in Figure 5 , AdaIN #REFR transfers the style patterns but losses important content details (1st, 2nd rows).", "text_before_citation": ["Quantitative comparison.", "We randomly collect 17, 124 content images from ImageNet #OTHEREFR and 17, 124 style images from WikiArt #OTHEREFR which are separated from the training set to generate 17, 124 stylization results.", "Similar to Stytr 2 [8], we use the mean euclidean distance and the mean instance statistics difference mentioned in 3.4 as the metrics for content preservation and stylization degree.", "As presented in Table 1 , compared with existing methods, UniST achieves the best performance in both content and style differences.", "Qualitative comparison."], "text_after_citation": ["SANet #OTHEREFR fails to align the distribution of the style patterns, leading to the distorted object boundaries (1st, 3rd, 5th rows) and inconsistent content background structures (2nd, 4th rows).", "The stylization of Linear #OTHEREFR is not satisfactory enough, resulting relatively light migration effects such as pink in the 5th row.", "As with the linear transformation, MCCNet #OTHEREFR learns the correlation of each channel through the transformed self-attention, slightly improving the transfer effect, but there are serious overflow problems around the object boundaries (1st, 3rd, 4th rows).", "MAST #OTHEREFR distorts the content background structure with excessive style transfer (3rd, 4th rows).", "Based on WCT #OTHEREFR in our setting, Artflow #OTHEREFR leads to conspicuous vertical artifacts at the edges of the generated results(3rd, 5th rows)."], "citing_paper_content": {"title": "Two Birds, One Stone: A Unified Framework For Joint Learning Of Image And Video Style Transfers", "abstract": "Current arbitrary style transfer models are limited to either image or video domains. In order to achieve satisfying image and video style transfers, two different models are inevitably required with separate training processes on image and video domains, respectively. In this paper, we show that this can be precluded by introducing UniST, a Unified Style Transfer framework for both images and videos. At the core of UniST is a domain interaction transformer (DIT), which first explores context information within the specific domain and then interacts contextualized domain information for joint learning. In particular, DIT enables exploration of temporal information from videos for the image style transfer task and meanwhile allows rich appearance texture from images for video style transfer, thus leading to mutual benefits. Considering heavy computation of traditional multihead self-attention, we present a simple yet effective axial multi-head self-attention (AMSA) for DIT , which improves computational efficiency while maintains style transfer performance. To verify the effectiveness of UniST, we conduct extensive experiments on both image and video style transfer tasks and show that UniST performs favorably against state-of-the-art approaches on both tasks. Our code and results will be released."}, "cited_paper_content": {"title": "Arbitrary Style Transfer In Real-Time With Adaptive Instance Normalization", "abstract": "Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network."}, "keywords": ["style patterns"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As presented in Table 1, compared with existing methods, UniST achieves the best performance in both content and style differences, utilizing a unified framework for joint learning of image and video style transfers #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve arbitrary style transfer in real-time, recent works have proposed fast approximations with feed-forward neural networks, but these models are usually restricted to a fixed set of styles; however, the authors of UniST have presented a novel adaptive instance normalization (AdaIN) layer that enables arbitrary style transfer in real-time without the restriction to a pre-defined set of styles #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of adaptive instance normalization (AdaIN) layer in #REFR enables arbitrary style transfer in real-time, which overcomes the limitation of fixed style patterns in existing fast approximations, as demonstrated in the manuscript.", "given_citation_intent": "background", "given_keywords": ["style patterns"]}]}
{"citing_id": "2304.13915v1", "cited_id": "quant-ph/0608142", "section_title": "Faster Estimation Of Stabilizer Fidelity", "citation": "We note that Aaronson studied PAC learning of quantum states in the so-called realizable setting #REFR .", "text_before_citation": ["Finally, we remark that the problem we solve is similar in spirit to the agnostic probably approximately correct (PAC) learning framework #OTHEREFR .", "In the agnostic PAC model, a learner is given labeled training data {(x 1 , y 1 ), . . .", ", (x m , y m )} from some unknown distribution D, as well as some concept class C to choose a hypothesis from.", "The goal of the learner is to find a hypothesis function h \u2208 C that approximates the best fit for the training data, even though no function in C will necessarily fit the training data perfectly.", "In an analogous fashion, our algorithm finds a stabilizer state |\u03c6 that approximates the best fit for |\u03c8 over the set of stabilizer states, which need not contain |\u03c8 ."], "text_after_citation": ["However, agnostic PAC learning of quantum states has not yet appeared in the literature."], "citing_paper_content": {"title": "Improved Stabilizer Estimation Via Bell Difference Sampling", "abstract": "We study the complexity of learning quantum states in various models with respect to the stabilizer formalism and obtain the following results: \u2022 We prove that \u2126(n) T-gates are necessary for any Clifford+T circuit to prepare computationally pseudorandom quantum states, an exponential improvement over the previously known bound. This bound is asymptotically tight if linear-time quantum-secure pseudorandom functions exist. \u2022 Given an n-qubit pure quantum state |\u03c8 that has fidelity at least \u03c4 with some stabilizer state, we give an algorithm that outputs a succinct description of a stabilizer state that witnesses fidelity at least \u03c4 \u2212 \u03b5. The algorithm uses O(n/(\u03b5 2 \u03c4 4)) samples and exp O(n/\u03c4 4) /\u03b5 2 time. In the regime of \u03c4 constant, this algorithm estimates stabilizer fidelity substantially faster than the na\u00efve exp(O(n 2))-time brute-force algorithm over all stabilizer states. \u2022 We improve the soundness analysis of the stabilizer state property testing algorithm due to Gross, Nezami, and Walter [Comms. Math. Phys. 385 (2021)]. As an application, we exhibit a tolerant property testing algorithm for stabilizer states. The underlying algorithmic primitive in all of our results is Bell difference sampling. To prove our results, we establish and/or strengthen connections between Bell difference sampling, symplectic Fourier analysis, and graph theory."}, "cited_paper_content": {"title": "The Learnability Of Quantum States", "abstract": "A method of cladding a steel roll, such as a felt roll with a non-corrosive shell, ensures that the shell is tightly fitted to the core and is free to expand and contract on the core to avoid distortion problems. The method comprises the steps of preparing two stainless steel shells each having a length equivalent to the circumference of the core plus a predetermined extra, wrapping the shells about the core, side by side, leaving a predetermined circumferential gap between the shells, heating and clamping the shells tightly around the core until the longitudinal joints each attain a predetermined longitudinal gap, welding the longitudinal gaps, and circumferential gap between the shells while the shells are hot, ensuring the sheets are welded together and not welded to the core."}, "keywords": ["quantum states"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To prove our results, we establish and/or strengthen connections between Bell difference sampling, symplectic Fourier analysis, and graph theory, and cite the reference paper on the learnability of quantum states #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide additional context about the problem of learning quantum states, we cite the reference paper \"The Learnability Of Quantum States\" #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Quantum states play a central role in our study, and to learn more about their learnability, the authors cite the reference paper titled \"The Learnability Of Quantum States\" #REFR.", "given_citation_intent": "background", "given_keywords": ["quantum states"]}]}
{"citing_id": "2303.14360v1", "cited_id": "1604.01685", "section_title": "Datasets And Implementation Details", "citation": "Cityscapes #REFR is an autonomous driving dataset that contains urban street scenes recorded from 50 different cities with precise pixel-wise annotations of 19 semantic categories.", "text_before_citation": [], "text_after_citation": ["DensePASS #OTHEREFR is a panoramic dataset and contains 2,000 images for training and 100 precise annotated images for testing.", "WildPASS #OTHEREFR is a panoramic dataset designed to capture diverse scenes from all around the globe and contains 2500 panoramas. Evaluation.", "We take the mean Intersection-over-Union (mIoU) as the evaluation metric in both the source and target domains.", "Our framework is evaluated on the DenseP-ASS / WildPASS validation set via test at a single scale, and the resolution is 400 \u00d7 2048. Implementation details.", "Our framework is implemented with Pytorch and trained on multiple NVIDIA GPUs."], "citing_paper_content": {"title": "Both Style And Distortion Matter: Dual-Path Unsupervised Domain Adaptation For Panoramic Semantic Segmentation", "abstract": "The ability of scene understanding has sparked active research for panoramic image semantic segmentation. However, the performance is hampered by distortion of the equirectangular projection (ERP) and a lack of pixel-wise annotations. For this reason, some works treat the ERP and pinhole images equally and transfer knowledge from the pinhole to ERP images via unsupervised domain adaptation (UDA). However, they fail to handle the domain gaps caused by: 1) the inherent differences between camera sensors and captured scenes; 2) the distinct image formats (e.g., ERP and pinhole images). In this paper, we propose a novel yet flexible dual-path UDA framework, DPPASS, taking ERP and tangent projection (TP) images as inputs. To reduce the domain gaps, we propose cross-projection and intra-projection training. The cross-projection training includes tangent-wise feature contrastive training and prediction consistency training. That is, the former formulates the features with the same projection locations as positive examples and vice versa, for the models' awareness of distortion, while the latter ensures the consistency of cross-model predictions between the ERP and TP. Moreover, adversarial intra-projection training is proposed to reduce the inherent gap, between the features of the pinhole images and those of the ERP and TP images, respectively. Importantly, the TP path can be freely removed after training, leading to no additional inference cost. Extensive experiments on two benchmarks show that our DPPASS achieves +1.06% mIoU increment than the state-of-the-art approaches. https: //vlis2022.github.io/cvpr23/DPPASS"}, "cited_paper_content": {"title": "The Cityscapes Dataset For Semantic Urban Scene Understanding", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."}, "keywords": ["precise pixel-wise annotations", "autonomous driving dataset"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the lack of pixel-wise annotations in panoramic image semantic segmentation, some works have treated equirectangular projection (ERP) and pinhole images equally and transferred knowledge from the pinhole to ERP images via unsupervised domain adaptation (UDA) (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the lack of pixel-wise annotations for semantic urban scene understanding, Cityscapes dataset was introduced, which provides a benchmark suite and large-scale dataset for pixel-level and instance-level semantic labeling, and exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the lack of precise pixel-wise annotations in urban scene understanding, the authors of this manuscript could benefit from using the Cityscapes dataset, which provides high-quality annotations and is a benchmark autonomous driving dataset. #REFR", "given_citation_intent": "background", "given_keywords": ["precise pixel-wise annotations", "autonomous driving dataset"]}]}
{"citing_id": "2304.07159v1", "cited_id": "2003.12039", "section_title": "Arxiv:2304.07159V1 [Cs.Cv] 14 Apr 2023", "citation": "Recent supervised learning work RAFT #REFR demonstrates the strength of recurrent blocks in optical flow estimation.", "text_before_citation": ["In continuous motion of objects, the loss of pixels information due to occlusion and blurring, which may not be a problem in supervised learning due to the presence of ground truth from God's view, is untraceable and tricky in unsupervised learning.", "The predictive coding structure based on temporal dynamics has at least the following potential advantages for alleviating the problem.", "First, Based on the motion temporal smoothness, temporal dynamic networks can propagate the previous motion prior through hidden states to provide a more reliable reference for untraceable pixels.", "Second, the motion estimation of the occluded object is practically equivalent to the prediction of the location of the occluded object in the latter frame, which has a strong connection with predictive coding mechanism as one of the motivations for introducing temporal dynamics.", "Accordingly, this work constructs an CNN-RNN based model that understands optical flow estimation in dynamic environments."], "text_after_citation": ["Instead of recurrent refinement of a single frame, we introduce both spatial and temporal recurrences into the model, constructing a spatial-temporal dual recurrent block with predictive coding arrangement.", "The final resulting network has a small number of parameters yet stably converges in the sequence of arbitrary length and demonstrates higher performance in ablation study compared to other models. We trained the model in the temporal causal sequence.", "To make the model correctly grasp the object occlusion, brightness variation, color variation, and various content blurring caused by the movement in the dynamic environment, three training enhancers based on self-supervised learning are proposed, including Dynamic Occlusion Enhancer (DOE), Content Variation Enhancer (CVE), and Spatial Variation Enhancer (SVE).", "In which DOE extracts sub-object blocks from the original data distribution and simulates the random natural motion of objects in multi-frame images, allowing the model to understand object occlusion patterns over prolonged temporal environments.", "Furthermore, we propose a mixed supervision strategy by combining the unsupervised loss and self-supervised loss in the DOE, which simultaneously provides reliable supervision on the occluded region and the occlusion per se, facilitating a better generalization of the network to occlusion scenes."], "citing_paper_content": {"title": "Unsupervised Learning Optical Flow In Multi-Frame Dynamic Environment Using Temporal Dynamic Modeling", "abstract": "For visual estimation of optical flow, a crucial function for many vision tasks, unsupervised learning, using the supervision of view synthesis, has emerged as a promising alternative to supervised methods, since ground-truth flow is not readily available in many cases. However, unsupervised learning is likely to be unstable when pixel tracking is lost due to occlusion and motion blur, or the pixel matching is impaired due to variation in image content and spatial structure over time. In natural environments, dynamic occlusion or object variation is a relatively slow temporal process spanning several frames. We, therefore, explore the optical flow estimation from multiple-frame sequences of dynamic scenes, whereas most of the existing unsupervised approaches are based on temporal static models. We handle the unsupervised optical flow estimation with a temporal dynamic model by introducing a spatial-temporal dual recurrent block based on the predictive coding structure, which feeds the previous high-level motion prior to the current optical flow estimator. Assuming temporal smoothness of optical flow, we use motion priors of the adjacent frames to provide more reliable supervision of the occluded regions. To grasp the essence of challenging scenes, we simulate various scenarios across long sequences, including dynamic occlusion, content variation, and spatial variation, and adopt self-supervised distillation to make the model understand the object's motion patterns in a prolonged dynamic environment. Experiments on KITTI 2012, KITTI 2015, Sintel Clean, and Sintel Final datasets demonstrate the effectiveness of our methods on unsupervised optical flow estimation. The proposal reaches state-of-the-art performance with advantages in memory overhead."}, "cited_paper_content": {"title": "Raft: Recurrent All-Pairs Field Transforms For Optical Flow", "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance, with strong cross-dataset generalization and high efficiency in inference time, training speed, and parameter count. Code is available \\url{https://github.com/princeton-vl/RAFT}."}, "keywords": ["optical flow estimation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To alleviate the problem of untraceable pixels due to occlusion and blurring in unsupervised optical flow estimation, this work proposes a CNN-RNN based model that utilizes a predictive coding structure based on temporal dynamics to propagate the previous motion prior through hidden states and provide a more reliable reference for occluded regions. The proposed method achieves state-of-the-art performance on several datasets, including KITTI and Sintel, and is based on the Recurrent All-Pairs Field Transforms (RAFT) architecture introduced in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of untraceable pixels in unsupervised optical flow estimation, the authors propose a temporal dynamic model that propagates the previous motion prior through hidden states to provide a more reliable reference, which is based on the motion temporal smoothness and has a strong connection with the predictive coding mechanism #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of optical flow estimation in dynamic environments, this work proposes a CNN-RNN based model that utilizes a temporal dynamic model to propagate the previous motion prior through hidden states, providing a more reliable reference for untraceable pixels in the presence of occlusion and blurring (#REFR).", "given_citation_intent": "background", "given_keywords": ["optical flow estimation"]}]}
{"citing_id": "2303.02673v2", "cited_id": "1812.05920", "section_title": "A. Time Branch", "citation": "Motivated by the Sinc-Net's #REFR promising performance and interpretability, the time branch of our framework is designed based on SincNet.", "text_before_citation": ["The time branch is designed to extract the time-domain feature from the raw speech signals."], "text_after_citation": ["Following SincNet, the first layer of the time branch is designed as the learnable band pass filters to model the frequency characteristics for speaker recognition. A band pass filter can be formulated as:", "g[n, f 1 , f 2 ] = 2f 2 sinc(2\u03c0f 2 n) \u2212 2f 2 sinc(2\u03c0f 1 n), (1)", "where f 1 /f 2 is the low/high cutoff frequency, sinc(x) = sin x/x.", "By designing the filters as band pass filters, the model has fewer parameters and better interpretability.", "Except for the first layer, all other layers in our time branch are the typical 1dimensional convolutional layer (Conv), followed by the batch normalization layer (BN) #OTHEREFR and ReLU layer (ReLU) #OTHEREFR ."], "citing_paper_content": {"title": "Time-Frequency Network For Robust Speaker Recognition", "abstract": "The wide deployment of speech-based biometric systems usually demands high-performance speaker recognition algorithms. However, most of the prior works for speaker recognition either process the speech in the frequency domain or time domain, which may produce suboptimal results because both time and frequency domains are important for speaker recognition. In this paper, we attempt to analyze the speech signal in both time and frequency domains and propose the timefrequency network (TFN) for speaker recognition by extracting and fusing the features in the two domains. Based on the recent advance of deep neural networks, we propose a convolution neural network to encode the raw speech waveform and the frequency spectrum into domain-specific features, which are then fused and transformed into a classification feature space for speaker recognition. Experimental results on the publicly available datasets TIMIT and LibriSpeech show that our framework is effective to combine the information in the two domains and performs better than the state-of-the-art methods for speaker recognition."}, "cited_paper_content": {"title": "Speech And Speaker Recognition From Raw Waveform With Sincnet", "abstract": "Deep neural networks can learn complex and abstract representations, that are progressively obtained by combining simpler ones. A recent trend in speech and speaker recognition consists in discovering these representations starting from raw audio samples directly. Differently from standard hand-crafted features such as MFCCs or FBANK, the raw waveform can potentially help neural networks discover better and more customized representations. The high-dimensional raw inputs, however, can make training significantly more challenging. This paper summarizes our recent efforts to develop a neural architecture that efficiently processes speech from audio waveforms. In particular, we propose SincNet, a novel Convolutional Neural Network (CNN) that encourages the first layer to discover meaningful filters by exploiting parametrized sinc functions. In contrast to standard CNNs, which learn all the elements of each filter, only low and high cutoff frequencies of band-pass filters are directly learned from data. This inductive bias offers a very compact way to derive a customized front-end, that only depends on some parameters with a clear physical meaning. Our experiments, conducted on both speaker and speech recognition, show that the proposed architecture converges faster, performs better, and is more computationally efficient than standard CNNs."}, "keywords": ["time branch", "Sinc-Net's promising performance"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The time branch is designed to extract the time-domain feature from the raw speech signals, and recent efforts have proposed novel neural architectures, such as SincNet, that efficiently process speech from audio waveforms #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To extract time-domain features from raw speech signals, the authors proposed a novel Convolutional Neural Network (CNN) called SincNet, which efficiently processes speech from audio waveforms by exploiting parametrized sinc functions, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To extract the time-domain feature from the raw speech signals, the authors designed the time branch, which is inspired by SincNet's promising performance in processing speech from audio waveforms. #REFR", "given_citation_intent": "method", "given_keywords": ["time branch", "Sinc-Net's promising performance"]}]}
{"citing_id": "2303.10976v1", "cited_id": "1904.00537", "section_title": "Comparison With State-Of-The-Art Methods", "citation": "Our ADP also got +2.6%/+7.6% improvement in Rank-1 and +8.7%/+10.5% in mAP compared with SOTA partial Re-ID method VPM #REFR .", "text_before_citation": ["Though our proposed module is designed to solve occlusion problems rather than holistic problems, we still get comparable performance in holistic datasets compared with other holistic Re-ID methods.", "For example, the proposed ADP can achieve +0.3%/+1.6% improvement in Rank-1 and +0.9%/+3.1% in mAP on Market-1501 and DukeMTMC-ReID datasets, respectively, Table 4 .", "Ablation study of the dual-path loss used in DPC on Occluded-Duke dataset.", "AM denotes the module use shared angular softmax with single-path structure, while DP represents the module with an asymmetric dual-path structure.", "compared with the state-of-the-art method ISP #OTHEREFR ."], "text_after_citation": ["In comparison with occluded state-ofthe-art methods, our method is naturally at a disadvantage in performance due to the reason that PFD #OTHEREFR is based on part feature while our method is based on global feature.", "But we still achieved a very competitive and close performance to PFD."], "citing_paper_content": {"title": "Attention Disturbance And Dual-Path Constraint Network For Occluded Person Re-Identification", "abstract": "Occluded person re-identification (Re-ID) aims to address the potential occlusion problem when matching occluded or holistic pedestrians from different camera views. Many methods use the background as artificial occlusion and rely on attention networks to exclude noisy interference. However, the significant discrepancy between simple background occlusion and realistic occlusion can negatively impact the generalization of the network. To address this issue, we propose a novel transformer-based Attention Disturbance and Dual-Path Constraint Network (ADP) to enhance the generalization of attention networks. Firstly, to imitate real-world obstacles, we introduce an Attention Disturbance Mask (ADM) module that generates an offensive noise, which can distract attention like a realistic occluder, as a more complex form of occlusion. Secondly, to fully exploit these complex occluded images, we develop a Dual-Path Constraint Module (DPC) that can obtain preferable supervision information from holistic images through dualpath interaction. With our proposed method, the network can effectively circumvent a wide variety of occlusions using the basic ViT baseline. Comprehensive experimental evaluations conducted on person re-ID benchmarks demonstrate the superiority of ADP over state-of-the-art methods."}, "cited_paper_content": {"title": "Perceive Where To Focus: Learning Visibility-Aware Part-Level Features For Partial Person Re-Identification", "abstract": "This paper considers a realistic problem in person re-identification (re-ID) task, i.e., partial re-ID. Under partial re-ID scenario, the images may contain a partial observation of a pedestrian. If we directly compare a partial pedestrian image with a holistic one, the extreme spatial misalignment significantly compromises the discriminative ability of the learned representation. We propose a Visibility-aware Part Model (VPM) for partial re-ID, which learns to perceive the visibility of regions through self-supervision. The visibility awareness allows VPM to extract region-level features and compare two images with focus on their shared regions (which are visible on both images). VPM gains two-fold benefit toward higher accuracy for partial re-ID. On the one hand, compared with learning a global feature, VPM learns region-level features and thus benefits from fine-grained information. On the other hand, with visibility awareness, VPM is capable to estimate the shared regions between two images and thus suppresses the spatial misalignment. Experimental results confirm that our method significantly improves the learned feature representation and the achieved accuracy is on par with the state of the art."}, "keywords": ["SOTA partial Re-ID"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of extreme spatial misalignment in partial person re-identification, the proposed Visibility-aware Part Model (VPM) learns to perceive the visibility of regions through self-supervision, allowing it to extract region-level features and compare two images with focus on their shared regions, which significantly improves the learned feature representation and achieves state-of-the-art accuracy #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Visibility-aware Part Model (VPM) in #REFR significantly improves the learned feature representation for partial person re-identification by learning region-level features and estimating shared regions between two images, which suppresses the spatial misalignment.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed ADP method achieves state-of-the-art performance on occluded person re-identification datasets, and even obtains comparable results on holistic datasets, such as Market-1501 and DukeMTMC-ReID, with an improvement of +0.3%/+1.6% in Rank-1 and +0.9%/+3.1% in mAP, respectively, compared to #REFR, which is a state-of-the-art method for partial re-ID.", "given_citation_intent": "result", "given_keywords": ["SOTA partial Re-ID"]}]}
{"citing_id": "2303.15772v1", "cited_id": "1810.03993", "section_title": "Transparency In Ai", "citation": "Formative works like data sheets and model cards #REFR brought this approach to the fore, complementing evaluations by articulating design decisions and developer positions involved in creating assets.", "text_before_citation": ["ImageNet; #OTHEREFR , others stress-test models [e.g.", "CheckList; #OTHEREFR or adversarially identify failures [e.g.", "red-teaming; #OTHEREFR and still others characterize models along a range of dimensions [e.g. HELM; .", "In general, while some efforts expand evaluation to datasets #OTHEREFR , Swayamdipta et al., 2020 #OTHEREFR or adopt methodologies from human-computer interaction to consider human factors like user experience [Lee et al., 2022a,b] , for the most part, evaluation aims to characterize a specific model in isolation.", "Second, documentation is a growing practice for specifying metadata about the broader context that situates model and system development."], "text_after_citation": ["Subsequent efforts have enriched these approaches to make these documentation artifacts more useful, accessible, or otherwise aligned to specific informational needs [Crisan et al., 2022] .", "14 In general, documentation efforts aim to contextualize a specific asset against a broader social backdrop, often with an inclination towards how the asset came to be and with greater uptake for research-centric assets to our knowledge.", "Third, analyses and critiques have become increasingly relevant, showcasing much of the latent and oft-overlooked underpinnings of AI development and deployment.", "These works often bring questions of values and power to the fore, frequently appealing to concepts or methodologies from the social sciences and disciplines beyond computer science [e.g.", "#OTHEREFR , Koch et al., 2021 #OTHEREFR ."], "citing_paper_content": {"title": "Ecosystem Graphs: The Social Footprint Of Foundation Models", "abstract": "Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm. stanford.edu/ecosystem-graphs: as of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers."}, "cited_paper_content": {"title": "Model Cards For Model Reporting", "abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation."}, "keywords": ["data sheets", "model cards"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In order to encourage transparent reporting of machine learning models, the authors of Model Cards For Model Reporting propose a framework called model cards, which provide benchmarked evaluation in a variety of conditions and disclose the context in which models are intended to be used, among other relevant information #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To encourage transparent model reporting, the authors of the manuscript propose a framework called model cards, which provide benchmarked evaluation in a variety of conditions and disclose the context in which models are intended to be used, among other relevant information #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To encourage transparent model reporting, the authors of the manuscript propose the use of data sheets, which are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, and model cards, which disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information #REFR.", "given_citation_intent": "background", "given_keywords": ["data sheets", "model cards"]}]}
{"citing_id": "2303.11408v1", "cited_id": "1405.0312", "section_title": "Gender Bias Analysis Through Text Markers", "citation": "In total, 97.66% of the captions generated contained gender-marked terms, versus #REFR appearance predictions: this is consistent given the fact that VQA mostly consists of single word predictions, whereas captions are full sentences.", "text_before_citation": ["As described in Section 3.1, we used captions and open-ended VQA to obtain textual representations of the generated images, whose discrete nature makes identifying trends related to social attribute variation more tractable.", "More specifically, we base our evaluation on the likelihood that a caption or VQA answer for a given profession contains gender-marked words such as 'man' and 'woman' or gender-unspecified descriptors such as 'person' or the profession name -we present these results in Table 1 ."], "text_after_citation": ["To put the percentage of predictions that contain gender-marked terms such as 'man' and 'woman' into perspective, we compare them with the percentages of men and women in these professions provided by the BLS #OTHEREFR .", "We find that Dall\u2022E 2 has the largest discrepancy compared to the BLS-provided numbers, with its captions mentioning women on average 27% less, and its VQA mentioning them 25% less, and Stable Diffusion v.1.4 having the least (approximately 9% for both captions and VQA).", "The professions with the biggest discrepancy between the BLS and both captions and VQA across all models are: clerk (57 and 55% less), data entry keyer (55/53% less) and real estate broker (52/54% less), whereas those that have more captions that mention women are: singer (29/36% more), cleaner (20/16% more) and dispatcher (19/16% more).", "Very few of the generated image captions mention gender-neutral terms such as 'person' (an average of less than 1% of captions for any of the models, distributed equally across professions), and none use the 'non-binary' gender marker.", "Also, less than 0.5% of captions and 2% of VQA generations explicitly mention the profession in the prompt, but it is interesting to note that a single profession, police officer, had explicit mentions of the profession name in 80.95% of captions."], "citing_paper_content": {"title": "Stable Bias: Analyzing Societal Representations In Diffusion Models", "abstract": "As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems' outputs; since artificial depictions of fictive humans have no inherent gender or ethnicity nor do they belong to socially-constructed groups, we need to look beyond common categorizations of diversity or representation. To address this need, we propose a new method for exploring and quantifying social biases in TTI systems by directly comparing collections of generated images designed to showcase a system's variation across social attributes-gender and ethnicity-and target attributes for bias evaluation-professions and gender-coded adjectives. Our approach allows us to (i) identify specific bias trends through visualization tools, (ii) provide targeted scores to directly compare models in terms of diversity and representation, and (iii) jointly model interdependent social variables to support a multidimensional analysis. We use this approach to analyze over 96,000 images generated by 3 popular TTI systems (Dall\u2022E 2 , Stable Diffusion v 1.4 and v 2) and find that all three significantly over-represent the portion of their latent space associated with whiteness and masculinity across target attributes; among the systems studied, Dall\u2022E 2 shows the least diversity, followed by Stable Diffusion v2 then v1.4."}, "cited_paper_content": {"title": "Microsoft Coco: Common Objects In Context", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."}, "keywords": ["appearance predictions", "captions"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The dataset used in this study contains photos of 91 common objects in their natural context, labeled using per-instance segmentations to aid in precise object localization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The dataset used in this study contains photos of 91 common objects in their natural context, labeled using per-instance segmentations, and is comparable to other datasets such as PASCAL, ImageNet, and SUN in terms of statistical analysis, as described in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our approach for evaluating social biases in TTI systems is based on the likelihood that a caption or VQA answer for a given profession contains gender-marked words such as 'man' and 'woman' or gender-unspecified descriptors such as 'person' or the profession name, and we present these results in Table 1. This evaluation is supported by the appearance predictions and captions obtained from the dataset presented in Microsoft Coco: Common Objects In Context #REFR.", "given_citation_intent": "result", "given_keywords": ["appearance predictions", "captions"]}]}
{"citing_id": "2304.09839v1", "cited_id": "1905.08197", "section_title": "F. Simulation Results", "citation": "The results show that our code has a lower edit distance error compared to the code in #REFR , for a higher code rate.", "text_before_citation": ["Note that this improvement comes at the expense of a lower code rate; however, different trade-offs can be achieved by varying the parameters \u03b4 and .", "In Fig.", "3 , we also compare the performance our code to the VT-based code in #OTHEREFR in the regime where the average number of deletions per segment is a constant (i.e., \u03b1 = 1).", "The code in #OTHEREFR is constructed by concatenating n/ = 17 VT codewords each of size = 59, resulting in a code with rate \u2248 0.9.", "The parameters of our code are set to \u03b4 = 3, n = 994, and = 71, resulting in a code with rate \u2248 0.934."], "text_after_citation": [], "citing_paper_content": {"title": "Optimal Codes Detecting Deletions In Concatenated Binary Strings Applied To Trace Reconstruction", "abstract": "Consider two or more strings x 1 , x 2 ,. .. , that are concatenated to form x = x 1 , x 2 ,. .. . Suppose that up to \u03b4 deletions occur in each of the concatenated strings. Since deletions alter the lengths of the strings, a fundamental question to ask is: how much redundancy do we need to introduce in x in order to recover the boundaries of x 1 , x 2 ,. . .? This boundary problem is equivalent to the problem of designing codes that can detect the exact number of deletions in each concatenated string. In this work, we answer the question above by first deriving converse results that give lower bounds on the redundancy of deletion-detecting codes. Then, we present a marker-based code construction whose redundancy is asymptotically optimal in \u03b4 among all families of deletion-detecting codes, and exactly optimal among all block-by-block decodable codes. To exemplify the usefulness of such deletion-detecting codes, we apply our code to trace reconstruction and design an efficient coded reconstruction scheme that requires a constant number of traces."}, "cited_paper_content": {"title": "Coding For Deletion Channels With Multiple Traces", "abstract": "Motivated by the sequence reconstruction problem from traces in DNA-based storage, we consider the problem of designing codes for the deletion channel when multiple observations (or traces) are available to the decoder. We propose simple binary and non-binary codes based on Varshamov-Tenengolts (VT) codes. The proposed codes split the codeword in blocks and employ a VT code in each block. The availability of multiple traces helps the decoder to identify deletion-free copies of a block, and to avoid mis-synchronization while decoding. The encoding complexity of the proposed scheme is linear in the codeword length; the decoding complexity is linear in the codeword length, and quadratic in the number of deletions and the number of traces. The proposed scheme offers an explicit low-complexity technique for correcting deletions using multiple traces."}, "keywords": ["higher code rate", "code"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In Fig. 3, we also compare the performance our code to the VT-based code in #REFR in the regime where the average number of deletions per segment is a constant (i.e., \u03b1 = 1).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed scheme for correcting deletions using multiple traces offers an explicit low-complexity technique, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed code achieves a higher code rate than the VT-based code in #OTHEREFR, while still providing an explicit low-complexity technique for correcting deletions using multiple traces. #REFR", "given_citation_intent": "result", "given_keywords": ["higher code rate", "code"]}]}
{"citing_id": "2304.14828v1", "cited_id": "1908.08196", "section_title": "Iii. Interview For Engineering Experience", "citation": "Participants who were categorized as elite developers were invited through email or text messages sent to public channels of their respective organizations #REFR .", "text_before_citation": ["The study employed a purposive sampling technique and a semi-structured approach to conduct interviews with participants selected from a list of OSS projects with bot deployment #OTHEREFR ."], "text_after_citation": ["The interview process consisted of 10 pre-defined questions grouped into three main sections, namely introduction, workflow, and expectations of SE bots (see Tab. I).", "All interviews, which lasted between 25 to 45 minutes each, were conducted across 2021 and 2022.", "Participants' responses were analyzed using an open coding procedure, and the raw interview transcripts were annotated for thematic analysis to extract meaningful findings #OTHEREFR Ch.4] .", "To ensure coding schema validity, two researchers assessed one participant's transcript, and substantial agreement was achieved (Cohen's \u03ba = 0.66).", "The study's primary objectives were to determine the workflow and expectations of SE bots, which were the major findings of the research."], "citing_paper_content": {"title": "Optimizing Workflow For Elite Developers: Perspectives On Leveraging Se Bots", "abstract": "Small-scale automation services in Software Engineering, known as SE Bots, have gradually infiltrated every aspect of daily software development with the goal of enhancing productivity and well-being. While leading the OSS development, elite developers have often burned out from holistic responsibilities in projects and looked for automation support. Building on prior research in BotSE and our interviews with elite developers, this paper discusses how to design and implement SE bots that integrate into the workflows of elite developers and meet their expectations. We present six main design guidelines for implementing SE bots for elite developers, based on their concerns about noise, security, simplicity, and other factors. Additionally, we discuss the future directions of SE bots, especially in supporting elite developers' increasing workload due to rising demands."}, "cited_paper_content": {"title": "Unveiling Elite Developers' Activities In Open Source Projects", "abstract": "Open-source developers, particularly the elite developers, maintain a diverse portfolio of contributing activities. They do not only commit source code but also spend a significant amount of effort on other communicative, organizational, and supportive activities. However, almost all prior research focuses on a limited number of specific activities and fails to analyze elite developers' activities in a comprehensive way. To bridge this gap, we conduct an empirical study with fine-grained event data from 20 large open-source projects hosted on GitHub. Thus, we investigate elite developers' contributing activities and their impacts on project outcomes. Our analyses reveal three key findings: (1) they participate in a variety of activities while technical contributions (e.g., coding) accounting for a small proportion only; (2) they tend to put more effort into supportive and communicative activities and less effort into coding as the project grows; (3) their participation in non-technical activities is negatively associated with the project's outcomes in term of productivity and software quality. These results provide a panoramic view of elite developers' activities and can inform an individual's decision making about effort allocation, thus leading to finer project outcomes. The results also provide implications for supporting these elite developers."}, "keywords": ["elite developers"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The study employed a purposive sampling technique and a semi-structured approach to conduct interviews with participants selected from a list of OSS projects with bot deployment, and to investigate elite developers' contributing activities and their impacts on project outcomes, as reported in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To conduct a comprehensive analysis of elite developers' activities in open-source projects, the authors of this manuscript employed a fine-grained event data from 20 large open-source projects hosted on GitHub #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To investigate the contributing activities of elite developers in open-source projects, the authors of this paper conducted an empirical study with fine-grained event data from 20 large open-source projects hosted on GitHub #REFR. Specifically, they found that elite developers participate in a variety of activities, with technical contributions accounting for a small proportion only, and tend to put more effort into supportive and communicative activities as the project grows, which can inform an individual's decision-making about effort allocation #OTHEREFR.", "given_citation_intent": "method", "given_keywords": ["elite developers"]}]}
{"citing_id": "2304.01560v1", "cited_id": "1909.06492", "section_title": "I. Introduction", "citation": "Then, collected real data displays a sharp change in the true harvesting function, e.g., Figure 4 in #REFR .", "text_before_citation": ["Our previous work #OTHEREFR accomplishes the learning approach and theoretical guarantees simultaneously by using splines and arXiv:2304.01560v1 [cs.IT] 4 Apr 2023 local polynomials.", "The work is motivated by the fact that for low-dimensional functions, classic function interpolation methods perform as good as deep learning regression but also provide theoretical guarantees.", "Specifically, assuming that a smooth harvesting function-formally, belonging to a class of Sobolev functions-is known only at sample points, it characterizes asymptotic performance loss of information rate and energy incurred by such sampled knowledge #OTHEREFR .", "Instead of deep learning regression, it suggests using the spline and local polynomial methods to reconstruct the unknown harvesting function and using it as a proxy for the true one.", "The results are promising as performance losses from the true optimal performance are asymptotically negligible with the number of samples, but the smoothness assumption is restrictive because the harvesting function occasionally varies very quickly as a signal varies, e.g., due to diode's mode transition, hardware defects, or unexpected resonance of circuits if frequency modulation is adopted."], "text_after_citation": ["Even when the true function is (formally Sobolev) smooth, such sharp change implies a large magnitude of its derivative, which is in turn captured by a hidden constant factor of the loss characterization that depends on derivatives #OTHEREFR . Therefore, the resulting loss bounds are vacuous.", "In such a case, it is more reasonable to model it as a function of bounded variation that may have discontinuities, but its total variation is bounded.", "This is a larger class of functions than the Sobolev functions, for which the spline reconstruction method does not perform well.", "This work considers a class of harvesting functions of bounded variation, which may have a finite number of jumps (discontinuities).", "In addition, we assume that the harvesting function is accessible only at discrete sample points, either in the absence of noise or in the presence of noise."], "citing_paper_content": {"title": "Information And Energy Transmission With Wavelet-Reconstructed Harvesting Functions", "abstract": "In practical simultaneous information and energy transmission (SIET), the exact energy harvesting function is usually unavailable because an energy harvesting circuit is nonlinear and nonideal. In this work, we consider a SIET problem where the harvesting function is accessible only at experimentally-taken sample points and study how close we can design SIET to the optimal system with such sampled knowledge. Assuming that the harvesting function is of bounded variation that may have discontinuities, we separately consider two settings where samples are taken without and with additive noise. For these settings, we propose to design a SIET system as if a waveletreconstructed harvesting function is the true one and study its asymptotic performance loss of energy and information delivery from the true optimal one. Specifically, for noiseless samples, it is shown that designing SIET as if the wavelet-reconstructed harvesting function is the truth incurs asymptotically vanishing energy and information delivery loss with the number of samples. For noisy samples, we propose to reconstruct wavelet coefficients via soft-thresholding estimation. Then, we not only obtain similar asymptotic losses to the noiseless case but also show that the energy loss by wavelets is asymptotically optimal up to a logarithmic factor."}, "cited_paper_content": {"title": "Learning To Communicate And Energize: Modulation, Coding And Multiple Access Designs For Wireless Information-Power Transmission", "abstract": "The explosion of the number of low-power devices in the next decades calls for a re-thinking of wireless network design, namely, unifying wireless transmission of information and power so as to make the best use of the RF spectrum, radiation, and infrastructure for the dual purpose of communicating and energizing. This paper provides a novel learning-based approach towards such wireless network design. To that end, a parametric model of a practical energy harvester, accounting for various sources of nonlinearities, is proposed using a nonlinear regression algorithm applied over collected real data. Relying on the proposed model, the learning problem of modulation design for Simultaneous Wireless Information-Power Transmission (SWIPT) over a point-to-point link is studied. Joint optimization of the transmitter and the receiver is implemented using Neural Network (NN)-based autoencoders. The results reveal that by increasing the receiver power demand, the baseband transmit modulation constellation converges to an On-Off keying signalling. Utilizing the observations obtained via learning, an algorithmic SWIPT modulation design is proposed. It is observed via numerical results that the performance loss of the proposed modulations are negligible compared to the ones obtained from learning. Extension of the studied problem to learning modulation design for multi-user SWIPT scenarios and coded modulation design for point-to-point SWIPT are considered. The major conclusion of this work is to utilize learning-based results to design non learning-based algorithms, which perform as well. In particular, inspired by the results obtained via learning, an algorithmic approach for coded modulation design is proposed, which performs very close to its learning counterparts, and is significantly superior due to its high real-time adaptability to new system design parameters."}, "keywords": ["true harvesting function"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose a learning-based approach for simultaneous wireless information-power transmission (SWIPT) over a point-to-point link, and optimize the transmitter and receiver using neural network-based autoencoders #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To design a simultaneous wireless information-power transmission (SWIPT) system with a practical energy harvester, the authors of this manuscript propose a novel learning-based approach that relies on a parametric model of the harvester and joint optimization of the transmitter and receiver using neural network-based autoencoders, as described in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reconstruct the unknown harvesting function, the authors propose using the spline and local polynomial methods and using it as a proxy for the #KEYWORD(true harvesting function) #REFR.", "given_citation_intent": "background", "given_keywords": ["true harvesting function"]}]}
{"citing_id": "2303.01679v1", "cited_id": "1903.05700", "section_title": "Results", "citation": "We reported TPR at 0.1% and 1% FPR for comparison to #REFR , where it can be seen our model performed better in both.", "text_before_citation": ["The table also contains results using loss as a performance metric for SOREL-20M and EMBER-2018; this is to show the difference in F1-score and loss as a performance metric in the final stage, this will be brought up in the discussion section of this section.", "Some other works shown in the tables only report a subset of the metrics, but are still shown for comparison.", "In particular, for SOREL-20M, Table 4 shows our AUC results are on par with the FFNN ensemble from #OTHEREFR and slightly exceeds #OTHEREFR , the work that presented the auxiliary model heads for SOREL-20M.", "Our model significantly exceeds the AUC under 0.1% FPR of the only other work #OTHEREFR , which reported this parameter.", "The accuracy of our model is similar but higher than #OTHEREFR ."], "text_after_citation": ["With respect to EMBER-2018 in Table 5 , #OTHEREFR performs slightly better in their reported metrics, AUC and accuracy, whereas the rest of their metrics are not reported.", "Our model chosen in the final training phase is similar to other results in AUC and accuracy, surpassing #OTHEREFR in AUC, and surpassing both #OTHEREFR , #OTHEREFR in accuracy.", "The AUC under 0.1% FPR of our model far surpasses the results of #OTHEREFR .", "The TPR at 0.1% is the only reported metric of #OTHEREFR , which is significantly Fig. 4 . ROC using F1-Score for Selection higher than our results.", "Due to limited metrics provided by other related works, it is difficult to compare the efficacy of our AutoML method in a holistic sense."], "citing_paper_content": {"title": "Automated Machine Learning For Deep Learning Based Malware Detection", "abstract": "Deep learning (DL) has proven to be effective in detecting sophisticated malware that is constantly evolving. Even though deep learning has alleviated the feature engineering problem, finding the most optimal DL model, in terms of neural architecture search (NAS) and the model's optimal set of hyper-parameters, remains a challenge that requires domain expertise. In addition, many of the proposed state-of-the-art models are very complex and may not be the best fit for different datasets. A promising approach, known as Automated Machine Learning (AutoML), can reduce the domain expertise required to implement a custom DL model. AutoML reduces the amount of human trial-and-error involved in designing DL models, and in more recent implementations can find new model architectures with relatively low computational overhead. Research on the feasibility of using AutoML for malware detection is very limited. This work provides a comprehensive analysis and insights on using AutoML for static and online malware detection. For static, our analysis is performed on two widely used malware datasets: SOREL-20M to demonstrate efficacy on large datasets; and EMBER-2018, a smaller dataset specifically curated to hinder the performance of machine learning models. In addition, we show the effects of tuning the NAS process parameters on finding a more optimal malware detection model on these static analysis datasets. Further, we also demonstrate that AutoML is performant in online malware detection scenarios using Convolutional Neural Networks (CNNs) for cloud IaaS. We compare an AutoML technique to six existing state-of-the-art CNNs using a newly generated online malware dataset with and without other applications running in the background during malware execution. We show that the AutoML technique is more performant than the state-of-the-art CNNs with little overhead in finding the architecture. In general, our experimental results show that the performance of AutoML based static and online malware detection models are on par or even better than state-of-the-art models or hand-designed models presented in literature."}, "cited_paper_content": {"title": "Aloha: Auxiliary Loss Optimization For Hypothesis Augmentation", "abstract": "Malware detection is a popular application of Machine Learning for Information Security (ML-Sec), in which an ML classifier is trained to predict whether a given file is malware or benignware. Parameters of this classifier are typically optimized such that outputs from the model over a set of input samples most closely match the samples' true malicious/benign (1/0) target labels. However, there are often a number of other sources of contextual metadata for each malware sample, beyond an aggregate malicious/benign label, including multiple labeling sources and malware type information (e.g., ransomware, trojan, etc.), which we can feed to the classifier as auxiliary prediction targets. In this work, we fit deep neural networks to multiple additional targets derived from metadata in a threat intelligence feed for Portable Executable (PE) malware and benignware, including a multi-source malicious/benign loss, a count loss on multi-source detections, and a semantic malware attribute tag loss. We find that incorporating multiple auxiliary loss terms yields a marked improvement in performance on the main detection task. We also demonstrate that these gains likely stem from a more informed neural network representation and are not due to a regularization artifact of multi-target learning. Our auxiliary loss architecture yields a significant reduction in detection error rate (false negatives) of 42.6% at a false positive rate (FPR) of $10^{-3}$ when compared to a similar model with only one target, and a decrease of 53.8% at $10^{-5}$ FPR."}, "keywords": ["model"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our model significantly exceeds the AUC under 0.1% FPR of the only other work #REFR, which reported this parameter.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our model significantly exceeds the AUC under 0.1% FPR of the only other work, which reported this parameter, and this improvement is consistent with the findings in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our model significantly exceeds the AUC under 0.1% FPR of the only other work, which reported this parameter, and this improvement is consistent with the findings in #REFR.", "given_citation_intent": "result", "given_keywords": ["model"]}]}
{"citing_id": "2304.11034v2", "cited_id": "0912.2625", "section_title": "I. Introduction", "citation": "Whether the infinite clique problem over arbitrary \u03c9-synchronous relations is decidable is a longstanding open problem #REFR .", "text_before_citation": ["In fact, the infinite clique problem for arbitrary synchronous relations was shown to be NL-complete #OTHEREFR (later simplified in #OTHEREFR ).", "However, in certain settings we need to exploit the fact that \u223c R j is the complement of an equivalence relation \u223c R j .", "#OTHEREFR While the authors of #OTHEREFR did not analyze the complexity of their algorithm, it is easy to see that their algorithm runs in elementary time for fixed arity k.", "For example, L\u00f6ding and Spinrath #OTHEREFR have shown that the infinite clique problem for \u03c9-synchronous co-equivalence relations is decidable in double exponential time.", "This yields a double (triple) exponential time algorithm for the \u03c9recognizability problem for \u03c9-synchronous relations given by (non)deterministic \u03c9-automata."], "text_after_citation": ["Another example where the difference between co-equivalence relations and arbitrary relations becomes apparent is the case of tree-automatic relations.", "It was proven in #OTHEREFR that the infinite clique problem for treeautomatic relations is EXP-complete; however, restricted to complements of transitive relations the infinite clique problem becomes P-complete.", "This yields optimal complexity for the recognizability problem for tree-automatic relations: Recognizability is P-complete for relations given by deterministic bottom-up or top-down tree automata, and EXP-complete for nondeterministic tree automata.", "Contributions: We provide improved complexity and new decidability results for the membership problems in subclasses of rational relations over finite and infinite words.", "To do so, we refine the existing analyses in #OTHEREFR , #OTHEREFR and identify patterns in the transducers which witness non-membership in the subclass."], "citing_paper_content": {"title": "Revisiting Membership Problems In Subclasses Of Rational Relations", "abstract": "We revisit the membership problem for subclasses of rational relations over finite and infinite words: Given a relation R in a class C2, does R belong to a smaller class C1? The subclasses of rational relations that we consider are formed by the deterministic rational relations, synchronous (also called automatic or regular) relations, and recognizable relations. For almost all versions of the membership problem, determining the precise complexity or even decidability has remained an open problem for almost two decades. In this paper, we provide improved complexity and new decidability results. (i) Testing whether a synchronous relation over infinite words is recognizable is NL-complete (PSPACE-complete) if the relation is given by a deterministic (nondeterministic) \u03c9-automaton. This fully settles the complexity of this recognizability problem, matching the complexity of the same problem over finite words. (ii) Testing whether a deterministic rational binary relation is recognizable is decidable in polynomial time, which improves a previously known double exponential time upper bound. For relations of higher arity, we present a randomized exponential time algorithm. (iii) We provide the first algorithm to decide whether a deterministic rational relation is synchronous. For binary relations the algorithm even runs in polynomial time."}, "cited_paper_content": {"title": "Is Ramsey'S Theorem Omega-Automatic?", "abstract": "We study the existence of infinite cliques in omega-automatic (hyper-)graphs. It turns out that the situation is much nicer than in general uncountable graphs, but not as nice as for automatic graphs. More specifically, we show that every uncountable omega-automatic graph contains an uncountable co-context-free clique or anticlique, but not necessarily a context-free (let alone regular) clique or anticlique. We also show that uncountable omega-automatic ternary hypergraphs need not have uncountable cliques or anticliques at all."}, "keywords": ["arbitrary \u03c9-synchronous relations", "infinite clique problem"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In certain settings, the complement of an equivalence relation can be exploited, as shown by the authors of #REFR. While their algorithm's complexity was not analyzed, it runs in elementary time for fixed arity k.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' study on the existence of infinite cliques in omega-automatic graphs #REFR provides valuable insights into the complexity of recognizing synchronous relations over infinite words.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The infinite clique problem for arbitrary \u03c9-synchronous relations has been studied in the literature #REFR, where it was shown that every uncountable \u03c9-automatic graph contains an uncountable co-context-free clique or anticlique, but not necessarily a context-free (let alone regular) clique or anticlique.", "given_citation_intent": "background", "given_keywords": ["arbitrary \u03c9-synchronous relations", "infinite clique problem"]}]}
{"citing_id": "2304.03424v1", "cited_id": "1908.09048", "section_title": "Introduction", "citation": "Unfortunately, they are often neglected by operators due to the difficulties of assessment even though job slowdowns are inevitable #REFR .", "text_before_citation": ["Figure 1 shows a set of recurring jobs in Cosmos #OTHEREFR , a Big Data analytics platform at Microsoft, submitted with different frequencies.", "We can see that some jobs have more stable runtime while some have occasional slow downs with non-regular patterns.", "But, it is not apparent why such variations are happening, how they can be mitigated, or how likely it is for the next job run to be an outlier compared to historic runs.", "In production systems, jobs are often scheduled or pipelined with strong data dependencies (jobs using other jobs' output data as inputs) #OTHEREFR .", "Stability and predictability of job runtimes are important factors that affect the fundamental design and architecture of data processing pipelines."], "text_after_citation": ["Even with massive amounts of telemetry data, cloud providers still default to a manual triage process due to the difficulty of capturing the compounding factors that impact job runtime and its stability, which is not scalable and error-prone.", "Although prior works #OTHEREFR have empirically characterized runtime variation, they do not propose methods to predict the variation nor the likelihood of a new run being an outlier compared to the average or median runtimes.", "Other works such as Griffon #OTHEREFR used machine learning models to predict the minor slowdown in runtimes for a limited number of job templates.", "They are unable to predict significant slowdowns that appear as outliers.", "As ML models are notoriously bad at handling outliers especially with a low existence, prior time-series based approaches #OTHEREFR are not applicable."], "citing_paper_content": {"title": "Runtime Variation In Big Data Analytics", "abstract": "The dynamic nature of resource allocation and runtime conditions on Cloud can result in high variability in a job's runtime across multiple iterations, leading to a poor experience. Identifying the sources of such variation and being able to predict and adjust for them is crucial to cloud service providers to design reliable data processing pipelines, provision and allocate resources, adjust pricing services, meet SLOs and debug performance hazards. In this paper, we analyze the runtime variation of millions of production SCOPE jobs on Cosmos, an exabyte-scale internal analytics platform at Microsoft. We propose an innovative 2-step approach to predict job runtime distribution by characterizing typical distribution shapes combined with a classification model with an average accuracy of >96%, out-performing traditional regression models and better capturing long tails. We examine factors such as job plan characteristics and inputs, resource allocation, physical cluster heterogeneity and utilization, and scheduling policies. To the best of our knowledge, this is the first study on predicting categories of runtime distributions for enterprise analytics workloads at scale. Furthermore, we examine how our methods can be used to analyze what-if scenarios, focusing on the impact of resource allocation, scheduling, and physical cluster provisioning decisions on a job's runtime consistency and predictability. CCS Concepts: \u2022 Computer systems organization \u2192 Cloud computing; \u2022 Computing methodologies \u2192 Causal reasoning and diagnostics; \u2022 Information systems \u2192 Data analytics."}, "cited_paper_content": {"title": "Griffon: Reasoning About Job Anomalies With Unlabeled Data In Cloud-Based Platforms", "abstract": "Microsoft's internal big data analytics platform is comprised of hundreds of thousands of machines, serving over half a million jobs daily, from thousands of users. The majority of these jobs are recurring and are crucial for the company's operation. Although administrators spend significant effort tuning system performance, some jobs inevitably experience slowdowns, i.e., their execution time degrades over previous runs. Currently, the investigation of such slowdowns is a labor-intensive and error-prone process, which costs Microsoft significant human and machine resources, and negatively impacts several lines of businesses. In this work, we present Griffin, a system we built and have deployed in production last year to automatically discover the root cause of job slowdowns. Existing solutions either rely on labeled data (i.e., resolved incidents with labeled reasons for job slowdowns), which is in most cases non-existent or non-trivial to acquire, or on time-series analysis of individual metrics that do not target specific jobs holistically. In contrast, in Griffin we cast the problem to a corresponding regression one that predicts the runtime of a job, and show how the relative contributions of the features used to train our interpretable model can be exploited to rank the potential causes of job slowdowns. Evaluated over historical incidents, we show that Griffin discovers slowdown causes that are consistent with the ones validated by domain-expert engineers, in a fraction of the time required by them."}, "keywords": ["job slowdowns"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Stability and predictability of job runtimes are important factors that affect the fundamental design and architecture of data processing pipelines. To address the issue of job slowdowns, Microsoft's internal big data analytics platform uses Griffin, a system that automatically discovers the root cause of job slowdowns by casting the problem to a corresponding regression one that predicts the runtime of a job #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the root causes of runtime variations in recurring jobs, Griffin #REFR, a system that automatically discovers the reasons for job slowdowns, was developed and deployed in production by Microsoft, which significantly reduced the time and resources required for investigation.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The investigation of job slowdowns is a labor-intensive and error-prone process, which costs significant human and machine resources, and negatively impacts several lines of businesses #REFR.", "given_citation_intent": "background", "given_keywords": ["job slowdowns"]}]}
{"citing_id": "2304.11217v1", "cited_id": "1908.09635", "section_title": "A Framework For Describing And Analyzing Fairness: Acrocpolis", "citation": "In an expos\u00e9 by ProPublica, the system was found to yield different results based on race, in effect recommending harsher penalties and higher bail payments for Black people than white people #REFR .", "text_before_citation": ["Describing the above categories is, in many cases, not sufficient to assess or describe the fairness of a process.", "Indeed, fairness assessments often hinge on the relationships between components.", "For this reason, Links, denoting such components, are an integral part of the framework.", "The next sections detail the different elements of ACROCPoLis.", "As a running example we use the well-known COMPAS case, where an AI decision support system ostensibly evaluated the recidivism risk of court defendants based on various demographic data."], "text_after_citation": ["In particular, the perspectives of both Northpointe (the company that built the COMPAS system) and ProPublica will be described, as well as the factors these Actors took as most relevant, revealing the differences in the fairness assessments they provided."], "citing_paper_content": {"title": "Acrocpolis: A Descriptive Framework For Making Sense Of Fairness", "abstract": "Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders."}, "cited_paper_content": {"title": "A Survey On Bias And Fairness In Machine Learning", "abstract": "With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields."}, "keywords": ["race"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For this reason, Links, denoting such components, are an integral part of the framework (Author et al., Year). The next sections detail the different elements of ACROCPoLis. As a running example we use the well-known COMPAS case, where an AI decision support system ostensibly evaluated the recidivism risk of court defendants based on various demographic data.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the survey on bias and fairness in machine learning, \"it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations\" (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the survey on bias and fairness in machine learning, researchers have attempted to address the biases that AI applications can contain, particularly in sensitive environments where decisions can have life-changing consequences for certain groups or populations, such as those based on #REFR race.", "given_citation_intent": "result", "given_keywords": ["race"]}]}
{"citing_id": "2304.11467v1", "cited_id": "1806.08159", "section_title": "Implications Of The Performance Anomalies Found", "citation": "A similar observation has been shown in IRN #REFR , but they mainly focus on innetwork behaviors.", "text_before_citation": ["For example, many anomalies (e.g., #9 and #12) show that the host limitation can slow down RNIC's outbound rate (dispatching received data to host memory).", "This makes the receiver cannot consume packets as fast as the sender sends.", "Without end-to-end flow control, the Ro-CEv2 now can only rely on PFC, the hop-by-hop flow control mechanism.", "PFC helps to avoid such overflow packet drop but can cause catastrophic consequences #OTHEREFR .", "Note that RDMA congestion control #OTHEREFR mainly targets innetwork congestion, so it is orthogonal."], "text_after_citation": ["Collie shows that, in addition to switches, the hosts can also generate PFC pause frames, which requires attention when deploying RDMA in production."], "citing_paper_content": {"title": "Collie: Finding Performance Anomalies In Rdma Subsystems", "abstract": "High-speed RDMA networks are getting rapidly adopted in the industry for their low latency and reduced CPU overheads. To verify that RDMA can be used in production, system administrators need to understand the set of application workloads that can potentially trigger abnormal performance behaviors (e.g., unexpected low throughput, PFC pause frame storm). We design and implement Collie, a tool for users to systematically uncover performance anomalies in RDMA subsystems without the need to access hardware internal designs. Instead of individually testing each hardware device (e.g., NIC, memory, PCIe), Collie is holistic, constructing a comprehensive search space for application workloads. Collie then uses simulated annealing to drive RDMA-related performance and diagnostic counters to extreme value regions to find workloads that can trigger performance anomalies. We evaluate Collie on combinations of various RDMA NIC, CPU, and other hardware components. Collie found 15 new performance anomalies. All of them are acknowledged by the hardware vendors. 7 of them are already fixed after we reported them. We also present our experience in using Collie to avoid performance anomalies for an RDMA RPC library and an RDMA distributed machine learning framework."}, "cited_paper_content": {"title": "Revisiting Network Support For Rdma", "abstract": "The advent of RoCE (RDMA over Converged Ethernet) has led to a significant increase in the use of RDMA in datacenter networks. To achieve good performance, RoCE requires a lossless network which is in turn achieved by enabling Priority Flow Control (PFC) within the network. However, PFC brings with it a host of problems such as head-of-the-line blocking, congestion spreading, and occasional deadlocks. Rather than seek to fix these issues, we instead ask: is PFC fundamentally required to support RDMA over Ethernet? ::: We show that the need for PFC is an artifact of current RoCE NIC designs rather than a fundamental requirement. We propose an improved RoCE NIC (IRN) design that makes a few simple changes to the RoCE NIC for better handling of packet losses. We show that IRN (without PFC) outperforms RoCE (with PFC) by 6-83% for typical network scenarios. Thus not only does IRN eliminate the need for PFC, it improves performance in the process! We further show that the changes that IRN introduces can be implemented with modest overheads of about 3-10% to NIC resources. Based on our results, we argue that research and industry should rethink the current trajectory of network support for RDMA."}, "keywords": ["IRN"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Without end-to-end flow control, the Ro-CEv2 now can only rely on PFC, the hop-by-hop flow control mechanism. PFC helps to avoid such overflow packet drop but can cause catastrophic consequences (Chen et al., 2018). Note that RDMA congestion control mainly targets innetwork congestion, so it is orthogonal.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The reference paper #REFR shows that the need for PFC is not a fundamental requirement for supporting RDMA over Ethernet and proposes an improved RoCE NIC design that outperforms RoCE with PFC by 6-83% for typical network scenarios.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The improved RoCE NIC (IRN) design proposed in #REFR eliminates the need for PFC and outperforms RoCE (with PFC) by 6-83% for typical network scenarios, as shown in the reference paper.", "given_citation_intent": "result", "given_keywords": ["IRN"]}]}
{"citing_id": "2304.09839v1", "cited_id": "1905.08197", "section_title": "B. Contributions", "citation": "For this regime, our code has a lower redundancy compared to the \u0398(log n) redundancy of the code introduced in #REFR .", "text_before_citation": ["We focus on the regime where p = k/n \u03b1 , where k > 1 and \u03b1 \u2208 (0.5, 1] are constants.", "We use the deletion-detecting code introduced in Section IV-B (Construction 1) to design a novel code C \u2286 F n 2 for trace reconstruction. The contributions that we present are the following.", "\u2022 For all \u03b1 \u2208 (0.5, 1], the redundancy of our code is o(n 1\u2212\u03b1 log n), i.e., grows strictly slower than the optimal redundancy of a deletion-correcting code, which is \u0398(n 1\u2212\u03b1 log n) for d = \u0398(np) = \u0398(n 1\u2212\u03b1 ).", "Furthermore, for the particular regime where the average number of deletions is fixed, i.e., \u03b1 = 1 and p = k/n, we show that our code can efficiently reconstruct any codeword x \u2208 C with any redundancy that is \u03c9(1).", "This regime is similar to the regime where the number of deletions per trace is fixed, which was studied in #OTHEREFR ."], "text_after_citation": ["\u2022 The reconstruction algorithm that we propose runs in linear time O(n).", "\u2022 We provide a theoretical guarantee on the performance of our code in the form of an upper bound on the probability of reconstruction error.", "This bound shows that for all \u03b1 \u2208 (0.5, 1], and for any codeword x \u2208 C , the probability of reconstruction error vanishes asymptotically in n for a constant number of traces t = O(1).", "\u2022 We evaluate the numerical performance of our code in terms of the edit distance error, which was adopted as a performance metric in several recent related works, e.g., #OTHEREFR - #OTHEREFR .", "We compare the performance of our code to the code in #OTHEREFR and to a simple coded version of the Bitwise Majority Alignment (BMA) reconstruction algorithm #OTHEREFR ."], "citing_paper_content": {"title": "Optimal Codes Detecting Deletions In Concatenated Binary Strings Applied To Trace Reconstruction", "abstract": "Consider two or more strings x 1 , x 2 ,. .. , that are concatenated to form x = x 1 , x 2 ,. .. . Suppose that up to \u03b4 deletions occur in each of the concatenated strings. Since deletions alter the lengths of the strings, a fundamental question to ask is: how much redundancy do we need to introduce in x in order to recover the boundaries of x 1 , x 2 ,. . .? This boundary problem is equivalent to the problem of designing codes that can detect the exact number of deletions in each concatenated string. In this work, we answer the question above by first deriving converse results that give lower bounds on the redundancy of deletion-detecting codes. Then, we present a marker-based code construction whose redundancy is asymptotically optimal in \u03b4 among all families of deletion-detecting codes, and exactly optimal among all block-by-block decodable codes. To exemplify the usefulness of such deletion-detecting codes, we apply our code to trace reconstruction and design an efficient coded reconstruction scheme that requires a constant number of traces."}, "cited_paper_content": {"title": "Coding For Deletion Channels With Multiple Traces", "abstract": "Motivated by the sequence reconstruction problem from traces in DNA-based storage, we consider the problem of designing codes for the deletion channel when multiple observations (or traces) are available to the decoder. We propose simple binary and non-binary codes based on Varshamov-Tenengolts (VT) codes. The proposed codes split the codeword in blocks and employ a VT code in each block. The availability of multiple traces helps the decoder to identify deletion-free copies of a block, and to avoid mis-synchronization while decoding. The encoding complexity of the proposed scheme is linear in the codeword length; the decoding complexity is linear in the codeword length, and quadratic in the number of deletions and the number of traces. The proposed scheme offers an explicit low-complexity technique for correcting deletions using multiple traces."}, "keywords": ["code"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We use the deletion-detecting code introduced in Section IV-B (Construction 1) to design a novel code C \u2286 F n 2 for trace reconstruction. The contributions that we present are the following. \u2022 For all \u03b1 \u2208 (0.5, 1], the redundancy of our code is o(n 1\u2212\u03b1 log n), i.e., grows strictly slower than the optimal redundancy of a deletion-correcting code, which is \u0398(n 1\u2212\u03b1 log n) for d = \u0398(np) = \u0398(n 1\u2212\u03b1 ). Furthermore, for the particular regime where the average number of deletions is fixed, i.e., \u03b1 = 1 and p = k/n, we show that our code can efficiently reconstruct any codeword x \u2208 C with any redundancy that is \u03c9(1). This regime is similar to the regime where the number of deletions per trace is fixed, which was studied in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To design a novel code for trace reconstruction, the authors propose simple binary and non-binary codes based on Varshamov-Tenengolts (VT) codes, which split the codeword in blocks and employ a VT code in each block, and show that the proposed scheme offers an explicit low-complexity technique for correcting deletions using multiple traces #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To design a novel code for trace reconstruction, the authors used a deletion-detecting code based on Varshamov-Tenengolts codes proposed in #REFR, which splits the codeword in blocks and employs a VT code in each block, allowing for efficient correction of deletions using multiple traces.", "given_citation_intent": "background", "given_keywords": ["code"]}]}
{"citing_id": "2304.11818v1", "cited_id": "1912.04958", "section_title": "Impact Of Multiple Transformer Layers On Training Convergence:", "citation": "There seems to be a contradiction with the conclusion on the generative model focusing on StyleGAN #REFR : the model becomes more robust with more parameters.", "text_before_citation": ["One drawback of the vanilla Transformer model in style transfer is that the multi-layer structure can lead to difficult training convergence. As shown in Fig.", "16 , with more layers adopted, the loss may converge more slowly, and it even fails in the 5-layer case."], "text_after_citation": ["In fact, instead of generating new contents unconditionally in StyleGAN, style transfer aims to preserve contents and migrate style patterns at the same time.", "Stacking more layers in Transformer models may increase the complexity of the transfer function and tends to learn more abstract information.", "Thus, with more layers, it becomes harder to preserve original content structures during training.", "Sharing parameters for different layers kills three birds with one stone: it makes a light-weight, easy-to-train, and easy-to-control model."], "citing_paper_content": {"title": "Master: Meta Style Transformer For Controllable Zero-Shot And Few-Shot Artistic Style Transfer", "abstract": "specific style. Text-guided few-shot style transfer is firstly achieved with the proposed framework. Extensive experiments demonstrate the superiority of Master under both zero-shot and few-shot style transfer settings."}, "cited_paper_content": {"title": "Analyzing And Improving The Image Quality Of Stylegan", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."}, "keywords": ["StyleGAN"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One possible citation sentence that reflects the citation intent of \"Method\" and incorporates relevant keywords is:\n\nTo address the difficult training convergence issue in multi-layer style transfer models, the proposed Master framework leverages a path length regularizer that encourages good conditioning in the mapping from latent codes to images, as suggested in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed changes in model architecture and training methods in StyleGAN, including the redesigned generator normalization and the path length regularizer, have significantly improved the image quality and conditioning of the mapping from latent codes to images, as demonstrated in the state-of-the-art results achieved by Master in zero-shot and few-shot style transfer. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed changes in model architecture and training methods, including the redesign of generator normalization and regularization of the generator to encourage good conditioning in the mapping from latent codes to images, have significantly improved the image quality of StyleGAN, as demonstrated in the state-of-the-art results achieved by the model #REFR.", "given_citation_intent": "result", "given_keywords": ["StyleGAN"]}]}
{"citing_id": "2304.13597v1", "cited_id": "1201.0490", "section_title": "Results", "citation": "The similarity between groups is higher for shade than for bark, but the similarity within groups is higher The training and test subsets were randomly selected (using the train_test_split function with test_size=0.5 and random_state=0; sci-kit learn library; #REFR .", "text_before_citation": ["The proxigram overlays of bark and shade highlight similar differences in their group structures.", "Whereas the proxigram of the contextualized embeddings of shade identifies multiple interconnected groups, the one for bark highlights two separate groups with very few connections between them, supporting previous verbal descriptions of these lexicographic categories (e.g. Rodd, 2020).", "To evaluate quantitatively the degree to which the contextual embeddings for bark and shade contain meaningful, disambiguating information in the multidimensional space, we computed the intra-and inter-group cosine similarity of their contextual embeddings (with 'group' defined by translation labels; see Figure 4c ).", "This analysis revealed greater similarity within each group than between groups ( = \u22120.03, < 0.01, = \u22120.31, = \u2212186.27, < 0.001; = 0.50; \u210e = 0.56).", "Moreover, we found a significant interaction between group status and type of ambiguity on the similarity of contextual embeddings ( = \u22120.01, < 0.01, = \u22120.08, = \u221246.4, < 0.001)."], "text_after_citation": ["The classifier had an accuracy of 94% and 74% in distinguishing the meanings of bark and shade respectively.", "To avoid any potential for circularity (e.g.", "if the translation algorithm is based on technology similar to BERT), we repeated the classification analysis using ground truth labels derived from the human judgments described above.", "We took as ground truth the meanings selected for each context by at least two of our human raters (where this was not 'other') and", "excluded all other cases."], "citing_paper_content": {"title": "Shades Of Meaning 1 Shades Of Meaning: Uncovering The Geometry Of Ambiguous Word Representations Through Contextualised Language Models", "abstract": "Lexical ambiguity presents a profound and enduring challenge to the language sciences. Researchers for decades have grappled with the problem of how language users learn, represent and process words with more than one meaning. Our work offers new insight into psychological understanding of lexical ambiguity through a series of simulations that capitalise on recent advances in contextual language models. These models have no grounded understanding of the meanings of words at all; they simply learn to predict words based on the surrounding context provided by other words. Yet, our analyses show that their representations capture fine-grained meaningful distinctions between unambiguous, homonymous, and polysemous words that align with lexicographic classifications and psychological theorising. These findings provide quantitative support for modern psychological conceptualisations of lexical ambiguity and raise new challenges for understanding of the way that contextual information shapes the meanings of words across different timescales."}, "cited_paper_content": {"title": "Scikit-Learn: Machine Learning In Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."}, "keywords": ["similarity", "sci-kit learn library"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate quantitatively the degree to which the contextual embeddings for bark and shade contain meaningful, disambiguating information in the multidimensional space, we computed the intra-and inter-group cosine similarity of their contextual embeddings (with 'group' defined by translation labels; see Figure 4c). This analysis revealed greater similarity within each group than between groups ( = \u22120.03, < 0.01, = \u22120.31, = \u2212186.27, < 0.001; = 0.50; \u210e = 0.56). Moreover, we found a significant interaction between group status and type of ambiguity on the similarity of contextual embeddings ( = \u22120.01, < 0.01, = \u22120.08, = \u221246.4, < 0.001). The machine learning algorithms used in this analysis were implemented using the Scikit-Learn library #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compute the intra-and inter-group cosine similarity of contextual embeddings, the authors used a specific machine learning package, Scikit-Learn, which integrates a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compute the intra-and inter-group cosine similarity of contextual embeddings, the authors utilized the sci-kit learn library (#REFR). This analysis revealed greater similarity within each group than between groups, supporting the meaningful distinctions between unambiguous, homonymous, and polysemous words captured by the contextual embeddings, as well as the lexicographic classifications and psychological theorizing (keywords: similarity, sci-kit learn library).", "given_citation_intent": "method", "given_keywords": ["similarity", "sci-kit learn library"]}]}
{"citing_id": "2304.10020v1", "cited_id": "1801.08618", "section_title": "Dnn Partition", "citation": "JointDNN #REFR ] is a new method that allows computation on either platform for each layer independently of the other layers; this may allow one more partition point across the mobile device and cloud.", "text_before_citation": ["The DNN is split into parts at the granularity of layers, input data, or sub-layers.", "Layer Partition. In general, the DNN is split into two parts.", "Neurosurgeon #OTHEREFR , Edgent #OTHEREFR , JALAD #OTHEREFR , and PriPro #OTHEREFR are typical partition methods that split a DNN into two parts.", "Some DNN partition approaches #OTHEREFR are similar in that the output of the partition is only two parts.", "In addition, the DNN can be subdivided into two more parts at the granularity of layers."], "text_after_citation": ["However, compared with the general solutions, the approach in #OTHEREFR is more flexible with regard to the fine-grained DNN computation partitioning mechanism. The number of DNNs influences DNN partition.", "The joint optimization of multiple DNN partition and scheduling for mobile cloud #OTHEREFR splits each DNN into two parts at the granularity of each DNN layer and the different partition points.", "Moreover, to address the multiple DNN partition problem, a DNN partition strategy with layer partition operations is also considered to be efficient #OTHEREFR .", "Data Partition.", "DNN partition can be employed at the granularity of the input data."], "citing_paper_content": {"title": "A Survey On Deep Neural Network Partition Over Cloud, Edge And End Devices", "abstract": "\"Deep neural network (DNN) partition\" is a research problem that involves splitting a DNN into multiple parts and offloading them to specific locations. Because of the recent advancement in multi-access edge computing and edge intelligence, DNN partition has been considered as a powerful tool for improving DNN inference performance when the computing resources of edge and end devices are limited and the remote transmission of data from these devices to clouds is costly. This paper provides a comprehensive survey on the recent advances and challenges in DNN partition approaches over the cloud, edge, and end devices based on a detailed literature collection. We review how DNN partition works in various application scenarios, and provide a unified mathematical model of the DNN partition problem. We developed a five-dimensional classification framework for DNN partition approaches, consisting of deployment locations, partition granularity, partition constraints, optimization objectives, and optimization algorithms. Each existing DNN partition approache can be perfectly defined in this framework by instantiating each dimension into specific values. In addition, we suggest a set of metrics for comparing and evaluating the DNN partition approaches. Based on this, we identify and discuss research challenges that have not yet been investigated or fully addressed. We hope that this work helps DNN partition researchers by highlighting significant future research directions in this domain."}, "cited_paper_content": {"title": "Jointdnn: An Efficient Training And Inference Engine For Intelligent Mobile Cloud Computing Services", "abstract": "Deep learning models are being deployed in many mobile intelligent applications. End-side services, such as intelligent personal assistants, autonomous cars, and smart home services often employ either simple local models on the mobile or complex remote models on the cloud. However, recent studies have shown that partitioning the DNN computations between the mobile and cloud can increase the latency and energy efficiencies. In this paper, we propose an efficient, adaptive, and practical engine, JointDNN, for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase. JointDNN not only provides an energy and performance efficient method of querying DNNs for the mobile side but also benefits the cloud server by reducing the amount of its workload and communications compared to the cloud-only approach. Given the DNN architecture, we investigate the efficiency of processing some layers on the mobile device and some layers on the cloud server. We provide optimization formulations at layer granularity for forward- and backward-propagations in DNNs, which can adapt to mobile battery limitations and cloud server load constraints and quality of service. JointDNN achieves up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respectively."}, "keywords": ["partition point", "cloud"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The DNN can be subdivided into two more parts at the granularity of layers. JointDNN #REFR is an efficient and practical engine for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase, which achieves up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respectively.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To split a DNN into parts, JointDNN #REFR proposes an efficient, adaptive, and practical engine for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase, achieving up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "JointDNN is an efficient engine for collaborative computation between a mobile device and cloud for DNNs in both inference and training phase, which provides optimization formulations at layer granularity for forward- and backward-propagations in DNNs, and achieves up to 18 and 32 times reductions on the latency and mobile energy consumption of querying DNNs compared to the status-quo approaches, respectively, making it a valuable #REFR method for partition point and cloud.", "given_citation_intent": "method", "given_keywords": ["partition point", "cloud"]}]}
{"citing_id": "2303.07470v1", "cited_id": "1901.10351", "section_title": "B. Performance Results", "citation": "We also compare our results with an in-memory NVM accelerator #REFR where the M V M Dynamic operations are executed in the temporal 1-D SIMD lanes instead of the NVM tiles due to limited endurance.", "text_before_citation": ["With the proposed sequence blocking (SB=64) dataflow."], "text_after_citation": ["First, without sequence blocking, we observe upto 10x benefits on average for both GLUE and SQuAD benchmarks over GPU.", "This is because X-Former is a highly parallel spatial in-memory architecture.", "Moreover, we also observe improvements compared to a traditional NVM in-memory accelerator since the Attention Engine is more efficient in processing the MVM operation compared to the 1-D SIMD module (vector-vector operations).", "Next, we notice additional improvements with sequence blocking, upto 85x and 81x for GLUE and SQuAD benchmarks respectively because the Attention Engine is inactive only until the Projection Engine processes a subset of the input instead of the entire sequence length.", "Therefore, both the engines work efficiently with higher utilization, resulting in better throughput."], "citing_paper_content": {"title": "X-Former: In-Memory Acceleration Of Transformers", "abstract": "Transformers have achieved great success in a wide variety of natural language processing (NLP) tasks due to the attention mechanism, which assigns an importance score for every word relative to other words in a sequence. However, these models are very large, often reaching hundreds of billions of parameters, and therefore require a large number of DRAM accesses. Hence, traditional deep neural network (DNN) accelerators such as GPUs and TPUs face limitations in processing Transformers efficiently. In-memory accelerators based on non-volatile memory promise to be an effective solution to this challenge, since they provide high storage density while performing massively parallel matrix vector multiplications within memory arrays. However, attention score computations, which are frequently used in Transformers (unlike CNNs and RNNs), require matrix vector multiplications (MVM) where both operands change dynamically for each input. As a result, conventional NVM-based accelerators incur high write latency and write energy when used for Transformers, and further suffer from the low endurance of most NVM technologies. To address these challenges, we present X-Former, a hybrid in-memory hardware accelerator that consists of both NVM and CMOS processing elements to execute transformer workloads efficiently. To improve the hardware utilization of X-Former, we also propose a sequence blocking dataflow, which overlaps the computations of the two processing elements and reduces execution time. Across several benchmarks, we show that X-Former achieves upto 85x and 7.5x improvements in latency and energy over a NVIDIA GeForce GTX 1060 GPU and upto 10.7x and 4.6x improvements in latency and energy over a state-of-theart in-memory NVM accelerator."}, "cited_paper_content": {"title": "Puma: A Programmable Ultra-Efficient Memristor-Based Accelerator For Machine Learning Inference", "abstract": "Memristor crossbars are circuits capable of performing analog matrix-vector multiplications, overcoming the fundamental energy efficiency limitations of digital logic. They have been shown to be effective in special-purpose accelerators for a limited set of neural network applications. We present the Programmable Ultra-efficient Memristor-based Accelerator (PUMA) which enhances memristor crossbars with general purpose execution units to enable the acceleration of a wide variety of Machine Learning (ML) inference workloads. PUMA's microarchitecture techniques exposed through a specialized Instruction Set Architecture (ISA) retain the efficiency of in-memory computing and analog circuitry, without compromising programmability. We also present the PUMA compiler which translates high-level code to PUMA ISA. The compiler partitions the computational graph and optimizes instruction scheduling and register allocation to generate code for large and complex workloads to run on thousands of spatial cores. We have developed a detailed architecture simulator that incorporates the functionality, timing, and power models of PUMA's components to evaluate performance and energy consumption. A PUMA accelerator running at 1 GHz can reach area and power efficiency of 577 GOPS/s/mm 2 and 837~GOPS/s/W, respectively. Our evaluation of diverse ML applications from image recognition, machine translation, and language modelling (5M-800M synapses) shows that PUMA achieves up to 2,446\u00d7 energy and 66\u00d7 latency improvement for inference compared to state-of-the-art GPUs. Compared to an application-specific memristor-based accelerator, PUMA incurs small energy overheads at similar inference latency and added programmability."}, "keywords": ["memory"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed sequence blocking (SB=64) dataflow can overlap the computations of the two processing elements and reduce execution time, achieving significant improvements in latency and energy efficiency compared to state-of-the-art GPUs and NVM accelerators #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "With the proposed sequence blocking (SB=64) dataflow, X-Former achieves up to 85x and 7.5x improvements in latency and energy over a NVIDIA GeForce GTX 1060 GPU and up to 10.7x and 4.6x improvements in latency and energy over a state-of-the-art in-memory NVM accelerator, demonstrating significant performance gains in comparison to the state-of-the-art; #REFR provides a general-purpose accelerator that enhances memristor crossbars with general-purpose execution units to enable the acceleration of a wide variety of Machine Learning (ML) inference workloads.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of high write latency and write energy in NVM-based accelerators, X-Former proposes a hybrid in-memory hardware accelerator that consists of both NVM and CMOS processing elements, achieving up to 85x and 7.5x improvements in latency and energy over a NVIDIA GeForce GTX 1060 GPU and up to 10.7x and 4.6x improvements in latency and energy over a state-of-the-art in-memory #REFR accelerator, respectively, with the proposed sequence blocking (SB=64) dataflow and massively parallel matrix vector multiplications within memory arrays.", "given_citation_intent": "result", "given_keywords": ["memory"]}]}
{"citing_id": "2303.18083v2", "cited_id": "1412.1193", "section_title": "Natural Gradient Descent", "citation": "Finally and anecdotically, the NGD method can also be viewed as an approximate Newton method since the FIM and the GGN matrix are equivalent when the model predictive distribution P y|x (\u03b8) belongs to exponential family distributions #REFR .", "text_before_citation": ["This is useful in high-dimensional cases where the choice of parameters is more or less arbitrary.", "Strictly speaking, invariance with respect to parameters only occurs at the continuous level, i.e., in the limit of \u03b1 k \u2192 0.", "This minor drawback does not undermine the theoretical soundness of the method.", "In fact, the real drawback of the NGD (2.15)-(2.16) lies in the cost of computing and inverting the Fisher matrix.", "This is why it is capital to consider suitable approximations of the FIM such as KFAC (cf. \u00a72.3)."], "text_after_citation": [], "citing_paper_content": {"title": "Analysis And Comparison Of Two-Level Kfac Methods For Training Deep Neural Networks", "abstract": "As a second-order method, the Natural Gradient Descent (NGD) has the ability to accelerate training of neural networks. However, due to the prohibitive computational and memory costs of computing and inverting the Fisher Information Matrix (FIM), efficient approximations are necessary to make NGD scalable to Deep Neural Networks (DNNs). Many such approximations have been attempted. The most sophisticated of these is KFAC, which approximates the FIM as a block-diagonal matrix, where each block corresponds to a layer of the neural network. By doing so, KFAC ignores the interactions between different layers. In this work, we investigate the interest of restoring some low-frequency interactions between the layers by means of two-level methods. Inspired from domain decomposition, several two-level corrections to KFAC using different coarse spaces are proposed and assessed. The obtained results show that incorporating the layer interactions in this fashion does not really improve the performance of KFAC. This suggests that it is safe to discard the off-diagonal blocks of the FIM, since the block-diagonal approach is sufficiently robust, accurate and economical in computation time."}, "cited_paper_content": {"title": "New Insights And Perspectives On The Natural Gradient Method", "abstract": "Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of approximate 2nd-order optimization method, where the Fisher information matrix can be viewed as an approximation of the Hessian. This perspective turns out to have significant implications for how to design a practical and robust version of the method. Additionally, we make the following contributions to the understanding of natural gradient and 2nd-order methods: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used \"empirical\" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods, which we show still holds for certain choices of the curvature matrix other than the Fisher, but notably not the Hessian."}, "keywords": ["approximate Newton method"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The real drawback of the NGD (2.15)-(2.16) lies in the cost of computing and inverting the Fisher matrix. This is why it is capital to consider suitable approximations of the FIM such as KFAC (cf. \u00a72.3) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the computational and memory costs of computing and inverting the Fisher Information Matrix (FIM) in Natural Gradient Descent (NGD), suitable approximations of the FIM such as KFAC have been proposed and used in high-dimensional cases where the choice of parameters is arbitrary (New Insights And Perspectives On The Natural Gradient Method). #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the computational burden of computing and inverting the Fisher matrix, many efficient approximations have been proposed, including the sophisticated KFAC method that approximates the FIM as a block-diagonal matrix, but recent research has also explored the use of approximate Newton methods #REFR.", "given_citation_intent": "method", "given_keywords": ["approximate Newton method"]}]}
{"citing_id": "2303.08824v1", "cited_id": "1810.03961", "section_title": "Ii. System Model", "citation": "As revealed by #REFR , the optimal value of reflecting attenuation is \u03b2 sn = 1, \u2200s, n to maximize the received power and simplify the hardware implementation.", "text_before_citation": ["In contrast to hand-held UE, a surface mounted on a vehicle is generally high enough to get the line-of-sight (LOS) path of the BS without any blockage, resulting in Rician fading, i.e.,", "EQUATION", "with the Rician factor \u0393, the LOS component H LOS , the multipath component H N LOS consisting of independent entries that follow CN (0, 1), and the BS-IRVS path loss \u03c3 2 s , s \u2208 S.", "Each reflecting surface is equipped with a smart controller, which can adaptively adjust the phase shift of each element in terms of the CSI acquired through periodic channel estimation #OTHEREFR .", "We write c sn = \u03b2 sn e j\u03c6sn to denote the reflection coefficient of the n th element of the s th surface, with the induced phase shift \u03c6 sn \u2208 [0, 2\u03c0) and the amplitude attenuation \u03b2 sn \u2208 [0, 1]."], "text_after_citation": ["Because of severe path loss, the signals that are reflected by the IRVS twice or more are negligible.", "By ignoring impairments such as channel aging #OTHEREFR and phase noise #OTHEREFR , the k th UE observes the received signal", "EQUATION", "where x denotes the vector of transmitted signals over the BS antenna array, P d expresses the power constraint of the BS, n k is additive white Gaussian noise (AWGN) with zero mean and variance \u03c3 2 n , namely n k \u223c CN (0, \u03c3 2 n ).", "Define the diagonal phase-shift matrix for surface s as \u03a6 s = diag e j\u03c6s1 , e j\u03c6s2 , . . . , e j\u03c6 sNs , \u2200s \u2208 S, Fig. 2 . The structure of a TDMA frame."], "citing_paper_content": {"title": "Intelligent Reflecting Vehicle Surface: A Novel Irs Paradigm For Moving Vehicular Networks", "abstract": "Intelligent reflecting surface (IRS) has recently received much attention from the research community due to its potential to achieve high spectral and power efficiency costeffectively. In addition to traditional cellular networks, the use of IRS in vehicular networks is also considered. Prior works on IRSaided vehicle-to-everything communications focus on deploying reflection surfaces on the facades of buildings along the road for sidelink performance enhancement. This paper goes beyond the state of the art by presenting a novel paradigm coined Intelligent Reflecting Vehicle Surface (IRVS). It embeds a massive number of reflection elements on vehicles' surfaces to aid moving vehicular networks in military and emergency communications. We propose an alternative optimization method to optimize jointly active beamforming at the base station and passive reflection across multiple randomly-distributed vehicle surfaces. Performance evaluation in terms of sum spectral efficiency under continuous, discrete, and random phase shifts is conducted. Numerical results reveal that IRVS can substantially improve the capacity of a moving vehicular network."}, "cited_paper_content": {"title": "Intelligent Reflecting Surface Enhanced Wireless Network Via Joint Active And Passive Beamforming", "abstract": "Intelligent reflecting surface (IRS) is a revolutionary and transformative technology for achieving spectrum and energy efficient wireless communication cost-effectively in the future. Specifically, an IRS consists of a large number of low-cost passive elements each being able to reflect the incident signal independently with an adjustable phase shift so as to collaboratively achieve three-dimensional (3D) passive beamforming without the need of any transmit radio-frequency (RF) chains. In this paper, we study an IRS-aided single-cell wireless system where one IRS is deployed to assist in the communications between a multi-antenna access point (AP) and multiple single-antenna users. We formulate and solve new problems to minimize the total transmit power at the AP by jointly optimizing the transmit beamforming by active antenna array at the AP and reflect beamforming by passive phase shifters at the IRS, subject to users\u2019 individual signal-to-interference-plus-noise ratio (SINR) constraints. Moreover, we analyze the asymptotic performance of IRS\u2019s passive beamforming with infinitely large number of reflecting elements and compare it to that of the traditional active beamforming/relaying. Simulation results demonstrate that an IRS-aided MIMO system can achieve the same rate performance as a benchmark massive MIMO system without using IRS, but with significantly reduced active antennas/RF chains. We also draw useful insights into optimally deploying IRS in future wireless systems."}, "keywords": ["attenuation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Each reflecting surface is equipped with a smart controller, which can adaptively adjust the phase shift of each element in terms of the CSI acquired through periodic channel estimation ( #REFR ). We write c sn = \u03b2 sn e j\u03c6sn to denote the reflection coefficient of the n th element of the s th surface, with the induced phase shift \u03c6 sn \u2208 [0, 2\u03c0) and the amplitude attenuation \u03b2 sn \u2208 [0, 1].", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Intelligent reflecting surface (IRS) is a transformative technology that can achieve spectrum and energy efficient wireless communication cost-effectively, and the recent study on an IRS-aided single-cell wireless system in #REFR demonstrates its potential to minimize the total transmit power at the access point by jointly optimizing the transmit beamforming by active antenna array at the access point and reflect beamforming by passive phase shifters at the IRS.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The reflection coefficient of each element in the Intelligent Reflecting Vehicle Surface (IRVS) can be adaptively adjusted through periodic channel estimation, as described in #REFR, with the induced phase shift \u03c6sn \u2208 [0, 2\u03c0) and the amplitude attenuation \u03b2sn \u2208 [0, 1].", "given_citation_intent": "background", "given_keywords": ["attenuation"]}]}
{"citing_id": "2304.09244v1", "cited_id": "1710.10196", "section_title": "Generative Adversarial Networks (Gans).", "citation": "However, recent advances in GANs architecture and training techniques have shown promise in addressing these issues and improving the stability and performance of GANs #REFR .", "text_before_citation": ["In other words, each neural network tries to minimize its cost.", "In practice, GANs is commonly trained using gradient-based optimization on and , alternatively.", "Despite GANs exhibiting the capacity to produce realistic and high-quality images, GANs is notoriously difficult to train due to the adversarial nature of the training process.", "GANs requires and to compete with each other, which can lead to \"mode collapse\", i.e.", "finds a way to generate only a limited ranges of outputs to mislead . GANs is also sensitive to hyperparameters."], "text_after_citation": ["Variational Autoencoders (VAEs).", "Generative process using a decoder from simple trained autoencoders (AEs) is extremely difficult since its latent space is normally not \"well-organized\".", "Contrary to encoder in AEs (encoding an input to a single point in the latent space), VAEs [Kingma and Welling 2013] encodes an input as a distribution.", "VAEs is then trained along with a regularization term (Eq. 2), i.e. Kullback-Leibler (KL) divergence, to provide exploitable latent space.", "Formally, given x representing the input data, and z representing the compressed latent space representation."], "citing_paper_content": {"title": "Text-Guided Image-And-Shape Editing And Generation: A Short Survey", "abstract": "Image and shape editing are ubiquitous among digital artworks. Graphics algorithms facilitate artists and designers to achieve desired editing intents without going through manually tedious retouching. In the recent advance of machine learning, artists' editing intents can even be driven by text, using a variety of well-trained neural networks. They have seen to be receiving an extensive success on such as generating photorealistic images, artworks and human poses, stylizing meshes from text, or auto-completion given image and shape priors. In this short survey, we provide a overview over 50 papers on state-of-the-art (text-guided) image-and-shape generation techniques. We start with an overview on recent editing algorithms in the introduction. Then, we provide a comprehensive review on text-guided editing techniques for 2D and 3D independently, where each of its subsection begins with a brief background introduction. We also contextualize editing algorithms under recent implicit neural representations. Finally, we conclude the survey with the discussion over existing methods and potential research ideas."}, "cited_paper_content": {"title": "Progressive Growing Of Gans For Improved Quality, Stability, And Variation", "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset."}, "keywords": ["GANs"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Despite the difficulty in training GANs due to the adversarial nature of the process, recent research has proposed a new training methodology that greatly stabilizes and speeds up the training process, resulting in unprecedented image quality and variation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The adversarial nature of the training process in GANs can lead to \"mode collapse\", where the generator finds a way to generate only a limited range of outputs to mislead the discriminator. To address this issue, a new training methodology for GANs was proposed in #REFR, which progressively grows both the generator and discriminator, allowing for the modeling of increasingly fine details as training progresses and producing images of unprecedented quality.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The adversarial nature of training GANs, where the generator and discriminator compete with each other, can lead to \"mode collapse\" and sensitivity to hyperparameters, making GANs notoriously difficult to train. As a solution, the authors of #REFR proposed a new training methodology for GANs, which progressively grows both the generator and discriminator, stabilizing the training process and allowing for the production of high-quality images.", "given_citation_intent": "background", "given_keywords": ["GANs"]}]}
{"citing_id": "2304.09181v1", "cited_id": "1810.04805", "section_title": "Characterization Of Models", "citation": "As we can see in Table 10 , BERT #REFR have the highest Precision among the models we have tested.", "text_before_citation": ["We have used three different pretrained models and collected their Precision, Recall and F1 score."], "text_after_citation": ["BERT has Precision value of 0.92, compared to Precision value of 0.91 for both BERT Tiny and GPT #OTHEREFR models.", "In case of identifying the fraction of relevant items that can be retrieved, i.e. the Recall score,", "Language Model Precision Recall F1 score BERT Tiny 0.91 0.82 0.86 GPT #OTHEREFR 0.91 0.80 0.85 BERT 0.92 0.81 0.86 Table 10 .", "Model characterization with pre-trained language models BERT Tiny performs slightly higher 0.82 than the BERT model 0.81.", "GPT model has the lowest Recall score of 0.80 among these three models."], "citing_paper_content": {"title": "Large Language Models Based Automatic Synthesis Of Software Specifications", "abstract": "Software configurations play a crucial role in determining the behavior of software systems. In order to ensure safe and error-free operation, it is necessary to identify the correct configuration, along with their valid bounds and rules, which are commonly referred to as software specifications. As software systems grow in complexity and scale, the number of configurations and associated specifications required to ensure the correct operation can become large and prohibitively difficult to manipulate manually. Due to the fast pace of software development, it is often the case that correct software specifications are not thoroughly checked or validated within the software itself. Rather, they are frequently discussed and documented in a variety of external sources, including software manuals, code comments, and online discussion forums. Therefore, it is hard for the system administrator to know the correct specifications of configurations due to the lack of clarity, organization, and a centralized unified source to look at. To address this challenge, we propose SpecSyn a framework that leverages a state-of-the-art large language model to automatically synthesize software specifications from natural language sources. Our approach formulates software specification synthesis as a sequence-to-sequence learning problem and investigates the extraction of specifications from large contextual texts. This is the first work that uses a large language model for end-to-end specification synthesis from natural language texts. Empirical results demonstrate that our system outperforms prior state-of-the-art specification synthesis tool by 21% in terms of F1 score and can find specifications from single as well as multiple sentences."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["highest Precision", "BERT"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We have used three different pretrained models and collected their Precision, Recall and F1 score. To pre-train our models, we leveraged the Bidirectional Encoder Representations from Transformers (BERT) model introduced in the reference paper #REFR, which has shown state-of-the-art performance on a wide range of natural language processing tasks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We have used three different pretrained models and collected their Precision, Recall and F1 score. Our results demonstrate the effectiveness of our approach, with an F1 score outperforming the prior state-of-the-art specification synthesis tool by 21% #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We have used three different pretrained models and collected their Precision, Recall and F1 score. Our results show that the model based on BERT achieved the highest Precision, outperforming the other models. #REFR", "given_citation_intent": "result", "given_keywords": ["highest Precision", "BERT"]}]}
{"citing_id": "2304.04774v1", "cited_id": "1605.08803", "section_title": "B. Generative Model", "citation": "Some notable contributions in the field of flow-based models include the Real-valued Non-Volume-Preserving (Real NVP) model, introduced by Dinh et al #REFR , which proposed a flexible and efficient way to construct invertible transformations using neural networks.", "text_before_citation": ["Ma et al.", "#OTHEREFR firstly introduced GANs into pansharpening and performed unsupervised learning of spectral and spatial information based on the reversed loss function upon the network's output.", "Zero-GAN #OTHEREFR also relies on GAN and constructed a non-reference loss function, including an adversarial loss, spatial and spectral reconstruction losses, a spatial enhancement loss, and an average constancy loss to specifically supervise the learning process.", "Flow-based model is a type of generative model that learns the underlying distribution of data by transforming a simple input distribution into the target distribution through a series of invertible transformations.", "This is achieved by representing the probability density function of the target distribution as a sequence of flows, where each flow is an invertible transformation."], "text_after_citation": ["Another important contribution is Glow #OTHEREFR , which improved upon previous flow-based models by introducing an invertible 1\u00d71 convolutional operation that greatly increased the expressiveness of the model.", "Diffusion model are recently proposed for generation including conditional or unconditional generation #OTHEREFR , #OTHEREFR , #OTHEREFR , text-to-image translation #OTHEREFR , #OTHEREFR , image superresolution #OTHEREFR , image restoration #OTHEREFR , #OTHEREFR and other highlevel image manipulation tasks #OTHEREFR , #OTHEREFR , #OTHEREFR .", "Diffusion models can generate images with more details and higher fidelity than the GAN-based model and flow-based model.", "The advantages of diffusion models include stable training, minimal mode collapse, and the ability to train with only a single mean squared error (MSE) loss.", "In comparison to GAN-based models, which suffer from instability issues in their adversarial training, and flow-based models, which are limited in their network performance due to the requirement for reversibility, diffusion models are easier to train and design."], "citing_paper_content": {"title": "Ddrf: Denoising Diffusion Model For Remote Sensing Image Fusion", "abstract": "Denosing diffusion model, as a generative model, has received a lot of attention in the field of image generation recently, thanks to its powerful generation capability. However, diffusion models have not yet received sufficient research in the field of image fusion. In this article, we introduce diffusion model to the image fusion field, treating the image fusion task as imageto-image translation and designing two different conditional injection modulation modules (i.e., style transfer modulation and wavelet modulation) to inject coarse-grained style information and fine-grained high-frequency and low-frequency information into the diffusion UNet, thereby generating fused images. In addition, we also discussed the residual learning and the selection of training objectives of the diffusion model in the image fusion task. Extensive experimental results based on quantitative and qualitative assessments compared with benchmarks demonstrates state-of-the-art results and good generalization performance in image fusion tasks. Finally, it is hoped that our method can inspire other works and gain insight into this field to better apply the diffusion model to image fusion tasks. Code shall be released for better reproducibility."}, "cited_paper_content": {"title": "Density Estimation Using Real Nvp", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations."}, "keywords": ["neural networks", "Real-valued Non-Volume-Preserving (Real"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Flow-based models, such as real NVP, have been shown to be powerful generative models that learn the underlying distribution of data by transforming a simple input distribution into the target distribution through a series of invertible transformations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Flow-based models, such as real NVP, have been shown to be powerful and tractable for unsupervised learning of probabilistic models, with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Flow-based models, such as Real-valued Non-Volume-Preserving (Real NVP) transformations, have been shown to be powerful and invertible neural networks that can learn the underlying distribution of data, which could be useful for image fusion tasks (Ma et al. #REFR).", "given_citation_intent": "background", "given_keywords": ["neural networks", "Real-valued Non-Volume-Preserving (Real"]}]}
{"citing_id": "2304.09319v2", "cited_id": "0804.2543", "section_title": ". (3.10)", "citation": "Again, expressions (3.11) and (3.12) yield highly accurate numerical values using Bornemann's approach #REFR .", "text_before_citation": ["For a random matrix that has its eigenvalue n-point correlation function given as in Proposition 1.1 with a kernel K, the CDF and PDF of the second largest eigenvalue \u03bb 2 are", "F \u03bb2 (s) = tr (I \u2212 K) \u22121 K (s,\u221e) \u2022 det(I \u2212 K (s,\u221e) ), (3.11) f \u03bb2 (s) = K(s, s) \u2022 tr (I \u2212 K (s) ) \u22121 K (s) (s,\u221e) \u2022 det(I \u2212 K (s) (s,\u221e) ), (3.12)", "where the kernel K (s) is given as (1.4).", "Proposition 3.2 could be used with the Airy kernel (soft-edge) as well as finite N kernels like the Hermite kernel.", "In the hard-edge scaling the interval (0, s) is used instead of (s, \u221e) with the Bessel kernel for the second smallest eigenvalue."], "text_after_citation": ["Table 4 is the computed first four moments of the two smallest eigenvalues (\u03bb 1 < \u03bb 2 ) in the hard-edge and soft-edge using PDF formulas of Section 3.2 and (3.12)."], "citing_paper_content": {"title": "The Conditional Dpp Approach To Random Matrix Distributions", "abstract": "We present the conditional determinantal point process (DPP) approach to obtain new (mostly Fredholm determinantal) expressions for various eigenvalue statistics in random matrix theory. It is well-known that many (especially \u03b2 = 2) eigenvalue n-point correlation functions are given in terms of n \u00d7 n determinants, i.e., they are continuous DPPs. We exploit a derived kernel of the conditional DPP which gives the n-point correlation function conditioned on the event of some eigenvalues already existing at fixed locations. Using such kernels we obtain new determinantal expressions for the joint densities of the k largest eigenvalues, probability density functions of the k th largest eigenvalue, density of the first eigenvalue spacing, and more. Our formulae are highly amenable to numerical computations and we provide various numerical experiments. Several numerical values that required hours of computing time could now be computed in seconds with our expressions, which proves the effectiveness of our approach. We also demonstrate that our technique can be applied to an efficient sampling of DR paths of the Aztec diamond domino tiling. Further extending the conditional DPP sampling technique, we sample Airy processes from the extended Airy kernel. Additionally we propose a sampling method for non-Hermitian projection DPPs."}, "cited_paper_content": {"title": "On The Numerical Evaluation Of Fredholm Determinants", "abstract": "Some significant quantities in mathematics and physics are most naturally expressed as the Fredholm determinant of an integral operator, most notably many of the distribution functions in random matrix theory. Though their numerical values are of interest, there is no systematic numerical treatment of Fredholm determinants to be found in the literature. Instead, the few numerical evaluations that are available rely on eigenfunction expansions of the operator, if expressible in terms of special functions, or on alternative, numerically more straightforwardly accessible analytic expressions, e.g., in terms of Painleve transcendents, that have masterfully been derived in some cases. In this paper we close the gap in the literature by studying projection methods and, above all, a simple, easily implementable, general method for the numerical evaluation of Fredholm determinants that is derived from the classical Nystrom method for the solution of Fredholm equations of the second kind. Using Gauss-Legendre or Clenshaw-Curtis as the underlying quadrature rule, we prove that the approximation error essentially behaves like the quadrature error for the sections of the kernel. In particular, we get exponential convergence for analytic kernels, which are typical in random matrix theory. The application of the method to the distribution functions of the Gaussian unitary ensemble (GUE), in the bulk and the edge scaling limit, is discussed in detail. After extending the method to systems of integral operators, we evaluate the two-point correlation functions of the more recently studied Airy and Airy1 processes for the first time."}, "keywords": ["highly accurate numerical"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors present a simple and easily implementable method for the numerical evaluation of Fredholm determinants, which is derived from the classical Nystrom method for the solution of Fredholm equations of the second kind. Using Gauss-Legendre or Clenshaw-Curtis as the underlying quadrature rule, they prove that the approximation error essentially behaves like the quadrature error for the sections of the kernel. In particular, they get exponential convergence for analytic kernels, which are typical in random matrix theory. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To numerically evaluate Fredholm determinants, projection methods and a simple, easily implementable, general method derived from the classical Nystrom method for the solution of Fredholm equations of the second kind can be used, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The numerical evaluation of Fredholm determinants can be achieved with highly accurate numerical methods, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["highly accurate numerical"]}]}
{"citing_id": "2303.00634v1", "cited_id": "2001.09306", "section_title": "1) Radar Channel Parameter Estimation:", "citation": "Therefore, we propose the following theorem to further approximate the CRLBs in #REFR , thus obtaining the simplified expressions.", "text_before_citation": ["By substituting #OTHEREFR into 23, we have", "EQUATION", "Thus, the aged CRLBs on the predicted MSEs of \u03b8 k,n , distance d k,n , and", "EQUATION", "However, considering the matrix multiplication of polynomials inG k,n\u22121 and\u1e20 k,n,i , it is still hard to apply the aged radar tracking performance in #OTHEREFR to study the resource allocation scheme design."], "text_after_citation": ["Theorem 3.2: The aged CRLBs on the predicted MSEs of \u03b8 k,n , distance d k,n , and velocity v k,n for U R k in block n derived in (27) can be approximated by", "EQUATION", "EQUATION", "where", "a k,n = 1 + n\u22121 i=1v k,i T d k,i cos(\u03b8 k,i ) 2 , b k,n = 1 + n\u22121 i=2 1 + n\u22121 i \u2032 =iv k,i \u2032 T d k,i \u2032 cos(\u03b8 k,i \u2032 ) 2 , and\u03b8 k,n =\u03b8 k,n \u2212\u03d5 k ."], "citing_paper_content": {"title": "Impact Of Channel Aging On Dual-Function Radar-Communication Systems: Performance Analysis And Resource Allocation", "abstract": "In conventional dual-function radar-communication (DFRC) systems, the radar and communication channels are routinely estimated at fixed time intervals based on their worst-case operation scenarios. Such situation-agnostic repeated estimations cause significant training overhead and dramatically degrade the system performance, especially for applications with dynamic sensing/communication demands and limited radio resources. In this paper, we leverage the channel aging characteristics to reduce training overhead and to design a situation-dependent channel re-estimation interval optimization-based resource allocation for performance improvement in a multi-target tracking DFRC system. Specifically, we exploit the channel temporal correlation to predict radar and communication channels for reducing the need of training preamble retransmission. Then, we characterize the channel aging effects on the Cramer-Rao lower bounds (CRLBs) for radar tracking performance analysis and achievable rates with maximum ratio transmission (MRT) and zero-forcing (ZF) transmit beamforming for communication performance analysis. In particular, the aged CRLBs and achievable rates are derived as closed-form expressions with respect to the channel aging time, bandwidth, and power. Based on the analyzed results, we optimize these factors to maximize the average total aged achievable rate subject to individual target tracking precision demand, communication rate requirement, and other practical constraints. Since the formulated problem belongs to a non-convex problem, we develop an efficient one-dimensional search based optimization algorithm to obtain its suboptimal solutions. Finally, simulation results are presented to validate the correctness of the derived theoretical results and the effectiveness of the proposed allocation scheme."}, "cited_paper_content": {"title": "Radar-Assisted Predictive Beamforming For Vehicular Links: Communication Served By Sensing", "abstract": "In vehicular networks of the future, sensing and communication functionalities will be intertwined. In this paper, we investigate a radar-assisted predictive beamforming design for vehicle-to-infrastructure (V2I) communication by exploiting the dual-functional radar-communication (DFRC) technique. Aiming for realizing joint sensing and communication functionalities at road side units (RSUs), we present a novel extended Kalman filtering (EKF) framework to track and predict kinematic parameters of each vehicle. By exploiting the radar functionality of the RSU we show that the communication beam tracking overheads can be drastically reduced. To improve the sensing accuracy while guaranteeing the downlink communication sum-rate, we further propose a power allocation scheme for multiple vehicles. Numerical results have shown that the proposed DFRC based beam tracking approach significantly outperforms the communication-only feedback based technique in the tracking performance. Furthermore, the designed power allocation method is able to achieve a favorable performance trade-off between sensing and communication."}, "keywords": ["simplified expressions"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "However, considering the matrix multiplication of polynomials in G k,n\u22121 and \u1e20 k,n,i, it is still hard to apply the aged radar tracking performance in the reference paper for V2I communication based on radar-assisted predictive beamforming. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed radar-assisted predictive beamforming design for vehicle-to-infrastructure (V2I) communication in #REFR is a promising approach to reduce communication beam tracking overheads and improve sensing accuracy while guaranteeing the downlink communication sum-rate.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed extended Kalman filtering framework in #REFR can effectively track and predict kinematic parameters of each vehicle, which simplifies the expressions of the aged CRLBs on the predicted MSEs of \u03b8 k,n, distance d k,n, and EQUATION, and reduces the communication beam tracking overheads.", "given_citation_intent": "background", "given_keywords": ["simplified expressions"]}]}
{"citing_id": "2303.14115v1", "cited_id": "1502.03167", "section_title": "Normalization Layers", "citation": "Batch Normalization (BN) #REFR is the most common CNN normalization layer, which normalizes layer inputs using moments across the mini-batch dimension.", "text_before_citation": ["Normalization layers are essential in training Convolutional Neural Networks (CNNs), as they address the internal covariate shift of the network by normalizing the inputs to layers, so that the input distributions to each layer are stable during training #OTHEREFR ."], "text_after_citation": ["However, to achieve deterministic behavior during inference, the mini-batch variance and mean are replaced by the global population mean and variance, which are obtained during training using an exponential running average. This works for i.i.d. #OTHEREFR data, but in non-i.i.d.", "incremental learning, the BN estimates of the population mean and variance are heavily biased towards the most recent task, resulting in a significant loss of performance on old tasks #OTHEREFR .", "Continual Normalization (CN) alleviates this discrepancy by combining Group- #OTHEREFR and Batch Normalization #OTHEREFR ."], "citing_paper_content": {"title": "Principles Of Forgetting In Domain-Incremental Semantic Segmentation In Adverse Weather Conditions", "abstract": "Deep neural networks for scene perception in automated vehicles achieve excellent results for the domains they were trained on. However, in real-world conditions, the domain of operation and its underlying data distribution are subject to change. Adverse weather conditions, in particular, can significantly decrease model performance when such data are not available during training. Additionally, when a model is incrementally adapted to a new domain, it suffers from catastrophic forgetting, causing a significant drop in performance on previously observed domains. Despite recent progress in reducing catastrophic forgetting, its causes and effects remain obscure. Therefore, we study how the representations of semantic segmentation models are affected during domain-incremental learning in adverse weather conditions. Our experiments and representational analyses indicate that catastrophic forgetting is primarily caused by changes to low-level features in domain-incremental learning and that learning more general features on the source domain using pre-training and image augmentations leads to efficient feature reuse in subsequent tasks, which drastically reduces catastrophic forgetting. These findings highlight the importance of methods that facilitate generalized features for effective continual learning algorithms."}, "cited_paper_content": {"title": "Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}, "keywords": ["common CNN normalization"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Normalization layers are essential in training Convolutional Neural Networks (CNNs), as they address the internal covariate shift of the network by normalizing the inputs to layers, so that the input distributions to each layer are stable during training ( #OTHEREFR ). To achieve this, the authors of Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate Shift proposed a method that normalizes layer inputs as part of the model architecture and performs the normalization for each training mini-batch, allowing for higher learning rates and less careful initialization, and in some cases eliminating the need for Dropout ( #REFR ).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Normalization layers are essential in training Convolutional Neural Networks (CNNs), as they address the internal covariate shift of the network by normalizing the inputs to layers, so that the input distributions to each layer are stable during training. Batch Normalization, a method that draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch, has been shown to allow the use of much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Common CNN normalization techniques, such as batch normalization, have been shown to stabilize the input distributions to each layer during training, which is essential for effective training of Convolutional Neural Networks (CNNs) #REFR.", "given_citation_intent": "background", "given_keywords": ["common CNN normalization"]}]}
{"citing_id": "2304.11821v1", "cited_id": "1412.6980", "section_title": "B. Metrics And Implementation", "citation": "We use Adam optimizer #REFR and the learning rate is set to 0.002 initially.", "text_before_citation": ["To evaluate the performance of cooperative perception with communication interruption, we test the detection results with different V2X packet drop rates (PDRs) (0 \u223c 90%).", "The detection results are evaluated by Average Precision (AP) at Intersection-over-Union (IoU) thresholds of 0.50 and 0.70.", "To implement V2X-INCOP, we set the number of the past timestep k = 1, the width, length, and height of each voxel of the input to (0.4m, 0.4m, 4m) for OPV2V and Dair-V2X and (0.4m, 0.4m, 5m) for V2X-Sim.", "In terms of coefficients in the loss function, we set \u03b1 = 1, \u03b2 = 2 and \u03b3 = 10000."], "text_after_citation": ["For a fair comparison, we implement all baselines and SOTA methods with Pointpillars, the same backbone as our proposed V2X-INCOP.", "We train all the models using NVIDIA GeForce RTX 3090 GPU."], "citing_paper_content": {"title": "Interruption-Aware Cooperative Perception For V2X Communication-Aided Autonomous Driving", "abstract": "Cooperative perception enabled by V2X Communication technologies can significantly improve the perception performance of autonomous vehicles beyond the limited perception ability of the individual vehicles, therefore, improving the safety and efficiency of autonomous driving in intelligent transportation systems. However, in order to fully reap the benefits of cooperative perception in practice, the impacts of imperfect V2X communication, i.e., communication errors and disruptions, need to be understood and effective remedies need to be developed to alleviate their adverse impacts. Motivated by this need, we propose a novel INterruption-aware robust COoperative Perception (V2X-INCOP) solution for V2X communication-aided autonomous driving, which leverages historical information to recover missing information due to interruption. To achieve comprehensive recovery, we design a communication adaptive multi-scale spatial-temporal prediction model to extract multiscale features based on V2X communication conditions and capture the most significant information for the prediction of the missing information. To further improve recovery performance, we adopt a knowledge distillation framework to give direct supervision to the prediction model and a curriculum learning strategy to stabilize the training of the model. Our experiments on three public cooperative perception datasets demonstrate that our proposed method is effective in alleviating the impacts of communication interruption on cooperative perception. Index Terms-cooperative perception, 3D object detection, autonomous driving I. INTRODUCTION Autonomous Vehicles (AVs) and Automated Driving Systems (ADSs) promise to improve the safety, efficiency, and accessibility of future transport systems. AVs and ADSs rely on accurate and robust understanding of their surroundings, i.e., perception, for safe, deficient, and comfortable motion planning [60]. Poor perception can result in traffic accidents, threatening human safety [55], [56], and other forms of"}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["learning rate", "Adam optimizer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To implement V2X-INCOP, we set the number of the past timestep k = 1, the width, length, and height of each voxel of the input to (0.4m, 0.4m, 4m) for OPV2V and Dair-V2X and (0.4m, 0.4m, 5m) for V2X-Sim, and in terms of coefficients in the loss function, we set \u03b1 = 1, \u03b2 = 2 and \u03b3 = 10000. The method we used for optimization is Adam, which has been shown to work well in practice and compares favorably to other stochastic optimization methods #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve comprehensive recovery from communication interruption, the proposed INterruption-aware robust COoperative Perception (V2X-INCOP) solution leverages a communication adaptive multi-scale spatial-temporal prediction model based on adaptive estimates of lower-order moments, as introduced in the Adam method by Kingma and Ba #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the performance of V2X-INCOP, the authors implemented the Adam optimizer with a learning rate and hyper-parameters that require little tuning, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["learning rate", "Adam optimizer"]}]}
{"citing_id": "2304.12776v1", "cited_id": "1906.04284", "section_title": "Why Does S4 Perform Well On Lm But Not Mt?", "citation": "This is in line with the observations made by #REFR , who argue that Transformer LMs tend to pay more attention to the previous few tokens, emphasizing the importance of short-term memory over long-term memory.", "text_before_citation": ["A natural question to ask is why does S4 perform well on LM #OTHEREFR , but not on MT.", "Our intuition is that MT is a more challenging task.", "For LM, the model only needs to consider a shorter context to accurately predict the next token, whereas Table 6 : BLEU scores on test set for each architecture in 4 different language pairs. The \u2020 on TR-S4A indicates statistically significant results.", "for MT, it requires accurate access to the source sentence representations.", "As the length of the source sentence increases, a fixed-size state is insufficient to capture fine-grained representations of the source, and thus the model's performance suffers."], "text_after_citation": [], "citing_paper_content": {"title": "State Spaces Aren'T Enough: Machine Translation Needs Attention", "abstract": "Structured State Spaces for Sequences (S4) is a recently proposed sequence model with successful applications in various tasks, e.g. vision, language modeling, and audio. Thanks to its mathematical formulation, it compresses its input to a single hidden state, and is able to capture long range dependencies while avoiding the need for an attention mechanism. In this work, we apply S4 to Machine Translation (MT), and evaluate several encoder-decoder variants on WMT'14 and WMT'16. In contrast with the success in language modeling, we find that S4 lags behind the Transformer by approximately 4 BLEU points, and that it counter-intuitively struggles with long sentences. Finally, we show that this gap is caused by S4's inability to summarize the full source sentence in a single hidden state, and show that we can close the gap by introducing an attention mechanism."}, "cited_paper_content": {"title": "Analyzing The Structure Of Attention In A Transformer Language Model", "abstract": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads."}, "keywords": ["long-term memory"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As the length of the source sentence increases, a fixed-size state is insufficient to capture fine-grained representations of the source, and thus the model's performance suffers. One possible solution to this problem is to introduce an attention mechanism, as demonstrated in the GPT-2 small pretrained model #REFR, which analyzes the structure of attention in a Transformer language model and reveals highly specific patterns targeted by particular attention heads.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We find that S4's inability to summarize the full source sentence in a single hidden state causes a performance gap in MT, which can be closed by introducing an attention mechanism #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We find that a fixed-size state is insufficient to capture fine-grained representations of the source, especially for long sentences, which causes the model's performance to suffer #REFR. The reference paper shows that attention targets different parts of speech at different layer depths within the model and that the deepest layers of the model capture the most distant relationships, which could potentially address the issue of long-term memory.", "given_citation_intent": "result", "given_keywords": ["long-term memory"]}]}
{"citing_id": "2304.02089v1", "cited_id": "1602.01585", "section_title": "Dataset", "citation": "Following the strategy in References #REFR , we extracted the users' product purchasing behaviors based on their reviews, i.e., the products they reviewed are the ones they purchased.", "text_before_citation": ["Negative samples in the training dataset are set to those impressed products but not clicked by users, and posivtie samples are clicked.", "Amazon 2 As there are no large-scale public datasets in the personalized product search area, we use Amazon dataset as our experiment corpus, consistent with existing approaches #OTHEREFR .", "In our experiments, we adopted the five-core version provided, whereby the remaining users and products have at least five reviews, respectively.", "Besides, we selected three categories with different sizes: Clothing, Shoes & Jewelry, Toys & Games, Electronics.", "These datasets both contain several categories so that users may have different interests."], "text_after_citation": ["Our model uses the previously purchased products in a neighboring window size to model the short-term user interests.", "We further filtered the dataset to make sure each user has at least 20 purchased products (i.e., 20 reviews).", "Following the same paradigm used in #OTHEREFR , we construct queries for each purchased item with category information.", "This strategy is based on the finding that directed product search is users' search for a producer's name, a brand, or a set of terms describing product category.", "We partitioned each of the four datasets into three sets: training, validation and testing sets."], "citing_paper_content": {"title": "Hierarchically Fusing Long And Short-Term User Interests For Click-Through Rate Prediction In Product Search", "abstract": "Estimating Click-Through Rate (CTR) is a vital yet challenging task in personalized product search. However, existing CTR methods still struggle in the product search settings due to the following three challenges including how to more effectively extract users' short-term interests with respect to multiple aspects, how to extract and fuse users' long-term interest with short-term interests, how to address the entangling characteristic of long and short-term interests. To resolve these challenges, in this paper, we propose a new approach named Hierarchical Interests Fusing Network (HIFN), which consists of four basic modules namely Short-term Interests Extractor (SIE), Long-term Interests Extractor (LIE), Interests Fusion Module (IFM) and Interests Disentanglement Module (IDM). Specifically, SIE is proposed to extract user's short-term interests by integrating three fundamental interests encoders within it namely query-dependent, target-dependent and causal-dependent interest encoder, respectively, followed by delivering the resultant representation to the module LIE, where it can effectively capture user longterm interests by devising an attention mechanism with respect to the short-term interests from SIE module. In IFM, the achieved long and short-term interests are further fused in an adaptive manner, followed by concatenating it with original raw context features for the final prediction result. Last but not least, considering the entangling characteristic of long and short-term interests, IDM further devises a self-supervised framework to disentangle long and short-term interests. Extensive offline and online evaluations on a real-world e-commerce platform demonstrate the superiority of HIFN over state-of-the-art methods."}, "cited_paper_content": {"title": "Ups And Downs: Modeling The Visual Evolution Of Fashion Trends With One-Class Collaborative Filtering", "abstract": "Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset."}, "keywords": ["users' product purchasing"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As there are no large-scale public datasets in the personalized product search area, we use Amazon dataset as our experiment corpus, consistent with existing approaches ( #OTHEREFR ). In our experiments, we adopted the five-core version provided, whereby the remaining users and products have at least five reviews, respectively. Besides, we selected three categories with different sizes: Clothing, Shoes & Jewelry, Toys & Games, Electronics. These datasets both contain several categories so that users may have different interests. To effectively capture user's long-term interests and evolving trends within the community, we adopt the One-Class Collaborative Filtering setting and novel models proposed in Ups And Downs: Modeling The Visual Evolution Of Fashion Trends With One-Class Collaborative Filtering #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To effectively capture users' evolving fashion preferences, the authors of this paper proposed a novel method for One-Class Collaborative Filtering that combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To effectively extract users' short-term interests with respect to multiple aspects and address the entangling characteristic of long and short-term interests in personalized product search, the authors proposed a new approach named Hierarchical Interests Fusing Network (HIFN) that outperforms state-of-the-art methods on a real-world e-commerce platform #REFR, which combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community.", "given_citation_intent": "method", "given_keywords": ["users' product purchasing"]}]}
{"citing_id": "2304.13431v1", "cited_id": "1903.04561", "section_title": "Appendix D Derivation Of Regularization Terms", "citation": "CivilComments #REFR : CivilComments is a text classification task that aims to predict whether an online comment is toxic or non-toxic.", "text_before_citation": ["The remaining training samples include 3,498 samples from \"landbirds on land\", and 1,057 samples from \"waterbirds on water\".", "CelebA #OTHEREFR : CelebA includes a number of face images of celebrities.", "The classification labels are the hair colors of the celebrities including \"blond\" or \"not blond.\" The spurious attribute is gender, i.e., male or female.", "In CelebA, the minority groups are (\"blond\", male) and (\"not blond\", female).", "The number of samples for each group are 71,629 (dark hair, female), 66,874 (dark hair, male), 22,880 (blond hair, female), and 1,387 (blond hair, male)."], "text_after_citation": ["The spurious domain identifications are a number of demographic features, including male, female, LGBTQ, Christian, Muslim, other religion, Black, and White #OTHEREFR .", "There are 450,000 comments in CivilComments which are collected from online articles.", "The numbers of samples for training, validation, and test datasets are 269,038, 45,180, and 133,782, respectively. Fig.", "14 illustrates more images augmented by ICDA, in which samples with various backgrounds and viewpoints are generated.", "For example, ICDA generates dogs on the grass, in the water, and even on the sofa. In addition, we present more results for model visualization."], "citing_paper_content": {"title": "Implicit Counterfactual Data Augmentation For Deep Neural Networks", "abstract": "Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from a regularization aspect, with extensive experiments indicating that our method consistently improves the generalization performance of popular depth networks on multiple typical learning scenarios that require out-of-distribution generalization."}, "cited_paper_content": {"title": "Nuanced Metrics For Measuring Unintended Bias With Real Data For Text Classification", "abstract": "Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models."}, "keywords": ["text classification task"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To measure unintended bias in machine learning, a suite of threshold-agnostic metrics has been introduced that considers the various ways a classifier's score distribution can vary across designated groups, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Unintended bias in machine learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. To address this issue, nuanced metrics have been proposed that provide a more comprehensive view of unintended bias, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large, which is a critical issue in the text classification task. #REFR provides a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups.", "given_citation_intent": "background", "given_keywords": ["text classification task"]}]}
{"citing_id": "2303.02601v1", "cited_id": "1505.00468", "section_title": "Related Work", "citation": "VQA models Since the introductory work on VQA #REFR , several endeavors have extended this paradigm, either by suggesting advanced model architectures or by proposing more challenging datasets.", "text_before_citation": [], "text_after_citation": ["State-of-the-art models addressing the VQA task are mainly based on VL transformer backbones; thus, models such as ViLBERT #OTHEREFR , VisualBERT #OTHEREFR , FLAVA #OTHEREFR , ALBEF #OTHEREFR , ViLT #OTHEREFR and others have dominated the recent VQA literature demonstrating rapid improvements on relevant benchmark datasets.", "Regarding datasets, improvements on the original VQA (VQA-v2) suggest adding similar image pairs corresponding to the same question , but leading to diverging answers #OTHEREFR .", "Visual Genome (VG) is another large scale dataset including numerous scene images, object, attribute and relationship annotations, as well as visual question-answer pairs #OTHEREFR .", "Our approach is tested on both VQA-v2 and VG datasets.", "Other popular VQA datasets are Flickr30k-Entities #OTHEREFR , COCO-QA #OTHEREFR , Visual7W #OTHEREFR and others."], "citing_paper_content": {"title": "Knowledge-Based Counterfactual Queries For Visual Question Answering", "abstract": "Visual Question Answering (VQA) has been a popular task that combines vision and language, with numerous relevant implementations in literature. Even though there are some attempts that approach explainability and robustness issues in VQA models, very few of them employ counterfactuals as a means of probing such challenges in a model-agnostic way. In this work, we propose a systematic method for explaining the behavior and investigating the robustness of VQA models through counterfactual perturbations. For this reason, we exploit structured knowledge bases to perform deterministic, optimal and controllable word-level replacements targeting the linguistic modality, and we then evaluate the model's response against such counterfactual inputs. Finally, we qualitatively extract local and global explanations based on counterfactual responses, which are ultimately proven insightful towards interpreting VQA model behaviors. By performing a variety of perturbation types, targeting different parts of speech of the input question, we gain insights to the reasoning of the model, through the comparison of its responses in different adversarial circumstances. Overall, we reveal possible biases in the decision-making process of the model, as well as expected and unexpected patterns, which impact its performance quantitatively and qualitatively, as indicated by our analysis."}, "cited_paper_content": {"title": "Vqa: Visual Question Answering", "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing $$\\sim $$~0.25 M images, $$\\sim $$~0.76 M questions, and $$\\sim $$~10 M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa)."}, "keywords": ["VQA models", "VQA"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Visual Question Answering (VQA) is a popular task that combines vision and language, with numerous relevant implementations in literature. Even though there are some attempts that approach explainability and robustness issues in VQA models, very few of them employ counterfactuals as a means of probing such challenges in a model-agnostic way. In this work, the authors propose a systematic method for explaining the behavior and investigating the robustness of VQA models through counterfactual perturbations, which is based on structured knowledge bases to perform deterministic, optimal and controllable word-level replacements targeting the linguistic modality. The authors then evaluate the model's response against such counterfactual inputs and qualitatively extract local and global explanations based on counterfactual responses, which are ultimately proven insightful towards interpreting VQA model behaviors. By performing a variety of perturbation types, targeting different parts of speech of the input question, the authors gain insights to the reasoning of the model, through the comparison of its responses in different adversarial circumstances. Overall, the authors reveal possible biases in the decision-making process of the model, as well as expected and unexpected patterns, which impact its performance quantitatively and qualitatively, as indicated by our analysis (Vqa: Visual Question Answering #REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed task of Visual Question Answering (VQA) requires a system that has a more detailed understanding of the image and complex reasoning than a system producing generic image captions, and has been widely studied in literature #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Visual Question Answering (VQA) has been a popular task that combines vision and language, with numerous relevant implementations in literature, and has been shown to require a more detailed understanding of the image and complex reasoning than a system producing generic image captions, as demonstrated in #REFR.", "given_citation_intent": "background", "given_keywords": ["VQA models", "VQA"]}]}
{"citing_id": "2303.15768v1", "cited_id": "1912.04958", "section_title": "Exploring Stylegan For Face Swapping.", "citation": "In this section, we analyze the latent space of Style-GAN #REFR from the perspective of developing the face swapping model.", "text_before_citation": [], "text_after_citation": ["Then, we justify the proper combination of latents for a source attribute leakage-free model.", "h\u00d7w results in improved preservation of expression, head pose, and eye gazing, while the identity undergoes less change.", "Overall score is calculated by (ID sim) 3 * (HP dis) * (Exp dis) * (EG dis) which is standardized.", "Details for each score metric are described in the supplementary materials.", "Revisiting Latent Space of StyleGAN."], "citing_paper_content": {"title": "Robustswap: A Simple Yet Robust Face Swapping Model Against Attribute Leakage", "abstract": "Figure 1. Face swapping results of RobustSwap. Our model robustly synthesizes the swapped image from the in-the-wild source and target images, while preserving the target attributes and reflecting source identity. With our model, the target attributes (e.g., eye gazing, hair, background, expression, and head pose) are preserved successfully regardless of source identity images."}, "cited_paper_content": {"title": "Analyzing And Improving The Image Quality Of Stylegan", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."}, "keywords": ["face", "Style-GAN"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed changes in model architecture and training methods in StyleGAN have significantly improved the image quality and conditioning of the generator, as well as made it easier to attribute a generated image to a particular network #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed changes in model architecture and training methods in StyleGAN, including the redesign of generator normalization and regularization of the generator, have significantly improved the image quality and conditioning of the mapping from latent codes to images, as well as made the generator easier to invert, according to the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To synthesize the swapped image from the in-the-wild source and target images while preserving the target attributes and reflecting the source identity, the authors utilized a robust face swapping model called RobustSwap, which incorporates the Style-GAN architecture proposed by Karras et al. (#REFR).", "given_citation_intent": "background", "given_keywords": ["face", "Style-GAN"]}]}
{"citing_id": "2304.05246v1", "cited_id": "1901.10338", "section_title": "Introduction", "citation": "This testing set is not available in real experiments making it hard to trust their behavior online #REFR .", "text_before_citation": ["We kickstarted the image classification models using transfer learning or self-supervision, following the industry best practices. Reproducibility.", "Our framework is open source, and all experiments results are made available and can be easily run again.", "We provide the accuracies and other AL metrics for the most common AL samplers, along with all train, test, and initial batch indices used for those experiments.", "Online evaluation of sampling strategies.", "Research works rely on the area under the accuracy curve of a left-out test set to evaluate the performance of AL strategies."], "text_after_citation": ["OpenAL logs unsupervised metrics to improve the offline strategies' interpretability and be able to interpret their behavior online #OTHEREFR .", "We first start by describing the setup of our tasks, the model selection methodology, and the evaluation criteria for sampling strategies.", "Then we present the results of our experiments per strategy across all tasks.", "We finish by focusing on the metrics observed and how they explain the performance of some strategies.", "Finally we open new perspectives on AL experiments and how this benchmark could be useful and extended in the future."], "citing_paper_content": {"title": "Openal: Evaluation And Interpretation Of Active Learning Strategies", "abstract": "Despite the vast body of literature on Active Learning (AL), there is no comprehensive and open benchmark allowing for efficient and simple comparison of proposed samplers. Additionally, the variability in experimental settings across the literature makes it difficult to choose a sampling strategy, which is critical due to the one-off nature of AL experiments. To address those limitations, we introduce OpenAL, a flexible and open-source framework to easily run and compare sampling AL strategies on a collection of realistic tasks. The proposed benchmark is augmented with interpretability metrics and statistical analysis methods to understand when and why some samplers outperform others. Last but not least, practitioners can easily extend the benchmark by submitting their own AL samplers. Preprint. Under review."}, "cited_paper_content": {"title": "Limitations Of Assessing Active Learning Performance At Runtime", "abstract": "Classification algorithms aim to predict an unknown label (e.g., a quality class) for a new instance (e.g., a product). Therefore, training samples (instances and labels) are used to deduct classification hypotheses. Often, it is relatively easy to capture instances but the acquisition of the corresponding labels remain difficult or expensive. Active learning algorithms select the most beneficial instances to be labeled to reduce cost. In research, this labeling procedure is simulated and therefore a ground truth is available. But during deployment, active learning is a one-shot problem and an evaluation set is not available. Hence, it is not possible to reliably estimate the performance of the classification system during learning and it is difficult to decide when the system fulfills the quality requirements (stopping criteria). In this article, we formalize the task and review existing strategies to assess the performance of an actively trained classifier during training. Furthermore, we identified three major challenges: 1)~to derive a performance distribution, 2)~to preserve representativeness of the labeled subset, and 3) to correct against sampling bias induced by an intelligent selection strategy. In a qualitative analysis, we evaluate different existing approaches and show that none of them reliably estimates active learning performance stating a major challenge for future research for such systems. All plots and experiments are provided in a Jupyter notebook that is available for download."}, "keywords": ["testing set"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Research works rely on the area under the accuracy curve of a left-out test set to evaluate the performance of AL strategies. However, as pointed out by previous studies, it is difficult to reliably estimate the performance of the classification system during learning and to decide when the system fulfills the quality requirements. #REFR formalizes the task and reviews existing strategies to assess the performance of an actively trained classifier during training, identifying major challenges in deriving a performance distribution, preserving representativeness of the labeled subset, and correcting against sampling bias induced by an intelligent selection strategy.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of active learning strategies, research works rely on the area under the accuracy curve of a left-out test set, but as pointed out by previous work, it is difficult to reliably estimate the performance of the classification system during learning and to decide when the system fulfills the quality requirements, which is a major challenge for future research for such systems #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of AL strategies, research works rely on the area under the accuracy curve of a left-out testing set #REFR.", "given_citation_intent": "background", "given_keywords": ["testing set"]}]}
{"citing_id": "2305.02722v1", "cited_id": "1906.03609", "section_title": "Related Work 2.1 Knowledge Distillation", "citation": "To filter massive less informative supervision in background areas, Wang #REFR transferred effective knowledge in sparse imitation regions.", "text_before_citation": ["As the seminal work by Hinton #OTHEREFR , knowledge distillation is introduced to boost performance of tiny models.", "A line of works #OTHEREFR has verified its potential.", "To extend its application to object detection, MIMIC #OTHEREFR proposed to prevent divergence between RPN output of teacher ans student via 2 loss."], "text_after_citation": ["To handle imbalanced supervision from foreground and background pixels, Zhang #OTHEREFR and Yang #OTHEREFR encouraged student to learn from crucial pixels of foreground objects and the relationship between them.", "To separate informative feature in and out of bounding boxed, FRS #OTHEREFR presented feature-richness score to focus on important features.", "To further investigate sophisticated dependence, MasKD #OTHEREFR proposed receptive tokens to build pixel dependency masks for more effective distilling.", "Instead of spatial-wise, CWD #OTHEREFR exploited more semantical information in each channel with category-specific masks.", "For the sake of fusing different kinds of information, GID #OTHEREFR merged branches of feature/relation/response-based knowledge and developed an all-in-one framework."], "citing_paper_content": {"title": "Avatar Knowledge Distillation: Self-Ensemble Teacher Paradigm With Uncertainty", "abstract": "Knowledge distillation is an effective paradigm for boosting the performance of pocket-size model, especially when multiple teacher models are available, the student would break the upper limit again. However, it is not economical to train diverse teacher models for the disposable distillation. In this paper, we introduce a new concept dubbed Avatars for distillation, which are the inference ensemble models derived from the teacher. Concretely, (1) For each iteration of distillation training, various Avatars are generated by a perturbation transformation. We validate that Avatars own higher upper limit of working capacity and teaching ability, aiding the student model in learning diverse and receptive knowledge perspectives from the teacher model. (2) During the distillation, we propose an uncertainty-aware factor from the variance of statistical differences between the vanilla teacher and Avatars, to adjust Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge Distillation (AKD) is fundamentally different from existing methods and refines with the innovative view of unequal training. Comprehensive experiments demonstrate the effectiveness of our Avatars mechanism, which polishes up the state-of-the-art distillation methods for dense prediction without more extra computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation, respectively."}, "cited_paper_content": {"title": "Distilling Object Detectors With Fine-Grained Feature Imitation", "abstract": "State-of-the-art CNN based recognition models are often computationally prohibitive to deploy on low-end devices. A promising high level approach tackling this limitation is knowledge distillation, which let small student model mimic cumbersome teacher model's output to get improved generalization. However, related methods mainly focus on simple task of classification while do not consider complex tasks like object detection. We show applying the vanilla knowledge distillation to detection model gets minor gain. To address the challenge of distilling knowledge in detection model, we propose a fine-grained feature imitation method exploiting the cross-location discrepancy of feature response. Our intuition is that detectors care more about local near object regions. Thus the discrepancy of feature response on the near object anchor locations reveals important information of how teacher model tends to generalize. We design a novel mechanism to estimate those locations and let student model imitate the teacher on them to get enhanced performance. We first validate the idea on a developed lightweight toy detector which carries simplest notion of current state-of-the-art anchor based detection models on challenging KITTI dataset, our method generates up to 15% boost of mAP for the student model compared to the non-imitated counterpart. We then extensively evaluate the method with Faster R-CNN model under various scenarios with common object detection benchmark of Pascal VOC and COCO, imitation alleviates up to 74% performance drop of student model compared to teacher. Codes released at https://github.com/twangnh/Distilling-Object-Detectors"}, "keywords": ["sparse imitation regions"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To extend its application to object detection, MIMIC proposed a fine-grained feature imitation method exploiting the cross-location discrepancy of feature response #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "As MIMIC proposed to prevent divergence between RPN output of teacher and student via 2 loss, fine-grained feature imitation method is proposed in #REFR to distill knowledge in detection model by exploiting the cross-location discrepancy of feature response, which generates up to 15% boost of mAP for the student model compared to the non-imitated counterpart.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of distilling knowledge in object detection, MIMIC proposed a fine-grained feature imitation method exploiting the cross-location discrepancy of feature response, which estimates sparse imitation regions and lets the student model imitate the teacher on them to get enhanced performance #REFR.", "given_citation_intent": "background", "given_keywords": ["sparse imitation regions"]}]}
{"citing_id": "2304.10714v1", "cited_id": "1412.6980", "section_title": "B. Cifar Classification 1) Training", "citation": "In the meta-learner training, we use Adam #REFR to update \u03b8 meta , and the meta-optimization step size \u03b2 is fixed to 0.001, the meta-learning rate \u03b3 to 0.004, the metalearner training epochs ite 2 to 150.", "text_before_citation": ["Based on QST bases, training images are separated into D base and D meta . We adopt ResNet-18 #OTHEREFR as the baseline model.", "All settings and configurations in the base training of QAM are the same as those in the baseline model training.", "Specifically, SGD with Nesterov momentum is utilized as optimizer to update \u03b8 bnb and \u03b8 rem , and the learning rate \u03b1 is set to 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9.", "The learning rate dropped by 0.2 at 60, 120, and 160 epochs, the base training epochs ite 1 are set to 200, and the minibatch size is set to 128.", "All input images are padded by 4 pixels on each side with reflections of the original image and pre-processed with horizontal flips and random crops."], "text_after_citation": ["2) Results on CIFAR-10/100-J and CIFAR-10/100-J-S: We compare QAM to the baseline model, which trains ResNet-18 on all training images of CIFAR-10/100-J or CIFAR-10/100-J-S and updates the parameters depending on Equation (1). From the results shown in Fig. 8 and Fig. 9 , we obtain three observations. Firstly, QAM achieves significant improvements, demonstrating its effectiveness.", "Secondly, the improvements on images with slighter compression loss are larger.", "The reason is that the baseline model is badly misled by heavily compressed images, while QAM utilizes QAC to reduce the interference of quantization.", "Thirdly, QAM does not undermine the performance over images with heavy compression but achieves slight improvements.", "Moreover, we compare QAM with several state-of-the-art methods, including two ensemble learning based methods (CDI #OTHEREFR and ADL #OTHEREFR ) and two features alignment-based methods (FCT #OTHEREFR and ARF #OTHEREFR )."], "citing_paper_content": {"title": "Picking Up Quantization Steps For Compressed Image Classification", "abstract": "The sensitivity of deep neural networks to compressed images hinders their usage in many real applications, which means classification networks may fail just after taking a screenshot and saving it as a compressed file. In this paper, we argue that neglected disposable coding parameters stored in compressed files could be picked up to reduce the sensitivity of deep neural networks to compressed images. Specifically, we resort to using one of the representative parameters, quantization steps, to facilitate image classification. Firstly, based on quantization steps, we propose a novel quantization aware confidence (QAC), which is utilized as sample weights to reduce the influence of quantization on network training. Secondly, we utilize quantization steps to alleviate the variance of feature distributions, where a quantization aware batch normalization (QABN) is proposed to replace batch normalization of classification networks. Extensive experiments show that the proposed method significantly improves the performance of classification networks on CIFAR-10, CIFAR-100, and ImageNet."}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["meta-learning rate"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To update the parameters, we utilize the Adam optimizer #REFR, which is known for its computational efficiency and suitability for problems with large data and/or parameters.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To update the parameters, we utilize the Adam algorithm #REFR, which is well suited for problems that are large in terms of data and/or parameters and appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize SGD with Nesterov momentum as optimizer to update \u03b8 bnb and \u03b8 rem, and set the learning rate \u03b1 to 0.1, weight decay to 0.0005, dampening to 0, momentum to 0.9, and the learning rate dropped by 0.2 at 60, 120, and 160 epochs. The proposed method is based on quantization steps, and we introduce a novel quantization aware confidence (QAC) based on #REFR to reduce the influence of quantization on network training.", "given_citation_intent": "method", "given_keywords": ["meta-learning rate"]}]}
{"citing_id": "2303.10820v2", "cited_id": "1911.09930", "section_title": "Intensity Consistency Loss", "citation": "As with the original paper #REFR , we set \u03bb 1 , \u03bb 2 , \u03bb 3 , \u03bb 4 , \u03bb 5 , \u03bb 6 as 10.0, 0.1, 10.0, 0.1, 5.0, and 1.0, respectively.", "text_before_citation": ["Thus, the second term represents the loss function when F (I)/L and the luminance of the estimated shade F (S(I)) are correlated.", "EQUATION", "where m L is the mask, which is 1 for pixels with densified LiDAR intensity and 0 otherwise.", "In addition, s x and b x , where x \u2208 1, 2, are trainable parameters for adjusting the scale and bias of LiDAR intensity, respectively. In summary, IID-LI optimizes the loss function in Eq. #OTHEREFR .", "EQUATION"], "text_after_citation": ["In addition, we set \u03bb 7 as 20.0 in this paper."], "citing_paper_content": {"title": "Unsupervised Intrinsic Image Decomposition With Lidar Intensity", "abstract": "Intrinsic image decomposition (IID) is the task that decomposes a natural image into albedo and shade. While IID is typically solved through supervised learning methods, it is not ideal due to the difficulty in observing ground truth albedo and shade in general scenes. Conversely, unsupervised learning methods are currently underperforming supervised learning methods since there are no criteria for solving the ill-posed problems. Recently, light detection and ranging (LiDAR) is widely used due to its ability to make highly precise distance measurements. Thus, we have focused on the utilization of LiDAR, especially LiDAR intensity, to address this issue. In this paper, we propose unsupervised intrinsic image decomposition with LiDAR intensity (IID-LI). Since the conventional unsupervised learning methods consist of image-to-image transformations, simply inputting LiDAR intensity is not an effective approach. Therefore, we design an intensity consistency loss that computes the error between LiDAR intensity and gray-scaled albedo to provide a criterion for the ill-posed problem. In addition, LiDAR intensity is difficult to handle due to its sparsity and occlusion, hence, a LiDAR intensity densification module is proposed. We verified the estimating quality using our own dataset, which include RGB images, LiDAR intensity and human judged annotations. As a result, we achieved an estimation accuracy that outperforms conventional unsupervised learning methods."}, "cited_paper_content": {"title": "Unsupervised Learning For Intrinsic Image Decomposition From A Single Image.", "abstract": "Intrinsic image decomposition, which is an essential task in computer vision, aims to infer the reflectance and shading of the scene. It is challenging since it needs to separate one image into two components. To tackle this, conventional methods introduce various priors to constrain the solution, yet with limited performance. Meanwhile, the problem is typically solved by supervised learning methods, which is actually not an ideal solution since obtaining ground truth reflectance and shading for massive general natural scenes is challenging and even impossible. In this paper, we propose a novel unsupervised intrinsic image decomposition framework, which relies on neither labeled training data nor hand-crafted priors. Instead, it directly learns the latent feature of reflectance and shading from unsupervised and uncorrelated data. To enable this, we explore the independence between reflectance and shading, the domain invariant content constraint and the physical constraint. Extensive experiments on both synthetic and real image datasets demonstrate consistently superior performance of the proposed method."}, "keywords": ["\u03bb", "original paper"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of unsupervised intrinsic image decomposition, the authors propose a novel framework that directly learns the latent feature of reflectance and shading from unsupervised and uncorrelated data, as described in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of intrinsic image decomposition, the authors propose an unsupervised learning method that directly learns the latent feature of reflectance and shading from unsupervised and uncorrelated data, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of unsupervised intrinsic image decomposition, the authors propose a novel framework that directly learns the latent feature of reflectance and shading from unsupervised and uncorrelated data, as described in their original paper #REFR.", "given_citation_intent": "method", "given_keywords": ["\u03bb", "original paper"]}]}
{"citing_id": "2303.11749v2", "cited_id": "1504.08083", "section_title": "Decoupling Proposal Generation And Roi Classification.", "citation": "With the generated proposals, the RoI classification stage is trained in the Fast RCNN #REFR manner.", "text_before_citation": ["In contrast, the class-specific RoI classification stage cannot even work for novel categories.", "Even with language embeddings, it is still biased to base classes.", "The distinct properties affect their joint training since the sensitivity of the classification stage to novel classes hampers the universality ability of the proposal generation stage.", "Consequently, we decouple these two stages and train them separately to avoid such conflict.", "Specifically, the region proposal generation stage is initialized with traditional ImageNet pre-trained parameters and trained in a class-agnostic way. After training, it produces a series of region proposals."], "text_after_citation": ["This stage is initialized with imagetext pre-trained parameters for predicting unseen categories.", "These two kinds of pre-trained parameters also contain complementary features, which provide more comprehensive information for universal object detection.", "Class-agnostic localization network.", "To produce generalized proposals in the open world, we present the classagnostic localization network (CLN), as illustrated in Fig.  4 .", "Instead of a single RPN module, our CLN contains both the RPN and the RoI head to generate proposals for universal object detection."], "citing_paper_content": {"title": "Detecting Everything In The Open World: Towards Universal Object Detection", "abstract": "In this paper, we formally address universal object detection, which aims to detect every scene and predict every category. The dependence on human annotations, the limited visual information, and the novel categories in the open world severely restrict the universality of traditional detectors. We propose UniDetector, a universal object detector that has the ability to recognize enormous categories in the open world. The critical points for the universality of UniDetector are: 1) it leverages images of multiple sources and heterogeneous label spaces for training through the alignment of image and text spaces, which guarantees sufficient information for universal representations. 2) it generalizes to the open world easily while keeping the balance between seen and unseen classes, thanks to abundant information from both vision and language modalities. 3) it further promotes the generalization ability to novel categories through our proposed decoupling training manner and probability calibration. These contributions allow UniDetector to detect over 7k categories, the largest measurable category size so far, with only about 500 classes participating in training. Our UniDetector behaves the strong zero-shot generalization ability on largevocabulary datasets-it surpasses the traditional supervised baselines by more than 4% on average without seeing any corresponding images. On 13 public detection datasets with various scenes, UniDetector also achieves state-of-the-art performance with only a 3% amount of training data."}, "cited_paper_content": {"title": "Fast R-Cnn", "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn."}, "keywords": ["Fast RCNN manner"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To avoid conflict between the proposal generation and classification stages, the authors decoupled the two stages and trained them separately, with the region proposal generation stage initialized with traditional ImageNet pre-trained parameters and trained in a class-agnostic way #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently classify object proposals, the authors propose using Fast R-CNN, a Fast Region-based Convolutional Network method that builds on previous work and achieves higher mAP on PASCAL VOC 2012, as well as trains and tests faster than previous methods. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To avoid the conflict between the proposal generation stage and the classification stage, the authors decoupled the two stages and trained them separately in a class-agnostic way, which is similar to the Fast RCNN manner. #REFR", "given_citation_intent": "method", "given_keywords": ["Fast RCNN manner"]}]}
{"citing_id": "2303.02874v1", "cited_id": "2002.05646", "section_title": "Ii. Background", "citation": "The new challenge had focused on adversarial attack and defense against machine learning model there had been series of research survey which establish the need for protecting machine learning model against various forms of attach which make it to misclassify #REFR .", "text_before_citation": ["Adversarial machine learning deals with the study of attacks on machine learning algorithms, and of the defenses against such attacks.", "#OTHEREFR .Many years ago, the focus of machine learning engineers and scientist was on obtaining high www.ijacsa.thesai.org accuracy for correct prediction, and while this had greatly been resolve in the past few years."], "text_after_citation": ["During the training of machine learning model, it is usually assumed that the training and test data are generated from the same statistical distribution.", "This assumption makes the final model vulnerable to various forms of attack, majority of which includes evasion attacks, [3] data poisoning attacks, [4] Byzantine attacks #OTHEREFR and model extraction [6] ."], "citing_paper_content": {"title": "Adversarial Sampling For Fairness Testing In Deep Neural Network", "abstract": "In this research, we focus on the usage of adversarial sampling to test for the fairness in the prediction of deep neural network model across different classes of image in a given dataset. While several framework had been proposed to ensure robustness of machine learning model against adversarial attack, some of which includes adversarial training algorithm. There is still the pitfall that adversarial training algorithm tends to cause disparity in accuracy and robustness among different group. Our research is aimed at using adversarial sampling to test for fairness in the prediction of deep neural network model across different classes or categories of image in a given dataset. We successfully demonstrated a new method of ensuring fairness across various group of input in deep neural network classifier. We trained our neural network model on the original image, and without training our model on the perturbed or attacked image. When we feed the adversarial samplings to our model, it was able to predict the original category/ class of the image the adversarial sample belongs to. We also introduced and used the separation of concern concept from software engineering whereby there is an additional standalone filter layer that filters perturbed image by heavily removing the noise or attack before automatically passing it to the network for classification, we were able to have accuracy of 93.3%. Cifar-10 dataset have ten categories of dataset, and so, in order to account for fairness, we applied our hypothesis across each categories of dataset and were able to get a consistent result and accuracy."}, "cited_paper_content": {"title": "Adversarial Machine Learning -- Industry Perspectives", "abstract": "Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and we enumerate the gaps in perspective in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders who are tasked with securing ML systems as they are designed, developed and deployed ML systems. The goal of this paper is to engage researchers to revise and amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era."}, "keywords": ["adversarial attack"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Adversarial machine learning deals with the study of attacks on machine learning algorithms, and of the defenses against such attacks. In the context of securing machine learning systems, the reference paper #REFR enumerates the gaps in perspective and provides tactical and strategic tools for developers and security incident responders to protect, detect, and respond to attacks on their ML systems.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adversarial machine learning deals with the study of attacks on machine learning algorithms, and of the defenses against such attacks. In the context of securing machine learning systems, the authors of the reference paper found that industry practitioners lack the necessary tactical and strategic tools to protect, detect, and respond to attacks on their ML systems #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adversarial machine learning deals with the study of attacks on machine learning algorithms, and of the defenses against such attacks. In recent years, the focus has shifted towards obtaining high accuracy for correct prediction, but as highlighted in the reference paper #REFR, industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to adversarial attacks on their machine learning systems.", "given_citation_intent": "background", "given_keywords": ["adversarial attack"]}]}
{"citing_id": "2304.09023v1", "cited_id": "1201.1387", "section_title": "A Numerical Example", "citation": "In the feedback law (19), we observed no noticeable difference between having \u03b5 = 0 or \u03b5 being a small number, which agrees with the observations made in #REFR .", "text_before_citation": ["EQUATION", "where we pick \u03c6 0 = 1 8 and \u03b8 = \u03c0 4 .", "In Figure 5 , the mean trajectory of 100 realizations as well as sample trajectories of 10 realizations are shown with the resulting control Hamiltonians. All realizations are initialized with", "\u03c1 0 = 1 2 |0 0| + 1 2N i |i i", "i| , i.e., a mixed state with probability 1/2 of being in |0 0| and probability 1/2 in uniform superposition over all basis states."], "text_after_citation": ["Thus, in the simulations shown here we have \u03b5 = 0.", "Likewise, as in #OTHEREFR", "(2013) , we use a quadratic approximation of the nonlinear optimization problem in the feedback law (19) to speed up computations. In particular, we solve the quadratic program", "u k = arg min u\u2208[\u2212\u016b,\u016b] 1 2 Tr [H 1 , P ], H 1 \u03c1 k \u2212 \u03b5 4 i i|[H 1 , \u03c1 k ]|i 2 u 2 \u2212 i Tr [H 1 , P ]\u03c1 k u (29) with\u016b = 0.1.", "As can be seen from Figure 5 , the state converges to the state \u03c1 = |n n| in almost all cases within the first 1000 iterations."], "citing_paper_content": {"title": "Measurement-Based Control For Minimizing Energy Functions In Quantum Systems", "abstract": "In variational quantum algorithms (VQAs), the most common objective is to find the minimum energy eigenstate of a given energy Hamiltonian. In this paper, we consider the general problem of finding a sufficient control Hamiltonian structure that, under a given feedback control law, ensures convergence to the minimum energy eigenstate of a given energy function. By including quantum non-demolition (QND) measurements in the loop, convergence to a pure state can be ensured from an arbitrary mixed initial state. Based on existing results on strict control Lyapunov functions, we formulate a semidefinite optimization problem, whose solution defines a non-unique control Hamiltonian, which is sufficient to ensure almost sure convergence to the minimum energy eigenstate under the given feedback law and the action of QND measurements. A numerical example is provided to showcase the proposed methodology."}, "cited_paper_content": {"title": "Feedback Stabilization Of Discrete-Time Quantum Systems Subject To Non-Demolition Measurements With Imperfections And Delays", "abstract": "We consider a controlled quantum system whose finite dimensional state is governed by a discrete-time nonlinear Markov process. In open-loop, the measurements are assumed to be quantum non-demolition (QND). The eigenstates of the measured observable are thus the open-loop stationary states: they are used to construct a closed-loop supermartingale playing the role of a strict control Lyapunov function. The parameters of this supermartingale are calculated by inverting a Metzler matrix that characterizes the impact of the control input on the Kraus operators defining the Markov process. The resulting state feedback scheme, taking into account a known constant delay, provides the almost sure convergence of the controlled system to the target state. This convergence is ensured even in the case where the filter equation results from imperfect measurements corrupted by random errors with conditional probabilities given as a left stochastic matrix. Closed-loop simulations corroborated by experimental data illustrate the interest of such nonlinear feedback scheme for the photon box, a cavity quantum electrodynamics system."}, "keywords": ["feedback law"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed methodology ensures almost sure convergence to the minimum energy eigenstate under the given feedback law and the action of quantum non-demolition (QND) measurements, as shown in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The closed-loop simulations in Figure 5 demonstrate the effectiveness of the proposed nonlinear feedback scheme for quantum electrodynamics systems, ensuring almost sure convergence to the target state even in the presence of imperfect measurements and delays #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The closed-loop simulations in Figure 5 demonstrate the effectiveness of the proposed nonlinear feedback scheme for the photon box, a cavity quantum electrodynamics system, in ensuring almost sure convergence to the target state under a known constant delay and imperfect measurements corrupted by random errors with conditional probabilities given as a left stochastic matrix, as shown in #REFR. The resulting state feedback scheme is based on a closed-loop supermartingale constructed using the eigenstates of the measured observable, which serves as a strict control Lyapunov function and is calculated by inverting a Metzler matrix that characterizes the impact of the control input on the Kraus operators defining the Markov process.", "given_citation_intent": "result", "given_keywords": ["feedback law"]}]}
{"citing_id": "2305.00769v1", "cited_id": "1706.03762", "section_title": "B. Transformer Based Method For In Multimodal Emotion Recognition From Physiological Signal", "citation": "Since their introduction in 2016, Transformers #REFR have emerged as the preferred models in the field of Deep Learning.", "text_before_citation": ["However, these approaches are heavily dependent on the quality of handcrafted feature selection and feature extraction processes.", "To overcome this challenge, other studies #OTHEREFR - #OTHEREFR proposed the use of Deep Learning techniques for an end-to-end approach, where the model learns to extract features automatically without the need for pre-designed feature descriptors.", "With the advancement of Deep Learning, various state-ofthe-art techniques have been employed for Affective Analysis from Physiological signals. Santamaria et al.", "#OTHEREFR used Convolutional Neural Networks (CNNs) with 1D Convolution layers for Emotion Detection, while Harper et al.", "#OTHEREFR combined CNNs with frequently used Recurrent Neural Networks (RNNs) for Emotion Recognition from ECG signals."], "text_after_citation": ["Their robust performance in Natural Language Processing, a type of data that shares some similar characteristics with timeseries data, has demonstrated the potential of Transformers when applied to time-series signals.", "As a result, recent research in the Time-Series domain has utilized Transformers as the core module in their model architecture #OTHEREFR - #OTHEREFR .", "For physiological signals, some studies have proposed using Transformers and their variants for detecting emotions #OTHEREFR , #OTHEREFR - #OTHEREFR . In the works of Vazquez et al.", "#OTHEREFR , #OTHEREFR , they focused on applying pretrained Transformers for Multimodal signal processing.", "However, this is still a very basic application of Transformer modules. Wu et al. #OTHEREFR and Yang et al."], "citing_paper_content": {"title": "Multi-Scale Transformer-Based Network For Emotion Recognition From Multi Physiological Signals", "abstract": "This paper presents an efficient Multi-scale Transformer-based approach for the task of Emotion recognition from Physiological data, which has gained widespread attention in the research community due to the vast amount of information that can be extracted from these signals using modern sensors and machine learning techniques. Our approach involves applying a Multi-modal technique combined with scaling data to establish the relationship between internal body signals and human emotions. Additionally, we utilize Transformer and Gaussian Transformation techniques to improve signal encoding effectiveness and overall performance. Our model achieves decent results on the CASE dataset of the EPiC competition, with an RMSE score of 1.45."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["Deep Learning"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To overcome the challenge of handcrafted feature selection and extraction processes, some studies proposed the use of Deep Learning techniques for an end-to-end approach, where the model learns to extract features automatically without the need for pre-designed feature descriptors. For instance, the Transformer architecture proposed in the #REFR paper has shown superior performance in machine translation tasks, and could potentially be applied to Emotion Recognition from Physiological signals.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the challenge of handcrafted feature selection and extraction processes, recent studies have proposed the use of end-to-end deep learning techniques #REFR, such as the Transformer architecture, which has shown superior performance in machine translation tasks.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the challenge of handcrafted feature selection and extraction processes, Deep Learning techniques have been proposed for an end-to-end approach, where the model learns to extract features automatically, as demonstrated in the Attention Is All You Need paper #REFR.", "given_citation_intent": "background", "given_keywords": ["Deep Learning"]}]}
{"citing_id": "2303.17181v1", "cited_id": "1711.09078", "section_title": "A. Blending Network", "citation": "As discussed in the paper, we train a UNet on the Vimeo90K #REFR dataset to generate weight maps to blend the warped frames. Each scene in Vimeo90K consists of 7 frames.", "text_before_citation": ["In this section, we provide further details about the blending network training."], "text_after_citation": ["We use the corner frames (1 and 7) as reference frames and randomly select an intermediate frame from 2-6.", "We apply data augmentation techniques like random horizontal and vertical flip, and cropping to the sequence.", "We use Zhang et al.'s optical flow method #OTHEREFR to estimate the flow between intermediate and reference frames.", "The reference frames are warped using these flows and the weight maps from the UNet are used to smoothly blend these warped frames.", "We use the following loss to train our blending network:"], "citing_paper_content": {"title": "Implicit View-Time Interpolation Of Stereo Videos Using Multi-Plane Disparities And Non-Uniform Coordinates", "abstract": "In this paper, we propose an approach for view-time interpolation of stereo videos. Specifically, we build upon X-Fields that approximates an interpolatable mapping between the input coordinates and 2D RGB images using a convolutional decoder. Our main contribution is to analyze and identify the sources of the problems with using X-Fields in our application and propose novel techniques to overcome these challenges. Specifically, we observe that X-Fields struggles to implicitly interpolate the disparities for large baseline cameras. Therefore, we propose multi-plane disparities to reduce the spatial distance of the objects in the stereo views. Moreover, we propose non-uniform time coordinates to handle the non-linear and sudden motion spikes in videos. We additionally introduce several simple, but important, improvements over X-Fields. We demonstrate that our approach is able to produce better results than the state of the art, while running in near real-time rates and having low memory and storage costs."}, "cited_paper_content": {"title": "Video Enhancement With Task-Oriented Flow", "abstract": "Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution."}, "keywords": ["Vimeo90K dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To train the blending network, the authors utilized a task-oriented flow (TOFlow) motion representation, which was learned in a self-supervised, task-specific manner #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train the blending network, the authors proposed a task-oriented flow (TOFlow) method for motion representation, which outperforms traditional optical flow on standard benchmarks and their Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train the blending network, the authors utilized the Vimeo90K dataset and proposed a novel task-oriented flow (TOFlow) method for motion representation, which outperformed traditional optical flow on standard benchmarks as well as their own dataset in various video processing tasks. #REFR", "given_citation_intent": "method", "given_keywords": ["Vimeo90K dataset"]}]}
{"citing_id": "2303.08444v1", "cited_id": "1701.01909", "section_title": "Introduction", "citation": "Furthermore, recurrent neural networks have been employed in the research #REFR to learn the temporal information of objects to improve the matching accuracy.", "text_before_citation": ["Most of online multi-object tracking methods are tracking by detection #OTHEREFR , #OTHEREFR , which rely on object detection algorithms for object recognition and objection location. Then additional \uff0a Corresponding author:", "Email: deemozen@163.com convolutional neural network branches are used to obtain the features of detected objects.", "Therefore, the similarity between detected objects can be obtained based on their features.", "Besides, the Kalman filter algorithm #OTHEREFR is usually used to estimate the motion information of objects.", "Finally, Hungarian matching algorithm #OTHEREFR is used to match the objects between frames in terms of the obtained appearance similarity and motion information."], "text_after_citation": ["More recently, some works #OTHEREFR - #OTHEREFR use sophisticated object detection architectures to obtain the outstanding tracking accuracy.", "In this paper, we focus on exploring effective and efficient matching algorithms without using appearance features of objects.", "Even with simple object detection models, our proposed algorithm can ensure fast training speed and online tracking.", "The matching process usually necessitates the use of numerous pieces of information about the objects, such as object motion #OTHEREFR and appearance features #OTHEREFR .", "However, the extraction of appearance features is time-consuming, and it is difficult to achieve real-time object tracking."], "citing_paper_content": {"title": "Real-Time Multi-Object Tracking Based On Bi-Directional Matching", "abstract": "\uff1aIn recent years, anchor-free object detection models combined with matching algorithms are used to achieve real-time muti-object tracking and also ensure high tracking accuracy. However, there are still great challenges in multi-object tracking. For example, when most part of a target is occluded or the target just disappears from images temporarily, it often leads to tracking interruptions for most of the existing tracking algorithms. Therefore, this study offers a bi-directional matching algorithm for multiobject tracking that makes advantage of bi-directional motion prediction information to improve occlusion handling. A stranded area is used in the matching algorithm to temporarily store the objects that fail to be tracked. When objects recover from occlusions, our method will first try to match them with objects in the stranded area to avoid erroneously generating new identities, thus forming a more continuous trajectory. Experiments show that our approach can improve the multi-object tracking performance in the presence of occlusions. In addition, this study provides an attentional up-sampling module that not only assures tracking accuracy but also accelerates training speed. In the MOT17 challenge, the proposed algorithm achieves 63.4% MOTA, 55.3% IDF1, and 20.1 FPS tracking speed."}, "cited_paper_content": {"title": "Tracking The Untrackable: Learning To Track Multiple Cues With Long-Term Dependencies", "abstract": "The majority of existing solutions to the Multi-Target Tracking (MTT) problem do not combine cues over a long period of time in a coherent fashion. In this paper, we present an online method that encodes long-term temporal dependencies across multiple cues. One key challenge of tracking methods is to accurately track occluded targets or those which share similar appearance properties with surrounding objects. To address this challenge, we present a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window. Our method allows to correct data association errors and recover observations from occluded states. We demonstrate the robustness of our data-driven approach by tracking multiple targets using their appearance, motion, and even interactions. Our method outperforms previous works on multiple publicly available datasets including the challenging MOT benchmark."}, "keywords": ["matching accuracy", "recurrent neural networks"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Most of online multi-object tracking methods are tracking by detection #OTHEREFR , #OTHEREFR , which rely on object detection algorithms for object recognition and objection location. Then additional convolutional neural network branches are used to obtain the features of detected objects. Therefore, the similarity between detected objects can be obtained based on their features. Besides, the Kalman filter algorithm #OTHEREFR is usually used to estimate the motion information of objects. Finally, Hungarian matching algorithm #OTHEREFR is used to match the objects between frames in terms of the obtained appearance similarity and motion information. To address the challenge of accurately tracking occluded targets or those which share similar appearance properties with surrounding objects, a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window is proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of accurately tracking occluded targets or those which share similar appearance properties with surrounding objects, the authors propose a structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple cues over a temporal window, as presented in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the matching accuracy of multi-object tracking, some recent studies have proposed to use recurrent neural networks (RNN) to encode long-term temporal dependencies across multiple cues #REFR.", "given_citation_intent": "method", "given_keywords": ["matching accuracy", "recurrent neural networks"]}]}
{"citing_id": "2304.11251v1", "cited_id": "1705.03439", "section_title": "Open Questions And Future Directions", "citation": "An initial promising result along these lines is #REFR , but there is substantial need for new research for broad classes of models and corresponding variational families.", "text_before_citation": ["Uncertainty quantification of the VB posterior It is well-known that variational posteriors tend to underestimate uncertainty of the posterior, so a central open question is how one can construct computationally efficient VB posteriors producing (a) credible balls with valid frequentist coverage and/or (b) posterior covariance matching that of the true posterior.", "There is limited work on theory for statistical inference using the variational posterior, including credible intervals and hypothesis testing.", "For this, we need theorems to reveal a limiting distribution of the variational posterior as the sample size goes to infinity, just as the Bernsteinvon Mises (BvM) theorem guarantees that the original posterior distribution converges to a Gaussian distribution under certain regularity conditions."], "text_after_citation": ["Theoretical guarantees of gradient-based algorithms Existing theoretical guarantees for VB only apply to the global solution of the variational optimization problem.", "In practice, this optimization problem tends to be highly nonconvex and algorithms are only guaranteed to converge to local optima.", "For certain variational families and model classes, these local optima can be dramatically different, so that there is a large sensitivity to the starting point of the algorithm.", "It is of critical importance to obtain guarantees on the algorithms being used and not just on inaccessible global optima.", "For example, can one obtain general theoretical guarantees for gradient-based black-box variational inference with or without warm-start conditions?"], "citing_paper_content": {"title": "Machine Learning And The Future Of Bayesian Computation", "abstract": "Bayesian models are a powerful tool for studying complex data, allowing the analyst to encode rich hierarchical dependencies and leverage prior information. Most importantly, they facilitate a complete characterization of uncertainty through the posterior distribution. Practical posterior computation is commonly performed via MCMC, which can be computationally infeasible for high dimensional models with many observations. In this article we discuss the potential to improve posterior computation using ideas from machine learning. Concrete future directions are explored in vignettes on normalizing flows, Bayesian coresets, distributed Bayesian inference, and variational inference."}, "cited_paper_content": {"title": "Frequentist Consistency Of Variational Bayes", "abstract": "ABSTRACTA key challenge for modern Bayesian statistics is how to perform scalable inference of posterior distributions. To address this challenge, variational Bayes (VB) methods have emerged as a p..."}, "keywords": ["corresponding variational families"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of scalable inference of posterior distributions, variational Bayes (VB) methods have emerged as a popular approach #REFR. However, it is well-known that variational posteriors tend to underestimate uncertainty of the posterior, and there is limited work on theory for statistical inference using the variational posterior, including credible intervals and hypothesis testing.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Variational Bayes (VB) methods have emerged as a popular approach for scalable inference of posterior distributions, and their frequentist consistency has been studied extensively in the literature #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Variational Bayes (VB) methods have emerged as a popular approach for scalable inference of posterior distributions, and recent work has focused on constructing computationally efficient VB posteriors that produce credible balls with valid frequentist coverage and posterior covariance matching that of the true posterior, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": ["corresponding variational families"]}]}
{"citing_id": "2305.02247v1", "cited_id": "1509.01240", "section_title": "Stepsize \u0397 T", "citation": "Our results are applicable to convex, nonconvex and strongly-convex losses, as they appear in the second column of Table 1 , and hold essentially verbatim to those in prior work #REFR , initially developed for SGD with batch size 1.", "text_before_citation": ["An important implication of our results is that different mini-batch schedules mainly affect the optimization error (as appears in #OTHEREFR ) rather than the generalization error for the class of Lipschitz and smooth losses.", "We also note that in the smooth (possibly non-Lipschitz, nonconvex) case, the question of optimality of batch schedules other than that of full-batch GD (which we prove to be optimal) remains open.", "More specifically, we establish upper bounds on the generalization error which are uniform over the class of mini-batch GD training schemes with either deterministic or stochastic, dataindependent, but otherwise arbitrary batch schedules; this class includes both deterministic and stochastic training as special cases.", "For instance, the set of algorithms under consideration includes but is not limited to classical SGD with mini-batch of any size m, full-batch GD, round-robin deterministic batch selection, SGD with random reshuffling, SGD with single shuffling, and incremental gradient methods #OTHEREFR .", "For this general set of algorithms, we prove generalization error bounds under a unified framework with respect to any data-independent (randomized or deterministic) batch scheduling rule."], "text_after_citation": ["To evaluate the performance of the algorithms under consideration, we are also interested in deriving minimax -over mini-batch GD algorithms (inf) and learning problem instances (sup)lower bounds directly on the generalization error.", "Prior works have already shown lower bounds on uniform stability (while taking a supremum over the data-set) for non-smooth losses #OTHEREFR and Lipschitz-smooth losses #OTHEREFR , for SGD and full-batch GD.", "However, lower bounds on uniform stability do not imply a lower bound on the generalization error; uniform stability is sufficient but not necessary for generalization.", "Further, recent work on the class of smooth convex losses #OTHEREFR considered the construction of specific -but not fully parameterized-examples to derive lower bounds on the corresponding excess risk (i.e., generalization + optimization), and showed tightness also for the cases of SGD #OTHEREFR and full-batch GD #OTHEREFR .", "Additionally, the construction of an elaborate example to lower bound the excess risk of the (S)GD algorithm appeared in #OTHEREFR ."], "citing_paper_content": {"title": "Select Without Fear: Almost All Mini-Batch Schedules Generalize Optimally", "abstract": "We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/stronglyconvex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones."}, "cited_paper_content": {"title": "Train Faster, Generalize Better: Stability Of Stochastic Gradient Descent", "abstract": "We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. ::: Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit."}, "keywords": ["strongly-convex losses"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice (Train Faster, Generalize Better: Stability Of Stochastic Gradient Descent).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our findings are consistent with the stability analysis of stochastic gradient descent (SGD) by Bousquet and Elisseeff (2002), which shows that SGD is algorithmically stable under standard Lipschitz and smoothness assumptions, and that models trained by SGD with few iterations have vanishing generalization error #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our findings are consistent with the stability bounds for strongly-convex losses derived in Train Faster, Generalize Better: Stability Of Stochastic Gradient Descent #REFR.", "given_citation_intent": "result", "given_keywords": ["strongly-convex losses"]}]}
{"citing_id": "2303.14457v1", "cited_id": "1805.06485", "section_title": "Overall Loss Function", "citation": "FK loss is proposed in #REFR to alleviate the accumulative errors of rotations in local coordinates.", "text_before_citation": ["KL Loss.", "As common in CVAE, we regularize the posterior distribution to normal distribution by optimizing the Kullback-Leibler divergence:", "EQUATION", "where q(\u2022 | \u2022) denotes the inference posterior (encoder).", "FK loss."], "text_after_citation": ["We calculate global positions by local quaternions with forward kinematics (FK) and get the average L1 norm between the calculated positions and real positions. It The FK loss function is:", "EQUATION", "where r represents the global root position. Adversarial Loss.", "We use a generator-discriminator architecture and employ a pair of long-short discriminators #OTHEREFR to improve the motion quality.", "The discriminator is in the form of Least Square GAN #OTHEREFR ."], "citing_paper_content": {"title": "Diverse Motion In-Betweening With Dual Posture Stitching", "abstract": "Figure 1. Three motion transitions generated by our method. The poses are rendered every ten frames. Black: key frames. Gray: generated transitions."}, "cited_paper_content": {"title": "Quaternet: A Quaternion-Based Recurrent Model For Human Motion", "abstract": "Deep learning for predicting or generating 3D human pose sequences is an active research area. Previous work regresses either joint rotations or joint positions. The former strategy is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angle or exponential map parameterizations. The latter requires re-projection onto skeleton constraints to avoid bone stretching and invalid configurations. This work addresses both limitations. Our recurrent network, QuaterNet, represents rotations with quaternions and our loss function performs forward kinematics on a skeleton to penalize absolute position errors instead of angle errors. On short-term predictions, QuaterNet improves the state-of-the-art quantitatively. For long-term generation, our approach is qualitatively judged as realistic as recent neural strategies from the graphics literature."}, "keywords": ["rotations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As common in CVAE, the authors regularize the posterior distribution to normal distribution by optimizing the Kullback-Leibler divergence (KL Loss) #REFR. In addition, their approach addresses the limitations of previous work by representing rotations with quaternions and penalizing absolute position errors instead of angle errors with their recurrent network, QuaterNet, which improves the state-of-the-art quantitatively on short-term predictions and is judged as realistic as recent neural strategies from the graphics literature for long-term generation.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "QuaterNet, a recurrent network that represents rotations with quaternions and penalizes absolute position errors instead of angle errors, has been shown to improve short-term predictions and generate realistic long-term human motion sequences, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Quaternet, a recurrent network that represents rotations with quaternions, could be a promising solution to the limitations of previous work that regresses joint rotations and is prone to error accumulation along the kinematic chain, as well as discontinuities when using Euler angle or exponential map parameterizations, as mentioned in #REFR.", "given_citation_intent": "background", "given_keywords": ["rotations"]}]}
{"citing_id": "2303.02648v1", "cited_id": "1409.0473", "section_title": "Related Work", "citation": "There have been multiple modifications on attention mechanisms including Soft attention #REFR which focuses on specific parts of the input data when making the prediction.", "text_before_citation": ["Markov-Logic Network(MLN) #OTHEREFR uses probabilistic inference to estimate uncertainty in generated captions.", "Captions are passed,attention mechanism is used to add visual context and is passed to MLN. This approach generated captions close to human-generated captions.", "Initially, Image Captioning was tackled using Recurrent Neural Networks (RNNs) #OTHEREFR and Long-Short Term Memory (LSTM) networks.", "Although these methods generated captions effectively, they faced the issue of vanishing gradients where the gradients decreased as the model processed longer sequences.", "To address this challenge, researchers proposed the use of an Attention Mechanism in conjunction with RNNs or LSTMs, allowing the network to focus on the most important parts of the input and make predictions based on relevant information."], "text_after_citation": ["The word \"soft\" implies it uses continuous distribution to weigh the importance of the different parts of input data.", "On the contrary Hard attention selects one region at random and only focuses on that particular area.", "Global attention #OTHEREFR which considers the entire input data globally without giving preference to any specific part.", "It allows the model to learn from all parts of the input and make predictions based on a holistic understanding of the data.", "In Local Attention #OTHEREFR prior to computing the attention weight in the windows to the left and right of the selected point for alignment, first determines a position for alignment. It then gives the context vector a weight."], "citing_paper_content": {"title": "Comparative Study Of Transformer And Lstm Network With Attention Mechanism On Image Captioning", "abstract": "In a globalized world at the present epoch of generative intelligence, most of the manual labour tasks are automated with increased efficiency. This can support businesses to save time and money. A crucial component of generative intelligence is the integration of vision and language. Consequently, image captioning become an intriguing area of research. There have been multiple attempts by the researchers to solve this problem with different deep learning architectures, although the accuracy has increased, but the results are still not up to standard. This study buckles down to the comparison of Transformer and LSTM with attention block model on MS-COCO dataset, which is a standard dataset for image captioning. For both the models we have used pretrained Inception-V3 CNN encoder for feature extraction of the images. The Bilingual Evaluation Understudy score (BLEU) is used to checked the accuracy of caption generated by both models. Along with the transformer and LSTM with attention block models,CLIP-diffusion model, M2-Transformer model and the X-Linear Attention model have been discussed with state of the art accuracy."}, "cited_paper_content": {"title": "Neural Machine Translation By Jointly Learning To Align And Translate", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."}, "keywords": ["Soft attention"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of vanishing gradients in Recurrent Neural Networks (RNNs) and Long-Short Term Memory (LSTM) networks, researchers proposed the use of an Attention Mechanism in conjunction with these models, allowing the network to focus on the most important parts of the input and make predictions based on relevant information #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of the basic encoder-decoder architecture, recent research has proposed to allow a model to automatically search for relevant parts of a source sentence using a soft-alignment mechanism, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of vanishing gradients in Recurrent Neural Networks (RNNs) and Long-Short Term Memory (LSTM) networks, researchers proposed the use of an Attention Mechanism, which allows the network to focus on the most important parts of the input and make predictions based on relevant information, and this approach has been extended by using Soft attention #REFR.", "given_citation_intent": "background", "given_keywords": ["Soft attention"]}]}
{"citing_id": "2303.10600v1", "cited_id": "1905.09198", "section_title": "Extension To Robin Type Transmission Conditions", "citation": "Similarly to the case of Dirichlet transmission conditions, we apply the restriction operator to problem #REFR .", "text_before_citation": ["where the operators A and B are defined as for (3) in (6), #OTHEREFR and the operator C takes the form,", "EQUATION", "If A and B are continuous operators, A is elliptic, B satisfies the infsup condition of Theorem 2, and C is of the form #OTHEREFR , then, owing to Theorem 4.3.2 of #OTHEREFR , problem (20) admits a unique solution u \u2208", "EQUATION", "EQUATION"], "text_after_citation": ["Using the notation defined before, we obtain the following abstract problem: given F \u2208", "EQUATION", "EQUATION", "We note that in the particular case where C is the identity, as in #OTHEREFR , the perturbation term becomes \u03ba R T \u039b, R T w .", "The operators A, RB and RCR T satisfy the assumptions of Theorem 4.3.2 of #OTHEREFR ."], "citing_paper_content": {"title": "Reduced Lagrange Multiplier Approach For Non-Matching Coupling Of Mixed-Dimensional Domains", "abstract": "Many physical problems involving heterogeneous spatial scales, such as the flow through fractured porous media, the study of fiber-reinforced materials or the modeling of the small circulation in living tissues-just to mention a few examples-can be described as coupled partial differential equations defined in domains of heterogeneous dimensions that are embedded into each other. This formulation is a consequence of geometric model reduction techniques that transform the original problems defined in complex three-dimensional domains into more tractable ones. The definition and the approximation of coupling operators suitable for this class of problems is still a challenge. The main objective of this work is to develop a general mathematical framework for the analysis and the approximation of partial differential equations coupled by nonmatching constraints across different dimensions. Considering the non standard formulation of the coupling conditions, we focus on their enforcement using Lagrange multipliers. In this context we address in abstract and general terms the well posedness, the stability, the robustness of the problem with respect to the smallest characteristic length of the embedded domain. We also address the the numerical approximation of the problem and we discuss the inf-sup stability of the proposed numerical scheme for some representative configuration of the embedded domain. The main message of this work is twofold: from the standpoint of the theory of mixed-dimensional problems, we provide general and abstract mathematical tools to formulate coupled problems across dimensions. From the practical standpoint of the numerical approximation, we show the interplay between the mesh characteristic size, the dimension of the Lagrange multiplier space and the size of the inclusion in representative configurations interesting for applications. The latter analysis is complemented with illustrative numerical examples."}, "cited_paper_content": {"title": "Error Estimates In Weighted Sobolev Norms For Finite Element Immersed Interface Methods", "abstract": "When solving elliptic partial differential equations in a region containing immersed interfaces (possibly evolving in time), it is often desirable to approximate the problem using an independent background discretisation, not aligned with the interface itself. Optimal convergence rates are possible if the discretisation scheme is enriched by allowing the discrete solution to have jumps aligned with the surface, at the cost of a higher complexity in the implementation. A much simpler way to reformulate immersed interface problems consists in replacing the interface by a singular force field that produces the desired interface conditions, as done in immersed boundary methods. These methods are known to have inferior convergence properties, depending on the global regularity of the solution across the interface, when compared to enriched methods. In this work we prove that this detrimental effect on the convergence properties of the approximate solution is only a local phenomenon, restricted to a small neighbourhood of the interface. In particular we show that optimal approximations can be constructed in a natural and inexpensive way, simply by reformulating the problem in a distributionally consistent way, and by resorting to weighted norms when computing the global error of the approximation."}, "keywords": ["Dirichlet transmission conditions"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In particular we show that optimal approximations can be constructed in a natural and inexpensive way, simply by reformulating the problem in a distributionally consistent way, and by resorting to weighted norms when computing the global error of the approximation. This approach is similar to the one proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors reformulate the problem in a distributionally consistent way and use weighted norms to compute the global error of the approximation, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enforce Dirichlet transmission conditions across different dimensions, the authors utilize a weighted Sobolev norm approach, as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["Dirichlet transmission conditions"]}]}
{"citing_id": "2303.09843v1", "cited_id": "1810.00736", "section_title": "Dataset", "citation": "The MobileNet backbone and our ResNet-18 backbone have been shown to have very similar performance #REFR .", "text_before_citation": ["All of our experiments are based on the Cityscapes dataset #OTHEREFR during training and evaluation in the segmentation task, but they are used to evaluate the uncertainty outputs as they indicate the model's ability to distinguish between in-domain and out-of-domain samples.", "Table 1 and Table 2 outline a quantitative comparison between the student's and the teacher's Intersection over Union (IoU) as well as their predictive uncertainties.", "The results of Holder and Shafique #OTHEREFR have been included as they are the most relevant previous work on DE-based student-teacher distillation for efficient UQ.", "Their teacher is based on 25 DeepLabv3+ models with a MobileNet backbone #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "Dudes: Deep Uncertainty Distillation Using Ensembles For Semantic Segmentation", "abstract": "Deep neural networks lack interpretability and tend to be overconfident, which poses a serious problem in safety-critical applications like autonomous driving, medical imaging, or machine vision tasks with high demands on reliability. Quantifying the predictive uncertainty is a promising endeavour to open up the use of deep neural networks for such applications. Unfortunately, current available methods are computationally expensive. In this work, we present a novel approach for efficient and reliable uncertainty estimation which we call Deep Uncertainty Distillation using Ensembles for Segmentation (DUDES). DUDES applies student-teacher distillation with a Deep Ensemble to accurately approximate predictive uncertainties with a single forward pass while maintaining simplicity and adaptability. Experimentally, DUDES accurately captures predictive uncertainties without sacrificing performance on the segmentation task and indicates impressive capabilities of identifying wrongly classified pixels and out-of-domain samples on the Cityscapes dataset. With DUDES, we manage to simultaneously simplify and outperform previous work on Deep Ensemble-based Uncertainty Distillation."}, "cited_paper_content": {"title": "Benchmark Analysis Of Representative Deep Neural Network Architectures", "abstract": "This work presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online."}, "keywords": ["ResNet-18 backbone", "similar performance"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The results of Holder and Shafique (2019) have been included as they are the most relevant previous work on DE-based student-teacher distillation for efficient UQ. Their teacher is based on 25 DeepLabv3+ models with a MobileNet backbone, and to evaluate the uncertainty outputs, we use the Cityscapes dataset during training and evaluation in the segmentation task, as it indicates the model's ability to distinguish between in-domain and out-of-domain samples. Table 1 and Table 2 outline a quantitative comparison between the student's and the teacher's Intersection over Union (IoU) as well as their predictive uncertainties. The benchmark analysis of representative deep neural network architectures presented in the work by Canziani et al. (2016) provides a comprehensive view of the state-of-the-art DNNs for image recognition, which can be useful for selecting the appropriate architecture(s) for practical deployments and applications. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Holder and Shafique's work on Deep Ensemble-based student-teacher distillation for efficient uncertainty quantification (UQ) #REFR is the most relevant previous work to our study, and we compare our results with theirs in Table 1 and Table 2.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Holder and Shafique (2019) have also used a DeepLabv3+ model with a MobileNet backbone, and their results show that ResNet-18 backbone models can achieve similar performance with lower computational complexity. #REFR", "given_citation_intent": "result", "given_keywords": ["ResNet-18 backbone", "similar performance"]}]}
{"citing_id": "2304.05967v1", "cited_id": "1904.05419", "section_title": "Visualization Tools For Supporting Error Discovery.", "citation": "For example, FairVis #REFR uses visualizations to help ML developers discover model bias by investigating known subgroups and exploring similar groups in the tabular data.", "text_before_citation": ["Interactive visualization is a powerful method for helping ML developers explore and interpret their models #OTHEREFR .", "While many visualizations have been built to help practitioners evaluate models over time, one area of recent work has focused on designing and developing analytic tools for ML error discovery [e.g. #OTHEREFR ."], "text_after_citation": ["Similarly, Visual Auditor #OTHEREFR automatically surfaces underperforming subgroups and leverages graph visualizations to help practitioners interpret the relationships between subgroups and discover new errors.", "For image data, explAIner #OTHEREFR combines interactive visualization and post-hoc ML explanation techniques [e.g., #OTHEREFR to help practitioners diagnose problems with image classifiers.", "For text data, Seq2seq-Vis #OTHEREFR helps practitioners debug sequence-to-sequence models by visualizing the model's internal mechanisms throughout each of its inference stages.", "The success of these recent visual ML diagnosis systems highlights the outstanding potential of applying visualization techniques to help ML developers detect errors.", "Instead of visualizing a model's internals [e.g., #OTHEREFR , we treat ML models as black-box systems and focus on probing their behaviors on different data subsets."], "citing_paper_content": {"title": "Angler: Helping Machine Translation Practitioners Prioritize Model Improvements", "abstract": "Angler enables ML developers to easily explore and curate challenge sets for machine translation. (A) The Table View lists all challenge sets, allowing users to compare them by metrics such as sample count, model performance, and familiarity score. After selecting a set, (B) the Detail View allows users to further explore samples in this set across various dimensions. (B1) The Timeline enables users to query data samples by time. (B2) The Spotlight presents visualizations with linking and brushing to help users characterize the set from different angles. (B3) The Sentence List shows all selected data samples and allows users to further fine-tune before exporting this challenge set for downstream tasks."}, "cited_paper_content": {"title": "Fairvis: Visual Analytics For Discovering Intersectional Bias In Machine Learning", "abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems."}, "keywords": ["ML developers", "model bias"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Interactive visualization has been shown to be a powerful method for helping ML developers explore and interpret their models, and recent work has focused on designing and developing analytic tools for ML error discovery. One such tool is FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Interactive visualization has been shown to be a powerful method for helping ML developers explore and interpret their models, and recent work has focused on designing and developing analytic tools for ML error discovery, such as FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Interactive visualization is a powerful method for helping ML developers explore and interpret their models, and recent work has focused on designing and developing analytic tools for ML error discovery, as well as discovering biases in machine learning models #REFR.", "given_citation_intent": "background", "given_keywords": ["ML developers", "model bias"]}]}
{"citing_id": "2304.00483v1", "cited_id": "1810.04805", "section_title": "Introduction", "citation": "In this work, we focus on language models with a Transformer encoder architecture such as BERT #REFR , that extract relevant outputs from a domain-specific evidence-based text corpus.", "text_before_citation": ["Large Language Models (LLMs), such as Generative Pre-trained Transformer 3 (GPT-3) #OTHEREFR , generate text that is grammatically correct, fluent, and informative.", "However, there is little to no control over the data that were used for model training.", "Consequently, LLMs are prone to hallucinating and providing untruthful outputs .", "Ironically, this reflects LLMs' ability to be better at learning the training distribution and consequently follow inverse scaling law #OTHEREFR .", "And while some of the recent research efforts are focused on providing explanations of where the LLM's outputs came from #OTHEREFR , such research is in its infancy."], "text_after_citation": ["Deep neural networks trained on domain-specific datasets, including those used in Natural Language Processing (NLP), are most heavily dependent on the quality of the training dataset, which is usually small in size #OTHEREFR as it is costly to engage a large number of domain experts for annotation.", "It is thus important to create high-quality training data for language models to perform better.", "In this paper, we propose a data-centric framework for Machine Reading Comprehension (MRC) datasets that increases the original dataset quality by both (i) keeping the size of the original dataset fixed, and (ii) augmenting the original dataset by adding new training samples.", "MRC is a Natural Language Understanding (NLU) task.", "Its goal is to answer questions based on the information provided in a passage ."], "citing_paper_content": {"title": "A Data-Centric Framework For Improving Domain-Specific Machine Reading Comprehension Datasets", "abstract": "Low-quality data can cause downstream problems in high-stakes applications. Datacentric approach emphasizes on improving dataset quality to enhance model performance. High-quality datasets are needed for generalpurpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets 1. We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["language models"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Low-quality data can cause downstream problems in high-stakes applications. Data-centric approach emphasizes on improving dataset quality to enhance model performance. High-quality datasets are needed for general-purpose Large Language Models (LLMs) training, as well as for domain-specific models, which are usually small in size as it is costly to engage a large number of domain experts for their creation. Thus, it is vital to ensure high-quality domain-specific training data. In this paper, we propose a framework for enhancing the data quality of original datasets 1. We applied the proposed framework to four biomedical datasets and showed relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation to enhance the original dataset quality. Interestingly, recent research efforts have shown that pre-training deep bidirectional representations from unlabeled text can create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the quality of domain-specific training data, the proposed framework in this paper applies back translation to the original datasets, resulting in relative improvements of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the quality of domain-specific training data for language models, the authors propose a framework that enhances the data quality of original datasets, which has been shown to achieve relative improvement of up to 33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when using back translation #REFR. The proposed method from the reference paper can be useful for improving the quality of language models.", "given_citation_intent": "method", "given_keywords": ["language models"]}]}
{"citing_id": "2305.01691v1", "cited_id": "1202.1347", "section_title": "(2.9)", "citation": "Discretizations with finite lower bandwidths for the differential operators studied in this paper are provided by the ultraspherical spectral method #REFR .", "text_before_citation": ["Combining Theorems 2.2 and 2.3, we find that for any integer n we have (2.12) z \u2208 \u2126 \u03b3 n (z, T ) < \u2282 \u039b (T ).", "Moreover, since the convergence of \u03b3 n (z, T ) to T (z) \u22121 \u22121 is locally uniform, these approximations converge to \u039b (T ) as n \u2192 \u221e without spectral pollution or spectral invisibility.", "4 There are generally two ways to make this a practical computation:", "\u2022 If we have discretizations of the finite-rank operators T (z)P * n and T (z) * P * n that have finite lower bandwidths (or are well approximated by such matrices), we take rectangular truncations capturing the full range #OTHEREFR .", "With respect to the appropriate norms, which can differ in the domain and range space, the smallest singular values of the resulting matrices are the same as those of T (z)P * n and T (z) * P * n ."], "text_after_citation": ["\u2022 If we have discretizations of P n T (z) * T (z)P * n and P n T (z)T (z) * P * n , then we can compute their smallest singular values and take square roots to compute \u03b3 n #OTHEREFR .", "The first method should be preferred over the second wherever possible since it avoids the loss of precision owing to the square root.", "There are situations where the second method seems unavoidable #OTHEREFR .", "It can be shown that it is not always possible to compute \u039b (T ) by discretizing with square, finite sections P n T P * n of T #OTHEREFR .", "One must be careful if one wants to compute \u039b (T ) by discretizing first."], "citing_paper_content": {"title": "Avoiding Discretization Issues For Nonlinear Eigenvalue Problems", "abstract": "The first step when solving an infinite-dimensional eigenvalue problem is often to discretize it. We show that one must be extremely careful when discretizing nonlinear eigenvalue problems. Using examples, we show that discretization can: (1) introduce spurious eigenvalues, (2) entirely miss spectra, and (3) bring in severe ill-conditioning. While there are many eigensolvers for solving matrix nonlinear eigenvalue problems, we propose a solver for general holomorphic infinitedimensional nonlinear eigenvalue problems that avoids discretization issues, which we prove is stable and converges. Moreover, we provide an algorithm that computes the problem's pseudospectra with explicit error control, allowing verification of computed spectra. The algorithm and numerical examples are publicly available in infNEP, which is a software package written in MATLAB."}, "cited_paper_content": {"title": "A Fast And Well-Conditioned Spectral Method", "abstract": "A spectral method is developed for the direct solution of linear ordinary differential equations with variable coefficients. The method leads to matrices which are almost banded, and a numerical solver is presented that takes O(m^2n) operations, where m is the number of Chebyshev points needed to resolve the coefficients of the differential operator and n is the number of Chebyshev coefficients needed to resolve the solution to the differential equation. We prove stability of the method by relating it to a diagonally preconditioned system which has a bounded condition number, in a suitable norm. For Dirichlet boundary conditions, this implies stability in the standard 2-norm. An adaptive QR factorization is developed to efficiently solve the resulting linear system and automatically choose the optimal number of Chebyshev coefficients needed to represent the solution. The resulting algorithm can efficiently and reliably solve for solutions that require as many as a million unknowns."}, "keywords": ["ultraspherical spectral method"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors utilized an adaptive QR factorization to efficiently solve the resulting linear system and automatically choose the optimal number of Chebyshev coefficients needed to represent the solution, as presented in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors efficiently solve linear ordinary differential equations with variable coefficients using an adaptive QR factorization method that automatically selects the optimal number of Chebyshev coefficients needed to represent the solution, as presented in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The ultraspherical spectral method proposed in #REFR can efficiently and reliably solve for solutions that require as many as a million unknowns, which avoids the discretization issues that can arise when solving nonlinear eigenvalue problems.", "given_citation_intent": "method", "given_keywords": ["ultraspherical spectral method"]}]}
{"citing_id": "2304.03756v1", "cited_id": "1706.04694", "section_title": "Related Work", "citation": "It has also been suggested that a lack of truthfulness in a robot's statement can negatively affect a human's evaluation of a robot's abilities #REFR .", "text_before_citation": ["Works in human factors have attempted to measure perceived safety through physiological signals that can indicate stress, fear, or anxiety #OTHEREFR .", "Some of this work has been applied to industrial manipulators #OTHEREFR , though none have been applied in a fast-paced HRC setting to the best of our knowledge.", "Communication in HRC -Prior work displayed that verbal communication in HRC affects collaboration as well as users' perception of the robot #OTHEREFR .", "Studies have shown that verbal communication between the robot and the user improves subjective and objective metrics on team performance and users prefer a speaking robot over a non-speaking one #OTHEREFR .", "Furthermore, #OTHEREFR found that autonomous agents who exchanged information regarding their intention (i.e., deliberative communication #OTHEREFR ) collaborated with humans better than those sharing information in regard to the world state."], "text_after_citation": ["Although these studies produced interesting findings about robot communication, the corresponding tasks did not have time pressure, deviating from real-time decision-making.", "In our experiment, the robot asserts its intention via a short verbal statement in a male/female voice to inform the participant that it will attempt to hit an incoming table tennis ball.", "The verbal communication is done in real-time by an agile robot conducting a high-speed striking maneuver.", "Deviating from prior literature, we additionally allow the human to communicate intent with the robot (bidirectional communication), creating a realistic, rapid, proximate HRC scenario where the human must reason about received communication, whether to communicate intent, which agent should hit the ball, and the ball trajectory, all within a short duration.", "Agile Robot Manipulation -Agile robotic systems that work alongside human beings is a long-standing goal in manufacturing, where high-speed robotic maneuvers are safely conducted with humans collaborating in proximity with the robot."], "citing_paper_content": {"title": "The Effect Of Robot Skill Level And Communication In Rapid, Proximate Human-Robot Collaboration", "abstract": "As high-speed, agile robots become more commonplace, these robots will have the potential to better aid and collaborate with humans. However, due to the increased agility and functionality of these robots, close collaboration with humans can create safety * Both authors contributed equally to this research."}, "cited_paper_content": {"title": "Planning With Verbal Communication For Human-Robot Collaboration", "abstract": "Human collaborators coordinate effectively their actions through both verbal and non-verbal communication. We believe that the the same should hold for human-robot teams. We propose a formalism that enables a robot to decide optimally between taking a physical action toward task completion and issuing an utterance to the human teammate. We focus on two types of utterances: verbal commands, where the robot asks the human to take a physical action, and state-conveying actions, where the robot informs the human about its internal state, which captures the information that the robot uses in its decision making. Human subject experiments show that enabling the robot to issue verbal commands is the most effective form of communicating objectives, while retaining user trust in the robot. Communicating information about the robot\u2019s state should be done judiciously, since many participants questioned the truthfulness of the robot statements when the robot did not provide sufficient explanation about its actions."}, "keywords": ["robot's abilities"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Furthermore, previous work has shown that enabling a robot to issue verbal commands is the most effective form of communicating objectives in human-robot collaboration, while retaining user trust in the robot #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Verbal communication has been shown to improve collaboration and user perception of the robot in human-robot collaboration (HRC) #REFR, and autonomous agents who exchanged information regarding their intention collaborated better with humans than those sharing information in regard to the world state #OTHEREFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Robots' abilities to communicate with humans through verbal commands and state-conveying actions have been shown to be effective in improving collaboration and user trust, as demonstrated in previous studies #REFR.", "given_citation_intent": "background", "given_keywords": ["robot's abilities"]}]}
{"citing_id": "2304.01224v1", "cited_id": "1201.0490", "section_title": "Discussion: Example Of Data Interaction Matrices", "citation": "In this section, we analyze different types of interaction. We select the Circle dataset #REFR .", "text_before_citation": [], "text_after_citation": ["Its input features are 2D points and it is a binary classification task.", "The distribution of the two classes represents two concentric circles, See Figure 3 . The two classes are balanced. Each class contains 300 generated points.", "In the matrix, the points are first sorted by their class (0 or 1).", "Within each class, the points are sorted based on their input feature x 1 , and then by x 2 .", "For example, the point at index 0 is a blue point (class 0) with the smallest x 1 value."], "citing_paper_content": {"title": "Optimizing Data Shapley Interaction Calculation From O(2 N ) To O(Tn 2 ) For Knn Models", "abstract": "With the rapid growth of data availability and usage, quantifying the added value of each training data point has become a crucial process in the field of artificial intelligence. The Shapley values have been recognized as an effective method for data valuation, enabling efficient training set summarization, acquisition, and outlier removal. In this paper, we introduce \"STI-KNN\", an innovative algorithm that calculates the exact pair-interaction Shapley values for KNN models in O(tn 2) time, which is a significant improvement over the O(2 n) time complexity of baseline methods. By using STI-KNN, we can efficiently and accurately evaluate the value of individual data points, leading to improved training outcomes and ultimately enhancing the effectiveness of artificial intelligence applications."}, "cited_paper_content": {"title": "Scikit-Learn: Machine Learning In Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."}, "keywords": ["Circle dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "With its wide range of machine learning algorithms and emphasis on ease of use, performance, and documentation, the Scikit-Learn module #REFR is a valuable resource for non-specialists in the field of artificial intelligence.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently implement the Shapley values for KNN models, the authors utilized the scikit-learn Python module #REFR, which integrates a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently evaluate the value of individual data points in the Circle dataset, the authors utilized the \"Scikit-Learn: Machine Learning In Python\" package, as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["Circle dataset"]}]}
{"citing_id": "2303.06155v1", "cited_id": "1909.09145", "section_title": "I. Introduction", "citation": "Moreover, in KD only the output of the teacher model is needed to transmit thus the transmission burden is much less than SL #REFR . Li et al.", "text_before_citation": ["As a promising technology, KD achieves success in many fields, since it can transfer knowledge in a big well-trained NN, named teacher model, to an arbitrary small NN, named student model, by providing the outputs of the big NN as soft labels for student model and using these soft labels as an additional gradient to train the student model.", "Therefore, the performance of the student model can be improved.", "Since in the KD method arbitrary NN can be used as a student model, thus different users can select the different models on demand in FL systems.", "Therefore, by using the KD method, the straggle in the training procedure of FL can be avoided.", "Because the NNs are not the same between different users, thus there is no need to directly aggregate the NNs of different users, even if the user with more computational resources performs several more training epochs, there is no gradient confusion among different user NNs."], "text_after_citation": ["combine migration learning and knowledge refinement for personalized federal learning, and each customer can use private data and a specific model for training. Zhou et al.", "proposed distilled one-shot federated learning (DOSFL), thus significantly reducing communication costs while ensuring model accuracy #OTHEREFR .", "Although the KD enables users to select different models based on their demands, however, in KD, a big teacher model is needed, and training such a model usually relay on a huge dataset.", "Therefore, just using one user's limited dataset cannot train the teacher model well. However.", "the teacher model is usually too big to train in users' CPU."], "citing_paper_content": {"title": "Digital Twin-Assisted Knowledge Distillation Framework For Heterogeneous Federated Learning", "abstract": "In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimization is used to allocate resources for users based on the output of Q-learning. Simulation results show the proposed DT-assisted KD framework and joint optimization method can significantly improve the average accuracy of users while reducing the total delay."}, "cited_paper_content": {"title": "Detailed Comparison Of Communication Efficiency Of Split Learning And Federated Learning", "abstract": "We compare communication efficiencies of two compelling distributed machine learning approaches of split learning and federated learning. We show useful settings under which each method outperforms the other in terms of communication efficiency. We consider various practical scenarios of distributed learning setup and juxtapose the two methods under various real-life scenarios. We consider settings of small and large number of clients as well as small models (1M - 6M parameters), large models (10M - 200M parameters) and very large models (1 Billion-100 Billion parameters). We show that increasing number of clients or increasing model size favors split learning setup over the federated while increasing the number of data samples while keeping the number of clients or model size low makes federated learning more communication efficient."}, "keywords": ["teacher model", "transmission burden"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As a promising technology, KD achieves success in many fields, since it can transfer knowledge in a big well-trained NN, named teacher model, to an arbitrary small NN, named student model, by providing the outputs of the big NN as soft labels for student model and using these soft labels as an additional gradient to train the student model. Therefore, the performance of the student model can be improved. Since in the KD method arbitrary NN can be used as a student model, thus different users can select the different models on demand in FL systems. Therefore, by using the KD method, the straggle in the training procedure of FL can be avoided. Because the NNs are not the same between different users, thus there is no need to directly aggregate the NNs of different users, even if the user with more computational resources performs several more training epochs, there is no gradient confusion among different user NNs (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the communication efficiency in federated learning, split learning and federated learning are two compelling distributed machine learning approaches that have been compared in terms of communication efficiency under various real-life scenarios, as shown in the study by #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed KD-driven training framework for FL allows each user to select its own neural network model and distill knowledge from a big #REFR[teacher model], which can significantly reduce the transmission burden in FL systems.", "given_citation_intent": "background", "given_keywords": ["teacher model", "transmission burden"]}]}
{"citing_id": "2303.06318v1", "cited_id": "1604.06174", "section_title": "Communication-Aware Activation Checkpointing (Cac)", "citation": "We now turn our attention to a second source of redundant communication in large model training, namely activation checkpointing #REFR .", "text_before_citation": [], "text_after_citation": ["Intermediate activations in a neural network generated during the forward pass need to be stashed in memory as they are required during the backward pass for the gradient computation.", "However, for large model training, storing all the activations can leads to tremendous memory overhead.", "Activation checkpointing alleviates this issue by storing only a subset of the activations, which are essentially just the input activations of every layer.", "During the backward pass of a layer, the remaining activations are re materialized from its stashed input activation by doing a local forward pass for that layer.", "Thus, activation checkpointing saves activation memory at the expense of a duplicate forward pass for every layer, and is almost always used for training large neural networks."], "citing_paper_content": {"title": "A Novel Tensor-Expert Hybrid Parallelism Approach To Scale Mixture-Of-Experts Training", "abstract": "A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4 \u2212 8\u00d7 larger base models than the current state-of-theart-DeepSpeed-MoE. We propose memory optimizations in the optimizer step, and communication optimizations that eliminate redundant movement of data. Removing these redundancies provides a speedup of nearly 21%. When training a 40 billion parameter MoE model (6.7 billion base model with 16 experts) on 128 V100 GPUs, our optimizations significantly improve the peak half precision flop/s from 20% to 27%."}, "cited_paper_content": {"title": "Training Deep Nets With Sublinear Memory Cost", "abstract": "We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O( \u221a n) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(logn) with as little as O(n logn) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences."}, "keywords": ["large model training"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To reduce the memory consumption of deep neural network training, the authors of this manuscript propose a systematic approach that costs O( \u221a n) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of memory consumption in deep neural network training, the authors of this manuscript could draw on the work of Zhang et al. who proposed a systematic approach to reduce memory consumption by designing an algorithm that costs O( \u221a n) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch, allowing for deeper and more complex models to be explored (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable the training of larger base models for Mixture-of-Experts (MoE) models, memory-efficient training algorithms are required, and recent work has proposed a systematic approach to reduce the memory consumption of deep neural network training, which can significantly reduce the memory cost of #REFR large model training.", "given_citation_intent": "background", "given_keywords": ["large model training"]}]}
{"citing_id": "2303.03950v1", "cited_id": "1907.01507", "section_title": "Introduction", "citation": "As shown in #REFR , in the underparametrized setting, there exist input data such that for all output data from a set of positive Lebesgue measure there does not exist minimizers in the optimization landscape.", "text_before_citation": ["However, for general measures \u00b5 not necessarily consisting of a finite number of Dirac measures, the literature on the existence of global minima is very limited.", "There exist positive results for the approximation of functions in the space", ") with shallow feedforward ANNs using heavyside activation #OTHEREFR , the approximation of one-dimensional Lipschitz continuous target functions with shallow feedforward ANNs using ReLU activation and the standard mean square error #OTHEREFR , and the approximation of multi-dimensional, continuous target functions with shallow residual ANNs using ReLU activation, see #OTHEREFR .", "Conversely, for several common (smooth) activations such as the standard logistic activation, softplus, arctan, hyperbolic tangent and softsign there, generally, do not exist minimizers in the optimization landscape for smooth target functions (or even polynomials), see #OTHEREFR and #OTHEREFR .", "This phenomenon can also be observed in empirical risk minimization for the hyperbolic tangent activation."], "text_after_citation": ["It remains an open problem whether for ReLU activation this phenomenon prevails.", "In this article, we prove, for the first time, existence results for shallow feedforward ReLU ANNs with multi-dimensional input space.", "Interestingly, minimizers exist under very mild assumptions on the optimization problem.", "This existence property indicates the robustness of ReLU activation and may be a reason for its success in practical applications. For the proof we proceed as follows.", "First, we show existence of minimizers in an extended target space that comprises of the representable responses of ANNs and additional discontinuous generalized responses."], "citing_paper_content": {"title": "On The Existence Of Optimal Shallow Feedforward Networks With Relu Activation", "abstract": "We prove existence of global minima in the loss landscape for the approximation of continuous target functions using shallow feedforward artificial neural networks with ReLU activation. This property is one of the fundamental artifacts separating ReLU from other commonly used activation functions. We propose a kind of closure of the search space so that in the extended space minimizers exist. In a second step, we show under mild assumptions that the newly added functions in the extension perform worse than appropriate representable ReLU networks. This then implies that the optimal response in the extended target space is indeed the response of a ReLU network."}, "cited_paper_content": {"title": "Best K-Layer Neural Network Approximations", "abstract": "We investigate the geometry of the empirical risk minimization problem for $k$-layer neural networks. We will provide examples showing that for the classical activation functions $\\sigma(x)= 1/\\bigl(1 + \\exp(-x)\\bigr)$ and $\\sigma(x)=\\tanh(x)$, there exists a positive-measured subset of target functions that do not have best approximations by a fixed number of layers of neural networks. In addition, we study in detail the properties of shallow networks, classifying cases when a best $k$-layer neural network approximation always exists or does not exist for the ReLU activation $\\sigma=\\max(0,x)$. We also determine the dimensions of shallow ReLU-activated networks."}, "keywords": ["minimizers"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the ReLU activation, it has been shown that there exists a positive-measured subset of target functions that do not have best approximations by a fixed number of layers of neural networks, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For the classical activation functions, it has been shown that there exists a positive-measured subset of target functions that do not have best approximations by a fixed number of layers of neural networks, as demonstrated in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For the ReLU activation, there exist minimizers in the optimization landscape for smooth target functions, which is shown in the approximation of one-dimensional Lipschitz continuous target functions with shallow feedforward ANNs using ReLU activation and the standard mean square error #REFR.", "given_citation_intent": "background", "given_keywords": ["minimizers"]}]}
{"citing_id": "2304.11625v1", "cited_id": "1707.00819", "section_title": "Macro-And Micro-Intervention", "citation": "This example violates the consistency condition set out in #REFR . We elaborate in Appendix A.", "text_before_citation": ["Further, U \u2192 V \u2208 E if U and V are micro-variables and U is a parent of V , and S \u2194S \u2208 C ifS is the aggregation of S.", "The first consequence of aggregation is that the aggregated variables may no longer show a well-defined causal relation: Example 1 Consider the cause-effect relation X \u2192 Y , with X = (X 1 , X 2 ) and Y = (Y 1 , Y 2 ). For j = 1, 2, set", "Y j := \u03b1 j X j .", "DefineX := \u03c0 X (X 1 , X 2 ) = X 1 + X 2 , and likewise\u0232 := \u03c0 Y (Y 1 , Y 2 ) = Y 1 + Y 2 .", "Then, for \u03b1 1 = \u03b1 2 , the effect of settingX tox on\u0232 is ill-defined without any further specification."], "text_after_citation": ["In reality, the case where \u03b1 1 = \u03b1 2 is rare, and the generic cases are when \u03b1 1 = \u03b1 2 . Since the operation 'settingX t\u014d", "x' is not well-defined a priori, its micro-realization needs to be specified whenever talking about it.", "More precisely, we need to specify a distribution according to which we randomize the micro state (the 'joint manipulation' in #OTHEREFR ): Definition 5 (macro intervention) Given an aggregation map, \u03c0 : X \u2192X , a (stochastic) macro-intervention atx, denoted do(x), is a probability measure over X , denoted", "P do \u03c0,x (X), such that \u03c0(x) =x \u2200x \u2208 supp(P do \u03c0,x (X))", "."], "citing_paper_content": {"title": "Meaningful Causal Aggregation And Paradoxical Confounding", "abstract": "In aggregated variables the impact of interventions is typically ill-defined because different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables. We show that this ill-definedness of causality on aggregated variables can turn unconfounded causal relations into confounded ones and vice versa, depending on the respective microrealization. We argue that it is practically infeasible to only use aggregated causal systems when we are free from this ill-definedness. Instead, we need to accept that macro causal relations are typically defined only with reference to the micro states. On the positive side, we show that causeeffect relations can be aggregated when the macro interventions are such that the distribution of micro states is the same as in the observational distribution and also discuss generalizations of this observation."}, "cited_paper_content": {"title": "Causal Consistency Of Structural Equation Models", "abstract": "Complex systems can be modelled at various levels of detail. Ideally, causal models of the same system should be consistent with one another in the sense that they agree in their predictions of the effects of interventions. We formalise this notion of consistency in the case of Structural Equation Models (SEMs) by introducing exact transformations between SEMs. This provides a general language to consider, for instance, the different levels of description in the following three scenarios: (a) models with large numbers of variables versus models in which the `irrelevant' or unobservable variables have been marginalised out; (b) micro-level models versus macro-level models in which the macro-variables are aggregate features of the micro-variables; (c) dynamical time series models versus models of their stationary behaviour. Our analysis stresses the importance of well specified interventions in the causal modelling process and sheds light on the interpretation of cyclic SEMs."}, "keywords": ["consistency condition"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors' analysis stresses the importance of well-specified interventions in the causal modeling process #REFR, which is relevant to the ill-definedness of causality on aggregated variables and the interpretation of cyclic SEMs discussed in the manuscript.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The ill-definedness of causality on aggregated variables can lead to confounded causal relations, as different micro-realizations of the same macro-intervention can result in different changes of downstream macro-variables (Causal Consistency Of Structural Equation Models). #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The notion of consistency in Structural Equation Models (SEMs) is formalized by introducing exact transformations between SEMs, which provides a general language to consider different levels of description, including models with large numbers of variables versus models in which the 'irrelevant' or unobservable variables have been marginalised out, micro-level models versus macro-level models, and dynamical time series models versus models of their stationary behavior. #REFR", "given_citation_intent": "background", "given_keywords": ["consistency condition"]}]}
{"citing_id": "2304.11357v1", "cited_id": "1907.08194", "section_title": "Introduction", "citation": "Currently, neuro-symbolic solutions rely either on costly pre-training methods or on additional supervision at the symbolic representation level provided by the neural network, in order to effectively utilize subsequent learning feedback from the logical component #REFR .", "text_before_citation": ["Recently, neuro-symbolic learning has received attention as a new approach for integrating symbolic-based and sub-symbolic methods based on neural networks.", "This integration provides new capabilities in terms of perception and reasoning."], "text_after_citation": ["This traditional top-down learning paradigm is subject to the problem of representational collapse.", "To gain a clearer understanding of the problem, imagine we have a tuple of three images, each of which contains a single digit (e.g., < 3, 5, 8 >).", "Along with this, we have information about the logical relationships between these digits (e.g., the third digit is the sum of the first two).", "Note that this task introduces less supervision compared to the digit addition experiment typically used in neuro-symbolic systems (i.e. the information about the sum is not provided).", "Current neuro-symbolic solutions can easily solve the task by mapping all input data onto the same symbol 0 and clearly solve the constrained task."], "citing_paper_content": {"title": "Learning Symbolic Representations Through Joint Generative And Discriminative Training", "abstract": "We introduce GEDI, a Bayesian framework that combines existing selfsupervised learning objectives with likelihood-based generative models. This framework leverages the benefits of both GEnerative and DIscriminative approaches, resulting in improved symbolic representations over standalone solutions. Additionally, GEDI can be easily integrated and trained jointly with existing neuro-symbolic frameworks without the need for additional supervision or costly pre-training steps. We demonstrate through experiments on real-world data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing selfsupervised learning strategies in terms of clustering performance by a significant margin. The symbolic component further allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime."}, "cited_paper_content": {"title": "Deepproblog: Neural Probabilistic Logic Programming", "abstract": "We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples."}, "keywords": ["neuro-symbolic solutions"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, neuro-symbolic learning has received attention as a new approach for integrating symbolic-based and sub-symbolic methods based on neural networks. This integration provides new capabilities in terms of perception and reasoning. DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates, has demonstrated the ability to support both symbolic and subsymbolic representations and inference, program induction, probabilistic programming, and learning from examples, making it a relevant tool for this approach. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Neuro-symbolic learning has recently gained attention for its ability to integrate symbolic-based and sub-symbolic methods, providing new capabilities in perception and reasoning. DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates, is a promising method for achieving this integration and has been shown to support both symbolic and subsymbolic representations and inference, program induction, probabilistic programming, and deep learning from examples. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Neuro-symbolic solutions have recently gained attention as a promising approach for integrating symbolic-based and sub-symbolic methods, and DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates, is a notable #REFR method that exploits the full expressiveness and strengths of both worlds.", "given_citation_intent": "method", "given_keywords": ["neuro-symbolic solutions"]}]}
{"citing_id": "2304.01430v1", "cited_id": "1901.07017", "section_title": "Preliminaries: Slot Attention Autoencoder", "citation": "Each slot is then decoded independently with a spatial broadcast decoder #REFR g producing slot reconstruc-tions\u00ce i 's and segmentation masks m i 's.", "text_before_citation": [", called slots, in the bottleneck, where n may change anytime during training or testing.", "Slots are initially sampled from a Gaussian with learned mean and standard deviation, without conditioning additional variables, including the slot ID.", "This affords changing the number of slots without re-training, and yields invariance to permutation of the ordering of the slots, which are updated iteratively using dot-product attention normalized over slots, which fosters competition among slots.", "The result is passed through a Gated Recurrent Unit (GRU) with a multi-layer perceptron (MLP) to yield the update residual for slots.", "All parameters are shared among slots to preserve permutation symmetry."], "text_after_citation": ["The final image reconstruction is\u00ce = n i=1\u00ce i m i where denotes elementwise multiplication.", "SAN is trained by minimizing a reconstruction loss (typically MSE) between I and\u00ce."], "citing_paper_content": {"title": "Divided Attention: Unsupervised Multi-Object Discovery With Contextually Separated Slots", "abstract": "We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-max optimization fosters the separation of objects and their assignment to different attention slots, leading to Divided Attention, or DivA. DivA outperforms recent unsupervised multi-object motion segmentation methods while tripling run-time speed up to 104FPS and reducing the performance gap from supervised methods to 12% or less. DivA can handle different numbers of objects and different image sizes at training and test time, is invariant to permutation of object labels, and does not require explicit regularization."}, "cited_paper_content": {"title": "Spatial Broadcast Decoder: A Simple Architecture For Learning Disentangled Representations In Vaes", "abstract": "We present a simple neural rendering architecture that helps variational autoencoders (VAEs) learn disentangled representations. Instead of the deconvolutional network typically used in the decoder of VAEs, we tile (broadcast) the latent vector across space, concatenate fixed X- and Y-\"coordinate\" channels, and apply a fully convolutional network with 1x1 stride. This provides an architectural prior for dissociating positional from non-positional features in the latent distribution of VAEs, yet without providing any explicit supervision to this effect. We show that this architecture, which we term the Spatial Broadcast decoder, improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space. It provides a particularly dramatic benefit when applied to datasets with small objects. We also emphasize a method for visualizing learned latent spaces that helped us diagnose our models and may prove useful for others aiming to assess data representations. Finally, we show the Spatial Broadcast Decoder is complementary to state-of-the-art (SOTA) disentangling techniques and when incorporated improves their performance."}, "keywords": ["segmentation masks", "spatial broadcast decoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors present a simple neural rendering architecture, called the Spatial Broadcast decoder, that improves disentangling, reconstruction accuracy, and generalization to held-out regions in data space for datasets with small objects #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The bottleneck architecture in our method uses slots to separate objects, and the learned disentangled representations in the Spatial Broadcast decoder #REFR could potentially improve the separation of positional and non-positional features in the slots.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the disentangling of positional and non-positional features in the latent distribution of VAEs, the authors of this manuscript employ a Spatial Broadcast decoder, which tiles the latent vector across space and applies a fully convolutional network with 1x1 stride, as proposed in the reference paper \"Spatial Broadcast Decoder: A Simple Architecture For Learning Disentangled Representations In Vaes\" (#REFR).", "given_citation_intent": "method", "given_keywords": ["segmentation masks", "spatial broadcast decoder"]}]}
{"citing_id": "2303.05164v1", "cited_id": "1702.04405", "section_title": "Implementation Details", "citation": "For the ScanNet-v2 dataset, we conduct experiments on the \"3D Semantic label with Limited Annotations\" benchmark #REFR , where only 20 fixed points are labeled in each room scene.", "text_before_citation": ["We train the segmentation model on Area 1, 2, 3, 4, 6 and test it on Area 5, respectively.", "The ScanNet-v2 dataset contains 1513 3D scans with 20 categories, which are divided into 1201, 312, and 100 scans for training, validation, and testing, respectively. Weak Annotation Scheme.", "On the S3DIS dataset, we follow existing methods #OTHEREFR to annotate the training data under the \"OTOC\" setting.", "For each object, we randomly select a point as the labeled one with the same probability.", "Consequently, only 0.02% of points have been labeled in the whole dataset."], "text_after_citation": ["Experiment Setting.", "If there is no special declaration, we implemented our proposed RAC-Net with the KPConv rigid backbone, which is same as the latest DAT model #OTHEREFR .", "We used the SGD to train the model with a batch size of 2 and a learning rate of 0.01.", "We set the confidence threshold \u03c4 as 0.7 and the uncertainty threshold \u03ba as 0.05.", "All experiments in this paper were conducted in an identical environment (Hardware: single NVIDIA RTX 3090 GPU; Software: PyTorch 1.7.0 and CUDA 11.0)."], "citing_paper_content": {"title": "Reliability-Adaptive Consistency Regularization For Weakly-Supervised Point Cloud Segmentation", "abstract": "Weakly-supervised point cloud segmentation with extremely limited labels is highly desirable to alleviate the expensive costs of collecting densely annotated 3D points. This paper explores to apply the consistency regularization that is commonly used in weakly-supervised learning, for its point cloud counterpart with multiple data-specific augmentations, which has not been well studied. We observe that the straightforward way of applying consistency constraints to weakly-supervised point cloud segmentation has two major limitations: noisy pseudo labels due to the conventional confidence-based selection and insufficient consistency constraints due to discarding unreliable pseudo labels. Therefore, we propose a novel Reliability-Adaptive Consistency Network (RAC-Net) to use both prediction confidence and model uncertainty to measure the reliability of pseudo labels and apply consistency training on all unlabeled points while with different consistency constraints for different points based on the reliability of corresponding pseudo labels. Experimental results on the S3DIS and ScanNet-v2 benchmark datasets show that our model achieves superior performance in weakly-supervised point cloud segmentation. The code will be released."}, "cited_paper_content": {"title": "Scannet: Richly-Annotated 3D Reconstructions Of Indoor Scenes", "abstract": "A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available \u2013 current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval."}, "keywords": ["\"3D Semantic label", "ScanNet-v2 dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We use the ScanNet-v2 dataset, which contains 1513 3D scans with 20 categories, to evaluate our weakly-supervised point cloud segmentation model. The dataset is divided into 1201, 312, and 100 scans for training, validation, and testing, respectively. To the best of our knowledge, ScanNet is the largest RGB-D video dataset with 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of limited labeled datasets in RGB-D scene understanding, the authors propose the use of ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations, which has been shown to achieve state-of-the-art performance on several 3D scene understanding tasks, including semantic voxel labeling, and CAD model retrieval. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train the segmentation model on the ScanNet-v2 dataset, the authors used a weak annotation scheme where only 0.02% of points were labeled in the whole dataset, and achieved superior performance in weakly-supervised point cloud segmentation compared to existing methods #OTHEREFR, thanks to their proposed Reliability-Adaptive Consistency Network (RAC-Net) that applies consistency training on all unlabeled points with different consistency constraints for different points based on the reliability of corresponding pseudo labels, as well as the availability of large, labeled datasets such as the ScanNet-v2 dataset containing 3D semantic labels #REFR.", "given_citation_intent": "method", "given_keywords": ["\"3D Semantic label", "ScanNet-v2 dataset"]}]}
{"citing_id": "2303.05812v1", "cited_id": "1812.10546", "section_title": "Results", "citation": "The results in Table 2 show that ALCIR consistently outperforms all baselines in terms of the accuracy measures on all three datasets, usually with a large margin #REFR .", "text_before_citation": ["6.2.1 Main Results.", "Table 2 reports the main results of our experiments, where the category is given to the recommenders.", "We name our proposed model ALCIR, which stands for Adversarial Learning for Complementary Item Recommendation.", "To analyze the contribution of the individual elements of our architecture, we report results for (a) ALCIR-Sup, which only consists of the supervised learning component from Section 4 and (b) ALCIR, which corresponds to the full model as described in Section 5."], "text_after_citation": ["We additionally observe that already the supervised component of our model (ALCIR-Supervised) performs better than the baselines.", "The adversarial component is then successful in even further increasing these already strong results.", "Considering the ranking of the other models, we notice that the popularity-based approach represents a baseline that can be difficult to beat.", "The DCF model and even the improved DCF-Hard model never reach the accuracy levels of the popularity-based method.", "P-Companion works better, but still does not reach the Hit Rate values of popular-item recommendations."], "citing_paper_content": {"title": "Semi-Supervised Adversarial Learning For Complementary Item Recommendation", "abstract": "Complementary item recommendations are a ubiquitous feature of modern e-commerce sites. Such recommendations are highly effective when they are based on collaborative signals like co-purchase statistics. In certain online marketplaces, however, e.g., on online auction sites, constantly new items are added to the catalog. In such cases, complementary item recommendations are often based on item side-information due to a lack of interaction data. In this work, we propose a novel approach that can leverage both item side-information and labeled complementary item pairs to generate effective complementary recommendations for cold items, i.e., for items for which no co-purchase statistics yet exist. Given that complementary items typically have to be of a different category than the seed item, we technically maintain a latent space for each item category. Simultaneously, we learn to project distributed item representations into these category spaces to determine suitable recommendations. The main learning process in our architecture utilizes labeled pairs of complementary items. In addition, we adopt ideas from Cycle Generative Adversarial Networks (CycleGAN) to leverage available item information even in case no labeled data exists for a given item and category. Experiments on three e-commerce datasets show that our method is highly effective. CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems."}, "cited_paper_content": {"title": "Deep Item-Based Collaborative Filtering For Sparse Implicit Feedback", "abstract": "Recommender systems are ubiquitous in the domain of e-commerce, used to improve the user experience and to market inventory, thereby increasing revenue for the site. Techniques such as item-based collaborative filtering are used to model users' behavioral interactions with items and make recommendations from items that have similar behavioral patterns. However, there are challenges when applying these techniques on extremely sparse and volatile datasets. On some e-commerce sites, such as eBay, the volatile inventory and minimal structured information about items make it very difficult to aggregate user interactions with an item. In this work, we describe a novel deep learning-based method to address the challenges. We propose an objective function that optimizes a similarity measure between binary implicit feedback vectors between two items. We demonstrate formally and empirically that a model trained to optimize this function estimates the log of the cosine similarity between the feedback vectors. We also propose a neural network architecture optimized on this objective. We present the results of experiments comparing the output of the neural network with traditional item-based collaborative filtering models on an implicit-feedback dataset, as well as results of experiments comparing different neural network architectures on user purchase behavior on eBay. Finally, we discuss the results of an A/B test that show marked improvement of the proposed technique over eBay's existing collaborative filtering recommender system."}, "keywords": ["three datasets", "large margin"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of sparse and volatile datasets in item-based collaborative filtering, a novel deep learning-based method was proposed in the reference paper #REFR, which optimizes a similarity measure between binary implicit feedback vectors between two items.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compare the performance of our proposed ALCIR model with traditional item-based collaborative filtering models, we conducted experiments on an implicit-feedback dataset, and the results are reported in Table 2. Our proposed ALCIR model outperforms the traditional models, demonstrating the effectiveness of our approach for complementary item recommendation on cold items. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compare the performance of our proposed ALCIR model with traditional item-based collaborative filtering models, we conducted experiments on three e-commerce datasets, including those with sparse and volatile data, and achieved significant improvements in recommendation accuracy, as shown in Table 2. #REFR's deep learning-based method, which optimizes a similarity measure between binary implicit feedback vectors, could address the challenges of sparse datasets and achieve a large margin of improvement over existing collaborative filtering recommender systems.", "given_citation_intent": "result", "given_keywords": ["three datasets", "large margin"]}]}
{"citing_id": "2303.15637v1", "cited_id": "1801.05039", "section_title": "Related Work", "citation": "Policy gradient approaches are a model-free algorithm suitable for offline RL, and are analyzed in #REFR .", "text_before_citation": ["Learning Controllers Offline Learning a controller from offline data is a familiar paradigm for control theorist and practitioners.", "It typically consists of system identification, followed by robust #OTHEREFR or certainty-equivalent #OTHEREFR ) control design, see Figure 2 .", "Recent work provides finite sample guarantees for such methods #OTHEREFR .", "Upper and lower bounds on the sample complexity of stabilization from offline data are presented in #OTHEREFR .", "The RL community has a similar paradigm, known as offline RL #OTHEREFR ."], "text_after_citation": ["Lower bounds on the variance of the gradient estimates in policy gradient approaches are supplied in #OTHEREFR .", "Lower bounds for offline linear control are also studied in #OTHEREFR with the objective of designing optimal experiments.", "We instead focus on the LQR setting to understand the dependence of the excess cost on interpretable system-theoretic quantities.", "Online LQR The problem of learning the optimal LQR controller online has a rich history beginning with\u00c5str\u00f6m and #OTHEREFR .", "Regret minimization was introduced in Lai (1986); #OTHEREFR ."], "citing_paper_content": {"title": "The Fundamental Limitations Of Learning Linear-Quadratic Regulators", "abstract": "We present a local minimax lower bound on the excess cost of designing a linear-quadratic controller from offline data. The bound is valid for any offline exploration policy that consists of a stabilizing controller and an energy bounded exploratory input. The derivation leverages a relaxation of the minimax estimation problem to Bayesian estimation, and an application of Van Trees' inequality. We show that the bound aligns with system-theoretic intuition. In particular, we demonstrate that the lower bound increases when the optimal control objective value increases. We also show that the lower bound increases when the system is poorly excitable, as characterized by the spectrum of the controllability gramian of the system mapping the noise to the state and the H \u221e norm of the system mapping the input to the state. We further show that for some classes of systems, the lower bound may be exponential in the state dimension, demonstrating exponential sample complexity for learning the linear-quadratic regulator offline."}, "cited_paper_content": {"title": "Global Convergence Of Policy Gradient Methods For The Linear Quadratic Regulator", "abstract": "Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model 2) they are an \"end-to-end\" approach, directly optimizing the performance metric of interest 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties. This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities."}, "keywords": ["Policy gradient approaches"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recent work provides finite sample guarantees for such methods #REFR. Upper and lower bounds on the sample complexity of stabilization from offline data are presented in #REFR. In contrast, #REFR shows that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities, even in the most basic continuous control problem of linear quadratic regulators.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent work provides finite sample guarantees for such methods (Li et al., 2017). Upper and lower bounds on the sample complexity of stabilization from offline data are presented in (Li et al., 2017). In particular, Li et al. show that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent work provides finite sample guarantees for such methods, including offline RL #REFR and #OTHEREFR, which leverage Policy gradient approaches.", "given_citation_intent": "method", "given_keywords": ["Policy gradient approaches"]}]}
{"citing_id": "2303.15948v1", "cited_id": "1808.05587", "section_title": "Introduction", "citation": "Yang (2020) devised a systematic method to compute such a kernel corresponding to a large number of neural network architectures, and #REFR examined the case of convolutional residual networks.", "text_before_citation": ["#OTHEREFR introduced deep kernels by allegory with deep networks, and #OTHEREFR", "(2018) showed that these kernels do indeed correspond to multi-layer neural networks with Gaussian process priors.", "They also showed that in practical settings, the Gaussian process limit arises rather quickly, with networks of width of order 100 behaving as Gaussian processes.", "Further work has demonstrated that Gaussian process behavior of deep networks remains when the network is trained using gradient descent. #OTHEREFR", "(2018) introduced the neural tangent kernel (NTK), which describes how a trained neural network exhibits Gaussian process behavior in the large-width regime."], "text_after_citation": ["A common theme in these works is that whilst Gaussian-process equivalents to large networks exist, they are often expensive to compute.", "For example in #OTHEREFR , constructing the kernel matrix for an image-recognition task took 10 hours on a 1000-core machine.", "This work brings together the computational method from #OTHEREFR", "(2020) with some of the advances in understanding the (kernels of) Gaussian processes connection to large DNNs.", "We first review some connections between RKHS eigenstructures and deep model structures; we clarify how the corresponding RKHS gives rise to polynomial kernels with spherical harmonics as the orthogonal basis; we introduce kernels of continuous depth so that depth may be estimated as a kernel hyper-parameter; and we introduce variational learning of spherical-harmonic phases, which enables scaling to larger input dimensions than previously."], "citing_paper_content": {"title": "Sparse Gaussian Processes With Spherical Harmonic Features Revisited", "abstract": "We revisit the Gaussian process model with spherical harmonic features and study connections between the associated RKHS, its eigenstructure, and deep models. Based on this, we introduce a new class of kernels which correspond to deep models of continuous depth. In our formulation, depth can be estimated as a kernel hyper-parameter by optimizing the evidence lower bound. Further, we introduce sparseness in the eigenbasis by variational learning of the spherical harmonic phases. This enables scaling to larger input dimensions than previously, while also allowing for learning of high frequency variations. We validate our approach on machine learning benchmark datasets."}, "cited_paper_content": {"title": "Deep Convolutional Networks As Shallow Gaussian Processes", "abstract": "We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike\"deep kernels\", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84% classification error on MNIST, a new record for GPs with a comparable number of parameters."}, "keywords": ["convolutional residual networks"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors show that the output of a convolutional neural network with an appropriate prior over the weights and biases is a Gaussian process in the limit of infinitely many convolutional filters, extending similar results for dense networks. The equivalent kernel can be computed exactly and has very few parameters, making it efficient to evaluate. This result is relevant to the study of deep models and their connection to Gaussian processes, as discussed by #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors utilized an appropriate prior over the weights and biases of a convolutional neural network to show that its output is a Gaussian process, which can be computed efficiently with very few parameters, as demonstrated by #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "#OTHEREFR (2018) demonstrated that the output of a residual convolutional neural network with an appropriate prior over the weights and biases is a Gaussian process, and the equivalent kernel can be computed exactly with very few parameters, making it efficient to evaluate for a pair of images, as shown in the manuscript. #REFR", "given_citation_intent": "method", "given_keywords": ["convolutional residual networks"]}]}
{"citing_id": "2305.01482v1", "cited_id": "1904.09751", "section_title": "I. Introduction", "citation": "However, this loss tends to generate repetitive and generic content #REFR and does not take into account synonyms, various sentences structures or the semantic closeness.", "text_before_citation": ["Following this idea, the Automated Audio Captioning (AAC) task appeared in 2017 #OTHEREFR and aims to create systems that generate a sentence written in natural language that describes an audio file.", "The audio can contain various sound events (human, natural, domestic, urban, music, effects...) of different lengths, recorded with different devices and in different scenes.", "The description can contain any kind of detail in the audio, with temporal or spatial relations between them (followed by, in the background...) or different characterizations (high-pitched, short, repetitive...).", "Since the descriptions are written by humans, we need to consider different words used to describe similar sounds (Birds are calling / chirping / singing / tweeting), different sentence structures (A door that needs to be oiled / A door with squeaky hinges), subjectivity (Man speaks in a foreign language), high-level descriptions (A vulgar man speaks / Unintelligible conversation), and vagueness (Someone speaks instead of A man gives a speech over a reverberating microphone).", "In AAC, most approaches use deep learning models trained with the standard Cross-Entropy (CE) loss."], "text_after_citation": ["Several studies introduced another criterion, the Self-Critical Sequence Training #OTHEREFR (SCST) used in reinforcement learning to fine-tune the model directly on a metric instead of the loss.", "This technique relies on sampling the next word to generate a new sentence.", "If this sentence has a higher score than the original one, the model is rewarded and the outputs probabilities for this new sentence are encouraged.", "However, this technique leads to degenerated sentences #OTHEREFR , with repetitive n-grams without syntactical correctness.", "Motivated by the limitations of CE and SCST, in this work, we attempted to add a Sentence Embedding Regression (SER) loss used in #OTHEREFR to improve our model."], "citing_paper_content": {"title": "Multitask Learning In Audio Captioning: A Sentence Embedding Regression Loss Acts As A Regularizer", "abstract": "In this work, we propose to study the performance of a model trained with a sentence embedding regression loss component for the Automated Audio Captioning task. This task aims to build systems that can describe audio content with a single sentence written in natural language. Most systems are trained with the standard Cross-Entropy loss, which does not take into account the semantic closeness of the sentence. We found that adding a sentence embedding loss term reduces overfitting, but also increased SPIDEr from 0.397 to 0.418 in our first setting on the AudioCaps corpus. When we increased the weight decay value, we found our model to be much closer to the current stateof-the-art methods, with a SPIDEr score up to 0.444 compared to a 0.475 score. Moreover, this model uses eight times less trainable parameters. In this training setting, the sentence embedding loss has no more impact on the model performance."}, "cited_paper_content": {"title": "The Curious Case Of Neural Text Degeneration", "abstract": "Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration \u2014 output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is the best decoding strategy for generating long-form text that is both high-quality \u2014 as measured by human evaluation \u2014 and as diverse as human-written text."}, "keywords": ["semantic closeness", "generic content"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In AAC, most approaches use deep learning models trained with the standard Cross-Entropy (CE) loss. However, recent research has shown that maximization-based decoding methods such as beam search can lead to degeneration in text generation from a language model, resulting in output text that is bland, incoherent, or repetitive #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Most approaches in Automated Audio Captioning use deep learning models trained with the standard Cross-Entropy loss, but recent research has shown that maximization-based decoding methods such as beam search can lead to degeneration in text generation tasks #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Most approaches in Automated Audio Captioning use deep learning models trained with the standard Cross-Entropy loss, which does not consider the #KEYWORD1 of the sentence. To address this issue, the authors propose a model trained with a sentence embedding regression loss component, which reduces overfitting and could potentially improve the #KEYWORD2 of the generated text #REFR.", "given_citation_intent": "background", "given_keywords": ["semantic closeness", "generic content"]}]}
{"citing_id": "2303.17270v1", "cited_id": "1602.04967", "section_title": "Elementary Turing Machines.", "citation": "The following result was proved in #REFR It permutes the positions of the m tape symbols according to \u03c0.", "text_before_citation": ["3.2. The oblivious Turing machines are finitely generated.", "In this section, we show that OBpZ d , n, kq is finitely generated.", "Our proof is based on the existence of strongly universal reversible gates for permutations of A m , which can be found in #OTHEREFR for the binary alphabet case, and generalized to other alphabets in #OTHEREFR .", "We need a finite generating set for permutations of Q\u02c6\u03a3 m , and hence the proof in #OTHEREFR has to be adjusted to account for non-homogeneous alphabet sizes (that is, due to possibly having n \u2030 k).", "Let us remark that the case n \" 1 is trivial: The group LPpZ d , 1, kq is finite and SHIFTpZ d , 1, kq is generated by the single step moves. We hence assume that n \u011b 2."], "text_after_citation": ["Now we can conjugate the prefix applicationf using a rewiring to getf \u03c0 \" r\u00b41 \u03c0\u02ddf\u02ddr \u03c0 , we callf \u03c0 an application of f in the coordinates Based on the decomposition in Figure 3 , we first conclude that any controlled 3-cycle f of Q\u02c6\u03a3 m is a composition of four applications of controlled swaps of Q\u02c6\u03a3 m\u00b42 .", "In the figure, the components of Q\u02c6\u03a3 m have been ordered in parallel horizontal wires, the Q-component being among the topmost three wires.", "Referring to the symbols in the illustration, the gate on the left is a generic 3-cycle ppszabcdw ptzabcdw qszabcdwq where one of the first three wires is the Q-component, a, b, c, d P \u03a3 and w P \u03a3 m\u00b46 .", "The proposed decomposition consists of two different controlled swaps p 1 and p 2 applied twice in the order f \" p 1 p 2 p 1 p 2 .", "Because p 1 and p 2 are involutions, the decomposition amounts to identity unless the input is of the form xyzabcdw where x P tp, qu and y P ts, tu."], "citing_paper_content": {"title": "The Group Of Reversible Turing Machines: Subgroups, Generators And Computability", "abstract": "We study an abstract group of reversible Turing machines. In our model, each machine is interpreted as a homeomorphism over a space which represents a tape filled with symbols and a head carrying a state. These homeomorphisms can only modify the tape at a bounded distance around the head, change the state and move the head in a bounded way. We study three natural subgroups arising in this model: the group of finite-state automata, which generalizes the topological full groups studied in topological dynamics and the theory of orbit-equivalence; the group of oblivious Turing machines whose movement is independent of tape contents, which generalizes lamplighter groups and has connections to the study of universal reversible logical gates; and the group of elementary Turing machines, which are the machines which are obtained by composing finite-state automata and oblivious Turing machines. We show that both the group of oblivious Turing machines and that of elementary Turing machines are finitely generated, while the group of finite-state automata and the group of reversible Turing machines are not. We show that the group of elementary Turing machines has undecidable torsion problem. From this, we also obtain that the group of cellular automata (more generally, the automorphism group of any uncountable one-dimensional sofic subshift) contains a finitely-generated subgroup with undecidable torsion problem. We also show that the torsion problem is undecidable for the topological full group of a full Z d-shift on a non-trivial alphabet if and only if d \u011b 2."}, "cited_paper_content": {"title": "Strongly Universal Reversible Gate Sets", "abstract": "It is well-known that the Toffoli gate and the negation gate together yield a universal gate set, in the sense that every permutation of $\\{0,1\\}^n$ can be implemented as a composition of these gates. Since every bit operation that does not use all of the bits performs an even permutation, we need to use at least one auxiliary bit to perform every permutation, and it is known that one bit is indeed enough. Without auxiliary bits, all even permutations can be implemented. We generalize these results to non-binary logic: If $A$ is a finite set of odd cardinality then a finite gate set can generate all permutations of $A^n$ for all $n$, without any auxiliary symbols. If the cardinality of $A$ is even then, by the same argument as above, only even permutations of $A^n$ can be implemented for large $n$, and we show that indeed all even permutations can be obtained from a finite universal gate set. We also consider the conservative case, that is, those permutations of $A^n$ that preserve the weight of the input word. The weight is the vector that records how many times each symbol occurs in the word. It turns out that no finite conservative gate set can, for all $n$, implement all conservative even permutations of $A^n$ without auxiliary bits. But we provide a finite gate set that can implement all those conservative permutations that are even within each weight class of $A^n$."}, "keywords": ["tape symbols"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section, we show that OBpZ d , n, kq is finitely generated based on the existence of strongly universal reversible gates for permutations of A m, which can be found in #REFR for the binary alphabet case, and generalized to other alphabets in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The existence of strongly universal reversible gates for permutations of A m, which can be found in #REFR, is used to obtain a finite generating set for permutations of Q\u02c6\u03a3 m in this section.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The existence of strongly universal reversible gates for permutations of non-homogeneous alphabet sizes is discussed in #REFR, providing background information for the finite generating set for permutations of Q\u02c6\u03a3 m in the context of the finitely generated group of oblivious Turing machines.", "given_citation_intent": "background", "given_keywords": ["tape symbols"]}]}
{"citing_id": "2304.04497v1", "cited_id": "1909.12201", "section_title": "4) Convergence Of The Inferred Network (Rq4):", "citation": "When \u03b7 = 0, our loss function in (8) boils down to the case in #REFR , which only takes into account the structure-community relationship without node metadata, leading to unsatisfactory performance.", "text_before_citation": ["\u2022 The AUC score between G (t) and G tends to increase with node queries.", "This suggests that EC-SiamNet is indeed effective in accurately inferring edges, leading to a fast convergence of the inferred network towards the underlying true network, as noted in Remark 1. 5) Effect of hyperparameters (RQ5): In Figs.", "6 and 7, we investigate the impact of two key hyperparameters, including \u03b7 and \u03bb, on the performance of META-CODE in terms of the NMI when the Facebook 348 and Engineering datasets are used.", "When a hyperparameter varies so that its effect is clearly revealed, another parameter is set to the pivot value addressed in Section V-D. Our findings are as follows.", "\u2022 The effect of \u03b7: From Fig 6, the maximum NMI is achieved at \u03b7 = 1.5 and \u03b7 = 1.0 on Facebook 348 and Engineering, respectively."], "text_after_citation": ["On the other hand, setting \u03b7 to a high value leads to relatively low performance for both datasets since the metadata-community relationship is over-emphasized during training.", "Hence, it is crucial to suitably determine the value of \u03b7 in guaranteeing satisfactory performance.", "\u2022 The effect of \u03bb: From Fig 7, the maximum NMI is achieved at \u03bb = 2.0 and \u03bb = 3.0 on Facebook 348 and Engineering, respectively.", "When \u03bb = 0, our node query strategy in (9) focuses only on selecting nodes belonging to more communities.", "In contrast, setting \u03bb to a high value leads to a negative effect on the performance because it over-emphasizes the selection of nodes distributed over diverse communities."], "citing_paper_content": {"title": "Graph Neural Network-Aided Exploratory Learning For Community Detection With Unknown Topology", "abstract": "In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a novel end-to-end solution for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via communityaffiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through comprehensive evaluations using five real-world datasets, we demonstrate that META-CODE exhibits (a) its superiority over benchmark community detection methods, (b) empirical evaluations as well as theoretical findings to see the effectiveness of our node query, (c) the influence of each module, and (d) its computational efficiency."}, "cited_paper_content": {"title": "Overlapping Community Detection With Graph Neural Networks", "abstract": "Community detection is a fundamental problem in machine learning. While deep learning has shown great promise in many graphrelated tasks, developing neural models for community detection has received surprisingly little attention. The few existing approaches focus on detecting disjoint communities, even though communities in real graphs are well known to be overlapping. We address this shortcoming and propose a graph neural network (GNN) based model for overlapping community detection. Despite its simplicity, our model outperforms the existing baselines by a large margin in the task of community recovery. We establish through an extensive experimental evaluation that the proposed model is effective, scalable and robust to hyperparameter settings. We also perform an ablation study that confirms that GNN is the key ingredient to the power of the proposed model."}, "keywords": ["structure-community relationship"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed META-CODE method leverages graph neural networks (GNNs) for node-level community-affiliation embeddings, which are trained using a reconstruction loss, and outperforms existing benchmarks for overlapping community detection in networks with unknown topology, as demonstrated in the comprehensive evaluations using real-world datasets #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of detecting overlapping communities in networks with unknown topology, META-CODE utilizes graph neural networks (GNNs) and outperforms existing baselines by a large margin, as demonstrated in the extensive experimental evaluation conducted by the authors #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed model in the reference paper #REFR addresses the shortcoming of existing approaches that focus on detecting disjoint communities, despite the well-known overlapping structure-community relationship in real graphs.", "given_citation_intent": "background", "given_keywords": ["structure-community relationship"]}]}
{"citing_id": "2304.01330v1", "cited_id": "1908.10084", "section_title": "Proposed Methodology", "citation": "This is because it performed incredibly well among the algorithms tested for this purpose and was the second most computationally efficient algorithm tested #REFR .", "text_before_citation": ["Statistical techniques are the simplest of the three types of document similarity algorithms.", "They compare text by first turning the sentences into vectors, and then comparing said vectors.", "The most used and most effective way of comparison is through cosine similarity, but other methods such as Euclidean distance are occasionally used.", "Of the possible preprocessors, Sentence-BERT was judged to be most ideal for the purposes of this paper."], "text_after_citation": ["Neural networks are another possible avenue of document similarity, and a very effective one.", "The basics of neural network-based techniques is that they are first fed training documents of pairs of texts that are either similar to each other or different.", "Gradually, the neural network learns which pairs of texts are similar and which pairs are different through understanding semantic information.", "Generally, the text is run through a tokenizer first, which allows the semantic information to be more understandable to the network.", "Some models, like BERT and XLNet, are pre-trained, which allows the network to have accurate predictions without needing long training times and massive corpora for each individual task."], "citing_paper_content": {"title": "A Comparison Of Document Similarity Algorithms", "abstract": "Document similarity is an important part of Natural Language Processing and is most commonly used for plagiarism-detection and text summarization. Thus, finding the overall most effective document similarity algorithm could have a major positive impact on the field of Natural Language Processing. This report sets out to examine the numerous document similarity algorithms, and determine which ones are the most useful. It addresses the most effective document similarity algorithm by categorizing them into 3 types of document similarity algorithms: statistical algorithms, neural networks, and corpus/knowledge-based algorithms. The most effective algorithms in each category are also compared in our work using a series of benchmark datasets and evaluations that test every possible area that each algorithm could be used in."}, "cited_paper_content": {"title": "Sentence-Bert: Sentence Embeddings Using Siamese Bert-Networks", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. ::: In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."}, "keywords": ["algorithms", "computationally efficient algorithm"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Of the possible preprocessors, Sentence-BERT was judged to be most ideal for the purposes of this paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For the purposes of this paper, Sentence-BERT was deemed the most ideal preprocessor for comparing text using cosine similarity, due to its ability to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity, as presented in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of computationally efficient algorithms for semantic similarity search, the authors of this manuscript suggest using Sentence-BERT (SBERT) which reduces the effort for finding the most similar pair from 65 hours with BERT to about 5 seconds with SBERT, while maintaining the accuracy from BERT. #REFR", "given_citation_intent": "method", "given_keywords": ["algorithms", "computationally efficient algorithm"]}]}
{"citing_id": "2303.10778v1", "cited_id": "1703.01541", "section_title": "Declarative Dtw Forward Pass", "citation": "Compared to Soft-DTW #REFR , which produces soft alignments, DecDTW outputs the optimal, continuous time warp.", "text_before_citation": ["Our DecDTW layer encodes an implicit function \u03c6 = DecDTW(x, y, \u03bb, s min , s max , b min , b max ), which yields the optimal warp given input signals x, y, regularisation \u03bb and constraints", "s min = {s min i } m i=1 , s max = {s max i } m i=1 , b min = {b min i } m i=1 , b max = {b max i } m i=1 .", "The warp \u03c6 can be used in a downstream loss J(\u03c6 ), e.g., the error between predicted warp \u03c6 and a ground truth warp \u03c6 gt .", "The GDTW discrepancy is recovered by setting J =f ."], "text_after_citation": ["The DecDTW forward pass solves the GDTW problem given by Equation 4 given the input parameters.", "We solve Equation 4 using a dynamic programming (DP) approach as proposed in #OTHEREFR instead of calling a general-purpose NLP solver; this is to minimise computation time.", "The DP approach uses the fact that \u03c6 lies on a compact subset of R m defined by the global bounds, allowing for efficient discretisation.", "Finding the globally optimal solution to the discretised version of Equation 4 can be done using DP.", "The resultant solution \u03c6 is an approximation to the solution of the continuous NLP, with the approximation error dependent on the resolution of discretisation."], "citing_paper_content": {"title": "Deep Declarative Dynamic Time Warping For End-To-End Learning Of Alignment Paths", "abstract": "This paper addresses learning end-to-end models for time series data that include a temporal alignment step via dynamic time warping (DTW). Existing approaches to differentiable DTW either differentiate through a fixed warping path or apply a differentiable relaxation to the min operator found in the recursive steps used to solve the DTW problem. We instead propose a DTW layer based around bilevel optimisation and deep declarative networks, which we name DecDTW. By formulating DTW as a continuous, inequality constrained optimisation problem, we can compute gradients for the solution of the optimal alignment (with respect to the underlying time series) using implicit differentiation. An interesting byproduct of this formulation is that DecDTW outputs the optimal warping path between two time series as opposed to a soft approximation, recoverable from Soft-DTW. We show that this property is particularly useful for applications where downstream loss functions are defined on the optimal alignment path itself. This naturally occurs, for instance, when learning to improve the accuracy of predicted alignments against ground truth alignments. We evaluate DecDTW on two such applications, namely the audio-to-score alignment task in music information retrieval and the visual place recognition task in robotics, demonstrating state-of-the-art results in both."}, "cited_paper_content": {"title": "Soft-Dtw: A Differentiable Loss Function For Time-Series", "abstract": "We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines. Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense."}, "keywords": ["soft alignments"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our DecDTW layer encodes an implicit function \u03c6 = DecDTW(x, y, \u03bb, s min , s max , b min , b max ), which yields the optimal warp given input signals x, y, regularisation \u03bb and constraints s min = {s min i } m i=1 , s max = {s max i } m i=1 , b min = {b min i } m i=1 , b max = {b max i } m i=1 . The warp \u03c6 can be used in a downstream loss J(\u03c6 ), e.g., the error between predicted warp \u03c6 and a ground truth warp \u03c6 gt . The GDTW discrepancy is recovered by setting J =f. Soft-DTW, a differentiable loss function for time-series, can be used to compute the soft-minimum of all alignment costs, and both its value and gradient can be computed with quadratic time/space complexity. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors utilized soft-DTW, a differentiable loss function for time-series, proposed in the reference paper #REFR, to cluster and average time series under the DTW geometry, achieving superior performance compared to existing baselines.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Soft-DTW is a differentiable loss function that computes the soft-minimum of all alignment costs, which is particularly well suited to average and cluster time series under the DTW geometry, and can be used for downstream loss functions such as #REFR for soft alignments.", "given_citation_intent": "method", "given_keywords": ["soft alignments"]}]}
{"citing_id": "2304.00133v1", "cited_id": "1807.06228", "section_title": "Surrogate Model Visualization", "citation": "RuleMatrix #REFR uses a matrix design and Sankey diagram visualization for the content of a rule list showing how data flows through the list.", "text_before_citation": ["Recent work has used surrogate models to approximate the behavior of complex ML models locally #OTHEREFR , globally #OTHEREFR , or on all scales #OTHEREFR ; all of these examples provide visual exploration of such surrogates as well.", "Closer to our work, SuRE #OTHEREFR uses hierarchical rules to describe the decision space of a given ML model and visually explore its results by an interactive hierarchical visualization of the extracted rules.", "However, when checking multiple intertwined rules in the form of if-else statements, participants evaluating SuRE almost always reasoned about less than two conditions at a time, a limitation we surpass with DEFORESTVIS due to the simple nature of one-level decision trees.", "Another interesting finding is that their system's users analyzed thoroughly the effect of predictions based on individual features, thus matching well the main design concept of our proposed tool.", "Di Castro and Bertini #OTHEREFR use a single surrogate decision tree to replicate a classification model's prediction and visualize it to propose simple yet effective explanations for the original model."], "text_after_citation": ["The problem with the above two VA tools is that they use a flat tabular layout #OTHEREFR which disregards the rules' hierarchical structure and the important feature-ordering information captured by the hierarchical structure of decision trees.", "In our approach, this is not a problem since AdaBoost produces one-level decision trees (stumps), and we sort stumps for the same feature on the \"importance\" extracted directly from the AdaBoost algorithm.", "DRIL #OTHEREFR presents a rule list for adjusting thresholds and examining relationships between rules and data.", "Our VA tool focuses on both the summarized rules and the decision stumps that serve as an extra explanation of how the aggregation of information occurs.", "StrategyAtlas #OTHEREFR , a hybrid approach, aims to explain individual data instances by aggregating multiple local surrogates #OTHEREFR ."], "citing_paper_content": {"title": "Deforestvis: Behavior Analysis Of Machine Learning Models With Surrogate Decision Stumps", "abstract": "Fig. 1: Components of DEFORESTVIS: (a.1) lollipop plot shows data-rounding effects in the fidelity score; (a.2) dot plot with lines that explains complexity increase as more decision stumps get added and (a.3) selective stump-based explanation; (b.1) segmented bar chart tells the predictive outcome and power of each segment based on automatically computed thresholds and (b.2) detailed stump-based explanation grid; (c.1) bar chart shows the impurity and weighted probability of each decision stump; (c.2) histogram shows the active rule's threshold and distribution of training instances; (d) projection aggregates the global behavior of instances; color shows the local behavior according to the currently selected decision stump; and (e) fragmented bar chart shows the per-feature contribution and influence level for each test case."}, "cited_paper_content": {"title": "Rulematrix: Visualizing And Understanding Classifiers With Rules", "abstract": "With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study."}, "keywords": ["diagram visualization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Closer to our work, SuRE uses hierarchical rules to describe the decision space of a given ML model and visually explore its results by an interactive hierarchical visualization of the extracted rules. However, when checking multiple intertwined rules in the form of if-else statements, participants evaluating SuRE almost always reasoned about less than two conditions at a time, a limitation we surpass with DEFORESTVIS due to the simple nature of one-level decision trees. Another interesting finding is that their system's users analyzed thoroughly the effect of predictions based on individual features, thus matching well the main design concept of our proposed tool (Di Castro and Bertini, #REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To help users with little expertise in machine learning to understand, explore and validate predictive models, Di Castro and Bertini (2019) proposed a single surrogate decision tree to replicate a classification model's prediction and visualize it to propose simple yet effective explanations for the original model #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of visualizing machine learning models for users with little expertise in machine learning, RuleMatrix, a matrix-based visualization of rules, was proposed in #REFR, which could be a useful reference for our work on diagram visualization.", "given_citation_intent": "method", "given_keywords": ["diagram visualization"]}]}
{"citing_id": "2303.04776v2", "cited_id": "1911.01414", "section_title": "Proof Of Theorem 4.7 And Summary Of Covers", "citation": "It is an interesting problem to develop a faster algorithm for computing d(\u03c1 * , \u03c0) than the one that can be derived from #REFR .", "text_before_citation": ["Finally, for certain applications it may be useful to efficiently compute our density statistic.", "For example, Even-Zohar and Leng #OTHEREFR devised a\u00d5(|\u03c0|)-time algorithm for computing d(\u03c4 * , \u03c0) for a permutation \u03c0, where \u03c4 * is the Bergsma-Dassios statistic defined in the introduction.", "This provides an efficient method for testing whether two [0, 1]-valued random variables X and Y with continuous joint cdf are independent.", "In fact, the results of #OTHEREFR can be used to efficiently compute d(\u03c1, \u03c0) for a large class of expressions \u03c1 \u2208 A, which they refer to as corner trees.", "As it turns out, \u03c1 * is not a corner tree, and so the results of #OTHEREFR are insufficient for computing d(\u03c1 * , \u03c0) in time\u00d5(|\u03c0|); however, using other algorithms in #OTHEREFR , one can do much better than the trivial bound of O(n 4 )."], "text_after_citation": ["This means that the aforementioned question of Graham on permutation quasirandomness was technically answered, in a different setting, almost 60 years before it was asked (and about 40 years before the term \"quasirandomness\" entered the combinatorial vernacular)."], "citing_paper_content": {"title": "Six Permutation Patterns Force Quasirandomness", "abstract": "A sequence \u03c0 1 , \u03c0 2 ,. .. of permutations is said to be quasirandom if the induced density of every permutation \u03c3 in \u03c0 n converges to 1/|\u03c3|! as n \u2192 \u221e. We prove that \u03c0 1 , \u03c0 2 ,. .. is quasirandom if and only if the density of each permutation \u03c3 in the set {123, 321, 2143, 3412, 2413, 3142} converges to 1/|\u03c3|!. Previously, the smallest cardinality of a set with this property, called a quasirandom-forcing set, was known to be between four and eight. In fact, we show that there is a single linear expression of the densities of the six permutations in this set which forces quasirandomness and show that this is best possible in the sense that there is no shorter linear expression of permutation densities with positive coefficients with this property. In the language of theoretical statistics, this expression provides a new nonparametric independence test for bivariate continuous distributions related to Spearman's \u03c1."}, "cited_paper_content": {"title": "Counting Small Permutation Patterns", "abstract": "A sample of n generic points in the xy-plane defines a permutation that relates their ranks along the two axes. Every subset of k points similarly defines a pattern, which occurs in that permutation. The number of occurrences of small patterns in a large permutation arises in many areas, including nonparametric statistics. It is therefore desirable to count them more efficiently than the straightforward ~O(n^k) time algorithm. ::: This work proposes new algorithms for counting patterns. We show that all patterns of order 2 and 3, as well as eight patterns of order 4, can be counted in nearly linear time. To that end, we develop an algebraic framework that we call corner tree formulas. Our approach generalizes the existing methods and allows a systematic study of their scope. ::: Using the machinery of corner trees, we find twenty-three independent linear combinations of order-4 patterns, that can be computed in time ~O(n). We also describe an algorithm that counts another 4-pattern, and hence all 4-patterns, in time ~O(n^(3/2)). ::: As a practical application, we provide a nearly linear time computation of a statistic by Yanagimoto (1970), Bergsma and Dassios (2010). This statistic yields a natural and strongly consistent variant of Hoeffding's test for independence of X and Y, given a random sample as above. This improves upon the so far most efficient ~O(n^2) algorithm."}, "keywords": ["d(\u03c1", "faster algorithm"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In fact, the results of #OTHEREFR can be used to efficiently compute d(\u03c1, \u03c0) for a large class of expressions \u03c1 \u2208 A, which they refer to as corner trees. As it turns out, \u03c1 * is not a corner tree, and so the results of #OTHEREFR are insufficient for computing d(\u03c1 * , \u03c0) in time\u00d5(|\u03c0|); however, using other algorithms in #OTHEREFR , one can do much better than the trivial bound of O(n 4 ). This work proposes new algorithms for counting patterns, including all patterns of order 2 and 3, as well as eight patterns of order 4, that can be counted in nearly linear time. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently count small permutation patterns, the authors propose new algorithms based on an algebraic framework called corner tree formulas, which generalize existing methods and allow a systematic study of their scope (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently compute d(\u03c1; a faster algorithm can be used as proposed in the work by #REFR, which presents new algorithms for counting patterns and provides a nearly linear time computation of a statistic by Yanagimoto, Bergsma, and Dassios, improving upon the previously most efficient algorithm of O(n^2).", "given_citation_intent": "method", "given_keywords": ["d(\u03c1", "faster algorithm"]}]}
{"citing_id": "2303.17610v3", "cited_id": "1906.04032", "section_title": "Flexible Parametric Distribution Estimation Using Modified Normalizing Flows", "citation": "This is contrary to #REFR , however, in our testing, this constraint relaxation works well and provides no tangible difference in performance.", "text_before_citation": ["d l,1 = \u2206 2 l,1 \u03b4 l,2 , d l,5 = \u2206 2 l,4 \u03b4 l,4 ,", "where \u2206 l,j and \u03b4 l,j are computed by", "\u2206 l,j = v l,j+1 \u2212 v l,j k l,j+1 \u2212 k l,j , \u03b4 l,j = v l,j+1 \u2212 v l,j\u22121 k l,j+1 \u2212 k l,j\u22121 .", "Due to our constraint on the minimal differences between consecutive knots and values we do not have to concern ourselves with specific edge cases outlined in #OTHEREFR where the differences in above equalities would be zero.", "Additionally, we do not restrict the spline knots to a predetermined interval."], "text_after_citation": ["It does, however, eliminate the need for parameter tuning as the knot ranges are determined automatically during optimization."], "citing_paper_content": {"title": "Ensemble Weather Forecast Post-Processing With A Flexible Probabilistic Neural Network Approach", "abstract": "Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a perlocation or per-lead-time basis, which limits their expressive power. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption made by many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibit state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed comparison of three variants of our novel post-processing method, we elucidate the reasons why our method outperforms per-lead-time-based approaches and approaches with distributional assumptions."}, "cited_paper_content": {"title": "Neural Spline Flows", "abstract": "A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images."}, "keywords": ["performance", "tangible difference"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To enhance the flexibility of our approach, we incorporate normalizing flows based on monotonic rational-quadratic splines, as proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate the effectiveness of their method in the context of the EUPPBench benchmark, where they conduct temperature forecast post-processing for stations in a sub-region of western Europe, and show that their novel method exhibits state-of-the-art performance on the benchmark, outclassing their previous, well-performing entry. Additionally, by providing a detailed comparison of three variants of their novel post-processing method, they elucidate the reasons why their method outperforms per-lead-time-based approaches and approaches with distributional assumptions #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate the effectiveness of their novel neural network-based method for ensemble forecast post-processing by incorporating normalizing flows as flexible parametric distribution estimators, which enables them to model varying forecast distributions in a mathematically exact way, resulting in a tangible difference in performance compared to per-lead-time-based approaches and approaches with distributional assumptions (#REFR).", "given_citation_intent": "result", "given_keywords": ["performance", "tangible difference"]}]}
{"citing_id": "2304.03548v1", "cited_id": "1810.09305", "section_title": "Experimental Settings", "citation": "WikiHow #REFR ) is a diverse summarization dataset extracted from the online knowledge base WikiHow, which is written by human authors.", "text_before_citation": ["We use three benchmark datasets representing different styles and domains as shown in Table 3 .", "CNN/DailyMail (Hermann et al., 2015) is the most popular single-document summarization dataset, comprising online news articles and human written highlights.", "XSum #OTHEREFR is an abstractive style summarization dataset built upon new articles with a one-sentence summary written by professional authors."], "text_after_citation": ["We adopt two baseline systems.", "BART #OTHEREFR is the abstractive baseline that we use to compare with our rewriterfuser model.", "BART-Rewriter is a stronger rewriter baseline that follows BERT-Rewriter #OTHEREFR but is fine-tuned on BART, so that we could have a fair comparison between our model and the previous rewriter.", "We train GEMINI using separated prefinetuning and fine-tuning steps.", "We prefinetuning the randomly-initialized parameters for 8 epochs, and then jointly fine-tune both the new and pretrained parameters until early stopping on development dataset."], "citing_paper_content": {"title": "Gemini: Controlling The Sentence-Level Writing Style For Abstractive Text Summarization", "abstract": "Human experts write summaries using different techniques, including rewriting a sentence in the document or fusing multiple sentences to generate a summary sentence. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a fuser to mimic the sentence rewriting and fusion techniques, respectively. GEM-INI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on various benchmark datasets, especially when the dataset has a balanced distribution of styles. Interestingly, empirical results show that the human writing style of each summary sentence is consistently predictable given its context."}, "cited_paper_content": {"title": "Wikihow: A Large Scale Text Summarization Dataset", "abstract": "Sequence-to-sequence models have recently gained the state of the art performance in summarization. However, not too many large-scale high-quality datasets are available and almost all the available ones are mainly news articles with specific writing style. Moreover, abstractive human-style systems involving description of the content at a deeper level require data with higher levels of abstraction. In this paper, we present WikiHow, a dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors. The articles span a wide range of topics and therefore represent high diversity styles. We evaluate the performance of the existing methods on WikiHow to present its challenges and set some baselines to further improve it."}, "keywords": ["diverse summarization dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We use three benchmark datasets representing different styles and domains as shown in Table 3. CNN/DailyMail (Hermann et al., 2015) is the most popular single-document summarization dataset, comprising online news articles and human written highlights. XSum #OTHEREFR is an abstractive style summarization dataset built upon new articles with a one-sentence summary written by professional authors. Additionally, to evaluate the performance of existing methods on datasets with higher levels of abstraction, we cite the Wikihow dataset #REFR, which presents challenges for abstractive human-style systems involving description of the content at a deeper level.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of the adaptive approach proposed in this manuscript, the authors used three benchmark datasets representing different styles and domains, including CNN/DailyMail and XSum, as well as WikiHow, a large-scale high-quality dataset of more than 230,000 article and summary pairs extracted and constructed from an online knowledge base written by different human authors, which was specifically designed to address the lack of data with higher levels of abstraction and diversity styles in summarization #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the diversity of summarization datasets, WikiHow provides a large-scale high-quality dataset of more than 230,000 article and summary pairs written by different human authors covering a wide range of topics, which can be used as a valuable resource for future research in this area. #REFR", "given_citation_intent": "method", "given_keywords": ["diverse summarization dataset"]}]}
{"citing_id": "2305.02247v1", "cited_id": "1509.01240", "section_title": "Proof Of Theorem 5 Lemma 3 Gives", "citation": "Note that Theorem 5 holds for any algorithm A R \u2208 A(\u03b7 t , T ), and extends established results in prior works for the SGD with batch size 1 #REFR .", "text_before_citation": ["w t+1 \u2212 w (i) t+1 \u2264 w t \u2212 w (i) t 1 Jt=J (i) t + w t \u2212 w (i) t 1 Jt =J (i) t + 2 m L\u03b7 t 1 Jt =J (i) t = w t \u2212 w (i) t + 2 m L\u03b7 t 1 Jt =J (i) t .", "By solving the recursion we find", "EQUATION", "Inequality #OTHEREFR and Lemma 2 give the on-average stability bound of the statement.", "This and inequality #OTHEREFR give the generalization error bound for convex smooth and Lipschitz loss for the final iterate for any batch schedule (see Algorithm 1), completing the proof."], "text_after_citation": ["Specifically, the generalization error bound of Theorem 5 holds for general stochastic or deterministic gradient schemes (Definition 1) and any batch size choice m.", "Next, we present a matching lower bound on the generalization error for the class of Lipschitz and smooth convex problems C L \u03b2 , instance-wise relative to the parameters L and \u03b2."], "citing_paper_content": {"title": "Select Without Fear: Almost All Mini-Batch Schedules Generalize Optimally", "abstract": "We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/stronglyconvex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones."}, "cited_paper_content": {"title": "Train Faster, Generalize Better: Stability Of Stochastic Gradient Descent", "abstract": "We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. ::: Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit."}, "keywords": ["batch size", "SGD"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "By applying the stability analysis of stochastic gradient methods (SGM) to the convex case, the authors of #REFR provide new insights into why multiple epochs of stochastic gradient methods generalize well in practice, and formally show that popular techniques for training large deep models are indeed stability-promoting.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "By applying the stability analysis of stochastic gradient method (SGM) from #REFR, the authors derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions, and provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "By applying the stability analysis of stochastic gradient method (SGM) to arbitrary nonadaptive batch schedules, the authors of the manuscript show that all such batch schedules generalize optimally, which is consistent with the findings in #REFR.", "given_citation_intent": "result", "given_keywords": ["batch size", "SGD"]}]}
{"citing_id": "2304.11448v1", "cited_id": "2003.08934", "section_title": "Comparisons", "citation": "As the atmospheric scattering coefficient \u03b2 increases, the results of Naive-NeRF #REFR are greatly affected, while our results are more robust to different haze concentrations. Table 3 .", "text_before_citation": ["Our model can still achieve comparable results under the premise that the training data is several orders of magnitude less than the comparison methods.", "Comparison of different haze concentrations.", "We added different concentrations of haze to the Lego scene according to ASM.", "Specifically, we set A to 0.8, and the atmospheric scattering coefficient \u03b2 increased from 0.04 to 0.36.", "As shown in Figure 6 , in Lego scene with different levels of haze, the results of Naive-NeRF #OTHEREFR are greatly affected, which indicates that the haze affects the 3D reconstruction to a large extent, and our model has a large impact on the different degrees of Haze has strong robustness. Estimation of parameters in ASM. As shown in Table 2 , Figure 6 . Quantitative results at different haze concentrations in Lego scene."], "text_after_citation": ["Quantitative results of ablation analysis on photometric errors of Lego scene, which shows that the combination of all the designed components is the best.", "in the four synthetic scenes, our model estimates the parameter values in ASM more accurately, which also shows the effectiveness of Dehazing-NeRF in modeling the image degradation process due to haze.", "The reason for the biased estimation of the parameter values in this model may be that the data lacks ground truth of parameters."], "citing_paper_content": {"title": "Dehazing-Nerf: Neural Radiance Fields From Hazy Images", "abstract": "Neural Radiance Field (NeRF) has received much attention in recent years due to the impressively high quality in 3D scene reconstruction and novel view synthesis. However, image degradation caused by the scattering of atmospheric light and object light by particles in the atmosphere can significantly decrease the reconstruction quality when shooting scenes in hazy conditions. To address this issue, we propose Dehazing-NeRF, a method that can recover clear NeRF from hazy image inputs. Our method simulates the physical imaging process of hazy images using an atmospheric scattering model, and jointly learns the atmospheric scattering model and a clean NeRF model for both image dehazing and novel view synthesis. Different from previous approaches, Dehazing-NeRF is an unsupervised method with only hazy images as the input, and also does not rely on hand-designed dehazing priors. By jointly combining the depth estimated from the NeRF 3D scene with the atmospheric scattering model, our proposed model breaks through the ill-posed problem of single-image dehazing while maintaining geometric consistency. Besides, to alleviate the degradation of image quality caused by information loss, soft margin consistency regularization, as well as atmospheric consistency and contrast discriminative loss, are addressed during the model training process. Extensive experiments demonstrate that our method outperforms the simple combination of single-image dehazing and NeRF on both image dehazing and novel view image synthesis."}, "cited_paper_content": {"title": "Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis", "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."}, "keywords": ["different haze concentrations"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As shown in Figure 6, in Lego scene with different levels of haze, the results of Naive-NeRF #REFR are greatly affected, which indicates that the haze affects the 3D reconstruction to a large extent, and our model has a large impact on the different degrees of Haze has strong robustness.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The results of our proposed Dehazing-NeRF model demonstrate strong robustness to different degrees of haze, which greatly affects the 3D reconstruction of Naive-NeRF #OTHEREFR, as shown in Figure 6, indicating the superiority of our method in image dehazing and novel view synthesis compared to prior work on neural rendering and view synthesis.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The results of Naive-NeRF #OTHEREFR are greatly affected by different haze concentrations in the Lego scene, indicating that the haze has a significant impact on 3D reconstruction; however, the proposed Dehazing-NeRF method shows strong robustness to different degrees of haze, as demonstrated in the supplementary video of the reference paper \"Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis\" when compared to the state-of-the-art neural rendering and view synthesis methods.", "given_citation_intent": "result", "given_keywords": ["different haze concentrations"]}]}
{"citing_id": "2303.06342v1", "cited_id": "1812.05784", "section_title": "I. Introduction", "citation": "In addition, the 4DRT-based RTNH achieves similar 3D object detection results to LiDAR point cloud-based (LPC-based) neural network, PointPillars #REFR , in road environments under clear weather conditions, and significantly outperforms the LPC-based network in adverse weather conditions such as sleet and heavy snow.", "text_before_citation": ["Due to the density, an RT provides rich information regarding the environment, but at a cost of a large amount of memory for storage and computations.", "With the availability of dense RTs, many studies #OTHEREFR , #OTHEREFR - #OTHEREFR have proposed RT-based object detection networks that achieve similar detection performance to camera and Lidarbased object detection networks.", "In particular, the K-Radar dataset #OTHEREFR provides a collection of 4D Radar Tensor (4DRT) that consists of power measurements along the Doppler, range, azimuth, and elevation dimensions.", "This is in contrast to the conventional 3D Radar tensor (3DRT) #OTHEREFR - #OTHEREFR that do not provide elevation information.", "The importance of the additional elevation information has been shown in #OTHEREFR , where the 4DRT-based Radar Tensor Network with Height (RTNH) significantly outperforms the Radar Tensor Network without height (RTN) in the 3D object detection task."], "text_after_citation": ["These results indicate the importance of 4D Radar sensors for a robust perception in adverse weather conditions.", "While the advantages of 4DRT-based networks are clear, it remains challenging to conduct experiments on the 4DRT data.", "This is mainly because of the size of the 4DRT data is prohibitively large (i.e., \u223c12TB).", "In the prior work #OTHEREFR , the size of the 4DRT data is reduced by performing a density reduction online during training, where the ouput density level is chosen arbitrarily.", "The effects of density reduction on the detection performance and memory consumption, however, remains largely unknown."], "citing_paper_content": {"title": "Enhanced K-Radar: Optimal Density Reduction To Improve Detection Performance And Accessibility Of 4D Radar Tensor-Based Object Detection", "abstract": "Recent works have shown the superior robustness of four-dimensional (4D) Radar-based three-dimensional (3D) object detection in adverse weather conditions. However, processing 4D Radar data remains a challenge due to the large data size, which require substantial amount of memory for computing and storage. In previous work, an online density reduction is performed on the 4D Radar Tensor (4DRT) to reduce the data size, in which the density reduction level is chosen arbitrarily. However, the impact of density reduction on the detection performance and memory consumption remains largely unknown. In this paper, we aim to address this issue by conducting extensive hyperparamter tuning on the density reduction level. Experimental results show that increasing the density level from 0.01% to 50% of the original 4DRT density level proportionally improves the detection performance, at a cost of memory consumption. However, when the density level is increased beyond 5%, only the memory consumption increases, while the detection performance oscillates below the peak point. In addition to the optimized density hyperparameter, we also introduce 4D Sparse Radar Tensor (4DSRT), a new representation for 4D Radar data with offline density reduction, leading to a significantly reduced raw data size. An optimized development kit for training the neural networks is also provided, which along with the utilization of 4DSRT, improves training speed by a factor of 17.1 compared to the stateof-the-art 4DRT-based neural networks."}, "cited_paper_content": {"title": "Pointpillars: Fast Encoders For Object Detection From Point Clouds", "abstract": "Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird\u2019s eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds."}, "keywords": ["4DRT-based RTNH", "LiDAR point cloud-based"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In the field of object detection from point clouds, PointPillars has been shown to outperform previous encoders with respect to both speed and accuracy by a large margin, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In contrast to LiDAR point cloud-based object detection, recent studies have shown that 4DRT-based RTNH significantly outperforms the Radar Tensor Network without height (RTN) in 3D object detection, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": ["4DRT-based RTNH", "LiDAR point cloud-based"]}]}
{"citing_id": "2303.11169v1", "cited_id": "1701.08398", "section_title": "Experiments", "citation": "Unlike some previous methods, we do not use any post-processing techniques like k-reciprocal re-ranking #REFR to refine our results.", "text_before_citation": ["On VeRi-776 and CityFlow-ReID, the initial learning rate is 1e-4 and the margin of triplet loss is set as 0.5 empirically.", "The number of training epochs is 80 and the learning rate is decreased by a factor of 0.1 at 20th, 40th, and 60th epoch.", "On VehicleID, the margin is 0.7 and the number of learning epochs is 120.", "The learning rate is increased linearly from 0 to 1e-4 during the first 10 epochs, decreased with cosine scheduler to 1e-7 at 100th epoch, and to 0 at the last epoch.", "Evaluation protocols."], "text_after_citation": ["We evaluate our approach by four widely used metrics in ReID literature, i.e., image-to-track retrieval mean Average Precision (tmAP) (if tracks are available in one dataset), image-to-image retrieval mAP (imAP), Top-1, and Top-5 accuracy.", "Particularly, we report both tmAP and imAP on VeRi-776 for comprehensive evaluation.", "These scores are shown as percentages and the best are marked in bold.", "In Table 1 , 2, and 3, ES (Y/N) indicates whether Extra Supervision besides ID labels is employed to train a corresponding method."], "citing_paper_content": {"title": "Self-Supervised Geometric Features Discovery Via Interpretable Attention For Vehicle Re-Identification And Beyond (Complete Version)", "abstract": "To learn distinguishable patterns, most of recent works in vehicle re-identification (ReID) struggled to redevelop official benchmarks to provide various supervisions, which requires prohibitive human labors. In this paper, we seek to achieve the similar goal but do not involve more human efforts. To this end, we introduce a novel framework, which successfully encodes both geometric local features and global representations to distinguish vehicle instances, optimized only by the supervision from official ID labels. Specifically, given our insight that objects in ReID share similar geometric characteristics, we propose to borrow self-supervised representation learning to facilitate geometric features discovery. To condense these features, we introduce an interpretable attention module, with the core of local maxima aggregation instead of fully automatic learning, whose mechanism is completely understandable and whose response map is physically reasonable. To the best of our knowledge, we are the first that perform self-supervised learning to discover geometric features. We conduct comprehensive experiments on three most popular datasets for vehicle ReID, i.e., VeRi-776, CityFlow-ReID, and VehicleID. We report our stateof-the-art (SOTA) performances and promising visualization results. We also show the excellent scalability of our approach on other ReID related tasks, i.e., person ReID and multi-target multi-camera (MTMC) vehicle tracking. The code is available at https://github.com/ ming1993li/Self-supervised-Geometric."}, "cited_paper_content": {"title": "Re-Ranking Person Re-Identification With K-Reciprocal Encoding", "abstract": "When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method."}, "keywords": ["k"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To improve the accuracy of person re-identification (re-ID) as a retrieval process, re-ranking is a critical step, especially for fully automatic, unsupervised solutions. Li et al. proposed a k-reciprocal encoding method to re-rank the re-ID results, which does not require any human interaction or labeled data, and is applicable to large-scale datasets. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To re-rank the re-ID results, Li et al. proposed a k-reciprocal encoding method that calculates a k-reciprocal feature by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance, and confirmed its effectiveness on large-scale datasets including Market-1501, CUHK03, MARS, and PRW. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To re-rank the re-ID results, the authors propose a k-reciprocal encoding method that encodes the k-reciprocal nearest neighbors of an image into a single vector, which is used for re-ranking under the Jaccard distance, and report its effectiveness on large-scale datasets such as Market-1501, CUHK03, MARS, and PRW. #REFR", "given_citation_intent": "method", "given_keywords": ["k"]}]}
{"citing_id": "2304.02383v1", "cited_id": "1201.0490", "section_title": "Embedded Fs Methods", "citation": "Random forests were grown with the Scikit-learn Python package #REFR and are composed of 500 trees each. We used the default parameters.", "text_before_citation": ["The CAE has been trained twice, once for selecting p and once for selecting 2p features. The model has been trained for 10 epochs.", "Each hidden layer is composed of 32 neurons, and followed by a 20% dropout.", "DeepPINK requires knockoff features, that we generated in two different ways depending on the nature of the dataset.", "For the DAG dataset, we relied on the framework of Model-X Knockoff features designed by Cand\u00e8s et al. #OTHEREFR . Details of Model-X Knockoff features are explained in Suppl. Mat. 1.2.", "For the remaining 4 datasets, knockoff features were generated by simply sampling a uniform distribution (the marginal distribution of the original data matrix X is multivariate but uniform)."], "text_after_citation": ["We selected features from trained random forests using the widely-used impurity-based feature importance scores, as implemented in Scikit-Learn.", "More specifically, the importance of a feature is defined as the total reduction in Gini impurity caused by all node splits involving that feature, averaged across all trees in the forest."], "citing_paper_content": {"title": "How Good Neural Networks Interpretation Methods Really Are? A Quantitative Benchmark", "abstract": "Saliency Maps (SMs) have been extensively used to interpret deep learning models decision by highlighting the features deemed relevant by the model. They are used on highly nonlinear problems, where linear feature selection (FS) methods fail at highlighting relevant explanatory variables. However, the reliability of gradient-based feature attribution methods such as SM has mostly been only qualitatively (visually) assessed, and quantitative benchmarks are currently missing, partially due to the lack of a definite ground truth on image data. Concerned about the apophenic biases introduced by visual assessment of these methods, in this paper we propose a synthetic quantitative benchmark for Neural Networks (NNs) interpretation methods. For this purpose, we built synthetic datasets with nonlinearly separable classes and increasing number of decoy (random) features, illustrating the challenge of FS in high-dimensional settings. We also compare these methods to conventional approaches such as mRMR or Random Forests. Our results show that our simple synthetic datasets are sufficient to challenge most of the benchmarked methods. TreeShap, mRMR and LassoNet are the best performing FS methods. We also show that, when quantifying the relevance of a few non linearly-entangled predictive features diluted in a large number of irrelevant noisy variables, neural network-based FS and interpretation methods are still far from being reliable."}, "cited_paper_content": {"title": "Scikit-Learn: Machine Learning In Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."}, "keywords": ["Random forests"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the remaining 4 datasets, knockoff features were generated by simply sampling a uniform distribution (the marginal distribution of the original data matrix X is multivariate but uniform) as described in the Scikit-Learn: Machine Learning In Python package #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate knockoff features for the remaining 4 datasets, the authors simply sampled a uniform distribution, as explained in the supplementary material #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compare the performance of gradient-based feature attribution methods with conventional approaches, the authors used Random Forests as one of the benchmarked methods, as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["Random forests"]}]}
{"citing_id": "2303.04473v1", "cited_id": "1806.01759", "section_title": "I. Introduction", "citation": "Another line of works perform point convolution #REFR [20] on radius neighborhood points to alleviate the impact of density.", "text_before_citation": ["These methods lack robustness to varying density point clouds, since density directly influences the k-nearest neighbor search.", "PointConv #OTHEREFR takes point density into account and uses the point position as input of the weight function, and employs another density function to re-weight the weighting function.", "However, this re-weighting neglects the internal correlation between point density and position.", "This deteriorates the network robustness and increases the computational complexity of convolution.", "These methods directly predict kernel weights to map input features to output features, which comes at a high computational cost and also increases the memory traffic during the learning process."], "text_after_citation": ["For example, MCC #OTHEREFR computes the point density by Kernel Density Estimation (KDE) to directly adjust convolution kernel weights, where the weight function is approximated by MLPs.", "This also has the same limitation as PointConv #OTHEREFR .", "KPConv #OTHEREFR associates convolutional weights with a series of explicit kernel points in a local region, and predicts the kernel points weight through a linear correlation function of relative position between kernel points and neighbor points.", "The linear coefficient is chosen according to the input density to characterize the influence of density to kernel weights.", "However, the linear coefficient must be set offline i.e., the convolution needs to be specifically optimized for different datasets according to their point density."], "citing_paper_content": {"title": "Danet: Density Adaptive Convolutional Network With Interactive Attention For 3D Point Clouds", "abstract": "Local features and contextual dependencies are crucial for 3D point cloud analysis. Many works have been devoted to designing better local convolutional kernels that exploit the contextual dependencies. However, current point convolutions lack robustness to varying point cloud density. Moreover, contextual modeling is dominated by non-local or self-attention models which are computationally expensive. To solve these problems, we propose density adaptive convolution, coined DAConv. The key idea is to adaptively learn the convolutional weights from geometric connections obtained from the point density and position. To extract precise context dependencies with fewer computations, we propose an interactive attention module (IAM) that embeds spatial information into channel attention along different spatial directions. DAConv and IAM are integrated in a hierarchical network architecture to achieve local density and contextual direction-aware learning for point cloud analysis. Experiments show that DAConv is significantly more robust to point density compared to existing methods and extensive comparisons on challenging 3D point cloud datasets show that our network achieves state-of-theart classification results of 93.6% on ModelNet40, competitive semantic segmentation results of 68.71% mIoU on S3DIS and part segmentation results of 86.7% mIoU on ShapeNet."}, "cited_paper_content": {"title": "Monte Carlo Convolution For Learning On Non-Uniformly Sampled Point Clouds", "abstract": "Deep learning systems extensively use convolution operations to process input data. Though convolution is clearly defined for structured data such as 2D images or 3D volumes, this is not true for other data types such as sparse point clouds. Previous techniques have developed approximations to convolutions for restricted conditions. Unfortunately, their applicability is limited and cannot be used for general point clouds. We propose an efficient and effective method to learn convolutions for non-uniformly sampled point clouds, as they are obtained with modern acquisition techniques. Learning is enabled by four key novelties: first, representing the convolution kernel itself as a multilayer perceptron; second, phrasing convolution as a Monte Carlo integration problem, third, using this notion to combine information from multiple samplings at different levels; and fourth using Poisson disk sampling as a scalable means of hierarchical point cloud learning. The key idea across all these contributions is to guarantee adequate consideration of the underlying non-uniform sample distribution function from a Monte Carlo perspective. To make the proposed concepts applicable to real-world tasks, we furthermore propose an efficient implementation which significantly reduces the GPU memory required during the training process. By employing our method in hierarchical network architectures we can outperform most of the state-of-the-art networks on established point cloud segmentation, classification and normal estimation benchmarks. Furthermore, in contrast to most existing approaches, we also demonstrate the robustness of our method with respect to sampling variations, even when training with uniformly sampled data only. To support the direct application of these concepts, we provide a ready-to-use TensorFlow implementation of these layers at https://github.com/viscom-ulm/MCCNN."}, "keywords": ["point convolution"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of robustness to varying point cloud density, the proposed density adaptive convolution (DAConv) in this manuscript adaptively learns convolutional weights from geometric connections obtained from the point density and position, which is a novel approach compared to previous methods that directly predict kernel weights. However, this approach neglects the internal correlation between point density and position, which deteriorates network robustness and increases computational complexity. To overcome this limitation, the authors of the manuscript could consider using the Monte Carlo convolution method proposed in #REFR, which represents the convolution kernel itself as a multilayer perceptron and uses a Monte Carlo integration problem to combine information from multiple samplings at different levels.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of robustness to varying point cloud density, recent works have proposed methods such as PointConv, which takes point density into account and uses the point position as input of the weight function, but neglects the internal correlation between point density and position, leading to decreased network robustness and increased computational complexity of convolution. #REFR provides an efficient and effective method to learn convolutions for non-uniformly sampled point clouds, which guarantees adequate consideration of the underlying non-uniform sample distribution function from a Monte Carlo perspective, and outperforms most of the state-of-the-art networks on established point cloud segmentation, classification and normal estimation benchmarks.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of robustness to varying point cloud density, recent works have proposed methods such as PointConv and DAConv that take point density into account in the convolution process, with DAConv adapting the convolutional weights from geometric connections obtained from the point density and position, but neglecting the internal correlation between point density and position, which can deteriorate the network robustness and increase the computational complexity of convolution (#REFR).", "given_citation_intent": "background", "given_keywords": ["point convolution"]}]}
{"citing_id": "2303.00654v1", "cited_id": "1502.03167", "section_title": "Name Rm 23 Comments", "citation": "Normalizing Batch Normalization #REFR Yes Either add noise to Batch norm mean and mean of squares calculation; or use public data to calculate these statistics.", "text_before_citation": [], "text_after_citation": ["Layer Normalization #OTHEREFR No GroupNormalziation #OTHEREFR No Weight Normalization #OTHEREFR No Specialized Layers GNN #OTHEREFR Yes Node or edge level GNN layers augment the features of the instance (node or edge) with features and labels of their direct neighbours, making this process not private.", "Further, graph structure is also leaked through such aggregation #OTHEREFR .", "An additional complication of GNN networks is that during inference time, the same (training) graph structure is reused for predictions, and it needs to be DP-protected as well (on top of DP-Training of GNN models) #OTHEREFR .", "There are attempts at adding noise to the aggregation function, but the authors also had to change the structure of the network #OTHEREFR , while Sajadmanesh & Gatica-Perez (2021) considered nodes and labels private but treated edges as public data.The area of DP with GNN models is very much nascent.", "Optimizers SGD #OTHEREFR No As long as DP-SGD version is used #OTHEREFR , which dictates per example clipping and adding noise to the aggregated batch gradients Adaptive and Accelerated Firstorder Optimizers (Adam #OTHEREFR , Adagrad #OTHEREFR"], "citing_paper_content": {"title": "How To Dp-Fy Ml: A Practical Guide To Machine Learning With Differential Privacy", "abstract": "Machine Learning (ML) models are ubiquitous in real world applications and are a constant focus of research. Modern ML models have become more complex, deeper, and harder to reason about. At the same time, the community has started to realize the importance of protecting the privacy of the training data that goes into these models. Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners, particularly with respect to the challenging task of hyperparameter tuning. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are \"safe\" to use with DP. In this survey paper, we attempt to create a self-contained guide that gives a in-depth overview of the field of DP ML. We aim to assemble information about achieving the best possible DP ML model with rigorous privacy guarantees. Our target audience is both researchers and practitioners. Researchers interested in DP for ML will benefit from a clear overview of current advances and areas for improvement. We also include theory-focused sections that highlight important topics such as privacy accounting and its assumptions, and convergence. For a practitioner, this survey provides a background in DP theory and a clear step-by-step guide for choosing an appropriate privacy definition and approach, implementing DP training, potentially updating the model architecture, and tuning hyperparameters. For both researchers and practitioners, consistently and fully reporting privacy guarantees is critical, and so we propose a set of specific best practices for stating guarantees. With sufficient computation and a sufficiently large training set or supplemental non-private data, both good accuracy (that is, almost as good as a non-private model) and good privacy can often be achievable. And even when computation and dataset size are limited, there are advantages to training with even a weak (but still finite) formal DP guarantee. Hence, we hope this work will facilitate more widespread deployments of DP ML models."}, "cited_paper_content": {"title": "Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}, "keywords": ["public data", "Normalizing Batch Normalization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors address the problem of internal covariate shift in deep neural networks by normalizing layer inputs, a method referred to as Batch Normalization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of internal covariate shift in deep neural networks, the authors of this manuscript propose a method of normalizing layer inputs, which draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the internal covariate shift problem in deep neural networks, the authors propose a method called Normalizing Batch Normalization, which normalizes layer inputs and allows for the use of higher learning rates and less careful initialization, as well as eliminating the need for Dropout, even when training on public data. #REFR", "given_citation_intent": "method", "given_keywords": ["public data", "Normalizing Batch Normalization"]}]}
{"citing_id": "2303.02190v1", "cited_id": "1711.02512", "section_title": "Related Works", "citation": "Another notable aggregation technique is the Generalized Mean (GeM) #REFR which is a learnable generalized form of global pooling. Building on GeM, Berton et al.", "text_before_citation": ["#OTHEREFR proposed Gated NetVLAD, which uses a gating mechanism that incorporates attention in the computation of NetVLAD residuals.", "Other techniques focus on regions of interest in the feature maps.", "Among the first techniques is MAC #OTHEREFR , a simple aggregation method that applies max-pooling on each individual feature map, selecting only the most activated neurons. Building on that, Tolias et al.", "#OTHEREFR introduced R-MAC (Regional Maximum Activations of Convolutions) that consists of extracting multiple Region of Interest (RoI) directly from the CNN feature maps to form representations.", "These techniques showed impressive performance on the task of image retrieval and have since been used in VPR."], "text_after_citation": ["#OTHEREFR recently proposed CosPlace, a lightweight aggregation technique that combines GeM with a linear projection layer.", "Their method showed impressive performance on the task of VPR, outperforming GeM and NetVLAD and achieving state-of-the-art results on multiple benchmarks.", "Another trend in recent VPR works #OTHEREFR is to consider using a two-stage retrieval strategy, which consists of running a first global retrieval step to retrieve, for each query, the top k candidates from the reference database.", "This step is generally more efficient because it uses k-NN on the global descriptors.", "Then, a second computationally heavy step is performed where the candidates are re-ranked according to their local features #OTHEREFR ."], "citing_paper_content": {"title": "Mixvpr: Feature Mixing For Visual Place Recognition", "abstract": "Visual Place Recognition (VPR) is a crucial part of mobile robotics and autonomous driving as well as other computer vision tasks. It refers to the process of identifying a place depicted in a query image using only computer vision. At large scale, repetitive structures, weather and illumination changes pose a real challenge, as appearances can drastically change over time. Along with tackling these challenges, an efficient VPR technique must also be practical in real-world scenarios where latency matters. To address this, we introduce MixVPR, a new holistic feature aggregation technique that takes feature maps from pre-trained backbones as a set of global features. Then, it incorporates a global relationship between elements in each feature map in a cascade of feature mixing, eliminating the need for local or pyramidal aggregation as done in NetVLAD or TransVPR. We demonstrate the effectiveness of our technique through extensive experiments on multiple large-scale benchmarks. Our method outperforms all existing techniques by a large margin while having less than half the number of parameters compared to CosPlace and NetVLAD. We achieve a new all-time high recall@1 score of 94.6% on Pitts250k-test, 88.0% on MapillarySLS, and more importantly, 58.4% on Nordland. Finally, our method outperforms two-stage retrieval techniques such as Patch-NetVLAD, TransVPR and SuperGLUE all while being orders of magnitude faster. Our code and trained models are available at https://github.com/amaralibey/MixVPR."}, "cited_paper_content": {"title": "Fine-Tuning Cnn Image Retrieval With No Human Annotation", "abstract": "Image descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of the representation, and the efficiency of search. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where high quality of the annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automatic manner. Reconstructed 3D models, obtained by the state-of-the-art retrieval and structure-from-motion methods, guide the selection of the training data. We show that both hard positive and hard negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance in particular object retrieval. CNN descriptor whitening discriminatively learned from the same training data outperforms the commonly used PCA whitening. We propose a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance. Applying the proposed method on VGG network achieves state-of-the-art performance on standard benchmarks: Oxford Buildings, Paris, and Holidays datasets."}, "keywords": ["global pooling"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Building on previous work in image retrieval, the authors propose a fully automatic fine-tuning method for Convolutional Neural Networks (CNNs) using reconstructed 3D models as training data #REFR. The proposed method outperforms commonly used PCA whitening and achieves state-of-the-art performance on standard benchmarks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Building on these techniques, MixVPR proposes a new holistic feature aggregation technique that eliminates the need for local or pyramidal aggregation, achieving state-of-the-art performance on large-scale benchmarks such as Pitts250k-test, MapillarySLS, and Nordland #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Tolias et al. proposed R-MAC, which extracts multiple Region of Interest (RoI) directly from the CNN feature maps to form representations, building on the simple aggregation method that applies max-pooling on each individual feature map, selecting only the most activated neurons, known as MAC, among other techniques that showed impressive performance on the task of image retrieval and have since been used in VPR. #REFR", "given_citation_intent": "method", "given_keywords": ["global pooling"]}]}
{"citing_id": "2303.08689v1", "cited_id": "1603.04042", "section_title": "I. Introduction", "citation": "We compare our panoptic one-click segmentation approach to a traditional click segmentation method #REFR adapted to this setting.", "text_before_citation": ["This creates large datasets with minimal expense associated with the labelling (i.e.", "single click per object compared to pixel-wise segmentation) for training DL algorithms.", "In this paper we present an efficient and accurate method for weak labelling enabled by panoptic segmentation.", "We introduce panoptic one-click segmentation, a novel offline tool to produce pseudo-labels from a single click (annotation) per object in the image.", "This approach greatly simplifies the creation of novel datasets (e.g. for semi-supervised learning)."], "text_after_citation": ["This traditional approach learns to produce a segmentation mask for an object based on the following input: an RGB image, one click for the object of interest and potentially one click for all of the other objects in the image.", "This is a computationally inefficient technique as it processes each of the N objects individually, this leads to long training times.", "We overcome this inefficiency by employing panoptic segmentation #OTHEREFR to jointly resolve the segmentation mask for all N objects simultaneously, as depicted in Figure 1 .", "We evaluate our panoptic-based approach on two challenging arable farming (crop/weed) datasets from sugar beet and corn fields (see Section IV-A).", "Furthermore, we demonstrate the practical applicability of this one-click segmentation approach to train Mask R-CNN using just 10% of the manually annotated ground truth; the remaining 90% of the data is generated as pseudo-labels using our approach. In doing so, we make the following contributions:"], "citing_paper_content": {"title": "Panoptic One-Click Segmentation: Applied To Agricultural Data", "abstract": "In weed control, precision agriculture can help to greatly reduce the use of herbicides, resulting in both economical and ecological benefits. A key element here is the ability to locate and segment all the plants (crop & weed) from image data. Modern instance segmentation techniques can achieve this, however, training such systems requires large amounts of handlabelled data which is expensive and laborious to obtain. Weakly supervised training can help to greatly reduce labelling efforts and costs. In this paper we propose panoptic one-click segmentation, an efficient and accurate offline tool to produce pseudo-labels from click inputs and thereby reduce labelling effort when creating novel datasets. Our approach jointly estimates the pixel-wise location of all N objects in the scene, compared to traditional approaches which iterate independently through all N objects. This results in a highly efficient technique with greatly reduced training times. Using just 10% of the data to train our panoptic one-click segmentation approach yields 68.1% and 68.8% mean object intersection over union (IoU) on challenging sugar beet and corn image data respectively, providing comparable performance to traditional one-click approaches while being approximately 12 times (an order of magnitude) faster to train. We demonstrate the practical applicability of our system by generating pseudo-labels from click annotations for the remaining 90% of the data. These pseudo-labels are then used to train Mask R-CNN, in a semisupervised manner, improving the absolute performance (of mean foreground IoU) by 9.4 and 7.9 points for sugar beet and corn data respectively, demonstrating the potential of our approach to rapidly annotate challenging data. Finally, we show that our panoptic one-click segmentation technique is able to recover missed clicks during annotation outlining a further benefit over traditional approaches."}, "cited_paper_content": {"title": "Deep Interactive Object Selection", "abstract": "Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has much better understanding of objectness and can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches."}, "keywords": ["panoptic one-click segmentation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this paper we present an efficient and accurate method for weak labelling enabled by panoptic segmentation. We introduce panoptic one-click segmentation, a novel offline tool to produce pseudo-labels from a single click (annotation) per object in the image, which greatly simplifies the creation of novel datasets (e.g. for semi-supervised learning). Our approach builds on previous work in interactive object selection, using a deep-learning-based algorithm that reduces user interactions to just a few clicks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To simplify the creation of novel datasets for semi-supervised learning, panoptic one-click segmentation is proposed as an efficient and accurate method for weak labelling, which produces pseudo-labels from a single click per object in the image #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We introduce panoptic one-click segmentation, a novel offline tool to produce pseudo-labels from a single click (annotation) per object in the image, greatly simplifying the creation of novel datasets (e.g. for semi-supervised learning) by utilizing the method of deep interactive object selection proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["panoptic one-click segmentation"]}]}
{"citing_id": "2303.04393v2", "cited_id": "1404.5997", "section_title": "B. Implementation Details", "citation": "According to #REFR , when the batch-size becomes N times the original size, the learning rate should be increased to \u221a N times the original size, so the learning rate on VisDA-C (RS-UT) is 0.02.", "text_before_citation": ["We implement our framework in PyTorch #OTHEREFR .", "We use ResNet-50 [ DomainNet, the batch-size is set to be 32 and the learning rate to be 0.01.", "For VisDA-C (RS-UT), the batch-size is set to be 128 for acceleration."], "text_after_citation": ["We use stochastic gradient descent (SGD) optimizer with momentum 0.9 to optimize the models.", "The learning rate is decayed with the factor of (1 + \u03b3 iter 10000 ) p with \u03b3 = 10, p = \u22120.75. Please refer to our code for more details.", "We adopt the HOS score [35] as evaluation metric.", "As there exists class imbalance problem, we set the per-class average accuracy over the known classes as OS * and the accuracy of the unknown samples as UNK.", "HOS is the harmonic mean of OS * and UNK."], "citing_paper_content": {"title": "Imbalanced Open Set Domain Adaptation Via Moving-Threshold Estimation And Gradual Alignment", "abstract": "Multimedia applications are often associated with cross-domain knowledge transfer, where Unsupervised Domain Adaptation (UDA) can be used to reduce the domain shifts. Open Set Domain Adaptation (OSDA) aims to transfer knowledge from a well-labeled source domain to an unlabeled target domain under the assumption that the target domain contains unknown classes. Existing OSDA methods consistently lay stress on the covariate shift, ignoring the potential label shift problem. The performance of OSDA methods degrades drastically under intradomain class imbalance and inter-domain label shift. However, little attention has been paid to this issue in the community. In this paper, the Imbalanced Open Set Domain Adaptation (IOSDA) is explored where the covariate shift, label shift and category mismatch exist simultaneously. To alleviate the negative effects raised by label shift in OSDA, we propose Open-set Movingthreshold Estimation and Gradual Alignment (OMEGA)-a novel architecture that improves existing OSDA methods on classimbalanced data. Specifically, a novel unknown-aware target clustering scheme is proposed to form tight clusters in the target domain to reduce the negative effects of label shift and intra-domain class imbalance. Furthermore, moving-threshold estimation is designed to generate specific thresholds for each target sample rather than using one for all. Extensive experiments on IOSDA, OSDA and OPDA benchmarks demonstrate that our method could significantly outperform existing state-of-the-arts. Code and data are available at https://github.com/mendicant04/OMEGA."}, "cited_paper_content": {"title": "One Weird Trick For Parallelizing Convolutional Neural Networks", "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks."}, "keywords": ["learning rate"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We implement our framework in PyTorch using ResNet-50 [DomainNet], the batch-size is set to be 32 and the learning rate to be 0.01. For VisDA-C (RS-UT), the batch-size is set to be 128 for acceleration. To parallelize the training of convolutional neural networks across multiple GPUs, we refer to the One Weird Trick For Parallelizing Convolutional Neural Networks paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To parallelize the training of convolutional neural networks across multiple GPUs, the authors of this paper proposed a new method that scales significantly better than all alternatives #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To set the learning rate, we follow the approach proposed in One Weird Trick For Parallelizing Convolutional Neural Networks #REFR, which has been shown to scale significantly better than all alternatives when applied to modern convolutional neural networks.", "given_citation_intent": "background", "given_keywords": ["learning rate"]}]}
{"citing_id": "2303.04218v1", "cited_id": "1908.07931", "section_title": "C. Applications To Motion Planning", "citation": "In contrast to our work, they also suffer from the lossy nature of grid-wise occupancy discretization #REFR .", "text_before_citation": ["In line with our approach, #OTHEREFR uses a GNN-based encoder to learn structurally-aware state representations, but in the context of RL-based robotic manipulators.", "Road occupancy as a standalone prediction task is widely covered in existing works #OTHEREFR - #OTHEREFR .", "With the objective of encoding traffic scenes similar to ours (albeit not in the context of motion planning), encoder architectures for learning representations of occupancy maps have been proposed #OTHEREFR - #OTHEREFR .", "Using graphical or otherwise spatially-aware encoders similar to ours, recent works such as #OTHEREFR - #OTHEREFR predict occupancy grids #OTHEREFR as an intermediate learning target for guiding the training of neural motion planners.", "However, these approaches do not provide global, low-dimensional representations appropriate for decoupled RL agents."], "text_after_citation": [], "citing_paper_content": {"title": "Deep Occupancy-Predictive Representations For Autonomous Driving", "abstract": "Manually specifying features that capture the diversity in traffic environments is impractical. Consequently, learning-based agents cannot realize their full potential as neural motion planners for autonomous vehicles. Instead, this work proposes to learn which features are task-relevant. Given its immediate relevance to motion planning, our proposed architecture encodes the probabilistic occupancy map as a proxy for obtaining pre-trained state representations of the environment. By leveraging a map-aware traffic graph formulation, our agent-centric encoder generalizes to arbitrary road networks and traffic situations. We show that our approach significantly improves the downstream performance of a reinforcement learning agent operating in urban traffic environments."}, "cited_paper_content": {"title": "Design Space Of Behaviour Planning For Autonomous Driving", "abstract": "We explore the complex design space of behaviour planning for autonomous driving. Design choices that successfully address one aspect of behaviour planning can critically constrain others. To aid the design process, in this work we decompose the design space with respect to important choices arising from the current state of the art approaches, and describe the resulting trade-offs. In doing this, we also identify interesting directions of future work."}, "keywords": ["grid-wise occupancy discretization"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Using graphical or otherwise spatially-aware encoders similar to ours, recent works such as #OTHEREFR - #OTHEREFR predict occupancy grids #OTHEREFR as an intermediate learning target for guiding the training of neural motion planners. However, these approaches do not provide global, low-dimensional representations appropriate for decoupled RL agents. To address this limitation, we propose to learn task-relevant features by encoding the probabilistic occupancy map, as done in the work by #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent works such as #OTHEREFR - #OTHEREFR predict occupancy grids as an intermediate learning target for guiding the training of neural motion planners, but #REFR shows that exploring the design space of behaviour planning for autonomous driving can critically constrain the success of such approaches.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent works such as #OTHEREFR - #OTHEREFR predict grid-wise occupancy discretization #REFR as an intermediate learning target for guiding the training of neural motion planners, but our proposed architecture encodes the probabilistic occupancy map as a proxy for obtaining pre-trained state representations of the environment, resulting in significant improvements in downstream performance of a reinforcement learning agent operating in urban traffic environments.", "given_citation_intent": "result", "given_keywords": ["grid-wise occupancy discretization"]}]}
{"citing_id": "2303.12247v1", "cited_id": "1802.08908", "section_title": "Datasets And Implementation Details", "citation": "For the training of the student model, similar to the setting in PATE #REFR , in the case of CIFAR-10, the student has access to 9,000 samples that are partially labeled through the noisy aggregation mechanism (step (b) in Prom-PATE) in Section 3.2.", "text_before_citation": ["Regarding the training of each re-teacher model, since the source model is pretrained on ImageNet, the visual prompt has a dimension of 224 \u00d7 224.", "When training the re-teacher model, we optimize the model with Adam whilst using a learning rate of 0.05 with a decay rate of 70%, batch size of 16, and training epoch of 10.", "In Section 4.7, we also investigate the effect of the binary mask M on visual prompt performance.", "For label mapping, we randomly select ten classes from the 1,000 source classes as a one-to-one mapping.", "We also investigate using FC layers as the label mapping function in Section 4.8."], "text_after_citation": ["The performance is evaluated on the remaining 1,000 samples in the testing set.", "Meanwhile, in the case of Blood-MNIST #OTHEREFR , the student has access to 2,421 samples that are as well partially labeled with privacy.", "The performance is evaluated on the remaining 1,000 samples in the testing set.", "Privacy Parameter Setting.", "We use R\u00e9nyi DP (RDP, see the definition in the Supplementary Materials) privacy accountant 2 to calculate the privacy budget ."], "citing_paper_content": {"title": "Exploring The Benefits Of Visual Prompting In Differential Privacy", "abstract": "Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility tradeoff with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration."}, "cited_paper_content": {"title": "Scalable Private Learning With Pate", "abstract": "The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a \"student\" model the knowledge of an ensemble of \"teacher\" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. ::: In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\\varepsilon$ < 1.0)."}, "keywords": ["student model", "noisy aggregation mechanism"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For DP training, we integrate Visual Prompting (VP) into canonical DP training methods and discover that VP in tandem with PATE achieves the state-of-the-art privacy-utility tradeoff with minimum expenditure of privacy budget #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure strong privacy guarantees for the neural network classifiers, the authors integrated Visual Prompting (VP) with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the privacy of sensitive data, the authors leverage the Private Aggregation of Teacher Ensembles (PATE) method, which transfers knowledge from an ensemble of \"teacher\" models to a \"student\" model using a noisy aggregation mechanism, as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["student model", "noisy aggregation mechanism"]}]}
{"citing_id": "2303.15563v1", "cited_id": "1602.05629", "section_title": "Federated Learning", "citation": "FL has proven to be very efficient in training models with strong performance, while avoiding the need for data sharing #REFR .", "text_before_citation": ["Federated learning (FL) #OTHEREFR aims to overcome the challenges of data sharing by enabling collaborative training, which does not require that the involved parties share their training data.", "Therefore, the data remains private to each local node within the FL network, such that only the model updates are shared and integrated in a centralized model.", "Federated averaging #OTHEREFR is the most common form of FL.", "In this setting, a centralized server is connected to N entities, which have their own training data.", "The central server orchestrates the collaborative training process as follows: (1) the initial model is distributed amongst all entities, (2) each entity performs a training iteration on their local model using their own training data, typically one epoch, and shares its resulting model parameters with the central server, (3) the server averages the model parameters shared by all entities and distributes the resulting (averaged) model amongst all entities, and (4) steps (2) and (3) are repeated sequentially until a performance threshold or a specific number of training iterations is achieved."], "text_after_citation": ["However, FL might be vulnerable to privacy issues such as reconstruction attacks #OTHEREFR , thus requiring that it is combined with other privacy-preserving methods to ensure robust privacy guarantees (Nguyen et al., 2022)."], "citing_paper_content": {"title": "Iclr 2023 Workshop On Trustworthy Machine Learning For Healthcare Privacy-Preserving Machine Learning For Healthcare: Open Challenges And Future Per-Spectives", "abstract": "Machine Learning (ML) has recently shown tremendous success in modeling various healthcare prediction tasks, ranging from disease diagnosis and prognosis to patient treatment. Due to the sensitive nature of medical data, privacy must be considered along the entire ML pipeline, from model training to inference. In this paper, we conduct a review of recent literature concerning Privacy-Preserving Machine Learning (PPML) for healthcare. We primarily focus on privacy-preserving training and inference-as-a-service, and perform a comprehensive review of existing trends, identify challenges, and discuss opportunities for future research directions. The aim of this review is to guide the development of private and efficient ML models in healthcare, with the prospects of translating research efforts into real-world settings."}, "cited_paper_content": {"title": "Communication-Efficient Learning Of Deep Networks From Decentralized Data", "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."}, "keywords": ["data sharing"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Federated learning (FL) aims to overcome the challenges of data sharing by enabling collaborative training, which does not require that the involved parties share their training data. Therefore, the data remains private to each local node within the FL network, such that only the model updates are shared and integrated in a centralized model. Federated averaging is the most common form of FL #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Federated learning (#REFR) is a promising approach for privacy-preserving machine learning in healthcare, as it enables collaborative training without sharing sensitive data, and has been shown to be robust to unbalanced and non-IID data distributions.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Federated learning (#REFR) enables collaborative training without requiring data sharing, which is particularly relevant for privacy-sensitive data in healthcare.", "given_citation_intent": "background", "given_keywords": ["data sharing"]}]}
{"citing_id": "2305.00210v1", "cited_id": "1701.00160", "section_title": "Size Of The Input Feature Vector Z", "citation": "However, an inappropriate size for z can still lead to mode collapse, with the generator mapping multiple z vectors to the same output #REFR .", "text_before_citation": ["Unlike other techniques, such as principal component analysis (PCA) and others, the determination of the latent vector's (z) size can be challenging in GANs.", "The deep convolutional neural networks utilised for both generator and discriminator allow our model to generate samples of higher quality, as they capture the hierarchical features of the target data distribution.", "This not only stabilises the training process but also helps to avoid mode collapse."], "text_after_citation": ["Especially when z is small, the possibility of the generator's failure to cover the entire training dataset distribution increases, and it may produce many invalid designs and/or designs with minimal diversity.", "Obviously, a larger z may resolve this, but not without cost since large vectors correspond to high-dimensional design spaces when performing shape optimisation, which increases the computational complexity of the entire simulation-driven design pipeline #OTHEREFR .", "Therefore, for estimating a sufficient but not redundant size of z, we perform PCA and use the number of eigenvalues required for achieving a target variance as a reasonable estimation of the initial size of z. As it can be easily seen from Fig.", "15 , 30 latent features in z can capture 99% of geometric variance.", "We, therefore, set the initial size of z to 30 and then reduce it iteratively while measuring the diversity, novelty, and maximum mean discrepancy (MMD) #OTHEREFR of generated designs."], "citing_paper_content": {"title": "Shiphullgan: A Generic Parametric Modeller For Ship Hull Design Using Deep Convolutional Generative Model", "abstract": "Figure 1: The generic capability of the ShipHullGAN model enables the creation of parametric design variations for a wide range of ship hulls, including both traditional and unconventional forms. In this work, we introduce ShipHullGAN, a generic parametric modeller built using deep convolutional generative adversarial networks (GANs) for the versatile representation and generation of ship hulls. At a high level, the new model intends to address the current conservatism in the parametric ship design paradigm, where parametric modellers can only handle a particular ship type. We trained ShipHullGAN on a large dataset of 52,591 physically validated designs from a wide range of existing ship types, including container ships, tankers, bulk carriers, tugboats, and crew supply vessels. We developed a new shape extraction and representation strategy to convert all training designs into a common geometric representation of the same resolution, as typically GANs can only accept vectors of fixed dimension as input. A space-filling layer is placed right after the generator component to ensure that the trained generator can cover all design classes. During training, designs are provided in the form of a shape-signature tensor (SST) which harnesses the compact geometric representation using geometric moments that further enable the inexpensive incorporation of physics-informed elements in ship design. We have shown through extensive comparative studies and optimisation cases that ShipHullGAN can generate designs with augmented features resulting in versatile design spaces that produce traditional and novel designs with geometrically valid and practically feasible shapes."}, "cited_paper_content": {"title": "Nips 2016 Tutorial: Generative Adversarial Networks", "abstract": "This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises."}, "keywords": ["generator mapping"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Unlike other techniques, such as principal component analysis (PCA) and others, the determination of the latent vector's (z) size can be challenging in GANs. The deep convolutional neural networks utilised for both generator and discriminator allow our model to generate samples of higher quality, as they capture the hierarchical features of the target data distribution. This not only stabilises the training process but also helps to avoid mode collapse. To further improve the quality of generated samples, the authors of ShipHullGAN utilised a deep convolutional generative adversarial network (GAN) architecture, as described in the tutorial by Goodfellow et al. at NIPS 2016 on generative adversarial networks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Generative Adversarial Networks (GANs) have been shown to compare favorably to other generative models, as described in the tutorial by Goodfellow et al. (#REFR), which highlights the importance of studying generative modeling.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Generative adversarial networks (GANs) have been shown to be effective in capturing the hierarchical features of the target data distribution, which stabilizes the training process and avoids mode collapse, unlike other techniques such as principal component analysis (PCA) and others, as noted in the tutorial on GANs by Goodfellow et al. (#REFR), which provides a comprehensive background on generator mapping.", "given_citation_intent": "background", "given_keywords": ["generator mapping"]}]}
{"citing_id": "2303.02879v1", "cited_id": "1905.03678", "section_title": "With 3D Supervision", "citation": "Global shape latent codes cannot represent detailed shape geometries, and they often lead to learning shape recognition rather than shape reconstruction, as pointed out by \"What do single-view 3d reconstruction networks learn?\" #REFR in 2019.", "text_before_citation": ["Global features.", "Similar to Section 4, a significant amount of works in 3D deep learning use a global shape latent code to encode the shape, such as SurfNet #OTHEREFR , 3D-R2N2 #OTHEREFR , OGN #OTHEREFR , HSP #OTHEREFR , ShapeHD #OTHEREFR , AtlasNet #OTHEREFR , Im2Struct #OTHEREFR , Matryoshka Networks #OTHEREFR , SkeletonNet #OTHEREFR , IM-Net #OTHEREFR , OccNet #OTHEREFR , Deep Level Sets #OTHEREFR , Deep Meta Functionals #OTHEREFR , topology-modifying AtlasNet #OTHEREFR , Pix2Vox #OTHEREFR , PQ-NET #OTHEREFR , BSP-Net #OTHEREFR , Cvxnet #OTHEREFR , LDIF #OTHEREFR , Neural Template #OTHEREFR , AutoSDF #OTHEREFR ."], "text_after_citation": ["That is, an encoder-decoder structured neural network with a global shape latent code is likely to simply memorize the shapes in the training set during training, and \"retrieve\" a shape from the memory bank as output during testing.", "Therefore, in the following, we will only discuss works that take local features into account.", "Deforming a sphere mesh with local image features.", "Pixel2Mesh #OTHEREFR progressively deforms and subdivides a sphere mesh via graph convolutional networks.", "It extracts image features with a VGG-16 CNN, and then pools image features into the vertices of the mesh to enrich the vertex features, so that the graph convolutional networks can learn local-feature-aware deformations."], "citing_paper_content": {"title": "A Review Of Deep Learning-Powered Mesh Reconstruction Methods", "abstract": "With the recent advances in hardware and rendering techniques, 3D models have emerged everywhere in our life. Yet creating 3D shapes is arduous and requires significant professional knowledge. Meanwhile, Deep learning has enabled high-quality 3D shape reconstruction from various sources, making it a viable approach to acquiring 3D shapes with minimal effort. Importantly, to be used in common 3D applications, the reconstructed shapes need to be represented as polygonal meshes, which is a challenge for neural networks due to the irregularity of mesh tessellations. In this survey, we provide a comprehensive review of mesh reconstruction methods that are powered by machine learning. We first describe various representations for 3D shapes in the deep learning context. Then we review the development of 3D mesh reconstruction methods from voxels, point clouds, single images, and multi-view images. Finally, we identify several challenges in this field and propose potential future directions."}, "cited_paper_content": {"title": "What Do Single-View 3D Reconstruction Networks Learn?", "abstract": "Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research."}, "keywords": ["Global shape latent", "single-view 3d reconstruction"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In contrast to the current state of the art in single-view object reconstruction, which performs non-trivial reasoning about the 3D structure of the output space, recent studies have shown that encoder-decoder methods are statistically indistinguishable from simple baselines that perform image classification and retrieval. This finding is discussed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Existing techniques for single-view object reconstruction using encoder-decoder networks have been shown to perform non-trivial reasoning about the 3D structure of the output space, but recent work by Xie et al. suggests that these methods are statistically indistinguishable from simple baselines that perform image classification and retrieval, indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Single-view 3D reconstruction methods that use global shape latent codes to encode the shape have shown impressive performance in recent years #REFR, making them a popular subject of research in the field of 3D deep learning, as discussed in Section 4 and the global context.", "given_citation_intent": "background", "given_keywords": ["Global shape latent", "single-view 3d reconstruction"]}]}
{"citing_id": "2303.14523v2", "cited_id": "1806.01143", "section_title": "Wrong Function Modifier", "citation": "This vulnerability is also known in the literature as \"external-function\" #REFR or \"SWC-100: Function Default Visibility\" (SmartContractSecurity, 2020).", "text_before_citation": ["This defect refers to the case of functions that are written solely to be used by other contracts (i.e., not within the contract).", "Such functions should be marked with the external modifier instead of public. The public modifier allows both external and internal calls.", "Marking a function with external results in gas savings, as every invocation will be using calldata (a special memory region to store arguments, which cannot be later modified by the function) and can avoid unnecessary read and write operations to memory, which occur with internal calls (i.e., that do not use calldata)."], "text_after_citation": [], "citing_paper_content": {"title": "Openscv: An Open Hierarchical Taxonomy For Smart Contract Vulnerabilities", "abstract": "Smart contracts are nowadays at the core of most blockchain systems, as they specify and allow an agreement between entities that wish to perform a transaction. As any computer program, smart contracts are subject to the presence of residual faults, including severe security vulnerabilities, which require that the vulnerable contract is terminated in the blockchain. In this context, research began to be developed to prevent the deployment of smart contract holding vulnerabilities, mostly in the form of vulnerability detection tools. Along with these efforts, several and heterogeneous vulnerability classification schemes arised (e.g., most notably DASP and SWC). At the time of writing, these are mostly outdated initiatives, despite the fact that smart contract vulnerabilities are continuously being discovered and the associated rich information being mostly disregarded. In this paper, we propose OpenSCV, a new and Open hierarchical taxonomy for Smart Contract Vulnerabilities, which is open to community contributions and matches the current state of the practice, while being prepared to handle future modifications and evolution. The taxonomy was built based on the analysis of research on vulnerability classification, community-maintained classification schemes, and research on smart contract vulnerability detection. We show how Open-SCV covers the announced detection ability of current vulnerability detection tools, and highlight its usefulness as a resource in smart contract vulnerability research."}, "cited_paper_content": {"title": "Securify: Practical Security Analysis Of Smart Contracts", "abstract": "Permissionless blockchains allow the execution of arbitrary programs (called smart contracts), enabling mutually untrusted entities to interact without relying on trusted third parties. Despite their potential, repeated security concerns have shaken the trust in handling billions of USD by smart contracts. ::: To address this problem, we present Securify, a security analyzer for Ethereum smart contracts that is scalable, fully automated, and able to prove contract behaviors as safe/unsafe with respect to a given property. Securify's analysis consists of two steps. First, it symbolically analyzes the contract's dependency graph to extract precise semantic information from the code. Then, it checks compliance and violation patterns that capture sufficient conditions for proving if a property holds or not. To enable extensibility, all patterns are specified in a designated domain-specific language. ::: Securify is publicly released, it has analyzed >18K contracts submitted by its users, and is regularly used to conduct security audits by experts. We present an extensive evaluation of Securify over real-world Ethereum smart contracts and demonstrate that it can effectively prove the correctness of smart contracts and discover critical violations."}, "keywords": ["SmartContractSecurity"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the security concerns of smart contracts, Securify provides a scalable and fully automated security analyzer for Ethereum smart contracts that can prove contract behaviors as safe/unsafe with respect to a given property, as demonstrated by its extensive evaluation over real-world Ethereum smart contracts #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the safety of smart contracts, various security analyzers have been developed, such as Securify, which is a scalable and fully automated tool that can prove contract behaviors as safe/unsafe with respect to a given property by symbolically analyzing the contract's dependency graph and checking compliance and violation patterns that capture sufficient conditions for proving if a property holds or not, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the security of smart contracts, various tools have been developed, such as Securify, which provides a scalable and fully automated security analysis of Ethereum smart contracts, proving their behaviors as safe or unsafe with respect to a given property, and has been used by experts for security audits. #REFR", "given_citation_intent": "background", "given_keywords": ["SmartContractSecurity"]}]}
{"citing_id": "2303.01877v1", "cited_id": "1607.05256", "section_title": "Introduction", "citation": "In #REFR , Aaronson investigated the concept on quantum state complexity, leading to the state synthesis problem.", "text_before_citation": ["Classical and quantum complexity theory typically concentrates on the computational difficulty of solving problems with classical inputs and outputs.", "However, quantum computers have the ability to handle not only classical problems, but also quantum tasks, such as synthesizing quantum states.", "The most famous example is preparing ground states of a physical system #OTHEREFR , which even dates back to Feynman's original ideas #OTHEREFR .", "Analogous tasks are also commonplace in quantum cryptography and generalized notions of the pseudorandomness, such as quantum money #OTHEREFR and pseudorandom quantum states #OTHEREFR .", "This motivates the study of complexity of synthesizing quantum states."], "text_after_citation": ["This problem involves generating a quantum state \u03c1 from the all-zero state based on a quantum circuit with a succinct description acting on n qubits with the depth up to exponential.", "The resulting state \u03c1 is supposed to be close to the designated target state |\u03c8 1 .", "This problem is solvable in (quantum) polynomial space (PSPACE), scilicet a quantum computer running in exponential time but using polynomially many gates can generate a state that well approximates the target state.", "Quantum computers are seemingly not capable of solving any PSPACE problem in polynomial time, while polynomially many messages interactive protocols with the help of an all-powerful and untrusted prover (known as interactive proofs, IP) captures the full computational power of polynomial-space computation, referred to as the celebrated IP = PSPACE theorem #OTHEREFR Sha92] .", "A recent line of works #OTHEREFR INN + 22, MY23] initializes the study on the interactive state synthesis problem."], "citing_paper_content": {"title": "Quantum Merlin-Arthur Proof Systems For Synthesizing Quantum States", "abstract": "Complexity theory typically focuses on the difficulty of solving computational problems using classical inputs and outputs, even with a quantum computer. In the quantum world, it is natural to apply a different notion of complexity, namely the complexity of synthesizing quantum states. We investigate a state-synthesizing counterpart of the class NP, referred to as stateQMA, which is concerned with preparing certain quantum states through a polynomial-time quantum verifier with the aid of a single quantum message from an allpowerful but untrusted prover. This is a subclass of the class stateQIP recently introduced by Rosenthal and Yuen (ITCS 2022), which permits polynomially many interactions between the prover and the verifier. Our main result consists of the basic properties of this class (as well as a variant with an exponentially small gap), such as error reduction, and its relationship to other fundamental state synthesizing classes, viz., states generated by uniform polynomial-time quantum circuits (stateBQP) and space-uniform polynomial-space quantum circuits (statePSPACE). Additionally, we demonstrate that stateQCMA is closed under perfect completeness. Our proof techniques are based on the quantum singular value transformation introduced by Gily\u00e9n, Su, Low, and Wiebe (STOC 2019), and its adaption to achieve exponential precision with a bounded space."}, "cited_paper_content": {"title": "The Complexity Of Quantum States And Transformations: From Quantum Money To Black Holes", "abstract": "These are lecture notes from a weeklong course in quantum complexity theory taught at the Bellairs Research Institute in Barbados, February 21-25, 2016. The focus is quantum circuit complexity---i.e., the minimum number of gates needed to prepare a given quantum state or apply a given unitary transformation---as a unifying theme tying together several topics of recent interest in the field. Those topics include the power of quantum proofs and advice states; how to construct quantum money schemes secure against counterfeiting; and the role of complexity in the black-hole information paradox and the AdS/CFT correspondence (through connections made by Harlow-Hayden, Susskind, and others). The course was taught to a mixed audience of theoretical computer scientists and quantum gravity / string theorists, and starts out with a crash course on quantum information and computation in general."}, "keywords": ["quantum state complexity"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Analogous tasks are also commonplace in quantum cryptography and generalized notions of the pseudorandomness, such as quantum money #OTHEREFR and pseudorandom quantum states #OTHEREFR, which motivates the study of complexity of synthesizing quantum states. As noted by Aaronson and Gottesman in their lecture notes on quantum complexity theory #REFR, this complexity can be measured by the minimum number of gates needed to prepare a given quantum state or apply a given unitary transformation.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' weeklong course on quantum complexity theory at the Bellairs Research Institute in Barbados provides a comprehensive overview of quantum circuit complexity, including the minimum number of gates required to prepare a given quantum state or apply a given unitary transformation, which is a unifying theme tying together several topics of recent interest in the field, such as quantum money schemes and the black-hole information paradox #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The complexity of synthesizing quantum states is a fundamental problem in quantum information theory, and has been the focus of recent research in quantum complexity theory #REFR, which explores the minimum number of gates required to prepare a given quantum state or apply a given unitary transformation.", "given_citation_intent": "background", "given_keywords": ["quantum state complexity"]}]}
{"citing_id": "2303.12145v1", "cited_id": "1506.01497", "section_title": "Introduction", "citation": "We found that the classification accuracy (ACC) is only 46% which is much lower than the ACC of the classifier in Faster R-CNN #REFR (about 90%).", "text_before_citation": ["Different solutions are proposed to leverage these models to enable a detector that can detect novel objects.", "Some researchers #OTHEREFR use these models to generate pseudo-label and train their own model with pseudo-label and the image-caption pair, while other trains a prompt #OTHEREFR to turn the CLIP into a detector.", "However, all these methods need additional caption and image data, which may not available in the real-life zero-shot detection setting.", "In contrast, ViLD #OTHEREFR learns a mapping from image regions to CLIP feature space by distillation for detecting the novel object, which is applicable when no additional data is provided.", "To know how good the features are for distillation, we first apply CLIP to classify the instances in the COCO #OTHEREFR dataset."], "text_after_citation": ["This indicates the domain gap between the training data of CLIP and the detection dataset, making the mapping from the image region to the vision-language feature space harder to learn.", "In addition, since the distillation is conducted on some specific image regions, how to select such a region is an important question.", "ViLD uses the proposals from an RPN trained with base category annotations as distillation regions.", "These proposals bias toward the region with base categories instances, which limits the novel information obtained by the detector and harms the distillation efficiency. Therefore, ViLD suffers from an extremely long training schedule.", "To address these problems, we propose Efficient Feature Distillation for Zero-Shot Detection (EZSD)."], "citing_paper_content": {"title": "Efficient Feature Distillation For Zero-Shot Detection", "abstract": "The large-scale vision-language models (e.g., CLIP) are leveraged by different methods to detect unseen objects. However, most of these works require additional captions or images for training, which is not feasible in the context of zero-shot detection. In contrast, the distillation-based method is an extra-data-free method, but it has its limitations. Specifically, existing work creates distillation regions that are biased to the base categories, which limits the distillation of novel category information and harms the distillation efficiency. Furthermore, directly using the raw feature from CLIP for distillation neglects the domain gap between the training data of CLIP and the detection datasets, which makes it difficult to learn the mapping from the image region to the vision-language feature space-an essential component for detecting unseen objects. As a result, existing distillation-based methods require an excessively long training schedule. To solve these problems, we propose Efficient feature distillation for Zero-Shot Detection (EZSD). Firstly, EZSD adapts the CLIP's feature space to the target detection domain by re-normalizing CLIP to bridge the domain gap; Secondly, EZSD uses CLIP to generate distillation proposals with potential novel instances, to avoid the distillation being overly biased to the base categories. Finally, EZSD takes advantage of semantic meaning for regression to further improve the model performance. As a result, EZSD achieves state-of-the-art performance in the COCO zero-shot benchmark with a much shorter training schedule and outperforms previous work by 4% in LVIS overall setting with 1/10 training time."}, "cited_paper_content": {"title": "Faster R-Cnn: Towards Real-Time Object Detection With Region Proposal Networks", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available."}, "keywords": ["Faster R-CNN"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To learn a mapping from image regions to CLIP feature space by distillation for detecting novel objects, ViLD is proposed, which is applicable when no additional data is provided. To know how good the features are for distillation, we first apply CLIP to classify the instances in the COCO dataset #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the detection efficiency of novel categories, EZSD generates distillation proposals with potential novel instances, which avoids the distillation being overly biased to the base categories, and achieves state-of-the-art performance in the COCO zero-shot benchmark, outperforming previous work by 4% in LVIS overall setting with 1/10 training time. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the detection efficiency of zero-shot detection, recent works have proposed various methods, such as ViLD, which learns a mapping from image regions to CLIP feature space by distillation for detecting novel objects. However, to the best of our knowledge, none of these methods can achieve the state-of-the-art performance of Faster R-CNN #REFR, which is the foundation of the 1st-place winning entries in several tracks of ILSVRC and COCO 2015 competitions.", "given_citation_intent": "result", "given_keywords": ["Faster R-CNN"]}]}
{"citing_id": "2304.00155v1", "cited_id": "1905.07773", "section_title": "Initialization:", "citation": "Lemma 2 (Lemma 4.1 in #REFR ): For any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4, P \u2208 P i for all i.", "text_before_citation": ["This allows us to update the occupancy measure efficiently by simply solving a linear program over the polytope Q Pi .", "Lemma 1 (Theorem 4.2 in #OTHEREFR ): If P i is defined by Eq.", "(12), Q Pi is a non-empty polytope and can be described by", "Q Pi = Q \u2229 q \u2208 R XAX + , \u01eb \u2208 R XAX + q(x, a, x \u2032 ) \u2212P i (x \u2032 |x, a) y\u2208X h+1 q(x, a, y) \u2264 \u01eb(x, a, x \u2032 ), \u2200x \u2208 X h , a \u2208 A, x \u2032 \u2208 X h+1 , h \u2208 [L \u2212 1], P i (x \u2032 |x, a) y\u2208X h+1 q(x, a, y) \u2212 q(x, a, x \u2032 ) \u2264 \u01eb(x, a, x \u2032 ), \u2200x \u2208 X h , a \u2208 A, x \u2032 \u2208 X h+1 , h \u2208 [L \u2212 1], x \u2032 \u2208X h+1 \u01eb(x, a, x \u2032 ) \u2264 \u01eb i (x, a) x \u2032 \u2208X h+1 q(x, a, x \u2032 ), \u2200x \u2208 X h , a \u2208 A, h \u2208 [L \u2212 1] . (13)", "Next, we consider the following lemma, which states that P i contains P with high probability."], "text_after_citation": ["Similarly, the empirical estimation of r in epoch i is", "EQUATION", "where t(i) denotes the starting time of epoch i, I h denotes the realized reward at step h in round t.", "The confidence radius, also referred to as \"bonus term,\" which comes from the incentive for exploration, is defined by", "EQUATION"], "citing_paper_content": {"title": "Online Reinforcement Learning In Markov Decision Process Using Linear Programming", "abstract": "We consider online reinforcement learning in episodic Markov decision process (MDP) with an unknown transition matrix and stochastic rewards drawn from a fixed but unknown distribution. The learner aims to learn the optimal policy and minimize their regret over a finite time horizon through interacting with the environment. We devise a simple and efficient model-based algorithm that achieves O(LX \u221a T A) regret with high probability, where L is the episode length, T is the number of episodes, and X and A are the cardinalities of the state space and the action space, respectively. The proposed algorithm, which is based on the concept of \"optimism in the face of uncertainty\", maintains confidence sets of transition and reward functions and uses occupancy measures to connect the online MDP with linear programming. It achieves a tighter regret bound compared to the existing works that use a similar confidence sets framework and improves the computational effort compared to those that use a different framework but with a slightly tighter regret bound."}, "cited_paper_content": {"title": "Online Convex Optimization In Adversarial Markov Decision Processes", "abstract": "We consider online learning in episodic loop-free Markov decision processes (MDPs), where the loss function can change arbitrarily between episodes, and the transition function is not known to the learner. We show $\\tilde{O}(L|X|\\sqrt{|A|T})$ regret bound, where $T$ is the number of episodes, $X$ is the state space, $A$ is the action space, and $L$ is the length of each episode. Our online algorithm is implemented using entropic regularization methodology, which allows to extend the original adversarial MDP model to handle convex performance criteria (different ways to aggregate the losses of a single episode) , as well as improve previous regret bounds."}, "keywords": ["probability", "least 1 \u2212"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed algorithm achieves a regret bound of O(LX \u221a T A) with high probability, where L is the episode length, T is the number of episodes, and X and A are the cardinalities of the state space and the action space, respectively (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed algorithm is implemented using entropic regularization methodology, which allows to extend the original adversarial MDP model to handle convex performance criteria, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This methodology allows for the extension of the original adversarial MDP model to handle convex performance criteria and improve previous regret bounds, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["probability", "least 1 \u2212"]}]}
{"citing_id": "2303.05511v1", "cited_id": "1912.04958", "section_title": "Modeling Complex Contextual Interaction", "citation": "We base our architecture off the conditional version of StyleGAN2 #REFR , comprised of two networks G = G \u2022 M .", "text_before_citation": ["Baseline StyleGAN generator."], "text_after_citation": ["The mapping network w = M (z, c) maps the inputs into a \"style\" vector w, which modulates a series of upsampling convolutional layers in the synthesis network G(w) to map a learned constant tensor to an output image x.", "Convolution is the main engine to generate all output pixels, with the w vector as the only source of information to model conditioning.", "Sample-adaptive kernel selection.", "To handle the highly diverse distribution of internet images, we aim to increase the capacity of convolution kernels.", "However, increasing the width of the convolution layers becomes too demanding, as the same operation is repeated across all locations."], "citing_paper_content": {"title": "Scaling Up Gans For Text-To-Image Synthesis", "abstract": "The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL\u2022E 2, autoregressive and diffusion models became the new standard for largescale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na\u00efvely increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations."}, "cited_paper_content": {"title": "Analyzing And Improving The Image Quality Of Stylegan", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."}, "keywords": ["StyleGAN2"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The baseline StyleGAN generator can be improved by redesigning the generator normalization, revisiting progressive growing, and regularizing the generator to encourage good conditioning in the mapping from latent codes to images, as proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To redesign the generator normalization and improve the conditioning in the mapping from latent codes to images, the authors could refer to the changes proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the baseline StyleGAN generator, the authors of this manuscript propose changes in both model architecture and training methods, including redesigning the generator normalization, revisiting progressive growing, and regularizing the generator to encourage good conditioning in the mapping from latent codes to images (#REFR).", "given_citation_intent": "method", "given_keywords": ["StyleGAN2"]}]}
{"citing_id": "2304.12130v1", "cited_id": "1803.08664", "section_title": "Reconstruction Performance", "citation": "In general, our CNDE-based methods are more stable over a long prediction period and show much better performance than other baselines. #REFR We also observe that the temporal model (e.g.", "text_before_citation": ["According to Table 1 and Table 2 , the CNDE-based methods in general outperform other baselines for velocity components { , , }.", "We also have several observations from these tables: (1) When comparing the CNDE-based methods with SR baselines and DCS/MS model, we observe that these baseline methods cannot recover turbulent flow well and get the worse performance in terms of SSIM and Laplacian difference. Temporal Analysis.", "In the temporal analysis from the FIT dataset, we show the change in performance as we reconstruct DNS data over 20 time steps after the training data.", "The performance change using SSIM and Laplacian difference is shown in Fig. 4 and Fig. 5 , respectively.", "Several observations are highlighted from these results: (1) With larger time intervals between training data and prediction data, the performance becomes worse."], "text_after_citation": ["LSTM) can bring significant improvement in longterm prediction by comparing DCS/MS with CTN.", "(3) By comparing CNDEp-based methods and CNDE-based methods, the CNDE-based methods using refinement outperform the CNDEp-based methods without using this adjustment over time.", "It demonstrates that the refinement method can adjust the state bias in the long-term prediction of flow data.", "#OTHEREFR We also find that CNDEp-based methods achieve better performance after the 5th time steps compared with the temporal baseline CTN model.", "It proves the advantage of the proposed RKTU structure in the long-term prediction."], "citing_paper_content": {"title": "Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics And Test-Time Refinement", "abstract": "Simulating turbulence is critical for many societally important applications in aerospace engineering, environmental science, the energy industry, and biomedicine. Large eddy simulation (LES) has been widely used as an alternative to direct numerical simulation (DNS) for simulating turbulent flows due to its reduced computational cost. However, LES is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the spatio-temporal complexity of turbulent flows. In this work, we propose a new physics-guided neural network for reconstructing the sequential DNS from low-resolution LES data. The proposed method leverages the partial differential equation that underlies the flow dynamics in the design of spatio-temporal model architecture. A degradation-based refinement method is also developed to enforce physical constraints and further reduce the accumulated reconstruction errors over long periods. The results on two different types of turbulent flow data confirm the superiority of the proposed method in reconstructing the high-resolution DNS data and preserving the physical characteristics of flow transport."}, "cited_paper_content": {"title": "Fast, Accurate, And Lightweight Super-Resolution With Cascading Residual Network", "abstract": "In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods."}, "keywords": ["CNDE-based methods"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "When comparing the CNDE-based methods with SR baselines and DCS/MS model, we observe that these baseline methods cannot recover turbulent flow well and get the worse performance in terms of SSIM and Laplacian difference. This highlights the need for more accurate and lightweight deep networks for image super-resolution, as proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of reconstructing DNS from low-resolution LES data, the proposed physics-guided neural network in this work leverages a cascading mechanism upon a residual network, achieving comparable performance to state-of-the-art methods with much fewer parameters and operations (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The CNDE-based methods outperform other baselines for velocity components { , , }, and when comparing with SR baselines and DCS/MS model, these baseline methods cannot recover turbulent flow well and get the worse performance in terms of SSIM and Laplacian difference (#REFR).", "given_citation_intent": "method", "given_keywords": ["CNDE-based methods"]}]}
{"citing_id": "2304.00433v1", "cited_id": "1710.10196", "section_title": "D. Mcmc-Gan Implementation Details", "citation": "More details of the ProGAN architecture used in this study can be found in #REFR .", "text_before_citation": ["The ProGANs were trained on 4 NVIDIA Quadro RTX 8000 GPUs by use of Tensorflow #OTHEREFR .", "A stochastic gradient method that employs the Adam algorithm #OTHEREFR was employed as the optimizer to train the ProGANs.", "A ProGAN architecture with the initial image resolution of 4 \u00d7 4 and a 64dimensional latent space were employed."], "text_after_citation": ["The ProGANs were trained by use of the publicly available ProGAN code (https://github.com/tkarras/ progressive_growing_of_gans).", "After the ProGAN was trained, the MCMC technique with pCN proposal was employed to sample the GAN's latent variable for use in computing the IO test statistic.", "The corresponding proposal density function and the acceptance probability were defined in Eq. 15 and Eq. 16, respectively.", "Each Markov chain was run for 200,000 iterations on a single NVIDIA Quadro RTX 8000 GPU.", "A burn-in period corresponding to the first 10,000 iterations was discarded from each Markov chain."], "citing_paper_content": {"title": "Ideal Observer Computation By Use Of Markov-Chain Monte Carlo With Generative Adversarial Networks", "abstract": "Medical imaging systems are often evaluated and optimized via objective, or task-specific, measures of image quality (IQ) that quantify the performance of an observer on a specific clinically-relevant task. The performance of the Bayesian Ideal Observer (IO) sets an upper limit among all observers, numerical or human, and has been advocated for use as a figure-of-merit (FOM) for evaluating and optimizing medical imaging systems. However, the IO test statistic corresponds to the likelihood ratio that is intractable to compute in the majority of cases. A sampling-based method that employs Markov-Chain Monte Carlo (MCMC) techniques was previously proposed to estimate the IO performance. However, current applications of MCMC methods for IO approximation have been limited to a small number of situations where the considered distribution of to-be-imaged objects can be described by a relatively simple stochastic object model (SOM). As such, there remains an important need to extend the domain of applicability of MCMC methods to address a large variety of scenarios where IO-based assessments are needed but the associated SOMs have not been available. In this study, a novel MCMC method that employs a generative adversarial network (GAN)-based SOM, referred to as MCMC-GAN, is described and evaluated. The MCMC-GAN method was quantitatively validated by use of test-cases for which reference solutions were available. The results demonstrate that the MCMC-GAN method can extend the domain of applicability of MCMC methods for conducting IO analyses of medical imaging systems."}, "cited_paper_content": {"title": "Progressive Growing Of Gans For Improved Quality, Stability, And Variation", "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset."}, "keywords": ["ProGAN architecture"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The ProGANs were trained on 4 NVIDIA Quadro RTX 8000 GPUs by use of Tensorflow #REFR. A stochastic gradient method that employs the Adam algorithm #REFR was employed as the optimizer to train the ProGANs. A ProGAN architecture with the initial image resolution of 4 \u00d7 4 and a 64dimensional latent space were employed. The key idea of growing both the generator and discriminator progressively was proposed in the reference paper, which greatly stabilizes the training and allows for the production of images of unprecedented quality.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train the ProGANs, a stochastic gradient method that employs the Adam algorithm was employed as the optimizer, and a ProGAN architecture with the initial image resolution of 4 \u00d7 4 and a 64-dimensional latent space were employed #REFR. Additionally, a novel MCMC method that employs a generative adversarial network (GAN)-based SOM, referred to as MCMC-GAN, is described and evaluated in this study.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train the ProGANs, a stochastic gradient method that employs the Adam algorithm was employed, and a ProGAN architecture with the initial image resolution of 4 \u00d7 4 and a 64-dimensional latent space were used #REFR. Additionally, the authors of the manuscript could benefit from the new training methodology for generative adversarial networks proposed in the reference paper, which grows both the generator and discriminator progressively, allowing for unprecedented image quality and stability, and could be applied to the ProGAN architecture.", "given_citation_intent": "method", "given_keywords": ["ProGAN architecture"]}]}
{"citing_id": "2303.17595v1", "cited_id": "1902.10811", "section_title": "Replicating Imagenet Annotations", "citation": "The candidate images for the original dataset are crawled from Google, MSN, Yahoo, and Flickr #REFR .", "text_before_citation": ["The original annotation procedure consists of the following four stages [77, #OTHEREFR .", "We replicate only the crowdsourcing stages (2) and (3) that are directly related to the generation of annotation byproducts.", "Our replication is based on the description in the original ImageNet [77] and ImageNetV2 #OTHEREFR papers.", "For stage (1), we use the 1,000-class subset of the original 21,841 WordNet concepts #OTHEREFR , corresponding to the ILSVRC2012 subset, also known as the ImageNet1K [77] .", "Preparing candidate images I cand c for each class c \u2208 C."], "text_after_citation": ["The search keywords are formulated by combining the class names and their \"synsets\" in WordNet #OTHEREFR Figure 3 : ImageNet annotation interface. We replicate the interface in #OTHEREFR .", "Annotators read the category description in the instruction panel, select all the images corresponding to \"French bulldog\", and click on the submit button.", "the front-end and back-end to record and save the annotation byproducts in the database.", "The annotation interface and crowdsourcing details are explained in Appendix C.1. Number of annotators per image.", "The original ImageNet annotation procedure presents each image to 10 annotators for more precise annotations. This would require 240k USD for the annotation."], "citing_paper_content": {"title": "Neglected Free Lunch -Learning Image Classifiers Using Annotation Byproducts", "abstract": "Supervised learning of image classifiers distills human knowledge into a parametric model f \u03b8 through pairs of images and corresponding labels {(X i , Y i)} N i=1. We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves the generalisability and robustness of the learned models. Compared to the original supervised learning, LUAB does not require extra annotation costs. ImageNet-AB and COCO-AB are at github.com/naverai/NeglectedFreeLunch."}, "cited_paper_content": {"title": "Do Imagenet Classifiers Generalize To Imagenet?", "abstract": "We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly \"harder\" images than those found in the original test sets."}, "keywords": ["candidate images", "original dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves the generalisability and robustness of the learned models (He et al., 2019 #REFR). Compared to the original supervised learning, LUAB does not require extra annotation costs. ImageNet-AB and COCO-AB are at github.com/naverai/NeglectedFreeLunch.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the generalizability of the classification models, the authors built new test sets for CIFAR-10 and ImageNet datasets, as described in #REFR, and found that the models' inability to generalize to slightly \"harder\" images caused accuracy drops of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prepare candidate images for each class, the authors followed the original dataset creation processes [77#REFR], which involved crowdsourcing stages related to the generation of annotation byproducts.", "given_citation_intent": "method", "given_keywords": ["candidate images", "original dataset"]}]}
{"citing_id": "2303.13322v1", "cited_id": "1910.09034", "section_title": "I. Introduction", "citation": "The drawback of #REFR is the ignorance of correlated patterns in historical samples of the load and renewable generation.", "text_before_citation": ["The iterative methods in #OTHEREFR - #OTHEREFR focus on solving a particular instance of the UC problem.", "Therefore, they can be practical in daily electricity market operations, but they encounter significant performance challenges in the context of short-term operations.", "Alternatively, the notion of umbrella constraint was introduced when identifying redundant constraints which do not alter the feasibility region of the original UC problem when they are removed from the original problem #OTHEREFR .", "Similarly, references #OTHEREFR - #OTHEREFR use a bound tightening technique #OTHEREFR and solve two optimization problems for each transmission line in the power system over linear programming (LP) relaxations of the feasible region to remove as many redundant constraints as possible from the full UC formulation.", "Also, the authors of #OTHEREFR used Clarkson's redundancy removal algorithm that is based on LP and requires solving multiple maximization problems #OTHEREFR for each possible line flow contingency constraint in a SCOPF."], "text_after_citation": ["Recently, an analytical approach was proposed by #OTHEREFR based on a constraint generation algorithm and heuristics to eliminate nonbinding constraints from UC problems on top of redundant constraints.", "In addition, the variability and uncertainty of renewable power generation have introduced new challenges to the UC problem #OTHEREFR .", "The representation of uncertainty can take the form of scenarios or uncertainty sets for stochastic and robust optimization approaches, respectively.", "The computational burden of stochastic programming prompts to migrate towards tractable approaches for optimization under uncertainty, namely robust and chance-constrained optimization #OTHEREFR , #OTHEREFR .", "Moreover, renewable energy sources (RES) manifest crosscorrelation over space and time."], "citing_paper_content": {"title": "Tight Data-Driven Linear Relaxations For Constraint Screening In Robust Unit Commitment", "abstract": "and with the Groupe d'\u00e9tudes et de recherche en analyse des d\u00e9cisions (GERAD),"}, "cited_paper_content": {"title": "Fast Security-Constrained Optimal Power Flow Through Low-Impact And Redundancy Screening", "abstract": "Network-constrained dispatch decisions are not only restricted by the thermal limits of the power lines, but also by complex security requirements related to contingency scenarios, e.g. the N-1-criterion. For real-world networks, the security-constrained optimal power flow (SCOPF) leads to a prohibitive increase in complexity even for linearized (DC) power flow equations. This paper presents a set of methods for fast, yet exact, SCOPF computation by reducing the power flow and contingency constraints to a minimal subset. The constraint reduction is performed by an iterative algorithm with performance guarantees based on both general algebraic theory and exploiting peculiarities of the optimal dispatch problem. The method is showcased on the classic IEEE 118-bus test system and on a full-scale real-world system. The numerical experiments rely on open-source software and data and our implementation is published supplementary to the paper."}, "keywords": ["renewable generation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Similarly, the authors of #REFR used a constraint reduction method to solve the security-constrained optimal power flow problem, which involves reducing the power flow and contingency constraints to a minimal subset, resulting in fast yet exact computation.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Similarly, the authors of #REFR used a constraint reduction method to solve the security-constrained optimal power flow problem, which reduces the power flow and contingency constraints to a minimal subset, resulting in fast yet exact computation.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently handle the complex security requirements related to contingency scenarios in power systems, the authors of #REFR proposed a set of methods for fast and exact computation of security-constrained optimal power flow by reducing the power flow and contingency constraints to a minimal subset, which can be useful for renewable generation.", "given_citation_intent": "background", "given_keywords": ["renewable generation"]}]}
{"citing_id": "2303.05777v1", "cited_id": "1604.07379", "section_title": "Introduction", "citation": "Our model can handle multi-label inpainting: we extend a previous method #REFR to include tissue labels within dropout regions rather than a binary mask.", "text_before_citation": ["Additionally, since the dropout masks contain not just CSF but all tissue types, the model learns to synthesize tissue boundaries realistically.", "Since our downstream goal is cortical surface reconstruction, a plausible GM/CSF boundary is critical.", "Once trained, we apply our model to synthetically atrophied images obtained using a registration-based method. Our main contributions are:", "1.", "The use of CSF inpainting for synthetic cortical atrophy induction is the first of its kind, and provides an interesting new challenge for the MIDL community. 2."], "text_after_citation": ["3.", "We combine several existing 2D methods and extend them to 3D, including a selfsupervised dropout scheme #OTHEREFR , edge priors #OTHEREFR , and sinusoidal positional encoding (SPE) #OTHEREFR . 4.", "Our model synthesizes better images than 2D methods and can adapt to unseen data.", "Figure 2: Self-supervised training pipeline for a T1w.", "We pre-processed (green) T1w and FLAIR images (gray) to obtain 5 input channels."], "citing_paper_content": {"title": "Self-Supervised Csf Inpainting With Synthetic Atrophy For Improved Accuracy Validation Of Cortical Surface Analyses", "abstract": "Accuracy validation of cortical thickness measurement is a difficult problem due to the lack of ground truth data. To address this need, many methods have been developed to synthetically induce gray matter (GM) atrophy in an MRI via deformable registration, creating a set of images with known changes in cortical thickness. However, these methods often cause blurring in atrophied regions, and cannot simulate realistic atrophy within deep sulci where cerebrospinal fluid (CSF) is obscured or absent. In this paper, we present a solution using a self-supervised inpainting model to generate CSF in these regions and create images with more plausible GM/CSF boundaries. Specifically, we introduce a novel, 3D GAN model that incorporates patch-based dropout training, edge map priors, and sinusoidal positional encoding, all of which are established methods previously limited to 2D domains. We show that our framework significantly improves the quality of the resulting synthetic images and is adaptable to unseen data with fine-tuning. We also demonstrate that our resulting dataset can be employed for accuracy validation of cortical segmentation and thickness measurement."}, "cited_paper_content": {"title": "Context Encoders: Feature Learning By Inpainting", "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods."}, "keywords": ["dropout regions", "binary mask"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Additionally, since the dropout masks contain not just CSF but all tissue types, the model learns to synthesize tissue boundaries realistically. Since our downstream goal is cortical surface reconstruction, a plausible GM/CSF boundary is critical. Once trained, we apply our model to synthetically atrophied images obtained using a registration-based method. Our main contributions are: 1. The use of CSF inpainting for synthetic cortical atrophy induction is the first of its kind, and provides an interesting new challenge for the MIDL community. 2. We have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss, and found that the latter produces much sharper results because it can better handle multiple modes in the output #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To synthesize realistic cortical atrophy within deep sulci where CSF is obscured or absent, the authors propose a self-supervised inpainting model that generates CSF in these regions and creates images with more plausible GM/CSF boundaries, using a novel 3D GAN model that incorporates patch-based dropout training, edge map priors, and sinusoidal positional encoding, all of which are established methods previously limited to 2D domains (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To synthesize tissue boundaries realistically, the authors used a novel 3D GAN model that incorporates patch-based dropout training and edge map priors, as well as sinusoidal positional encoding, which were previously limited to 2D domains (#REFR).", "given_citation_intent": "method", "given_keywords": ["dropout regions", "binary mask"]}]}
{"citing_id": "2303.08509v1", "cited_id": "1808.04218", "section_title": "Rq3: Efficiency", "citation": "However, after the substi-tute model is well trained with sufficient data #REFR , BagAmmo performs more efficiently and exhibits its advantage. . We can draw two conclusions from this figure. First, the package-level classifier is more difficult to attack.", "text_before_citation": ["The vertical axis reflects perturbation ratio, and the horizontal axis indicates the number of queries.", "It can be seen that in all cases, BagAmmo always has a higher convergence speed.", "Moreover, BagAmmo always requires fewer queries before the perturbation ratio is kept below a certain threshold (e.g., 0.1).", "Note that the difference between two methods in the initial phase is relatively small.", "It is because the substitute model has not been well trained in this phase."], "text_after_citation": ["This is because package-level FCGs contain much more nodes than family-level FCGs, resulting in a larger search space for BagAmmo.", "Second, using the substitute model reduces the number of queries in almost all cases and helps enhance the attack efficiency."], "citing_paper_content": {"title": "Black-Box Adversarial Example Attack Towards Fcg Based Android Malware Detection Under Incomplete Feature Information", "abstract": "The function call graph (FCG) based Android malware detection methods have recently attracted increasing attention due to their promising performance. However, these methods are susceptible to adversarial examples (AEs). In this paper, we design a novel black-box AE attack towards the FCG based malware detection system, called BagAmmo. To mislead its target system, BagAmmo purposefully perturbs the FCG feature of malware through inserting \"never-executed\" function calls into malware code. The main challenges are twofold. First, the malware functionality should not be changed by adversarial perturbation. Second, the information of the target system (e.g., the graph feature granularity and the output probabilities) is absent. To preserve malware functionality, BagAmmo employs the try-catch trap to insert function calls to perturb the FCG of malware. Without the knowledge about feature granularity and output probabilities, BagAmmo adopts the architecture of generative adversarial network (GAN), and leverages a multi-population co-evolution algorithm (i.e., Apoem) to generate the desired perturbation. Every population in Apoem represents a possible feature granularity, and the real feature granularity can be achieved when Apoem converges. Through extensive experiments on over 44k Android apps and 32 target models, we evaluate the effectiveness, efficiency and resilience of BagAmmo. BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance. Moreover, BagAmmo outperforms the state-of-the-art attack SRL in attack success rate."}, "cited_paper_content": {"title": "Android Hiv: A Study Of Repackaging Malware For Evading Machine-Learning Detection", "abstract": "Machine learning-based solutions have been successfully employed for the automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features ( e.g. , requested permissions, API calls, etc. ), and the perturbations can only be implemented by simply modifying application\u2019s manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning-based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK\u2019s Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK."}, "keywords": ["package-level classifier"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The study presented in #REFR proposes a new attacking method that generates adversarial examples of Android malware and successfully evades detection by current machine learning models, even those that rely on semantic features such as control-flow-graph.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, outperforming the state-of-the-art attack SRL in attack success rate, as demonstrated in the experiments on over 44k Android apps and 32 target models, as reported in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance, outperforming the state-of-the-art attack SRL in attack success rate (#REFR), which demonstrates the effectiveness and resilience of the proposed method against package-level classifiers.", "given_citation_intent": "result", "given_keywords": ["package-level classifier"]}]}
{"citing_id": "2304.05215v1", "cited_id": "1406.2661", "section_title": "B. Self-Supervised Learning In Remote Sensing", "citation": "Generative adversarial networks (GANs) #REFR and reconstruction with auto-encoding are primarily used for representation learning.", "text_before_citation": ["However, further attempts are being made to learn multi-sensor and temporal information #OTHEREFR , #OTHEREFR , #OTHEREFR .", "4) Generative learning: The primary goal of generative learning is to train a model to reconstruct an original input even when it is partially corrupted.", "In general computer vision, removing spatial and temporal information from the original input is employed.", "Similarly, a comparable method is used in the field of remote sensing.", "Generation from missing spatial information involves learning features by reconstructing the corrupted area of the original image using the characteristics of the surrounding space."], "text_after_citation": ["In MARTA GANs #OTHEREFR , which is related to generative adversarial networks, the discriminator serves as the encoder for the scene classification model.", "On the other hand, numerous studies focus on reconstruction with auto-encoding #OTHEREFR , #OTHEREFR , #OTHEREFR .", "The auto-encoding method primarily employs a corrupted image as input with an encoder-decoder structure model using L1 or L2 distance metrics.", "Additionally, a study on reconstruction by reflecting the characteristics of ground sample distance (GSD), an essential feature in remote sensing images, has been conducted #OTHEREFR .", "Generation from missing temporal information is similar to video frame prediction in the computer vision field."], "citing_paper_content": {"title": "A Billion-Scale Foundation Model For Remote Sensing Images", "abstract": "As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billionscale foundation model in the remote sensing field. Furthermore, we propose an effective method for scaling up and fine-tuning a vision transformer in the remote sensing field. To evaluate general performance in downstream tasks, we employed the DOTA v2.0 and DIOR-R benchmark datasets for rotated object detection, and the Potsdam and LoveDA datasets for semantic segmentation. Experimental results demonstrated that, across all benchmark datasets and downstream tasks, the performance of the foundation models and data efficiency improved as the number of parameters increased. Moreover, our models achieve the state-of-the-art performance on several datasets including DIOR-R, Postdam, and LoveDA."}, "cited_paper_content": {"title": "Generative Adversarial Networks", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."}, "keywords": ["representation learning", "Generative adversarial networks"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Generation from missing spatial information involves learning features by reconstructing the corrupted area of the original image using the characteristics of the surrounding space. This approach has been successfully employed in the field of remote sensing using generative models such as Generative Adversarial Networks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Generative learning in remote sensing involves learning features by reconstructing the corrupted area of the original image using the characteristics of the surrounding space, and recent research has proposed a new framework for estimating generative models via an adversarial process, which can be trained with backpropagation and does not require any Markov chains or unrolled approximate inference networks during either training or generation of samples #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Generative adversarial networks have been widely used in representation learning #REFR, which involves training a generative model to capture the data distribution and a discriminative model to estimate the probability that a sample came from the training data rather than the generative model.", "given_citation_intent": "method", "given_keywords": ["representation learning", "Generative adversarial networks"]}]}
{"citing_id": "2304.03208v1", "cited_id": "1811.03600", "section_title": "Simple Batch Size + Learning Rate Scaling With \u039cp", "citation": "Further, when training models with a batch size smaller than the critical batch size, learning rate should be reduced linearly proportional to the reduction in batch size-consistent with the findings in #REFR .", "text_before_citation": ["More precisely, we find that \u00b5P learning rate transfers as long as each model size is trained with a batch size roughly consistent with or larger than the critical batch size.", "The closer the batch size is to the critical batch size for a given model, the better the loss will be when using the \u00b5Transferred learning rate."], "text_after_citation": ["We believe the LLaMa 13B model is better than Cerebras-GPT on downstream tasks because it was trained for 4\u00d7 more tokens, but were unable to get access to test the model ourselves.\u00a92023 Cerebras Systems Inc. All Rights Reserved.", "All Cerebras-GPT development and hyperparameter tuning was evaluated using the Pile validation set.3 Pile test loss is crossentropy in nats/token.", "We correct all crossentropy results for different vocabularies to be comparable to the GPT-2 vocabulary.\u00a92023 Cerebras Systems Inc. All Rights Reserved.", "Here, we report accuracy result from each model predictions using token-level probability, consistent with reported results in the GPT-NeoX paper.", "We report additional accuracy measures in Appendix C.2.\u00a92023 Cerebras Systems Inc. All Rights Reserved."], "citing_paper_content": {"title": "Cerebras-Gpt: Open Compute-Optimal Language Models Trained On The Cerebras Wafer-Scale Cluster", "abstract": "We study recent research advances that improve large language models through efficient pretraining and scaling, and open datasets and tools. We combine these advances to introduce Cerebras-GPT, a family of open compute-optimal language models scaled from 111M to 13B parameters. We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget). We characterize the predictable power-law scaling and compare Cerebras-GPT with other publicly-available models to show all Cerebras-GPT models have state-ofthe-art training efficiency on both pre-training and downstream objectives. We describe our learnings including how Maximal Update Parameterization (\u00b5P) can further improve large model scaling, improving accuracy and hyperparameter predictability at scale. We release our pre-trained models and code, making this paper the first open and reproducible work comparing compute-optimal model scaling to models trained on fixed dataset sizes. Cerebras-GPT models are available on HuggingFace: https://huggingface.co/cerebras. Recent research in large language models (LLMs) shows important advances that can improve LLM quality and efficiency. Scaling law studies show predictable and significant improvements in model performance by increasing model and dataset size (Hestness et al., 2017; Kaplan et al., 2020). Language models can also be improved just by training on more data (Hoffmann et al., 2022; Touvron et al., 2023). Recent works, such as Maximal Update Parameterization (\u00b5P), also show techniques to improve training stability and performance as models scale up (e.g., Bachlechner et al. (2020); Yang et al. (2021)). Concurrently with these advances, the research community has trained and released many open-source models. Models like GPT-J, GPT-NeoX, OPT, and Pythia have each held state-of-the-art accuracy for open source models for their size, and these models can be tested and used simply by downloading the pretrained weights ("}, "cited_paper_content": {"title": "Measuring The Effects Of Data Parallelism On Neural Network Training", "abstract": "Recent hardware developments have made unprecedented amounts of data parallelism available for accelerating neural network training. Among the simplest ways to harness next-generation accelerators is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured in the number of steps necessary to reach a goal out-of-sample error. Eventually, increasing the batch size will no longer reduce the number of training steps required, but the exact relationship between the batch size and how many training steps are necessary is of critical importance to practitioners, researchers, and hardware designers alike. We study how this relationship varies with the training algorithm, model, and data set and find extremely large variation between workloads. Along the way, we reconcile disagreements in the literature on whether batch size affects model quality. Finally, we discuss the implications of our results for efforts to train neural networks much faster in the future."}, "keywords": ["training models", "learning rate"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "More precisely, we find that \u00b5P learning rate transfers as long as each model size is trained with a batch size roughly consistent with or larger than the critical batch size. The closer the batch size is to the critical batch size for a given model, the better the loss will be when using the \u00b5Transferred learning rate (Measuring The Effects Of Data Parallelism On Neural Network Training). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We find extremely large variation between workloads in the relationship between batch size and how many training steps are necessary, as shown in the study by #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We find that increasing the batch size during training models will eventually no longer reduce the number of training steps required, but the exact relationship between the batch size and how many training steps are necessary is of critical importance to practitioners, researchers, and hardware designers alike, as shown in #REFR. Moreover, recent works, such as Maximal Update Parameterization (\u00b5P), also show techniques to improve training stability and performance as models scale up, including learning rate transfers, which can be used as long as each model size is trained with a batch size roughly consistent with or larger than the critical batch size.", "given_citation_intent": "result", "given_keywords": ["training models", "learning rate"]}]}
{"citing_id": "2303.11055v1", "cited_id": "1807.06514", "section_title": "C. Attention Mechanisms", "citation": "Bottleneck Attention module (BAM) #REFR also uses double attention modules to add their attention matrices to get the final attention map.", "text_before_citation": ["Attention mechanisms have attracted great attention since it was successfully applied in natural language processing tasks #OTHEREFR , which is mainly used to help networks concentrating on partial information of data flow and generating the corresponding dimensional weight maps.", "When it comes to the image processing task, there are mainly three types of attention mechanisms: channel attention (CA) #OTHEREFR , spatial attention (SA) #OTHEREFR and pixel attention (PA) #OTHEREFR , depending on their generated weight maps' dimension level.", "Self-Attention #OTHEREFR is also very popular apart from the three.", "Among the above mechanisms, channel attention is the first to propose in SE-Net #OTHEREFR and then polished in SK-Net #OTHEREFR .", "After that, attempts to incorporate both channel attention and spatial attention for stronger attention capability in Convolutional Block Attention Module (CBAM) #OTHEREFR are tried several times."], "text_after_citation": ["SA-Net #OTHEREFR splits the channel feature maps separately into the channel attention module and spatial attention module.", "Apart from these, there also exists the temporal attention, i.e. the attention in time level #OTHEREFR ."], "citing_paper_content": {"title": "Parameter-Free Channel Attention For Image Classification And Super-Resolution", "abstract": "The channel attention mechanism is a useful technique widely employed in deep convolutional neural networks to boost the performance for image processing tasks, e.g., image classification and image super-resolution. It is usually designed as a parameterized sub-network and embedded into the convolutional layers of the network to learn more powerful feature representations. However, current channel attention induces more parameters and therefore leads to higher computational costs. To deal with this issue, in this work, we propose a Parameter-Free Channel Attention (PFCA) module to boost the performance of popular image classification and image super-resolution networks, but completely sweep out the parameter growth of channel attention. Experiments on CIFAR-100, ImageNet, and DIV2K validate that our PFCA module improves the performance of ResNet on image classification and improves the performance of MSRResNet on image super-resolution tasks, respectively, while bringing little growth of parameters and FLOPs."}, "cited_paper_content": {"title": "Bam: Bottleneck Attention Module", "abstract": "Recent advances in deep neural networks have been developed via architecture search for stronger representational power. In this work, we focus on the effect of attention in general deep neural networks. We propose a simple and effective attention module, named Bottleneck Attention Module (BAM), that can be integrated with any feed-forward convolutional neural networks. Our module infers an attention map along two separate pathways, channel and spatial. We place our module at each bottleneck of models where the downsampling of feature maps occurs. Our module constructs a hierarchical attention at bottlenecks with a number of parameters and it is trainable in an end-to-end manner jointly with any feed-forward models. We validate our BAM through extensive experiments on CIFAR-100, ImageNet-1K, VOC 2007 and MS COCO benchmarks. Our experiments show consistent improvement in classification and detection performances with various models, demonstrating the wide applicability of BAM. The code and models will be publicly available."}, "keywords": ["double attention modules", "Bottleneck Attention module"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "When it comes to the image processing task, there are mainly three types of attention mechanisms: channel attention (CA) #OTHEREFR, spatial attention (SA) #OTHEREFR and pixel attention (PA) #OTHEREFR, depending on their generated weight maps' dimension level. Among the above mechanisms, channel attention is the first to propose in SE-Net and then polished in SK-Net. After that, attempts to incorporate both channel attention and spatial attention for stronger attention capability in Convolutional Block Attention Module (CBAM) #REFR are tried several times.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the attention capability of convolutional neural networks, the authors propose a Bottleneck Attention Module (BAM) that infers an attention map along two separate pathways, channel and spatial, and constructs a hierarchical attention at bottlenecks with a number of parameters, which is trainable in an end-to-end manner jointly with any feed-forward models #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To further enhance the attention capability of the convolutional block, recent studies have proposed double attention modules, such as Bottleneck Attention Module (BAM) #REFR, which infers an attention map along two separate pathways, channel and spatial, and constructs a hierarchical attention at bottlenecks with a number of parameters.", "given_citation_intent": "method", "given_keywords": ["double attention modules", "Bottleneck Attention module"]}]}
{"citing_id": "2303.08028v1", "cited_id": "1511.03575", "section_title": "Conclusion", "citation": "There is a lot of community interest in distributed and federated problems in machine learning #REFR .", "text_before_citation": [], "text_after_citation": ["We believe that the inference setting is under-studied despite a clear need in a number of different domains.", "This paper focuses its discussion on the execution layer for decentralized prediction systems.", "The paper contributes the communication primitives needed to make such systems work in practice.", "We find that decentralized prediction can give system designers new knobs to enable real-time predictions.", "Our results show: (1) a clear value for decentralization, (2) that EdgeServe is an effective architecture for supporting decentralization, and (3) core techniques in EdgeServe, such as lazy data routing, are broadly applicable to modern stream processing."], "citing_paper_content": {"title": "Edgeserve: An Execution Layer For Decentralized Prediction", "abstract": "The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can trade-off computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition."}, "cited_paper_content": {"title": "Federated Optimization:Distributed Optimization Beyond The Datacenter", "abstract": "We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed (unevenly) over an extremely large number of \\nodes, but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of utmost importance. ::: A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead, the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network, each of which has only a tiny fraction of data available totally; in particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, we assume that no device has a representative sample of the overall distribution. ::: We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization."}, "keywords": ["machine learning", "federated problems"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose EdgeServe, a machine learning system for decentralized prediction that relies on a low-latency message broker to route data through a network to nodes that can serve predictions, and on a series of novel optimizations that can trade-off computation, communication, and accuracy. To address the challenge of communication efficiency in this setting, the authors could consider the approach of Federated Optimization proposed by McMahan et al. in their paper on distributed optimization beyond the datacenter #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The problem of decentralized prediction, which involves aggregating relevant features from data sources collected on different nodes in a network, creates several systems challenges, including managing data routing, placing computation, and time-synchronization. As noted in the reference paper #REFR, Federated Optimization is a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed unevenly over an extremely large number of nodes, making communication efficiency of utmost importance.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Federated optimization is a relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed over an extremely large number of nodes, and communication efficiency is of utmost importance, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["machine learning", "federated problems"]}]}
{"citing_id": "2305.01486v1", "cited_id": "1608.01041", "section_title": "Comparison With State-Of-The-Art Methods", "citation": "Finally, on the FER+ #REFR dataset, ARBEx acquires an accuracy score of 93.09%, which significantly outperforms every other model tested.", "text_before_citation": ["Using five separate datasets -AffWild2, RAF-DB, JAFFE, FERG-DB, and FER+ (explained in 4.1), the table shows the comparison of the accuracy of multiple State-of-the-Art facial expression learning methods.", "In this study, the models SCN #OTHEREFR , RAN #OTHEREFR , RUL #OTHEREFR , EfficientFace #OTHEREFR , POSTER #OTHEREFR , POSTER++ #OTHEREFR , and ARBEx are compared.", "Upon investigation of the results, it is apparent that AR-BEx outperforms all other models across all datasets, attaining the highest accuracy scores for each dataset.", "Specifically, ARBEx earns an accuracy score of 72.48% on the AffWild2 #OTHEREFR dataset, which is significantly higher than POSTER++, which has an accuracy score of 69.18%.", "ARBEx outperforms every other model in the study, with accuracy scores on the RAF-DB #OTHEREFR , #OTHEREFR , FERG-DB #OTHEREFR and JAFFE #OTHEREFR , #OTHEREFR datasets of 92.47%, 98.18% and 96.67%, respectively."], "text_after_citation": ["Our novel reliability balancing section reduces all kinds of biases, resulting in exceptional performance in all circumstances."], "citing_paper_content": {"title": "Arbex: Attentive Feature Extraction With Reliability Balancing For Robust Facial Expression Learning", "abstract": "In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the model's discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an integral role in identifying accurate labels. This approach provides critical elements for improving the reliability of predictions and has a substantial positive effect on final prediction capabilities. Our adaptive model can be integrated with any deep neural network to forestall challenges in various recognition tasks. Our strategy outperforms current state-of-the-art methodologies, according to extensive experiments conducted in a variety of contexts."}, "cited_paper_content": {"title": "Training Deep Networks For Facial Expression Recognition With Crowd-Sourced Label Distribution", "abstract": "Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community."}, "keywords": ["FER+ dataset"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Using five separate datasets -AffWild2, RAF-DB, JAFFE, FERG-DB, and FER+ (explained in 4.1), the table shows the comparison of the accuracy of multiple State-of-the-Art facial expression learning methods. In this study, the models SCN, RAN, RUL, EfficientFace, POSTER, POSTER++, and ARBEx are compared. Upon investigation of the results, it is apparent that AR-BEx outperforms all other models across all datasets, attaining the highest accuracy scores for each dataset (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed ARBEx framework achieves the highest accuracy scores across all datasets compared to other State-of-the-Art facial expression learning methods, with an accuracy score of 72.48% on the AffWild2 dataset, which is significantly higher than POSTER++ (69.18%) #REFR. This result is consistent with the findings of the reference paper, which also demonstrated that the traditional majority voting scheme does not perform as well as the probabilistic label drawing and cross-entropy loss approaches that fully leverage the label distribution.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The enhanced FER+ dataset with multiple labels for each face image, shared by the authors of the reference paper #REFR, was used in the comparison of accuracy scores for multiple facial expression learning models, where ARBEx outperformed all other models across all datasets.", "given_citation_intent": "result", "given_keywords": ["FER+ dataset"]}]}
{"citing_id": "2303.03808v1", "cited_id": "2003.08934", "section_title": "Rendering Equation Encoding", "citation": "Second, a diverse of ASG functions can be produced to achieve much richer encoding compared with a few fixed basis encoding functions in existing methods #REFR .", "text_before_citation": ["Finally, we apply the sigmoid activation function to the combined color to obtain the viewdependent color:", "EQUATION", "We can also interpret the proposed rendering equation encoding as a more advanced view direction encoding method.", "Our rendering equation encoding has two-fold benefits compared with popular frequency encoding #OTHEREFR and sphere harmonics #OTHEREFR .", "First, every point now has its own independent encoding functions controlled by the predicted bandwidths in the ASGs, while the encoding functions are fixed for all points in existing works."], "text_after_citation": [], "citing_paper_content": {"title": "Multiscale Tensor Decomposition And Rendering Equation Encoding For View Synthesis", "abstract": "Rendering novel views from captured multi-view images has made considerable progress since the emergence of the neural radiance field. This paper aims to further advance the quality of view rendering by proposing a novel approach dubbed the neural radiance feature field (NRFF) which represents scenes in the feature space. We first propose a multiscale tensor decomposition scheme to organize learnable features so as to represent scenes from coarse to fine scales. We demonstrate many benefits of the proposed multiscale representation, including more accurate scene shape and appearance reconstruction, and faster convergence compared with the single-scale representation. Instead of encoding view directions to model view-dependent effects, we further propose to encode the rendering equation in the feature space by employing the anisotropic spherical Gaussian mixture predicted from the proposed multiscale representation. The proposed NRFF improves state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and NSVF synthetic datasets. A significant improvement has also been observed on the real-world Tanks & Temples dataset."}, "cited_paper_content": {"title": "Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis", "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."}, "keywords": ["much richer encoding"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We further propose to encode the rendering equation in the feature space by employing the anisotropic spherical Gaussian mixture predicted from the proposed multiscale representation, which improves state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and NSVF synthetic datasets #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To represent scenes in the feature space, the proposed neural radiance feature field (NRFF) method in this paper employs an anisotropic spherical Gaussian mixture predicted from the multiscale tensor decomposition scheme, which improves the rendering results by over 1 dB in PSNR on both the NeRF and NSVF synthetic datasets compared to the state-of-the-art methods #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed neural radiance feature field (NRFF) method represents scenes in the feature space, providing a much richer encoding than existing works, and achieves state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and NSVF synthetic datasets, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["much richer encoding"]}]}
{"citing_id": "2304.10263v1", "cited_id": "1703.06868", "section_title": "Evaluation", "citation": "We use an off-the-shelf multi-label classifier based on ResNet50 #REFR to obtain predicted logits.", "text_before_citation": ["w/o L multiID denotes the model without multi-view ID loss.", "ated by the average pose distance(APD), which is root mean squared error between the pose encodings estimated by the pretrained 3D face detector #OTHEREFR .", "Time metric indicates the average inference time (encoding time and generation time) for one image computed on one Tesla V100 GPU.", "Our method outperforms baselines on ID and APD metrics and is significantly faster than IDE-3D and 3D-Inv when inference.", "We show the comparison of face attribute editing against the baselines in Table 3 ."], "text_after_citation": ["Attribute altering (AA) measures the change of the desired attribute, which is the attribute logit change \u2206l t #OTHEREFR when detecting attribute t by the classifier (pretrained on CelebA #OTHEREFR ).", "\u2206l t is normalized by \u03c3(l) #OTHEREFR , which is the standard deviation calculated from the logits of CelebA-HQ dataset.", "We evaluate the precision of attribute editing with attribute dependency (AD) #OTHEREFR , which measures the degree of change on other attributes when modifying along a certain attribute editing direction, as measured by classifiers. Our method performs better than the previous method. More attribute editing results are provided in Appendix. Qualitative Evaluation.", "We present examples of the inversion and editing results in Figure 4 .", "We sample 9 images for each source image with yaw = [\u221230"], "citing_paper_content": {"title": "Preim3D: 3D Consistent Precise Image Attribute Editing From A Single Image", "abstract": "'an Satellite Control Center 3 RealAI black hair inversion smile age input eyeglasses shape Figure 1. 3D consistent precise inversion and editing. Our method enables reconstructing texture and geometry from a single real image and allows one to perform a list of attributes editing sequentially. The yaw angles of the second to sixth columns are [\u221230 \u2022 , \u221220 \u2022 , 0 \u2022 , 20 \u2022 , 30 \u2022 ]. The last column is the shape of the sixth column."}, "cited_paper_content": {"title": "Arbitrary Style Transfer In Real-Time With Adaptive Instance Normalization", "abstract": "Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network."}, "keywords": ["ResNet50"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve arbitrary style transfer in real-time, the authors of Preim3D cite the novel adaptive instance normalization (AdaIN) layer proposed in the reference paper, which aligns the mean and variance of the content features with those of the style features, enabling fast and flexible user controls with a single feed-forward neural network #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve arbitrary style transfer in real-time, the authors of Preim3D utilized a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features, as proposed in the reference paper \"#REFR\".", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve arbitrary style transfer in real-time, the authors of Preim3D utilized a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features, as proposed in the reference paper \"Arbitrary Style Transfer In Real-Time With Adaptive Instance Normalization\" (#REFR).", "given_citation_intent": "method", "given_keywords": ["ResNet50"]}]}
{"citing_id": "2304.10095v1", "cited_id": "1907.03133", "section_title": "D. Algorithm Summary And Analysis", "citation": "It is found that the transmit power is non-increasing after each iteration, which can be proved in a similar way as that in #REFR .", "text_before_citation": ["The proposed BCD-based algorithm for solving P1 is summarized in Algorithm 1."], "text_after_citation": ["Thus, the value of the objective function in P1 will converge to a stationary point. Then, we analyze the computational complexity of Algorithm 1.", "According to #OTHEREFR , the complexities of solving P2.1 and P3.4 are", "O( \u221a K + 2Q + N log( 1 \u01eb ) \u2022 (n 1 (K + 2Q + N 3 ) + n 2 1 (K + 2Q + N 2 ) + n 3 1 )) and O( \u221a 2K + 4Q + 2M log( 1 \u01eb ) \u2022 (n 2 (2K + 4Q) + n 2 2 (2K + 4Q) + n 2 (4M) + n 3", "2 )), respectively, where n 1 = N 2 and n 2 = 2M + K + 2Q.", "Therefore, the overall computational complexity of Algorithm 1 is O("], "citing_paper_content": {"title": "Transmit Power Minimization For Star-Ris Empowered Symbiotic Radio Communications", "abstract": "In this paper, we propose a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) empowered transmission scheme for symbiotic radio (SR) systems to make more flexibility for network deployment and enhance system performance. The STAR-RIS is utilized to not only beam the primary signals from the base station (BS) towards multiple primary users on the same side of the STAR-RIS, but also achieve the secondary transmission to the secondary users on another side. We consider both the broadcasting signal model and unicasting signal model at the BS. For each model, we aim for minimizing the transmit power of the BS by designing the active beamforming and simultaneous reflection and transmission coefficients under the practical phase correlation constraint. To address the challenge of solving the formulated problem, we propose a block coordinate descent based algorithm with the semidefinite relaxation, penalty dual decomposition and successive convex approximation methods, which decomposes the original problem into one sub-problem about active beamforming and the other sub-problem about simultaneous reflection and transmission coefficients, and iteratively solve them until the convergence is achieved. Numerical results indicate that the proposed scheme can reduce up to 150.6% transmit power compared to the backscattering device enabled scheme."}, "cited_paper_content": {"title": "Intelligent Reflecting Surface Assisted Non-Orthogonal Multiple Access", "abstract": "Intelligent reflecting surface (IRS) which consists of a large number of low-cost passive reflecting elements and can digitally manipulating electromagnetic waves, is a new and disruptive technology to achieve spectrum- and energy-efficient as well as cost-efficient wireless networks. In this paper, we consider an IRS-assisted non-orthogonal-multiple-access (NOMA) system in which a base station (BS) transmits superposed downlink signals to multiple users. In order to optimize the rate performance and ensure user fairness, we maximize the minimum decoding signal-to-interference-plus-noise-ratio (SINR) (i.e., equivalently the rate) of all users, by jointly optimizing the power allocation at the BS and the phase shifts at the passive IRS. However, the formulated problem is non-convex and difficult to be solved optimally. By leveraging the block coordinated decent, successive convex optimization and semidefinite relaxation techniques, an efficient algorithm is further proposed to obtain a sub-optimal solution. The convergence is proved and the complexity is analyzed for the proposed algorithm. Also, a low-complexity solving scheme is proposed. Simulation results show that the IRS can enhance the rate performance for downlink NOMA systems significantly even for the scenario in which users have the same or comparable channel strength(es), and the practical IRS with a 3-bit phase quantizer is sufficient to ensure the rate degradation of less than 3.4% compared to the case of continuous phase shifts."}, "keywords": ["transmit power"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed BCD-based algorithm for solving P1 is summarized in Algorithm 1, following the approach proposed in the reference paper #REFR, which considers an intelligent reflecting surface (IRS)-assisted non-orthogonal-multiple-access (NOMA) system and jointly optimizes the power allocation at the base station and the phase shifts at the passive IRS to maximize the minimum decoding signal-to-interference-plus-noise-ratio (SINR) of all users.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed algorithm efficiently solves the non-convex problem of maximizing the minimum decoding signal-to-interference-plus-noise-ratio of all users in an IRS-assisted NOMA system, as demonstrated by simulation results in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed algorithm for minimizing transmit power in the STAR-RIS empowered transmission scheme is shown to reduce up to 150.6% transmit power compared to the backscattering device enabled scheme, as demonstrated in simulation results #REFR.", "given_citation_intent": "result", "given_keywords": ["transmit power"]}]}
{"citing_id": "2304.07509v1", "cited_id": "1704.03165", "section_title": "Grl Under Homophily Assumption.", "citation": "They are shown to over-emphasize proximity information at the expense of structural information #REFR 38] .", "text_before_citation": ["Matrix Factorization & DeepWalk.", "Early unsupervised methods for learning node representation are traditionally based on matrix factorization and random walks.", "Matrix factorization methods calculate losses with handcrafted similarity metrics to build vector representations for each node with latent features #OTHEREFR .", "The inspiration for random walk-based unsupervised methods for learning node representation comes from the effectiveness of the NLP method.", "DeepWalk #OTHEREFR and Node2Vec #OTHEREFR optimize node embeddings by matching nodes' co-occurrence rate on short random walks over graphs."], "text_after_citation": ["Also, they are limited to preserving the similarity of adjacent nodes and cannot extend to heterophily settings. AutoEncoder-based.", "Graph autoencoders, e.g., GAE and VGAE [19] and their follow-up work ARVGE #OTHEREFR with an adversarial regularization framework use two-layer GCN as their encoder and consider that impose the topological closeness of nodes in the graph structure on the latent space by predicting the first-order neighbors.", "GAEs over-emphasize proximity information and are also based on the assumption that connected nodes should be more similar. Unsupervised GNNs.", "Unsupervised GNN-based methods #OTHEREFR , on the other hand, propagate features and aggregate them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM), where the node representations evolve over multiple rounds of propagation with becoming prohibitively similar.", "Uniform aggregation and update in GNNs ignore the difference in information between similar and dissimilar neighbors."], "citing_paper_content": {"title": "Multi-View Graph Representation Learning Beyond Homophily", "abstract": "Unsupervised graph representation learning(GRL) aims to distill diverse graph information into task-agnostic embeddings without label supervision. Due to a lack of support from labels, recent representation learning methods usually adopt self-supervised learning, and embeddings are learned by solving a handcrafted auxiliary task(so-called pretext task). However, partially due to the irregular non-Euclidean data in graphs, the pretext tasks are generally designed under homophily assumptions and cornered in the low-frequency signals, which results in significant loss of other signals, especially high-frequency signals widespread in graphs with heterophily. Motivated by this limitation, we propose a multi-view perspective and the usage of diverse pretext tasks to capture different signals in graphs into embeddings. A novel framework, denoted as Multi-view Graph Encoder(MVGE), is proposed, and a set of key designs are identified. More specifically, a set of new pretext tasks are designed to encode different types of signals, and a straightforward operation is propxwosed to maintain both the commodity and personalization in both the attribute and the structural levels. Extensive experiments on synthetic and real-world network datasets show that the node representations learned with MVGE achieve significant performance improvements in three different downstream tasks, especially on graphs with heterophily."}, "cited_paper_content": {"title": "Struc2Vec: Learning Node Representations From Structural Identity", "abstract": "Structural identity is a concept of symmetry in which network nodes are identified according to the network structure and their relationship to other nodes. Structural identity has been studied in theory and practice over the past decades, but only recently has it been addressed with representational learning techniques. This work presents struc2vec, a novel and flexible framework for learning latent representations for the structural identity of nodes. struc2vec uses a hierarchy to measure node similarity at different scales, and constructs a multilayer graph to encode structural similarities and generate structural context for nodes. Numerical experiments indicate that state-of-the-art techniques for learning node representations fail in capturing stronger notions of structural identity, while struc2vec exhibits much superior performance in this task, as it overcomes limitations of prior approaches. As a consequence, numerical experiments indicate that struc2vec improves performance on classification tasks that depend more on structural identity."}, "keywords": ["structural information"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Another recent approach that addresses the limitations of prior methods is struc2vec, which uses a hierarchy to measure node similarity at different scales and constructs a multilayer graph to encode structural similarities and generate structural context for nodes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recently, struc2vec has been proposed as a novel and flexible framework for learning latent representations for the structural identity of nodes, which exhibits much superior performance in capturing stronger notions of structural identity compared to state-of-the-art techniques for learning node representations #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent approaches for learning node representations fail to capture stronger notions of structural identity, which is crucial for classification tasks that depend more on #KEYWORD1. However, the proposed struc2vec framework in #REFR overcomes these limitations by using a hierarchy to measure node similarity at different scales and constructing a multilayer graph to encode structural similarities and generate structural context for nodes.", "given_citation_intent": "background", "given_keywords": ["structural information"]}]}
{"citing_id": "2305.02320v1", "cited_id": "1909.10351", "section_title": "Experimental Design", "citation": "In addition, we conduct experiments with TinyBERT (2 layers version) #REFR to assess the generalizability of our results.", "text_before_citation": ["For the lexical retrieval with BM25, we use the similarity function of Elasticsearch [9] .", "We cap the query length at 30 tokens and the passage length at 200 tokens following prior work #OTHEREFR .", "Table 2 shows a comparison of the effectiveness of CE human and CE ChatGPT .", "Please note that for both models, during inference, we evaluate their effectiveness in retrieving human responses in the supervised or zero-shot settings.", "We choose MiniLM (12 layers version) #OTHEREFR for the experiments due to its competitive results in comparison to BERT re-ranker #OTHEREFR while being three times smaller and six times faster."], "text_after_citation": [], "citing_paper_content": {"title": "Generating Synthetic Documents For Cross-Encoder Re-Rankers: A Comparative Study Of Chatgpt And Human Experts", "abstract": "We investigate the usefulness of generative Large Language Models (LLMs) in generating training data for cross-encoder re-rankers in a novel direction: generating synthetic documents instead of synthetic queries. We introduce a new dataset, ChatGPT-RetrievalQA, and compare the effectiveness of models fine-tuned on LLMgenerated and human-generated data. Data generated with generative LLMs can be used to augment training data, especially in domains with smaller amounts of labeled data. We build ChatGPT-RetrievalQA based on an existing dataset, human ChatGPT Comparison Corpus (HC3), consisting of public question collections with human responses and answers from ChatGPT. We fine-tune a range of cross-encoder re-rankers on either human-generated or ChatGPTgenerated data. Our evaluation on MS MARCO DEV, TREC DL'19, and TREC DL'20 demonstrates that cross-encoder re-ranking models trained on ChatGPT responses are statistically significantly more effective zero-shot re-rankers than those trained on human responses. In a supervised setting, the human-trained re-rankers outperform the LLM-trained re-rankers. Our novel findings suggest that generative LLMs have high potential in generating training data for neural retrieval models. Further work is needed to determine the effect of factually wrong information in the generated responses and test our findings' generalizability with open-source LLMs. We release our data, code, and cross-encoders checkpoints for future work. 1 2 We refer to the cross-encoders fine-tuned on ChatGPT-generated and humangenerated responses as CE ChatGPT and CE human , respectively."}, "cited_paper_content": {"title": "Tinybert: Distilling Bert For Natural Language Understanding", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT.TinyBERT is empirically effective and achieves more than 96% the performance of teacher BERTBASE on GLUE benchmark while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only about 28% parameters and about 31% inference time of them."}, "keywords": ["results", "TinyBERT"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To accelerate inference and reduce model size while maintaining accuracy, the authors propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To accelerate inference and reduce model size while maintaining accuracy, the authors propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce model size and accelerate inference while maintaining accuracy, the authors propose a novel transformer distillation method called TinyBERT, which achieves more than 96% of the performance of teacher BERTBASE on GLUE benchmark while being 7.5x smaller and 9.4x faster on inference, according to the results presented in #REFR.", "given_citation_intent": "method", "given_keywords": ["results", "TinyBERT"]}]}
{"citing_id": "2303.10322v1", "cited_id": "1905.07230", "section_title": "A. Inverse Ckf", "citation": "The forward gain matrix K k+1 in case of I-KF is deterministic #REFR .", "text_before_citation": ["K k+1 = \u03a3 xa k+1 \u03a3 a k+1 \u22121", ".", "The I-CKF recursions follow from the non-additive noise formulation of CKF #OTHEREFR with a higher (n x + n y )-dimensional cubature points.", "However, unlike forward CKF, these cubature points are generated only once for the time update taking into account the process noise statistics (covariance R).", "Remark 1."], "text_after_citation": ["For I-EKF #OTHEREFR , this matrix depends on only the linearized system model at the state estimates.", "Hence, I-KF and I-EKF treat K k+1 as a time-varying parameter of the inverse filter's state transition.", "But the gain matrix in CKF explicitly depends on the state estimates through the covariances computed from the generated cubature points.", "Thus, it is not treated as a parameter of (10).", "Remark 2."], "citing_paper_content": {"title": "Inverse Cubature And Quadrature Kalman Filters", "abstract": "Recent developments in counter-adversarial system research have led to the development of inverse stochastic filters that are employed by a defender to infer the information its adversary may have learned. Prior works addressed this inverse cognition problem by proposing inverse Kalman filter (I-KF) and inverse extended KF (I-EKF), respectively, for linear and non-linear Gaussian state-space models. However, in practice, many counter-adversarial settings involve highly non-linear system models, wherein EKF's linearization often fails. In this paper, we consider the efficient numerical integration techniques to address such nonlinearities and, to this end, develop inverse cubature KF (I-CKF) and inverse quadrature KF (I-QKF). We derive the stochastic stability conditions for the proposed filters in the exponential-mean-squared-boundedness sense. Numerical experiments demonstrate the estimation accuracy of our I-CKF and I-QKF with the recursive Cram\u00e9r-Rao lower bound as a benchmark."}, "cited_paper_content": {"title": "How To Calibrate Your Adversary'S Capabilities? Inverse Filtering For Counter-Autonomous Systems", "abstract": "We consider an adversarial Bayesian signal processing problem involving \u201cus\u201d and an \u201cadversary\u201d. The adversary observes our state in noise; updates its posterior distribution of our state and then chooses an action based on this posterior. Given knowledge of \u201cour\u201d state and sequence of adversary's actions observed in noise, we consider three problems: (i) How can the adversary's posterior distribution be estimated? Estimating the posterior is an inverse filtering problem involving a random measure - we formulate and solve several versions of this problem in a Bayesian setting. (ii) How can the adversary's observation likelihood be estimated? This tells us how accurate the adversary's sensors are. We compute the maximum likelihood estimator for the adversary's observation likelihood given our measurements of the adversary's actions where the adversary's actions are in response to estimating our state. (iii) How can the state be chosen by us to minimize the covariance of the estimate of the adversary's observation likelihood? \u201cOur\u201d state can be viewed as a probe signal which causes the adversary to act; so choosing the optimal state sequence is an input design problem. The above questions are motivated by the design of counter-autonomous systems: given measurements of the actions of a sophisticated autonomous adversary, how can our counter-autonomous system estimate the underlying belief of the adversary, predict future actions and therefore guard against these actions."}, "keywords": ["forward gain matrix"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors proposed inverse cubature KF (I-CKF) and inverse quadrature KF (I-QKF) to address highly non-linear system models in counter-adversarial settings, which are based on efficient numerical integration techniques #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The design of counter-autonomous systems requires the estimation of the adversary's observation likelihood, which can be computed using the maximum likelihood estimator proposed in #REFR, given the measurements of the adversary's actions in response to estimating our state.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The inverse filtering problem involving a random measure can be formulated and solved in a Bayesian setting to estimate the adversary's posterior distribution, as well as its observation likelihood, given our measurements of the adversary's actions where the adversary's actions are in response to estimating our state, which is crucial for the design of counter-autonomous systems that aim to guard against the actions of a sophisticated autonomous adversary. #REFR", "given_citation_intent": "background", "given_keywords": ["forward gain matrix"]}]}
{"citing_id": "2303.01728v2", "cited_id": "1705.10528", "section_title": "Analysis On Intervention-Based Rl", "citation": "To analyze the state distribution discrepancy in BC, we first introduce a useful lemma #REFR . Lemma 3.1.", "text_before_citation": ["Let T (s) = 1 denotes the teacher policy \u03c0 t takes control and T (s) = 0 means otherwise. Then \u03c0 b can be represented as", "\u03c0 b (\u2022|s) = T (s)\u03c0 t (\u2022|s) + (1 \u2212 T (s))\u03c0 s (\u2022|s).", "One issue with the joint control is that the student policy \u03c0 s is trained with samples collected by the behavior policy \u03c0 b , whose action distribution is not always aligned with \u03c0 s .", "A large state distribution discrepancy between two policies d \u03c0 b \u2212 d \u03c0s 1 can cause distributional shift and ruin the training.", "A similar problem exists in behavior cloning (BC), though in BC no intervention is involved and \u03c0 s learns from samples all collected by the teacher policy \u03c0 t ."], "text_after_citation": ["The state distribution discrepancy between the teacher policy \u03c0 t and the student policy \u03c0 s is bounded by their expected policy discrepancy:", "EQUATION", "We apply the lemma to the setting of intervention-based RL and derive a bound for", "d \u03c0 b \u2212 d \u03c0s 1 .", "Theorem 3.2."], "citing_paper_content": {"title": "Guarded Policy Optimization With Imperfect Online Demonstrations", "abstract": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C."}, "cited_paper_content": {"title": "Constrained Policy Optimization", "abstract": "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. ::: We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety."}, "keywords": ["state distribution discrepancy"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A similar problem exists in behavior cloning (BC), though in BC no intervention is involved and \u03c0 s learns from samples all collected by the teacher policy \u03c0 t (Schulman et al., 2015).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent advances in policy search algorithms have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. However, the proposed Constrained Policy Optimization (CPO) algorithm in this manuscript, which is the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration, allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The joint control of the teacher and student policies can cause distributional shift and ruin the training due to a large state distribution discrepancy between the two policies (d \u03c0 b \u2212 d \u03c0s 1), as discussed in the manuscript. To address this issue, the authors propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration, which allows training of neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["state distribution discrepancy"]}]}
{"citing_id": "2304.13539v1", "cited_id": "1409.1556", "section_title": "I. Introduction", "citation": "Furthermore, subsequently VGG #REFR This research was supported in part by the National Science Foundation under grant number CCF-1954749.", "text_before_citation": ["T ENSORS are multidimensional arrays indexed by three or more indices.", "An N th -order tensor is the tensor product of N vector spaces. Third-order tensors have three indices as shown in Fig. 1 .", "In special cases, first-order tensors represent vectors, and second-order tensors represent matrices.", "Convolutional neural networks (CNNs) have outperformed traditional techniques for image recognition tasks.", "In 2012, AlexNet #OTHEREFR achieved about 80% top-5 accuracy on the ImageNet dataset #OTHEREFR ."], "text_after_citation": ["and GoogleNet #OTHEREFR achieved about 90% top-5 accuracy with the same dataset.", "On the ImageNet dataset, ResNet #OTHEREFR with a depth of up to 152 layers achieved 3.57% top-5 error.", "Executing CNNs for computer vision tasks on mobile devices is gaining more and more attention.", "Common methods to reduce the size of CNNs include: sparsification #OTHEREFR - #OTHEREFR , quantization #OTHEREFR , #OTHEREFR , structural pruning #OTHEREFR - #OTHEREFR , and low-rank approximation #OTHEREFR - #OTHEREFR .", "The use of low-rank approximations is inspired by #OTHEREFR which showed that the neural network parameters are highly redundant."], "citing_paper_content": {"title": "Tensor Decomposition For Model Reduction In Neural Networks: A Review", "abstract": "Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices."}, "cited_paper_content": {"title": "Very Deep Convolutional Networks For Large-Scale Image Recognition", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."}, "keywords": ["subsequently VGG", "National Science Foundation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Convolutional neural networks have outperformed traditional techniques for image recognition tasks, achieving about 80% top-5 accuracy on the ImageNet dataset in 2012 with AlexNet #REFR. To further improve the accuracy, the authors of Very Deep Convolutional Networks For Large-Scale Image Recognition investigated the effect of the convolutional network depth on its accuracy and achieved a significant improvement by pushing the depth to 16-19 weight layers.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Convolutional neural networks (CNNs) have achieved state-of-the-art results in image recognition tasks, with the best-performing models having a depth of 16-19 weight layers #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In 2014, the VGG team achieved the first and second places in the ImageNet Challenge classification and localization tracks, respectively, by pushing the depth of their convolutional neural network to 16-19 weight layers #REFR, subsequently inspiring many researchers to explore deeper networks for better performance.", "given_citation_intent": "background", "given_keywords": ["subsequently VGG", "National Science Foundation"]}]}
{"citing_id": "2303.16948v1", "cited_id": "2003.14199", "section_title": "V. Simulation Results", "citation": "Thus, in Phase I, we solve problems (7), #REFR , and (8) to obtain the minimum cost, hence the optimal trajectory for Phase I.", "text_before_citation": ["The disruption in (5c) is evaluated with parameters \u03b3 x = 0.5, \u03b3 v = 0.5.", "When any of the problems (13), #OTHEREFR , or (15) is infeasible or whenever the optimal trajectory of C does not converge, we relax the terminal time with a relaxation rate \u03bb = 1.8.", "The numerical solutions to the optimization problems are obtained using an interior point optimizer (IPOPT) on an Intel(R) Core(TM) i7-8700 3.20GHz.", "\"Merge ahead of HDV\" policy.", "As discussed in Section III, in order for CAV C to evaluate the cost of this policy, it breaks down its trajectory into two phases if its initial position is behind the HDV."], "text_after_citation": ["The weights \u03b1 {t,u,v} in (7) and (9) are set to 0.55, 0.2, and 0.25, respectively.", "The maximum maneuver time is set as T = 15s.", "If any of the OCPs is infeasible in this phase, its corresponding cost is set as \"Inf \".", "The results are shown in Table I , where we see that, in this case, it is optimal for CAV C to travel with constant acceleration and t * 1 = 3.53s. Proceeding to Phase II, the initial conditions are", "EQUATION"], "citing_paper_content": {"title": "Cooperative Lane Changing In Mixed Traffic Can Be Robust To Human Driver Behavior", "abstract": "We derive time and energy-optimal control policies for a Connected Autonomous Vehicle (CAV) to complete lane change maneuvers in mixed traffic. The interaction between CAVs and Human-Driven Vehicles (HDVs) requires designing the best possible response of a CAV to actions by its neighboring HDVs. This interaction is formulated using a bilevel optimization setting with an appropriate behavioral model for an HDV's. Then, an iterated best response (IBR) method is used to determine a Nash equilibrium. However, we also show that when a common and simple-to-detect condition applies, the optimal lane-changing policy is in fact independent of HDV behavior with a CAV changing lanes by cooperating with another CAV in the target lane and always merging ahead of it. Thus, the dependence on the interaction between CAVs and HDVs may be eliminated in such cases. Simulation results are included to show the effectiveness of our controllers in terms of cost, safety guarantees, and disruption to the traffic flow when uncontrollable HDVs are present."}, "cited_paper_content": {"title": "Nonconvex Consensus Admm For Cooperative Lane Change Maneuvers Of Connected Automated Vehicles", "abstract": "Connected automated vehicles (CAV) offer huge potential to improve the performance of automated vehicles (AV) without communication capabilities, especially in situations when the vehicles (or agents) need to be cooperative to accomplish their maneuver. Lane change maneuvers in dense traffic, e.g., are very challenging for non-connected AVs. To alleviate this problem, we propose a holistic distributed lane change control scheme for CAVs which relies on vehicle-to-vehicle communication. The originally centralized optimal control problem is embedded into a consensus-based Alternating Direction Method of Multipliers framework to solve it in a distributed receding horizon fashion. Although agent dynamics render the underlying optimal control problem nonconvex, we propose a problem reformulation that allows to derive convergence guarantees. In the distributed setting, every agent needs to solve a nonlinear program (NLP) locally. To obtain a real time solution of the local NLPs, we utilize the optimization engine OpEn which implements the proximal averaged Newton method for optimal control (PANOC). Simulation results prove the efficacy and real time capability of our approach."}, "keywords": ["optimal trajectory"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To obtain a real time solution of the local NLPs, the authors utilize the optimization engine OpEn which implements the proximal averaged Newton method for optimal control (PANOC) (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain a real-time solution of the local nonlinear program, the authors utilized the optimization engine OpEn which implements the proximal averaged Newton method for optimal control (PANOC) as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain the optimal trajectory of C, the authors propose utilizing the optimization engine OpEn which implements the proximal averaged Newton method for optimal control (PANOC) as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["optimal trajectory"]}]}
{"citing_id": "2303.03633v1", "cited_id": "1906.00446", "section_title": "Appendix C. Detailed Network Architecture", "citation": "Arrows indicate the diseased areas in the entire reconstructions, which are diminished in the pseudo-normal reconstructions. Gd, gadolinium. et al., 2021; #REFR .", "text_before_citation": ["Moreover, the fourth row presents the ground-truth segmentation labels for the tumor-associated regions, and the fifth row shows the predicted segmentation labels that were decoded by the label decoder taking the abnormal ACs as input.", "Note that the segmentation was trained only for the tumor-associated regions and not for the normal anatomy-associated regions, as shown in the results.", "One may argue that the quality of the reconstructed images and segmentation labels was insufficient, as observed from the blurred and rounded appearance that did not recover the detailed image characteristics.", "These tendencies are reasonable because the image information is compressed because of the limited size of the latent representation, which raises a trade-off between the reconstruction qualities and latent size (Kobayashi The input images, the entire reconstructions from entire anatomy codes (ACs), and the pseudo-normal reconstructions from normal ACs are presented in the first, second, and third rows, respectively.", "The ground-truth and predicted segmentation labels are shown in the fourth and fifth rows, respectively."], "text_after_citation": ["Indeed, we did not pursue the generation quality of the reconstructions as a primary purpose because the lower dimension of the latent representations can be advantageous for computational efficiency in similarity search.", "Besides, although the detailed part of the image characteristics was not perfectly reproduced, it was still sufficient for recognizing the anatomical location and presence of abnormalities in the reconstructed images.", "Appendix D.2.", "Qualitative evaluation of the semantically organized latent space We applied t-distributed stochastic neighbor embedding (t-SNE) analysis (van der Maaten and Hinton, 2011) to evaluate how the latent space was organized according to the semantics by changing the margin parameter (see Section 2.2.3).", "We randomly selected 50 individual volumes from each dataset and extracted entire ACs and normal ACs in an image-wise manner."], "citing_paper_content": {"title": "Sketch-Based Medical Image Retrieval", "abstract": "The amount of medical images stored in hospitals is increasing faster than ever; however, utilizing the accumulated medical images has been limited. This is because existing content-based medical image retrieval (CBMIR) systems usually require example images to construct query vectors; nevertheless, example images cannot always be prepared. Besides, there can be images with rare characteristics that make it difficult to find similar example images, which we call isolated samples. Here, we introduce a novel sketch-based medical image retrieval (SBMIR) system that enables users to find images of interest without example images. The key idea lies in feature decomposition of medical images, whereby the entire feature of a medical image can be decomposed into and reconstructed from normal and abnormal features. By extending this idea, our SBMIR system provides an easy-to-use two-step graphical user interface: users first select a template image to specify a normal feature and then draw a semantic sketch of the disease on the template image to represent an abnormal feature. Subsequently, it integrates the two kinds of input to construct a query vector and retrieves reference images with the closest reference vectors. Using two datasets, ten healthcare professionals with various clinical backgrounds participated in the user test for evaluation. As a result, our SBMIR system enabled users to overcome previous challenges, including image retrieval based on fine-grained image characteristics, image retrieval without example images, and image retrieval for isolated samples. Our SBMIR system achieves flexible medical image retrieval on demand, thereby expanding the utility of medical image databases."}, "cited_paper_content": {"title": "Generating Diverse High-Fidelity Images With Vq-Vae-2", "abstract": "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity."}, "keywords": ["pseudo-normal reconstructions"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors' use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation, as described in #REFR, highlights the trade-off between reconstruction qualities and latent size when compressing image information into a limited size of the latent representation.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors explored the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation, which could be a potential solution to the trade-off between the reconstruction qualities and latent size (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors utilized a Vector Quantized Variational AutoEncoder (VQ-VAE) model to generate synthetic samples of higher coherence and fidelity, including #REFR for pseudo-normal reconstructions, which can be used for medical image retrieval systems.", "given_citation_intent": "background", "given_keywords": ["pseudo-normal reconstructions"]}]}
{"citing_id": "2303.09952v1", "cited_id": "2003.08934", "section_title": "I. Introduction", "citation": "In this paper, we propose a joint rendering mechanism that takes the MPI strategy for coarse sampling proposals and the MLP&volume-based rendering #REFR for fine sampling and rendering.", "text_before_citation": ["However, the rendering quality heavily relies on the number of image planes.", "It needs high-resolution 4D volumes to store the 4channel (RGB and volume density) image planes that cost a large amount of GPU memory in both training and prediction. Li Zhang is the corresponding author. (e-mail:lizhangfd@fudan.edu.cn).", "Yurui Chen, Chun Gu and Li Zhang are with the School of Data Science, Fudan University. Feihu Zhang is with University of Oxford. #OTHEREFR .", "It produces many blurs and floats in the occluded regions and at the object/depth edges.", "(c-d) Our method employs a joint rendering mechanism that preserves more image details and predicts sharp depth edges."], "text_after_citation": ["Then, both the coarse point samples and the fine samples are combined according to their geometry distribution to realize a more accurate joint rendering.", "More importantly, we introduce a depth teacher net that serves as the guidance for the joint rendering.", "The monocular depth teacher predicts dense pseudo depth maps that assist the consistent 3D geometry learning between the MPI, the fine volume, and the joint rendering.", "It also boosts the multi-view geometry consistency between the source view and the target novel views that helps handle the occlusions, reduce the blurs and floats, and render accurate depths.", "In the experiments, we verify the effectiveness of our method on three challenging real-scene datasets (RealEstate10K #OTHEREFR , NYU #OTHEREFR and NeRF-LLFF #OTHEREFR ) for novel view synthesis or depth estimation."], "citing_paper_content": {"title": "Single-View Neural Radiance Fields With Depth Teacher", "abstract": "Neural Radiance Fields (NeRF) have been proposed for photorealistic novel view rendering. However, it requires many different views of one scene for training. Moreover, it has poor generalizations to new scenes and requires retraining or finetuning on each scene. In this paper, we develop a new NeRF model for novel view synthesis using only a single image as input. We propose to combine the (coarse) planar rendering and the (fine) volume rendering to achieve higher rendering quality and better generalizations. We also design a depth teacher net that predicts dense pseudo depth maps to supervise the joint rendering mechanism and boost the learning of consistent 3D geometry. We evaluate our method on three challenging datasets. It outperforms state-of-the-art single-view NeRFs by achieving 5\u223c20% improvements in PSNR and reducing 20\u223c50% of the errors in the depth rendering. It also shows excellent generalization abilities to unseen data without the need to finetune on each new scene. Index Terms-single-view, novel view synthesis, multi-plane images, neural radiance field, volume rendering."}, "cited_paper_content": {"title": "Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis", "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."}, "keywords": ["MLP&volume-based rendering"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed method achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve higher rendering quality and better generalizations, the authors propose to combine planar rendering and volume rendering, as well as a depth teacher net that predicts dense pseudo depth maps to supervise the joint rendering mechanism and boost the learning of consistent 3D geometry (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Li et al. proposed a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views, which includes MLP&volume-based rendering, and is cited as #REFR.", "given_citation_intent": "method", "given_keywords": ["MLP&volume-based rendering"]}]}
{"citing_id": "2303.00720v1", "cited_id": "1607.06215", "section_title": "Introduction", "citation": "Contrary to its unimodal counterpart, CEM requires an additional relationship modeling step #REFR that represents data elements from different modalities on a shared embedding space before similarities can be established.", "text_before_citation": ["posters, banners), this results in a limited set of queries as the target fields needed to gather actionable insights do not appear in the document in the first place.", "A cross-modal entity matching (CEM) framework (defined in Section 2) can accelerate Alice's decision-making process in the previous example by augmenting the menu with supplementary data e.g., allergen information from the nutritional table in near-real-time.", "Briefly, a CEM framework maps a data element e 1 from one modality (e.g.", "printed menu) to a data element e 2 from another modality (e.g. relational table) if they represent the same real-world object(s).", "What makes this task challenging is measuring similarity between data elements across modalities."], "text_after_citation": ["This is particularly challenging as VRDs can be multimodal and extremely diverse in terms of their layout, format and content (see Fig. 2 for an example).", "Now let's consider a straightforward solution for the aforementioned task. Using a publicly available document understanding model (e.g.", "LayoutLMv2 #OTHEREFR ), we can extract a structured record from the menu corresponding to each item.", "We can subsequently identify matching tuples for a menu-item by performing fuzzyjoin between the nutritional table and its corresponding record.", "Unfortunately, this method requires significant effort at every intermediate step."], "citing_paper_content": {"title": "Cross-Modal Entity Matching For Visually Rich Documents", "abstract": "Visually rich documents (VRD) are physical/digital documents that utilize visual cues to augment their semantics. The information contained in these documents are often incomplete. Existing works that enable automated querying on VRDs do not take this aspect into account. Consequently, they support a limited set of queries. In this paper, we describe Juno-a multimodal framework that identifies a set of tuples from a relational database to augment an incomplete VRD with supplementary information. Our main contribution in this is an end-to-end-trainable neural network with bi-directional attention that executes this crossmodal entity matching task without any prior knowledge about the document type or the underlying database-schema. Exhaustive experiments on two heteroegeneous datasets show that Juno outperforms state-of-theart baselines by more than 6% in F1-score, while reducing the amount of human-effort in its workflow by more than 80%. To the best of our knowledge, ours is the first work that investigates the incompleteness of VRDs and proposes a robust framework to address it in a seamless way."}, "cited_paper_content": {"title": "A Comprehensive Survey On Cross-Modal Retrieval", "abstract": "In recent years, cross-modal retrieval has drawn much attention due to the rapid growth of multimodal data. It takes one type of data as the query to retrieve relevant data of another type. For example, a user can use a text to retrieve relevant pictures or videos. Since the query and its retrieved results can be of different modalities, how to measure the content similarity between different modalities of data remains a challenge. Various methods have been proposed to deal with such a problem. In this paper, we first review a number of representative methods for cross-modal retrieval and classify them into two main groups: 1) real-valued representation learning, and 2) binary representation learning. Real-valued representation learning methods aim to learn real-valued common representations for different modalities of data. To speed up the cross-modal retrieval, a number of binary representation learning methods are proposed to map different modalities of data into a common Hamming space. Then, we introduce several multimodal datasets in the community, and show the experimental results on two commonly used multimodal datasets. The comparison reveals the characteristic of different kinds of cross-modal retrieval methods, which is expected to benefit both practical applications and future research. Finally, we discuss open problems and future research directions."}, "keywords": ["shared embedding space"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To measure similarity between data elements across modalities, various methods have been proposed in the literature #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To measure similarity between data elements across modalities, various methods have been proposed in the literature #REFR, including real-valued representation learning and binary representation learning.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Measuring similarity between data elements across modalities is a challenging task, and various methods have been proposed to deal with such a problem, including learning real-valued common representations for different modalities of data or mapping different modalities of data into a common Hamming space #REFR (A Comprehensive Survey On Cross-Modal Retrieval). Specifically, the shared embedding space is a popular approach to address the cross-modal retrieval problem.", "given_citation_intent": "background", "given_keywords": ["shared embedding space"]}]}
{"citing_id": "2304.05943v1", "cited_id": "1812.08451", "section_title": "Conclusion", "citation": "In particular, machine learning techniques which were limited to Kitaev's codes in #REFR could be expanded to a much broader range of codes and circuits. Proof.", "text_before_citation": ["In the future, our scheme may be improved by designing a decoder that exploits the equivalence between different fault configurations like in the work of Pryadko #OTHEREFR .", "Design of better decoders for quantum LDPC codes may also improve our scheme.", "Conceptually, our work emphasizes and formally captures a circuit-centric approach of quantum error correction and fault tolerance.", "This circuit-centric approach is central to Floquet codes [17] and has also lead to new ideas for surface codes #OTHEREFR .", "Our formalism could be used to take this approach further by, for example, searching over the space of quantum codes and circuits."], "text_after_citation": ["Because S j belongs to the stabilizer group of the system right after level j , the faults corresponding to \u03b7 j +0.5 (S j ) do not flip any of the checks of the outcome code.", "Based on Lemma 4, this leads to the lemma for the sign j + 0.5.", "The same result holds for j \u2212 0.5 because a fault S j after the measurement of S j is equivalent to a fault S j before this measurement.", "A Pauli error on the input state also keeps the check operator outcomes trivial.", "Lemma 6 (Input error)."], "citing_paper_content": {"title": "Spacetime Codes Of Clifford Circuits", "abstract": "We propose a scheme for detecting and correcting faults in any Clifford circuit. The scheme is based on the observation that the set of all possible outcome bit-strings of a Clifford circuit is a linear code, which we call the \"outcome code\". From the outcome code we construct a corresponding stabilizer code, the \"spacetime code\". Our construction extends the circuit-tocode construction of Bacon, Flammia, Harrow and Shi [2], revisited recently by Gottesman [16], to include intermediate and multi-qubit measurements. With this correspondence, we reduce the problem of correcting faults in a circuit to the well-studied problem of correcting errors in a stabilizer code. More precisely, a most likely error decoder for the spacetime code can be transformed into a most likely fault decoder for the circuit. We give efficient algorithms to construct the outcome and spacetime codes. We also identify conditions under which these codes are LDPC, and give an algorithm to generate low-weight checks, which can then be combined with efficient LDPC code decoders."}, "cited_paper_content": {"title": "Optimizing Quantum Error Correction Codes With Reinforcement Learning", "abstract": "Quantum error correction is widely thought to be the key to fault-tolerant quantum computation. However, determining the most suited encoding for unknown error channels or specific laboratory setups is highly challenging. Here, we present a reinforcement learning framework for optimizing and fault-tolerantly adapting quantum error correction codes. We consider a reinforcement learning agent tasked with modifying a family of surface code quantum memories until a desired logical error rate is reached. Using efficient simulations with about 70 data qubits with arbitrary connectivity, we demonstrate that such a reinforcement learning agent can determine near-optimal solutions, in terms of the number of data qubits, for various error models of interest. Moreover, we show that agents trained on one setting are able to successfully transfer their experience to different settings. This ability for transfer learning showcases the inherent strengths of reinforcement learning and the applicability of our approach for optimization from off-line simulations to on-line laboratory settings."}, "keywords": ["Kitaev's codes", "codes"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our formalism could be used to take this approach further by, for example, searching over the space of quantum codes and circuits, as demonstrated in the work of Optimizing Quantum Error Correction Codes With Reinforcement Learning #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Reinforcement learning has been shown to be a promising approach for optimizing and fault-tolerantly adapting quantum error correction codes, as demonstrated in the work of Baireuther et al. #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Kitaev's codes have been widely used in quantum error correction, and recent work has shown that reinforcement learning can be used to optimize and adapt these codes for specific laboratory setups #REFR.", "given_citation_intent": "background", "given_keywords": ["Kitaev's codes", "codes"]}]}
{"citing_id": "2303.03260v1", "cited_id": "1412.6980", "section_title": "Neural Network Architecture", "citation": "Optimization is always conducted with Adam #REFR to make the comparison more straightforward, although L-BFGS leads to fewer iterations.", "text_before_citation": ["To increase the ratio between the number of indicator voxels and network parameters, twice as many filters are employed in the three-dimensional case. For more details on both networks, c.f. Appendix B.", "Nearest-Neighbor interpolation was used for upsampling, as it performed better than polynomial interpolation, as it preserves discontinuities needed to represent the voids.", "Due to their convergence accelerating behavior #OTHEREFR , adaptive activation functions are used.", "Specifically, PReLU #OTHEREFR was used for all layers except the last layer, where an adaptive Sigmoid was used to ensure an output in the range [0, 1].", "The output was then rescaled to the range [ , 1] to ensure numerical stability."], "text_after_citation": ["Gradient clipping #OTHEREFR was found to be essential to ensure an early convergence.", "Without it, a plateau was observed, followed by a strongly delayed breakthrough.", "In addition, a learning rate scheduler with a polynomial decay (b \u2022 epoch + 1) a was employed with the values a = \u22120.5, b = 0.2.", "The initial network weights are set with Glorot initialization #OTHEREFR ."], "citing_paper_content": {"title": "On The Use Of Neural Networks For Full Waveform Inversion", "abstract": "Neural networks have recently gained attention in solving inverse problems. One prominent methodology are Physics-Informed Neural Networks (PINNs) which can solve both forward and inverse problems. In the paper at hand, full waveform inversion is the considered inverse problem. The performance of PINNs is compared against classical adjoint optimization, focusing on three key aspects: the forward-solver, the neural network Ansatz for the inverse field, and the sensitivity computation for the gradient-based minimization. Starting from PINNs, each of these key aspects is adapted individually until the classical adjoint optimization emerges. It is shown that it is beneficial to use the neural network only for the discretization of the unknown material field, where the neural network produces reconstructions without oscillatory artifacts as typically encountered in classical full waveform inversion approaches. Due to this finding, a hybrid approach is proposed. It exploits both the efficient gradient computation with the continuous adjoint method as well as the neural network Ansatz for the unknown material field. This new hybrid approach outperforms Physics-Informed Neural Networks and the classical adjoint optimization in settings of two and three-dimensional examples."}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["Optimization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Due to their convergence accelerating behavior, adaptive activation functions are used. Specifically, PReLU was used for all layers except the last layer, where an adaptive Sigmoid was used to ensure an output in the range [0, 1]. The output was then rescaled to the range [ , 1] to ensure numerical stability. For more details on the adaptive optimization algorithm used, please refer to the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure efficient gradient computation and produce reconstructions without oscillatory artifacts, the authors propose a hybrid approach that exploits both the neural network Ansatz for the unknown material field and the continuous adjoint method #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure efficient convergence during the optimization process, the authors employed the Adam algorithm #REFR, which is based on adaptive estimates of lower-order moments and is well-suited for problems that are large in terms of data and/or parameters, such as full waveform inversion.", "given_citation_intent": "method", "given_keywords": ["Optimization"]}]}
{"citing_id": "2304.01811v1", "cited_id": "1512.03385", "section_title": "H.4. Experiment Results Of The Robustness Of The Harsanyinets", "citation": "Table 6 shows that the classification accuracy of the adversarial examples of HarsanyiNet is slightly higher than that of ResNet-18 #REFR .", "text_before_citation": ["We conducted more experiments to analyze the robustness of the HarsanyiNet.", "Specifically, we estimate the adversarial robustness of the classification performance and the adversarial robustness of the estimated Shapley values #OTHEREFR .", "To estimate the adversarial robustness of the classification performance on HarsanyiNet, we conducted experiments on the CIFAR-10 dataset to evaluate the model robustness by examining its classification accuracy on the test set of adversarial examples.", "To generate adversarial examples, we used the FGSM attack #OTHEREFR , a gradient-based method, with a maximum perturbation of 8/255 #OTHEREFR ."], "text_after_citation": ["To estimate the adversarial robustness of the estimated Shapley values on HarsanyiNet, we assessed the robustness of its estimated Shapley values by computing the 2 norm of the difference in Shapley values between the adversarial and natural examples, i.e., ||\u03c6 nat \u2212 \u03c6 adv || 2 , where \u03c6 nat denotes the Shapley values of natural examples, and \u03c6 adv denotes the Shapley values of adversarial examples.", "To calculate the Shapley values of the ResNet-18 model, we estimate Shapley values using the sampling algorithm with 1000 iterations.", "Table 6 shows that the adversarial robustness of the estimated Shapley values on HarsanyiNet (estimated by the 2 norm of the difference of Shapley values, the lower the better) is slightly higher than that of Both experiments indicate that HarsanyiNet has a robustness close to, or even slightly higher than, that of the traditional model."], "citing_paper_content": {"title": "Harsanyinet: Computing Accurate Shapley Values In A Single Forward Propagation", "abstract": "The Shapley value is widely regarded as a trustworthy attribution metric. However, when people use Shapley values to explain the attribution of input variables of a deep neural network (DNN), it usually requires a very high computational cost to approximate relatively accurate Shapley values in real-world applications. Therefore, we propose a novel network architecture, the HarsanyiNet, which makes inferences on the input sample and simultaneously computes the exact Shapley values of the input variables in a single forward propagation. The HarsanyiNet is designed on the theoretical foundation that the Shapley value can be reformulated as the redistribution of Harsanyi interactions encoded by the network."}, "cited_paper_content": {"title": "Deep Residual Learning For Image Recognition", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."}, "keywords": ["adversarial examples"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve a deeper neural network with ease of optimization, the authors of Harsanyinet proposed a residual learning framework that reformulates the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed HarsanyiNet architecture achieves exact computation of Shapley values in a single forward propagation, which significantly reduces the computational cost compared to existing methods #REFR, as demonstrated by the experiments on CIFAR-10 dataset.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed HarsanyiNet architecture achieves robustness against adversarial examples while computing exact Shapley values in a single forward propagation #REFR.", "given_citation_intent": "result", "given_keywords": ["adversarial examples"]}]}
{"citing_id": "2304.00731v1", "cited_id": "1806.10574", "section_title": "Introduction", "citation": "For example, in image classification applications #REFR , a partial area of the image is enough for interpretation of the classification results, not specific to each feature (pixel).", "text_before_citation": ["Therefore, interpretable machine learning has attracted more and more attention in the past years.", "The usual way to obtain interpretability is to build a black-box model first and then explain its behavior using the post-hoc methods.", "However, many researchers are skeptical about the reliability of this direction. Considering interpretability before modeling is encouraged #OTHEREFR .", "Some works of interpretable machine learning tend to propose a general model or algorithm #OTHEREFR , but it is unknown whether that model meets the requirements in the actual application scenarios.", "In application scenarios with high requirements for interpretation, it is unrealistic to seek a one-for-all model or interpretation technology, because different application scenarios need different interpretations."], "text_after_citation": ["For healthcare and criminal justice, the traditional scorecard is more acceptable for practitioners #OTHEREFR , but for credit evaluation, while the rule-based interpretation may be more user-friendly.", "RRL (Rule Representation Learner) #OTHEREFR provides a rule representation and learning framework, which has performance advantages over traditional decision tree methods.", "So we design a credit evaluation model based on RRL. The main contributions of the paper are as follows:", "\u2022 Focusing on the credit evaluation, we designed an interpretable loan credit evaluation model based on RRL.", "Our model can naturally extract accurate global and local explanations without using the post-hoc methods."], "citing_paper_content": {"title": "An Interpretable Loan Credit Evaluation Method Based On Rule Representation Learner", "abstract": "The interpretability of model has become one of the obstacles to its wide application in the high-stake fields. The usual way to obtain interpretability is to build a black-box first and then explain it using the post-hoc methods. However, the explanations provided by the posthoc method are not always reliable. Instead, we design an intrinsically interpretable model based on RRL(Rule Representation Learner) for the Lending Club dataset. Specifically, features can be divided into three categories according to their characteristics of themselves and build three sub-networks respectively, each of which is similar to a neural network with a single hidden layer but can be equivalently converted into a set of rules. During the training, we learned tricks from previous research to effectively train binary weights. Finally, our model is compared with the tree-based model. The results show that our model is much better than the interpretable decision tree in performance and close to other blackbox, which is of practical significance to both financial institutions and borrowers. More importantly, our model is used to test the correctness of the explanations generated by the post-hoc method, the results show that the post-hoc method is not always reliable."}, "cited_paper_content": {"title": "This Looks Like That: Deep Learning For Interpretable Image Recognition", "abstract": "When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models."}, "keywords": ["classification results", "image classification applications"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Therefore, interpretable machine learning has attracted more and more attention in the past years. The usual way to obtain interpretability is to build a black-box model first and then explain its behavior using the post-hoc methods. However, many researchers are skeptical about the reliability of this direction. Considering interpretability before modeling is encouraged (This Looks Like That: Deep Learning For Interpretable Image Recognition). Some works of interpretable machine learning tend to propose a general model or algorithm #REFR, but it is unknown whether that model meets the requirements in the actual application scenarios. In application scenarios with high requirements for interpretation, it is unrealistic to seek a one-for-all model or interpretation technology, because different application scenarios need different interpretations.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Interpretable deep learning models have become increasingly popular in recent years due to their ability to provide transparent explanations for their decisions, and ProtoPNet is one such model that has been shown to achieve comparable accuracy with its non-interpretable counterpart while providing a level of interpretability that is absent in other interpretable deep models #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For image classification applications, deep learning has achieved remarkable classification results in recent years #REFR.", "given_citation_intent": "background", "given_keywords": ["classification results", "image classification applications"]}]}
{"citing_id": "2303.01150v1", "cited_id": "1705.08926", "section_title": "B. Algorithm", "citation": "To do this, we build upon the COMA RL algorithm of Foerster et al. #REFR to our IPP application for UAV teams.", "text_before_citation": ["Our goal is to learn a policy enabling cooperative UAV team behaviour for the adaptive monitoring task."], "text_after_citation": ["COMA is an actor-critic algorithm using a centralised critic network to evaluate each agent's behaviour, described by the policy \u03c0(\u2022 | \u03c9 t i ) and parameterised by an actor network, and to optimise the policy accordingly.", "The critic evaluates the current policy \u03c0 by estimating the agent's i state-action value", "Q \u03c0 s t , (u t 1 , . . . , u t i , . . . , u t N )", "given the other agents' actions u t \u2212i .", "The critic network is trained on-policy via TD(\u03bb) #OTHEREFR to estimate the discounted return G t , introduced in Sec."], "citing_paper_content": {"title": "Multi-Uav Adaptive Path Planning Using Deep Reinforcement Learning", "abstract": "Efficient aerial data collection is important in many remote sensing applications. In large-scale monitoring scenarios, deploying a team of unmanned aerial vehicles (UAVs) offers improved spatial coverage and robustness against individual failures. However, a key challenge is cooperative path planning for the UAVs to efficiently achieve a joint mission goal. We propose a novel multi-agent informative path planning approach based on deep reinforcement learning for adaptive terrain monitoring scenarios using UAV teams. We introduce new network feature representations to effectively learn path planning in a 3D workspace. By leveraging a counterfactual baseline, our approach explicitly addresses credit assignment to learn cooperative behaviour. Our experimental evaluation shows improved planning performance, i.e. maps regions of interest more quickly, with respect to non-counterfactual variants. Results on synthetic and real-world data show that our approach has superior performance compared to state-of-theart non-learning-based methods, while being transferable to varying team sizes and communication constraints."}, "cited_paper_content": {"title": "Counterfactual Multi-Agent Policy Gradients", "abstract": "Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state."}, "keywords": ["UAV teams"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our goal is to learn a policy enabling cooperative UAV team behaviour for the adaptive monitoring task. To this end, we propose a novel multi-agent informative path planning approach based on deep reinforcement learning, which leverages a counterfactual baseline to address credit assignment and achieves improved planning performance compared to non-counterfactual variants #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable cooperative behaviour in our UAV team, we propose a novel multi-agent informative path planning approach based on deep reinforcement learning, which leverages a counterfactual baseline to address credit assignment, and is inspired by the counterfactual multi-agent policy gradients method proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable cooperative behaviour in our multi-UAV adaptive path planning approach, we leverage a counterfactual baseline that marginalizes out a single agent's action, while keeping the other agents' actions fixed #REFR.", "given_citation_intent": "method", "given_keywords": ["UAV teams"]}]}
{"citing_id": "2305.02177v1", "cited_id": "1706.03762", "section_title": "Revisiting Of Transformer", "citation": "Note that in \"ED-ATT\", Q is set to the output of the former decoder block while K, V are set to the output of the visual encoder, and such operation is called encoder-decoder attention #REFR . After stacking 6 blocks defined in Eq. (4), a language decoder is built.", "text_before_citation": ["For the first block, its input is the extracted visual feature set of the given image.", "The output of the last block will be input into the language decoder.", "For the decoder, it also stacks 6 identical blocks where each one contains two MHAs and an FFN.", "Given the output of the former decoder block X D and the output of the visual encoder X E , the next decoder block calculates its output:", "EQUATION"], "text_after_citation": ["For the first block in the decoder, X D in Eq.", "(4) is set to the word embedding set of the partially generated captions S = {s 1 , ..., s t } at the t-th time step.", "For all the decoder blocks, the input X E is set to the same value, which is the output of the visual encoder.", "The output of the last decoder block Z = {z 1 , ..., z t } is used to calculate the word distribution of the next word:", "EQUATION"], "citing_paper_content": {"title": "Transforming Visual Scene Graphs To Image Captions", "abstract": "We propose to Transform Scene Graphs (TSG) into more descriptive captions. In TSG, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixtureof-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. Since both the encoder and decoder are built based on the MHA, as a result, we construct a homogeneous encoderdecoder unlike the previous heterogeneous ones which usually apply Fully-Connectedbased GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TSG. The code is in: https: //anonymous.4open.science/r/ACL23_TSG."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["encoder-decoder attention"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the decoder, it also stacks 6 identical blocks where each one contains two MHAs and an FFN, inspired by the Transformer model proposed in \"Attention Is All You Need\" #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To design the Graph Neural Network (GNN) for embedding scene graphs, multi-head attention (MHA) is applied, which is inspired by the attention mechanism proposed in the Transformer model #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To design the homogeneous encoder-decoder architecture, the authors utilized the encoder-decoder attention mechanism proposed in the reference paper \"Attention Is All You Need\" #REFR.", "given_citation_intent": "method", "given_keywords": ["encoder-decoder attention"]}]}
{"citing_id": "2303.11162v1", "cited_id": "1409.1556", "section_title": "Downstream Applications", "citation": "Here, we perform retrieval by first translating a query sketch into the photo domain, and then finding its nearest neighbourhood feature match in the entire photo gallery using an Im-ageNet pre-trained VGG-16 #REFR feature extractor.", "text_before_citation": ["Fine-Grained SBIR: Fine-grained SBIR aims at retrieving a particular image given a query sketch #OTHEREFR ."], "text_after_citation": ["Hence, we essentially convert the sketch-based retrieval task into an image-based retrieval task.", "As seen in Table 4 , our method beats SOTA FG-SBIR schemes #OTHEREFR in terms of Acc.@q, which measures the percentage of sketches having a true-paired photo in the top-q retrieved list.", "Precise Semantic Editing: Local semantic image editing is a popular application of GAN inversion #OTHEREFR .", "Our method enables realistic semantic editing, where modifying one region of an input sketch, yields seamless local alterations in the generated images. Fig.", "7 depicts one such sketch editing episode where the user gradually changes the heel length via sketch, to observe consistent local changes in the output photo domain."], "citing_paper_content": {"title": "Picture That Sketch: Photorealistic Image Generation From Abstract Sketches", "abstract": "Edgemap Sketch Sketch Sketch Sketch Sketch Existing Methods Proposed Method Figure 1. (a) Set of photos generated by the proposed method. (b) While existing methods can generate faithful photos from perfectly pixel-aligned edgemaps, they fall short drastically in case of highly deformed and sparse free-hand sketches. In contrast, our autoregressive sketch-to-photo generation model produces highly photorealistic outputs from highly abstract sketches."}, "cited_paper_content": {"title": "Very Deep Convolutional Networks For Large-Scale Image Recognition", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."}, "keywords": ["photo domain", "Im-ageNet pre-trained VGG-16"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Fine-grained SBIR: Fine-grained SBIR aims at retrieving a particular image given a query sketch. In this context, the use of very deep convolutional networks with small convolution filters has shown a significant improvement in accuracy for large-scale image recognition tasks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of generating photorealistic images from abstract sketches, the authors propose an autoregressive sketch-to-photo generation model that produces highly realistic outputs from highly abstract sketches #REFR. This method builds upon the findings of the reference paper, which showed that increasing the depth of convolutional networks can significantly improve their accuracy in large-scale image recognition.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate highly photorealistic outputs from abstract sketches in the photo domain, the authors of this manuscript could benefit from using the Im-ageNet pre-trained VGG-16 method proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["photo domain", "Im-ageNet pre-trained VGG-16"]}]}
{"citing_id": "2303.04995v2", "cited_id": "1708.01641", "section_title": "Introduction", "citation": "The architecture and performance comparison among TVG methods: a) 3D TVG methods [14, 16, 18, 34, 43, 60-62, 64, 67, 69, 71, 73] , b) 2D TVG methods #REFR 7] , and c) TVP-based 2D TVG (Ours), d) overall performance comparison.", "text_before_citation": ["In recent years, we have witnessed great progress on temporal video grounding (TVG) #OTHEREFR .", "One key to this success comes from the fine-grained dense 3D visual features extracted by 3D convolutional neural networks (CNNs) (e.g., C3D #OTHEREFR and I3D #OTHEREFR ) since TVG tasks demand spatial-temporal context to locate the temporal interval of the moments described by the text query.", "However, due to the high cost of the dense 3D feature extraction, most existing TVG models only take these 3D visual features ex- Figure 1 ."], "text_after_citation": ["Ours is the most efficient (least inference time) and achieves competitive performance compared to 3D TVG methods.", "In contrast to existing TVG methods, which utilize dense video features extracted by non-trainable offline 3D CNNs and textual features, our proposed framework utilizes a trainable 2D CNN as the vision encoder to extract features from sparsely-sampled video frames with a universal set of frame-aware visual prompts and adds text prompts in textual feature space for end-to-end regression-based modeling.", "tracted by offline 3D CNNs as inputs instead of co-training during TVG model training.", "Although models using 3D visual features (that we call '3D methods or models') outperform these using the 2D features (that we call '2D methods or models'), a unique advantage of 2D methods is that extracting 2D visual features can significantly reduce the cost in TVG tasks #OTHEREFR .", "An efficient and lightweight solution with reasonable performance is also demanded in computer vision, NLP, and video-language tasks #OTHEREFR ."], "citing_paper_content": {"title": "Text-Visual Prompting For Efficient 2D Temporal Video Grounding", "abstract": "In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments on two benchmark datasets, Charades-STA and Ac-tivityNet Captions datasets, empirically show that the proposed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on ActivityNet Captions) and achieves 5\u00d7 inference acceleration over TVG using 3D visual features. Codes are available at Open.Intel. \"Person pours water into a glass.\" Text Query Video 2D CNN Textual Features"}, "cited_paper_content": {"title": "Localizing Moments In Video With Natural Language", "abstract": "We consider retrieving a specific temporal segment, or moment, from a video given a natural language text description. Methods designed to retrieve whole video clips with natural language determine what occurs in a video but not when. To address this issue, we propose the Moment Context Network (MCN) which effectively localizes natural language queries in videos by integrating local and global video features over time. A key obstacle to training our MCN model is that current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions. We demonstrate that MCN outperforms several baseline methods and believe that our initial results together with the release of DiDeMo will inspire further research on localizing video moments with natural language."}, "keywords": ["a) 3D TVG"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One key to the success of temporal video grounding (TVG) tasks is the use of fine-grained dense 3D visual features extracted by 3D convolutional neural networks (CNNs) (e.g., C3D and I3D). However, due to the high cost of the dense 3D feature extraction, most existing TVG models only take these 3D visual features ex- Figure 1. To address this issue, the authors propose a novel text-visual prompting (TVP) framework that allows for the effective co-training of vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of localizing natural language queries in videos, the Moment Context Network (MCN) proposed in #REFR integrates local and global video features over time, outperforming several baseline methods.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the high cost of dense 3D feature extraction in TVG, the proposed text-visual prompting (TVP) framework in this paper effectively co-trains vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features, as opposed to the fine-grained dense 3D visual features used in most existing TVG models #REFR.", "given_citation_intent": "method", "given_keywords": ["a) 3D TVG"]}]}
{"citing_id": "2303.12253v1", "cited_id": "1706.07068", "section_title": "Related Work", "citation": "While these techniques allowed for generating images with creative and novel artistic styles #REFR , none provided significant affordances to end-users for controlling what was generated outside of the training data scope.", "text_before_citation": ["A particularly relevant variation of this architecture is the Creative Adversarial Network (CAN) from Elgammal et al.", "#OTHEREFR , designed to generate images with novel artistic styles.", "This artwork was subsequently featured in multiple exhibitions #OTHEREFR where human observers could not distinguish the CAN-generated art from human-authored artwork. Outside of GANs, Gatys et al.", "#OTHEREFR introduced a method to apply learned artistic styles to random images (a technique now known as Neural Style Transfer #OTHEREFR ).", "Additionally, work originally designed to make convolutional neural networks more explainable, now referred to as Deep Dreams, became popular for generating art #OTHEREFR due to its ability to generate psychedelic versions of images #OTHEREFR ."], "text_after_citation": ["Mansimov et al.", "#OTHEREFR addressed this issue by showing that a generative model could produce novel images from natural text when conditioned on image captions.", "As text-to-image models rely on language modeling techniques, recent advances in the scaling of large language models #OTHEREFR , and Imagen #OTHEREFR .", "Fueled by the latest advances in text-to-image models, current image generation applications are becoming mainstream.", "With this broader adoption comes the question of how these new models' capabilities impact art practices, which we examine in this paper."], "citing_paper_content": {"title": "The Prompt Artists", "abstract": "Fig. 1. Prompt artists develop descriptive text-based prompts that are rendered by text-to-image models. Highly skilled prompt artists will develop 1) distinct visual concepts and styles (S1), 2) prompts that can also serve as titles of the art piece (\"prompts as art\", A1), and 3) \"prompt templates\" (A2), which encapsulate specific visual concepts to be customized by others. Artists strive to discover unique natural language that produces unique visual outputs (G1), and/or model \"glitches\" (G2) that can be elevated to artistic styles in their own right. Finally, some prompt artists validate the novelty of their work by conducting an image search for similar images (C1). This paper examines the art practices, artwork, and motivations of prolific users of the latest generation of text-to-image models. Through interviews, observations, and a user survey, we present a sampling of the artistic styles and describe the developed community of practice around generative AI. We find that: 1) the text prompt and the resulting image can be considered collectively as an art piece (prompts as art), and 2) prompt templates (prompts with \"slots\" for others to fill in with their own words) are developed to create generative art styles. We discover that the value placed by this community on unique outputs leads to artists seeking specialized vocabulary to produce distinctive art pieces (e.g., by reading architectural blogs to find phrases to describe images). We also find that some artists use \"glitches\" in the model that can be turned into artistic styles of their own right. From these findings, we outline specific implications for design regarding future prompting and image editing options."}, "cited_paper_content": {"title": "Can: Creative Adversarial Networks, Generating\"Art\"By Learning About Styles And Deviating From Style Norms", "abstract": "We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales."}, "keywords": ["novel artistic styles"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A particularly relevant variation of this architecture is the Creative Adversarial Network (CAN) from Elgammal et al. (insert citation here), designed to generate images with novel artistic styles.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "A relevant approach to generating novel artistic styles is the Creative Adversarial Network (CAN) from Elgammal et al. #REFR, which has been shown to produce art that is indistinguishable from human-authored artwork by human observers in multiple exhibitions.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "A relevant architecture for generating images with novel artistic styles is the Creative Adversarial Network (CAN) from Elgammal et al., which was featured in multiple exhibitions where human observers could not distinguish the CAN-generated art from human-authored artwork. #REFR", "given_citation_intent": "background", "given_keywords": ["novel artistic styles"]}]}
{"citing_id": "2304.01064v1", "cited_id": "1809.00219", "section_title": "A. Implementation Details", "citation": "We use the implementation of Residual Dense Block from #REFR , where \"ResidualDenseBlock-32\" refers to a Residual Dense Block with minimum channel 32.", "text_before_citation": ["In this section, we describe the details of our architecture and training settings. Network Architecture.", "Table 5 and Table 6 respectively show details of our encoder and decoder.", "In Table 5 , we describe the architecture of our UNet-based encoder #OTHEREFR .", "\"PixelUnshuffle 4x\" stands for the rearrangement of elements #OTHEREFR which downsamples the HR image by a factor of 4.", "\"3\u00d73, 64, LeakyReLU\" denotes a 2d convolution operation of kernel size 3, output channel 64, followed by a LeakyReLU operation."], "text_after_citation": ["We save the produced quantization tables and output coefficients into a single JPEG file using TorchJPEG #OTHEREFR .", "Building blocks are shown in brackets, with the number of blocks stacked.", "Downsampling is performed at the beginning of the downsampling block using max pooling of stride 2.", "After the first convolution in the upsampling block, we upsample the decoder feature with Pixel Shuffling Operation. Then, skip connections with the encoder features are conducted.", "In Table 6 , we show the details of our efficient decoder, which is developed based on EDSR #OTHEREFR ."], "citing_paper_content": {"title": "Hyperthumbnail: Real-Time 6K Image Rescaling With Rate-Distortion Optimization", "abstract": "Contemporary image rescaling aims at embedding a high-resolution (HR) image into a low-resolution (LR) thumbnail image that contains embedded information for HR image reconstruction. Unlike traditional image superresolution, this enables high-fidelity HR image restoration faithful to the original one, given the embedded information in the LR thumbnail. However, state-of-the-art image rescaling methods do not optimize the LR image file size for efficient sharing and fall short of real-time performance for ultra-high-resolution (e.g., 6K) image reconstruction. To address these two challenges, we propose a novel framework (HyperThumbnail) for real-time 6K rate-distortionaware image rescaling. Our framework first embeds an HR image into a JPEG LR thumbnail by an encoder with our proposed quantization prediction module, which minimizes the file size of the embedding LR JPEG thumbnail while maximizing HR reconstruction quality. Then, an efficient frequency-aware decoder reconstructs a high-fidelity HR image from the LR one in real time. Extensive experiments demonstrate that our framework outperforms previous image rescaling baselines in rate-distortion performance and can perform 6K image reconstruction in real time."}, "cited_paper_content": {"title": "Esrgan: Enhanced Super-Resolution Generative Adversarial Networks", "abstract": "The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at this https URL ."}, "keywords": ["Residual Dense Block"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To further enhance the visual quality, the authors of HyperThumbnail propose a novel framework for real-time 6K rate-distortion-aware image rescaling that embeds a high-resolution image into a JPEG LR thumbnail by an encoder with a quantization prediction module, which minimizes the file size of the embedding LR JPEG thumbnail while maximizing HR reconstruction quality #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the quality of the reconstructed high-resolution image, the authors utilized the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit, as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the visual quality of the reconstructed high-resolution image, the authors utilized the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["Residual Dense Block"]}]}
{"citing_id": "2303.10232v1", "cited_id": "1501.00092", "section_title": "Qualitative Comparisons", "citation": "By comparison, the image generated by the Bicubic interpolation contains serious noises, while the image predicted by SRCNN #REFR is more blurry than others.", "text_before_citation": ["2 , there exist dozens of obvious reddish and blackish noisy points on the white vertical ridge of the building in images predicted by the Bicubic and CNN-based methods.", "In contrast, for those images generated by Transformer-based methods, the existence of such severe noisy points is much less frequent.", "Specifically, images predicted by SwinIR #OTHEREFR and LSwinSR contain only two and one such noisy points, respectively.", "With the increase of the upsample scale, the available information for accurate reconstruction becomes more and more scarce, leading to great challenges for super-resolution algorithms. For example, in the top part of Fig.", "4 , only SwinIR #OTHEREFR and the proposed LSwinSR provide smooth results for the text in the image when upsampling \u00d74 scale."], "text_after_citation": ["For LP-KPN #OTHEREFR and NLSA #OTHEREFR , the problem is the unnecessary wrinkles in the reconstructed texts.", "When it comes to \u00d78 super-resolution, the super-resolution algorithms can only rely on very limited input information. As shown in Fig.", "7 , the details of the building structure are nearly indistinguishable for all methods, but the Transformer-based methods still provide a clearer contour.", "As the proposed LSwinSR is an improved and simplified version of SwinIR #OTHEREFR , the complexity and the inference speed between the two models are compared.", "As can be seen in Table 2, the complexity and the inference speed of our LSwinSR are slightly better than SwinIR #OTHEREFR when the window size is eight."], "citing_paper_content": {"title": "Lswinsr: Uav Imagery Super-Resolution Based On Linear Swin Transformer", "abstract": "Super-resolution, which aims to reconstruct high-resolution images from low-resolution images, has drawn considerable attention and has been intensively studied in computer vision and remote sensing communities. The super-resolution technology is especially beneficial for Unmanned Aerial Vehicles (UAV), as the amount and resolution of images captured by UAV are highly limited by physical constraints such as flight altitude and load capacity. In the wake of the successful application of deep learning methods in the super-resolution task, in recent years, a series of super-resolution algorithms have been developed. In this paper, for the super-resolution of UAV images, a novel network based on the state-of-the-art Swin Transformer is proposed with better efficiency and competitive accuracy. Meanwhile, as one of the essential applications of the UAV is land cover and land use monitoring, simple image quality assessments such as the Peak-Signal-to-Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) are not enough to comprehensively measure the performance of an algorithm. Therefore, we further investigate the effectiveness of super-resolution methods using the accuracy of semantic segmentation. The code will be available at https://github.com/ lironui/GeoSR."}, "cited_paper_content": {"title": "Image Super-Resolution Using Deep Convolutional Networks", "abstract": "We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality."}, "keywords": ["SRCNN", "Bicubic interpolation"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the super-resolution of UAV images, a novel network based on the state-of-the-art Swin Transformer is proposed with better efficiency and competitive accuracy (Liu et al., 2021). Specifically, images predicted by SwinIR #OTHEREFR and LSwinSR contain only two and one such noisy points, respectively.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed LSwinSR method demonstrates superior performance in reducing severe noisy points in super-resolved UAV images compared to traditional sparse-coding-based SR methods and CNN-based methods (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Compared to the Bicubic interpolation and CNN-based methods, the LSwinSR algorithm proposed in this paper achieves better super-resolution results with fewer noisy points, which is consistent with the findings in the SRCNN paper #REFR.", "given_citation_intent": "result", "given_keywords": ["SRCNN", "Bicubic interpolation"]}]}
{"citing_id": "2304.03370v1", "cited_id": "1211.1082", "section_title": "Bounds On The P\u2192Q Disagreement Coefficient", "citation": "We can apply the whitening transform from Theorem 16 of #REFR provided (1/20 + c 1 ) 1/C 1 \u2212 c 2 1 \u2264 1/9, where C 1 = e \u03b21 log 2 (d+1) and c 1 = e(C 1 \u2212 1) \u221a 2C 1 .", "text_before_citation": ["R d .", "Then for 0 \u2264 \u03b2 1 , \u03b2 2 \u2264 1 56 log 2 (d+1) , we have \u0398 P\u2192Q (\u03b5) = O(d 1/2+ \u03b2 2 2 ln 2 log(d/\u03b5)).", "Proof.", "Our proof builds on and generalizes the arguments used in the proof of Theorem 14 in #OTHEREFR .", "Let h \u2208 B P (h * , r), i.e. d(h, h * ) \u2264 r."], "text_after_citation": ["It may be verified that this condition holds for 0 \u2264 \u03b2 1 \u2264 1 56 log 2 (d+1) .", "Now, by Theorem 11 of #OTHEREFR we can bound the angle between their normal vectors as \u03b8(w h , w h * ) \u2264 cr where c is an absolute constant.", "Now if x \u2208 X has a large margin |w h * \u2022 x| \u2265 cr\u03b1 and small norm ||x|| \u2264 \u03b1, for some \u03b1 > 0, we have", "|w h \u2022 x \u2212 w h * \u2022 x| \u2264 ||w h \u2212 w h * || \u2022 ||x|| < cr\u03b1.", "Now the large margin condition |w h * \u2022 x| \u2265 cr\u03b1 implies w h , x w h * , x > 0, or h(x) = h * (x). Since h \u2208 B P (h * , r)"], "citing_paper_content": {"title": "", "abstract": "Machine learning algorithms are often used in environments which are not captured accurately even by the most carefully obtained training data, either due to the possibility of 'adversarial' test-time attacks, or on account of 'natural' distribution shift. For test-time attacks, we introduce and analyze a novel robust reliability guarantee, which requires a learner to output predictions along with a reliability radius \u03b7, with the meaning that its prediction is guaranteed to be correct as long as the adversary has not perturbed the test point farther than a distance \u03b7. We provide learners that are optimal in the sense that they always output the best possible reliability radius on any test point, and we characterize the reliable region, i.e. the set of points where a given reliability radius is attainable. We additionally analyze reliable learners under distribution shift, where the test points may come from an arbitrary distribution Q different from the training distribution P. For both cases, we bound the probability mass of the reliable region for several interesting examples, for linear separators under nearly log-concave and s-concave distributions, as well as for smooth boundary classifiers under smooth probability distributions."}, "cited_paper_content": {"title": "Active And Passive Learning Of Linear Separators Under Log-Concave Distributions", "abstract": "We provide new results concerning label efficient, polynomial time, passive and active learning of linear separators. We prove that active learning provides an exponential improvement over PAC (passive) learning of homogeneous linear separators under nearly log-concave distributions. Building on this, we provide a computationally efficient PAC algorithm with optimal (up to a constant factor) sample complexity for such problems. This resolves an open question concerning the sample complexity of efficient PAC algorithms under the uniform distribution in the unit ball. Moreover, it provides the first bound for a polynomial-time PAC algorithm that is tight for an interesting infinite class of hypothesis functions under a general and natural class of data-distributions, providing significant progress towards a longstanding open question. ::: We also provide new bounds for active and passive learning in the case that the data might not be linearly separable, both in the agnostic case and and under the Tsybakov low-noise condition. To derive our results, we provide new structural results for (nearly) log-concave distributions, which might be of independent interest as well."}, "keywords": ["Theorem", "C 1"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To derive our results, we provide new structural results for (nearly) log-concave distributions, which might be of independent interest as well. Our proof builds on and generalizes the arguments used in the proof of Theorem 14 in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To derive their results, the authors rely on new structural results for nearly log-concave distributions, which might be of independent interest as well #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prove the theorem, the authors build on and generalize the arguments used in the proof of Theorem 14 in #REFR.", "given_citation_intent": "method", "given_keywords": ["Theorem", "C 1"]}]}
{"citing_id": "2304.13597v1", "cited_id": "1810.04805", "section_title": "Method", "citation": "We obtained embeddings of each target word in each of these contexts using a pretrained version of BERT (model version: bert-large-uncased; PyTorch Transformers library; Wolf et al., 2019; #REFR .", "text_before_citation": ["Starting from the sentence in which the target word occurs, we compared the length of the target sentence (current context) and the length of its immediate broader context extended to the sentence before and after (broader context), a total of three sentences in the first instance.", "We then determined whether the current context or the broader context was closest to our desired 100-word window.", "If the broader context was closest, then we repeated the process (adding an additional sentence before and after the current context) until the current context was closest to 100 words.", "This process resulted in a set of naturalistic contexts with varying lengths of approximately 100-words (M=99.94; SD=16.34).", "In total, we retrieved 2,589,921 contexts from the BNC (an average of 3,368 contexts per target word; see supplementary Table 1 in the OSF site for this project for distribution of contexts across conditions)."], "text_after_citation": ["To derive contextual embeddings for our target words, we submitted all 2,589,921 contexts (as defined above) as input to the pre-trained model.", "We then extracted multidimensional representations of each target word in each context (i.e.", "the contextual embeddings) from the final layer of the language model.", "Each of the resulting 2,589,921 contextual embeddings had 1,024 dimensions.", "We ran simulation analyses of lexical ambiguity studies #OTHEREFR to determine whether the ambiguity structure of target words in these studies was apparent in the sample of BERT contextual embeddings obtained for each of these target words."], "citing_paper_content": {"title": "Shades Of Meaning 1 Shades Of Meaning: Uncovering The Geometry Of Ambiguous Word Representations Through Contextualised Language Models", "abstract": "Lexical ambiguity presents a profound and enduring challenge to the language sciences. Researchers for decades have grappled with the problem of how language users learn, represent and process words with more than one meaning. Our work offers new insight into psychological understanding of lexical ambiguity through a series of simulations that capitalise on recent advances in contextual language models. These models have no grounded understanding of the meanings of words at all; they simply learn to predict words based on the surrounding context provided by other words. Yet, our analyses show that their representations capture fine-grained meaningful distinctions between unambiguous, homonymous, and polysemous words that align with lexicographic classifications and psychological theorising. These findings provide quantitative support for modern psychological conceptualisations of lexical ambiguity and raise new challenges for understanding of the way that contextual information shapes the meanings of words across different timescales."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["contexts", "embeddings"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Starting from the sentence in which the target word occurs, we compared the length of the target sentence (current context) and the length of its immediate broader context extended to the sentence before and after (broader context), a total of three sentences in the first instance. We then determined whether the current context or the broader context was closest to our desired 100-word window. If the broader context was closest, then we repeated the process (adding an additional sentence before and after the current context) until the current context was closest to 100 words. This process resulted in a set of naturalistic contexts with varying lengths of approximately 100-words (M=99.94; SD=16.34). In total, we retrieved 2,589,921 contexts from the BNC (an average of 3,368 contexts per target word; see supplementary Table 1 in the OSF site for this project for distribution of contexts across conditions) which was pre-processed using the BERT model #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train deep bidirectional representations from unlabeled text, BERT jointly conditions on both left and right context in all layers, which allows for state-of-the-art models for a wide range of tasks with just one additional output layer #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate embeddings for our naturalistic contexts, we utilized the pre-trained BERT model introduced by Devlin et al. (2018), which is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. #REFR", "given_citation_intent": "method", "given_keywords": ["contexts", "embeddings"]}]}
{"citing_id": "2303.11296v1", "cited_id": "1912.04958", "section_title": "Proposed Method", "citation": "In this section, we present our method for anonymizing the identity of faces in a given real face dataset by optimizing the representations of the dataset's images in the latent space of a pre-trained StyleGAN2 #REFR .", "text_before_citation": [], "text_after_citation": ["More specifically, given a real dataset X R , we first create a fake dataset X F by randomly generating a large set (i.e., such that |X F | > |X R |) of fake images and obtaining the corresponding latent codes in the W + space of StyleGAN2, namely, W + F .", "Additionally, we obtain the latent codes of the real dataset in the W + space by inverting its images using e4e #OTHEREFR , arriving at a set of latent codes W + R .", "In order to obtain meaningful initial values for the latent codes that will be optimized to create the anonymized version of the real dataset, namely X A , we first pair the real images from the original set (i.e., X R ) with fake ones from the generated dataset (i.e., X F ) in the feature space of the ViT-based FaRL #OTHEREFR image encoder and use their latent codes for initializing the aforementioned trainable codes.", "The latent codes of the anonymized dataset are then optimized under the following objectives via two novel loss functions: (a) to be similar to the corresponding real ones, up to a certain margin, using the proposed identity loss (L id ), and (b) to preserve the facial attributes of the corresponding real ones by being pulled closer in the Figure 2 .", "Overview of the proposed method: optimizing the trainable portion of the latent code w i\u00c3 \u2208 R 5\u00d7512 to obfuscate the identity of the resulting synthetic image x i A with L id whilst preserving the facial attributes with Latt."], "citing_paper_content": {"title": "Attribute-Preserving Face Dataset Anonymization Via Latent Code Optimization", "abstract": "This work addresses the problem of anonymizing the identity of faces in a dataset of images, such that the privacy of those depicted is not violated, while at the same time the dataset is useful for downstream task such as for training machine learning models. To the best of our knowledge, we are the first to explicitly address this issue and deal with two major drawbacks of the existing state-of-the-art approaches, namely that they (i) require the costly training of additional, purpose-trained neural networks, and/or (ii) fail to retain the facial attributes of the original images in the anonymized counterparts, the preservation of which is of paramount importance for their use in downstream tasks. We accordingly present a task-agnostic anonymization procedure that directly optimizes the images' latent representation in the latent space of a pre-trained GAN. By optimizing the latent codes directly, we ensure both that the identity is of a desired distance away from the original (with an identity obfuscation loss), whilst preserving the facial attributes (using a novel feature-matching loss in FaRL's [49] deep feature space). We demonstrate through a series of both qualitative and quantitative experiments that our method is capable of anonymizing the identity of the images whilstcrucially-better-preserving the facial attributes. We make the code and the pre-trained models publicly available at: https://github.com/chi0tzp/FALCO."}, "cited_paper_content": {"title": "Analyzing And Improving The Image Quality Of Stylegan", "abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality."}, "keywords": ["pre-trained StyleGAN2"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To ensure the preservation of facial attributes during dataset anonymization, the authors directly optimize the images' latent representation in the latent space of a pre-trained GAN, as presented in their task-agnostic anonymization procedure. This is achieved by using a novel feature-matching loss in FaRL's deep feature space and an identity obfuscation loss, resulting in better preservation of facial attributes compared to existing state-of-the-art approaches. The authors cite the use of a pre-trained GAN in their work, which is based on changes in both model architecture and training methods to address characteristic artifacts in the style-based GAN architecture, as proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the images' latent representation in the latent space of a pre-trained GAN, the authors directly optimized the latent codes, ensuring both identity obfuscation and preservation of facial attributes, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the images' latent representation in the latent space of a pre-trained GAN, the authors directly optimize the latent codes, ensuring that the identity is of a desired distance away from the original while preserving the facial attributes, as described in the pre-trained StyleGAN2-based method proposed by Karras et al. in \"Analyzing And Improving The Image Quality Of StyleGAN\" (#REFR).", "given_citation_intent": "method", "given_keywords": ["pre-trained StyleGAN2"]}]}
{"citing_id": "2303.12675v1", "cited_id": "1904.08921", "section_title": "Method Description", "citation": "Unlike most SDF-based methods such as DeepSDF #REFR which directly predicts the values of signed distance functions via neural networks, our model outputs the learnable parameters of curves that can be applied to handle complex glyph outlines containing concave curves. Fig. 2 shows the pipeline of our VecFontSDF.", "text_before_citation": ["In order to directly reconstruct and synthesize vector fonts from input glyph images via neural networks, we need a learnable shape representation that precisely preserves geometry information.", "Previous methods that fully rely on sequential generative models such as SVG-VAE #OTHEREFR and DeepVecFont #OTHEREFR fail to perfectly handle this task mainly due to the ambiguity of control point selection.", "To address this problem, we propose a vector graphic reconstructor based on an interpretable distance function with differentiable rendering."], "text_after_citation": ["For data preparation, we need to pre-calculate the values of signed distance functions (SDFs) for each input glyph using the method described in Sec. 3.1.", "We have two types of precalculated SDFs: grid SDFs and contour SDFs.", "Grid SDFs mean that the sampling points used to calculate SDFs are located at all grid positions (i.e., pixels in a raster image).", "Contour SDFs mean that the sampling points are uniformly distributed near the contours of the input glyph.", "Then, a CNN encoder is trained to extract the features from input raster images and an SDF decoder is followed to predict the parameters for every parabolic curve."], "citing_paper_content": {"title": "Vecfontsdf: Learning To Reconstruct And Synthesize High-Quality Vector Fonts Via Signed Distance Functions", "abstract": "Font design is of vital importance in the digital content design and modern printing industry. Developing algorithms capable of automatically synthesizing vector fonts can significantly facilitate the font design process. However, existing methods mainly concentrate on raster image generation, and only a few approaches can directly synthesize vector fonts. This paper proposes an end-to-end trainable method, VecFontSDF, to reconstruct and synthesize high-quality vector fonts using signed distance functions (SDFs). Specifically, based on the proposed SDFbased implicit shape representation, VecFontSDF learns to model each glyph as shape primitives enclosed by several parabolic curves, which can be precisely converted to quadratic B\u00e9zier curves that are widely used in vector font products. In this manner, most image generation methods can be easily extended to synthesize vector fonts. Qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality results on several tasks, including vector font reconstruction, interpolation, and few-shot vector font synthesis, markedly outperforming the state of the art."}, "cited_paper_content": {"title": "Deep Parametric Shape Predictions Using Distance Fields", "abstract": "Many tasks in graphics and vision demand machinery for converting shapes into consistent representations with sparse sets of parameters; these representations facilitate rendering, editing, and storage. When the source data is noisy or ambiguous, however, artists and engineers often manually construct such representations, a tedious and potentially time-consuming process. While advances in deep learning have been successfully applied to noisy geometric data, the task of generating parametric shapes has so far been difficult for these methods. Hence, we propose a new framework for predicting parametric shape primitives using deep learning. We use distance fields to transition between shape parameters like control points and input data on a pixel grid. We demonstrate efficacy on 2D and 3D tasks, including font vectorization and surface abstraction."}, "keywords": ["VecFontSDF", "learnable parameters"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the ambiguity of control point selection in previous methods for vector font reconstruction, the authors propose a vector graphic reconstructor based on an interpretable distance function with differentiable rendering #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of control point selection ambiguity in previous methods, the authors propose a vector graphic reconstructor based on an interpretable distance function with differentiable rendering, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the ambiguity of control point selection in previous methods, the authors propose a vector graphic reconstructor based on an interpretable distance function with differentiable rendering, which is a learnable parameter in their end-to-end trainable method, VecFontSDF. #REFR", "given_citation_intent": "method", "given_keywords": ["VecFontSDF", "learnable parameters"]}]}
{"citing_id": "2303.18084v1", "cited_id": "1904.00229", "section_title": "B. Superpoint Detection", "citation": "For each superpoint\u015d i , we construct a local patch G i using a point-to-node strategy #REFR .", "text_before_citation": ["In this paper, we supervise the superpoints to fall in the locally significant region, which may also lead to redundant proposals located in the same significant region.", "Thus, we use a simple radius search-based filtering strategy to force only one proposal in the same region.", "We iteratively perform a radius search for each proposal and filter out the ones close to the search center.", "After that, we obtain the final superpoints\u015c with associated features\u0124.", "Note that we limit the offsets \u2206P to a certain range, which maintains the superpoints to be evenly distributed throughout the point cloud instead of only concentrated in so-called significant areas. It also avoids possible degeneracy #OTHEREFR ."], "text_after_citation": ["Specifically, each point is assigned to its nearest superpoint by:", "EQUATION", "There are two advantages of this strategy.", "First, it assigns every point to a specific superpoint without duplication or loss.", "Second, it adapts to different densities, which is particularly suitable for our case since our superpoints break the uniformity of the original sampling after adding offsets."], "citing_paper_content": {"title": "Rdmnet: Reliable Dense Matching Based Point Cloud Registration For Autonomous Driving", "abstract": "Point cloud registration is an important task in robotics and autonomous driving to estimate the ego-motion of the vehicle. Recent advances following the coarse-to-fine manner show promising potential in point cloud registration. However, existing methods rely on good superpoint correspondences, which are hard to be obtained reliably and efficiently, thus resulting in less robust and accurate point cloud registration. In this paper, we propose a novel network, named RDMNet, to find dense point correspondences coarse-to-fine and improve final pose estimation based on such reliable correspondences. Our RDMNet uses a devised 3D-RoFormer mechanism to first extract distinctive superpoints and generates reliable superpoints matches between two point clouds. The proposed 3D-RoFormer fuses 3D position information into the transformer network, efficiently exploiting point clouds' contextual and geometric information to generate robust superpoint correspondences. RDMNet then propagates the sparse superpoints matches to dense point matches using the neighborhood information for accurate point cloud registration. We extensively evaluate our method on multiple datasets from different environments. The experimental results demonstrate that our method outperforms existing state-of-the-art approaches in all tested datasets with a strong generalization ability."}, "cited_paper_content": {"title": "Usip: Unsupervised Stable Interest Point Detection From 3D Point Clouds", "abstract": "In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis of our USIP detector and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. this https URL"}, "keywords": ["superpoint\u015d"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our proposed method outperforms existing state-of-the-art approaches in all tested datasets with a strong generalization ability #REFR, which is consistent with the findings of the authors of the reference paper who also demonstrated the superior performance of their USIP detector on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect stable keypoints from 3D point clouds, the authors propose an unsupervised method called USIP detector, which outperforms existing hand-crafted and deep learning-based 3D keypoint detectors according to extensive experimental results on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D, and CAD models #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We use a simple radius search-based filtering strategy to force only one proposal in the same region, which ensures the superpoints\u015c to be evenly distributed throughout the point cloud instead of only concentrated in so-called significant areas, as proposed in the USIP detector #REFR.", "given_citation_intent": "method", "given_keywords": ["superpoint\u015d"]}]}
{"citing_id": "2303.10056v1", "cited_id": "1803.08664", "section_title": "Gluenet + Stable Diffusion", "citation": "This new ca- #REFR The cost of training a Stable Diffusion model is around 600K USD.", "text_before_citation": ["Our model showed competitive improvements in FID score and user study ranking compared to the baselines but it still required finetuning for the overall performance boost.", "Secondly, we aligned a multilingual language model, XLM-Roberta-L #OTHEREFR , using our approach, enabling multilingual text-to-image generation.", "It achieved competitive results of translation-based models under a significantly lower training cost.", "Finally, we demonstrated GlueNet's capability to bring new functionalities beyond text signals into existing T2I models.", "The alignment of the AudioClip #OTHEREFR encoder enables sound-to-image generation without requiring any parameter finetuning of the image generator."], "text_after_citation": ["pability allows the existing Stable Diffusion model to generate high-quality images that correspond to sound signals such as dogs barking and street music.", "This new capability goes beyond the traditional T2I generation and opens up new possibilities for creating multimedia content towards X-to-Image (X2I) generation.", "Our contributions can be summarized as follows:", "\u2022 To the best of our knowledge, this is the first work to consider the problem of efficiently aligning a pretrained audio model with a pre-trained T2I diffusion model for sound-to-image generation.", "\u2022 Extensive experiments on text-to-image generation benchmarks demonstrate the superiority of our model over the baseline LDM method on both image quality and language controllability."], "citing_paper_content": {"title": "Gluegen: Plug And Play Multi-Modal Encoders For X-To-Image Generation", "abstract": "Text-to-image (T2I) models based on diffusion processes have achieved remarkable success in controllable image generation using user-provided captions. However, the tight coupling between the current text encoder and image decoder in T2I models makes it challenging to replace or upgrade. Such changes often require massive fine-tuning or even training from scratch with the prohibitive expense. To address this problem, we propose GlueGen, which applies a newly proposed GlueNet model to align features from single-modal or multi-modal encoders with the latent space of an existing T2I model. The approach introduces a new training objective that leverages parallel corpora to align the representation spaces of different encoders. Empirical results show that GlueNet can be trained efficiently and enables various capabilities beyond previous state-of-the-art models: 1) multilingual language models such as XLM-Roberta can be aligned with existing T2I models, allowing for the generation of high-quality images from captions beyond English; 2) GlueNet can align multi-modal encoders such as AudioCLIP with the Stable Diffusion model, enabling sound-to-image generation; 3) it can also upgrade the current text encoder of the latent diffusion model for challenging case generation. By the alignment of various feature representations, the GlueNet allows for flexible and efficient integration of new functionality into existing T2I models and sheds light on X-to-image (X2I) generation. 1 * This work was done when Can Qin interned at Salesforce AI Research."}, "cited_paper_content": {"title": "Fast, Accurate, And Lightweight Super-Resolution With Cascading Residual Network", "abstract": "In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods."}, "keywords": ["Stable Diffusion model"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our model showed competitive improvements in FID score and user study ranking compared to the baselines but it still required finetuning for the overall performance boost. Secondly, we aligned a multilingual language model, XLM-Roberta-L #OTHEREFR , using our approach, enabling multilingual text-to-image generation. It achieved competitive results of translation-based models under a significantly lower training cost. Finally, we demonstrated GlueNet's capability to bring new functionalities beyond text signals into existing T2I models. The alignment of the AudioClip #OTHEREFR encoder enables sound-to-image generation without requiring any parameter finetuning of the image generator (Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network #REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of heavy computation in deep learning methods for image super-resolution, the authors proposed an accurate and lightweight deep network with a cascading mechanism upon a residual network, achieving performance comparable to that of state-of-the-art methods with much fewer parameters and operations #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable sound-to-image generation without requiring any parameter finetuning of the image generator, the authors aligned the AudioClip encoder with the Stable Diffusion model #REFR.", "given_citation_intent": "background", "given_keywords": ["Stable Diffusion model"]}]}
{"citing_id": "2304.02172v1", "cited_id": "1909.03382", "section_title": "Indefinite Defense", "citation": "Empirically, we found that for all strongly-connected and undirected graphs, the iterative algorithm in #REFR converges within N iterations, where N = |V| is the number of nodes.", "text_before_citation": ["x t+1 \u2208 Q (j) \u221e that is reachable from x t . Since Q (j)", "\u221e \u2286 P req (y (j) ), the defender can also defend the time step t + 1.", "Through mathematical induction, one can easily argue that being in Q (i) \u221e when y t\u22121 = y (i) guarantees indefinite defense for all no-splitting attacker strategies.", "The conditions on the graph that guarantee convergence of the iterative algorithm in #OTHEREFR as well as the conditions for the existence of such fixed point(s) is an ongoing research.", "Note that not all graphs have such a fixed point, for example, a sink graph (see Figure 13 in Appendix A) does not have one, since it requires infinite defender resources to guard indefinitely."], "text_after_citation": ["A follow-up work would be to establish the convergence guarantees."], "citing_paper_content": {"title": "Dynamic Adversarial Resource Allocation: The Ddab Game", "abstract": "This work proposes a dynamic and adversarial resource allocation problem in a graph environment, which is referred to as the dynamic Defender-Attacker Blotto (dDAB) game. A team of defender robots is tasked to ensure numerical advantage at every node in the graph against a team of attacker robots. The engagement is formulated as a discrete-time dynamic game, where the two teams reallocate their robots in sequence and each robot can move at most one hop at each time step. The game terminates with the attacker's victory if any node has more attacker robots than defender robots. Our goal is to identify the necessary and sufficient number of defender robots to guarantee defense. Through a reachability analysis, we first solve the problem for the case where the attacker team stays as a single group. The results are then generalized to the case where the attacker team can freely split and merge into subteams. Crucially, our analysis indicates that there is no incentive for the attacker team to split, which significantly reduces the search space for the attacker's winning strategies and also enables us to design defender counter-strategies using superposition. We also present an efficient numerical algorithm to identify the necessary and sufficient number of defender robots to defend a given graph. Finally, we present illustrative examples to verify the efficacy of the proposed framework."}, "cited_paper_content": {"title": "Characterizing The Interplay Between Information And Strength In Blotto Games", "abstract": "In this paper, we investigate informational asymmetries in the Colonel Blotto game, a game-theoretic model of competitive resource allocation between two players over a set of battlefields. The battlefield valuations are subject to randomness. One of the two players knows the valuations with certainty. The other knows only a distribution on the battlefield realizations. However, the informed player has fewer resources to allocate. We characterize unique equilibrium payoffs in a two battlefield setup of the Colonel Blotto game. We then focus on a three battlefield setup in the General Lotto game, a popular variant of the Colonel Blotto game. We characterize the unique equilibrium payoffs and mixed equilibrium strategies. We quantify the value of information - the difference in equilibrium payoff between the asymmetric information game and complete information game. We find information strictly improves the informed player's performance guarantee. However, the magnitude of improvement varies with the informed player's strength as well as the game parameters. Our analysis highlights the interplay between strength and information in adversarial environments."}, "keywords": ["undirected graphs"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors investigate informational asymmetries in the Colonel Blotto game, a game-theoretic model of competitive resource allocation between two players over a set of battlefields, and quantify the value of information - the difference in equilibrium payoff between the asymmetric information game and complete information game (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Colonel Blotto game is a game-theoretic model of competitive resource allocation between two players over a set of battlefields, where the informed player has fewer resources to allocate, and the value of information strictly improves the informed player's performance guarantee, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The conditions on the graph that guarantee convergence of the iterative algorithm in #OTHEREFR as well as the conditions for the existence of such fixed point(s) is an ongoing research, and undirected graphs are among the graphs that have been studied in the literature. #REFR", "given_citation_intent": "background", "given_keywords": ["undirected graphs"]}]}
{"citing_id": "2304.03147v1", "cited_id": "1811.09716", "section_title": "Robustness Of Neural Network Models", "citation": "The authors of #REFR provide theoretical evidence for a strong relationship between small curvature and large robustness, proposing an efficient regularizer that encourages small curvatures and leads to significant boosts in neural network robustness.", "text_before_citation": ["Several recent works (e.g., [8, 12, 15, 19, 21-24, 31, 33, 36, 37, 50, 79, 81, 83] ) have explored the issue of deep learning model robustness from an image or text perspective.", "In #OTHEREFR , the authors analyze model robustness by adding noise or perturbations to images and observing their impact on predicted results."], "text_after_citation": ["While most existing works focus on adding noise to the image input, our work instead focuses on adding noise to the text input #OTHEREFR .", "Specifically, we consider the semantically related BQs of a given MQ as a type of noise for the MQ, using these BQs to evaluate the robustness of VQA models."], "citing_paper_content": {"title": "Improving Visual Question Answering Models Through Robustness Analysis And In-Context Learning With A Chain Of Basic Questions", "abstract": "Deep neural networks have been critical in the task of Visual Question Answering (VQA), with research traditionally focused on improving model accuracy. Recently, however, there has been a trend towards evaluating the robustness of these models against adversarial attacks. This involves assessing the accuracy of VQA models under increasing levels of noise in the input, which can target either the image or the proposed query question, dubbed the main question. However, there is currently a lack of proper analysis of this aspect of VQA. This work proposes a new method that utilizes semantically related questions, referred to as basic questions, acting as noise to evaluate the robustness of VQA models. It is hypothesized that as the similarity of a basic question to the main question decreases, the level of noise increases. To generate a reasonable noise level for a given main question, a pool of basic questions is ranked based on their similarity to the main question, and this ranking problem is cast as a optimization problem. Additionally, this work proposes a novel robustness measure, , and two basic question datasets to standardize the analysis of VQA model robustness. The experimental results demonstrate that the proposed evaluation method effectively analyzes the robustness of VQA models. Moreover, the experiments show that in-context learning with a chain of basic questions can enhance model accuracy."}, "cited_paper_content": {"title": "Robustness Via Curvature Regularization, And Vice Versa", "abstract": "State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more \"linear\" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness."}, "keywords": ["neural network robustness"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Several recent works (e.g., [8, 12, 15, 19, 21-24, 31, 33, 36, 37, 50, 79, 81, 83]) have explored the issue of deep learning model robustness from an image or text perspective. In their paper \"Robustness Via Curvature Regularization, And Vice Versa\", the authors investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries, and propose a new regularizer that directly minimizes curvature of the loss surface to achieve robustness. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In their investigation of deep learning model robustness, the authors of this manuscript add noise or perturbations to images and observe their impact on predicted results, building on the work of several recent studies (e.g., [8, 12, 15, 19, 21-24, 31, 33, 36, 37, 50, 79, 81, 83]). Specifically, they propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training, as described in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Several recent works (e.g., [8, 12, 15, 19, 21-24, 31, 33, 36, 37, 50, 79, 81, 83]) have explored the issue of neural network robustness from an image or text perspective. In particular, the proposed method in this manuscript evaluates the robustness of Visual Question Answering models using semantically related questions as noise, which is similar to the approach taken in the curvature regularization method proposed in #REFR.", "given_citation_intent": "background", "given_keywords": ["neural network robustness"]}]}
{"citing_id": "2304.12303v1", "cited_id": "math/0610459", "section_title": "A.1 Planar Graphs", "citation": "Note that G(n, p) may not be an \u03b1-expander for fixed \u03b1 > 0: If edge probability p = (1 + )/n, then G(n, p) contains an induced path of length \u2126(log n) with high probability #REFR .", "text_before_citation": ["Indeed, initialize S as the empty set and keep adding components S i (that were not added already) until S has size at least (1/2 \u2212 )n.", "Note that S has cardinality no larger than n/2 and has at most \u03b1n/4 neighbors.", "For sufficiently small, this is a contradiction to G being an \u03b1-expander.", "Remark 4.", "Observe that the assertion in Lemma 4 applies also if G contains a sub-graph with \u2126(n) nodes which is an \u03b1-expander. Proof of Theorem 11."], "text_after_citation": ["Nevertheless, it is known that if p \u2265 (1 + )/n, then G(n, p) contains an \u03b1-expander with \u2126(n) nodes with high probability (see #OTHEREFR ). Lemmas 4 and 5 complete the proof.", "\u2020 Observe that as C, L > 0 the cost is always strictly positive."], "citing_paper_content": {"title": "Inoculation Strategies For Bounded Degree Graphs", "abstract": "We analyze a game-theoretic abstraction of epidemic containment played on an undirected graph G: each player is associated with a node in G and can either acquire protection from a contagious process or risk infection. After decisions are made, an infection starts at a random node v and propagates through all unprotected nodes reachable from v. It is known that the price of anarchy (PoA) in n-node graphs can be as large as \u0398(n). Our main result is a tight bound of order \u221a n\u2206 on the PoA, where \u2206 is the maximum degree of the graph. We also study additional factors that can reduce the PoA, such as higher thresholds for contagion and varying the costs of becoming infected vs. acquiring protection."}, "cited_paper_content": {"title": "The Mixing Time Of The Giant Component Of A Random Graph", "abstract": "We show that the total variation mixing time of the simple random walk on the giant component of supercritical Gn,p and Gn,m is i\u00be?log2n. This statement was proved, independently, by Fountoulakis and Reed. Our proof follows from a structure result for these graphs which is interesting in its own right. We show that these graphs are \"decorated expanders\" - an expander glued to graphs whose size has constant expectation and exponential tail, and such that each vertex in the expander is glued to no more than a constant number of decorations. \u00a9 2014 Wiley Periodicals, Inc. Random Struct. Alg., 45, 383-407, 2014"}, "keywords": ["\u03b1-expander", "length \u2126(log"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For sufficiently small, this is a contradiction to G being an \u03b1-expander (see #REFR). Remark 4. Observe that the assertion in Lemma 4 applies also if G contains a sub-graph with \u2126(n) nodes which is an \u03b1-expander.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This result is consistent with the findings of Fountoulakis and Reed who showed that the total variation mixing time of the simple random walk on the giant component of supercritical Gn,p and Gn,m is i\u00be?log2n #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This contradicts the fact that G is an \u03b1-expander, as shown in #REFR, where the authors proved that the total variation mixing time of the simple random walk on the giant component of supercritical Gn,p and Gn,m is of length \u2126(log n).", "given_citation_intent": "background", "given_keywords": ["\u03b1-expander", "length \u2126(log"]}]}
{"citing_id": "2305.01360v1", "cited_id": "1801.03924", "section_title": "Experimental Setup", "citation": "Moreover, we also calculate LPIPS #REFR , a popular deep-learning-based similarity metric for a more comprehensive evaluation.", "text_before_citation": ["To reduce computational consumption while maintaining image content as much as possible, we decrease the latter size to 49 \u00d7 288 \u00d7 288 by cropping.", "Compared Methods: We compare our model with three SR methods: #OTHEREFR Cubic interpolation, a mathematical method based on cubic polynomials; (2) IREM, a self-supervised SR method based on INR.", "(3) SMORE #OTHEREFR , a deep learning method based on self super-resolution (SRR) #OTHEREFR .", "Here Cubic interpolation is based on the scikit-image library of Python, while IREM #OTHEREFR and SMORE #OTHEREFR are reproduced following the original papers.", "Evaluation Metrics: PSNR and SSIM #OTHEREFR , two widely-used image quality evaluation metrics, are computed."], "text_after_citation": ["The slice-by-slice strategy is employed to calculate LPIPS #OTHEREFR since it is designed for 2D images.", "Specifically, we first extract the most central 30 2D slices from SR volumes along three orthogonal views (i.e., ten images per view).", "Then, we calculate LPIPS #OTHEREFR based on the 2D slices and average them to obtain the final scores."], "citing_paper_content": {"title": "Self-Supervised Arbitrary Scale Super-Resolution Framework For Anisotropic Mri", "abstract": "In this paper, we propose an efficient self-supervised arbitraryscale super-resolution (SR) framework to reconstruct isotropic magnetic resonance (MR) images from anisotropic MRI inputs without involving external training data. The proposed framework builds a training dataset using \"in-the-wild\" anisotropic MR volumes with arbitrary image resolution. We then formulate the 3D volume SR task as a SR problem for 2D image slices. The anisotropic volume's high-resolution (HR) plane is used to build the HR-LR image pairs for model training. We further adapt the implicit neural representation (INR) network to implement the 2D arbitrary-scale image SR model. Finally, we leverage the welltrained proposed model to up-sample the 2D LR plane extracted from the anisotropic MR volumes to their HR views. The isotropic MR volumes thus can be reconstructed by stacking and averaging the generated HR slices. Our proposed framework has two major advantages: (1) It only involves the arbitrary-resolution anisotropic MR volumes, which greatly improves the model practicality in real MR imaging scenarios (e.g., clinical brain image acquisition); (2) The INR-based SR model enables arbitrary-scale image SR from the arbitrary-resolution input image, which significantly improves model training efficiency. We perform experiments on a simulated public adult brain dataset and a real collected 7T brain dataset. The results indicate that our current framework greatly outperforms two well-known self-supervised models for anisotropic MR image SR tasks."}, "cited_paper_content": {"title": "The Unreasonable Effectiveness Of Deep Features As A Perceptual Metric", "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called\"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations."}, "keywords": ["popular deep-learning-based similarity"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate the image quality of our proposed framework, we computed two widely-used image quality evaluation metrics, PSNR and SSIM, and compared our results with those of previous methods. Specifically, we compared our model with three SR methods: Cubic interpolation, IREM, and SMORE. Cubic interpolation is based on the scikit-image library of Python, while IREM and SMORE are reproduced following the original papers. However, these widely-used metrics fail to account for many nuances of human perception. To address this issue, we suggest using perceptual losses based on deep features, as shown in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compare our model with three SR methods: #OTHEREFR Cubic interpolation, a mathematical method based on cubic polynomials; (2) IREM, a self-supervised SR method based on INR. (3) SMORE #OTHEREFR , a deep learning method based on self super-resolution (SRR) #OTHEREFR . Here Cubic interpolation is based on the scikit-image library of Python, while IREM #OTHEREFR and SMORE #OTHEREFR are reproduced following the original papers. Evaluation Metrics: PSNR and SSIM #OTHEREFR , two widely-used image quality evaluation metrics, are computed. Our proposed method outperforms all three methods by a significant margin, demonstrating the effectiveness of deep features as a perceptual metric for image super-resolution tasks. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of our proposed self-supervised arbitrary-scale super-resolution framework, we compared it with several other methods, including popular deep-learning-based similarity metrics #REFR, and found that our framework outperforms them by a significant margin.", "given_citation_intent": "method", "given_keywords": ["popular deep-learning-based similarity"]}]}
{"citing_id": "2304.09649v1", "cited_id": "1710.05370", "section_title": "Datasets", "citation": "The texts are a subset of the NoReC dataset #REFR , a multi-domain dataset of full-text professional reviews published in Norwegian online news sources.", "text_before_citation": ["NorQuAD A Norwegian question answering dataset for machine reading comprehension #OTHEREFR based on the SQuAD format #OTHEREFR .", "For a given question, the model must predict the correct span in a provided passage that answers the question.", "NorQuAD includes three domain splits: one sourced from the Norwegian Wikipedia (N = 2351), one from Norwegian news articles (N = 2398), and one split that combines both of them (N = 4749).", "We use an 80 \u2212 10 \u2212 10 split on all three domains for training, validation, and testing.", "NoReC fine A fine-grained sentiment analysis dataset for Norwegian (\u00d8vrelid et al., 2020) ."], "text_after_citation": ["Each sentence in NoReC fine is annotated for sentiment holders, targets, polar expressions, expression polarities, and polar intensities.", "A version for targeted sentiment analysis (TSA) is released on GitHub where only the sentiment targets are labeled. 4", "NoReC sent A sentence-level sentiment analysis dataset for Norwegian derived from NoReC fine #OTHEREFR .", "This dataset is generated by aggregating the entity sentiments in each sentence.", "The sentences are then labeled as either positive, negative, or neutral."], "citing_paper_content": {"title": "Brent: Bidirectional Retrieval Enhanced Norwegian Transformer", "abstract": "Retrieval-based language models are increasingly employed in questionanswering tasks. These models search in a corpus of documents for relevant information instead of having all factual knowledge stored in its parameters, thereby enhancing efficiency, transparency, and adaptability. We develop the first Norwegian retrieval-based model by adapting the REALM framework and evaluate it on various tasks. After training, we also separate the language model, which we call the reader, from the retriever components, and show that this can be fine-tuned on a range of downstream tasks. Results show that retrieval augmented language modeling improves the reader's performance on extractive question-answering, suggesting that this type of training improves language models' general ability to use context and that this does not happen at the expense of other abilities such as part-of-speech tagging, dependency parsing, named entity recognition, and lemmatization. Code, trained models, and data are made publicly available. 1 * The authors contributed equally to this work 1 https://github.com/ltgoslo/brent query Gauss regnet ut den n\u00f8yaktige posisjonen hvor en kunne forvente \u00e5 observere [MASK] igjen MLM Reader Database query encoder documents Retriever Allerede som barn viste Gauss seg som svaert begavet ... ledet Gauss til studiet av dvergplaneters bevegelse Gauss ble professor i astronomi i 1807 og var direkt\u00f8r for observatoriet i."}, "cited_paper_content": {"title": "Norec: The Norwegian Review Corpus", "abstract": "This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment analysis. The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1-6, as provided by the rating of the original author. This first release of the corpus comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for Norwegian Text), a project seeking to provide resources and tools for sentiment analysis and opinion mining for Norwegian. As resources for sentiment analysis have so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology."}, "keywords": ["Norwegian online news"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "NoReC, a Norwegian Review Corpus, was created for training and evaluating models for document-level sentiment analysis, and is a highly valuable addition to Norwegian language technology (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train their retrieval-based model, the authors used the Norwegian Review Corpus (NoReC) #REFR, a valuable resource for sentiment analysis and opinion mining for Norwegian, which comprises more than 35,000 reviews from various domains and is distributed using the CoNLL-U format.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train and evaluate models for document-level sentiment analysis on Norwegian online news, the authors used the Norwegian Review Corpus (NoReC) dataset, which comprises more than 35,000 reviews from major Norwegian news sources covering various domains, including literature, movies, video games, restaurants, music, and theater, as well as product reviews across a range of categories (#REFR).", "given_citation_intent": "method", "given_keywords": ["Norwegian online news"]}]}
{"citing_id": "2304.05051v1", "cited_id": "1806.08317", "section_title": "Downstream Tasks And Results", "citation": "Moreover, the differences between the results of our model and others are significant. Table 5 . CR and SCR results on FashionGen #REFR .", "text_before_citation": ["FashionSAP gets the SOTA performance as the comparable results shown in Tab. 2.", "We report the average result of 5 randomly chosen retrieval test sets and each of them contains 1k queries by following previous works.", "For each query in test sets, only one candidate is matched (positive), while the other 100 candidates are mismatched (negative) and chosen from the same subcategory.", "For the T2I task, there are 101 candidate images for each query text, and only one image in candidates is matched.", "In order to test the performance of our model thoroughly, we also evaluate our model in the full test set of FashionGen #OTHEREFR in Tab. 3 following #OTHEREFR . Our model also gets the SOTA performance."], "text_after_citation": [], "citing_paper_content": {"title": "Fashionsap: Symbols And Attributes Prompt For Fine-Grained Fashion Vision-Language Pre-Training", "abstract": "Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics. Firstly, we propose the fashion symbols, a novel abstract fashion concept layer, to represent different fashion items and to generalize various kinds of finegrained fashion features, making modelling fine-grained attributes more effective. Secondly, the attributes prompt method is proposed to make the model learn specific attributes of fashion items explicitly. We design proper prompt templates according to the format of fashion data. Comprehensive experiments are conducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, and Fash-ionSAP gets SOTA performances for four popular fashion tasks. The ablation study also shows the proposed abstract fashion symbols, and the attribute prompt method enables the model to acquire fine-grained semantics in the fashion domain effectively. The obvious performance gains from FashionSAP provide a new baseline for future fashion task research."}, "cited_paper_content": {"title": "Fashion-Gen: The Generative Fashion Dataset And Challenge", "abstract": "We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles. We provide baseline results on 1) high-resolution image generation, and 2) image generation conditioned on the given text descriptions. We invite the community to improve upon these baselines. In this paper, we also outline the details of a challenge that we are launching based upon this dataset."}, "keywords": ["FashionGen"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve the SOTA performance on the FashionGen dataset, the authors of Fashionsap utilized a novel abstract fashion concept layer and attribute prompt method, as reported in Tab. 3 following #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our model achieves state-of-the-art performance on the FashionGen dataset, outperforming the previous baseline by a significant margin (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our model achieves the SOTA performance on the full test set of FashionGen, as reported in Tab. 3 #REFR.", "given_citation_intent": "result", "given_keywords": ["FashionGen"]}]}
{"citing_id": "2303.03508v1", "cited_id": "1811.05340", "section_title": "Ii. Related Work", "citation": "Most tracking-based networks first do single-frame object detection and use the association for the detection reciprocally #REFR .", "text_before_citation": ["While memory networks, such as STDnet-ST #OTHEREFR , achieve high accuracies, their heavy architectures prohibit their deployment on embedded devices.", "Optical flow-based networks are faster, but their benefit over single-frame object detectors is limited #OTHEREFR . Figure 2 .", "Here, we have that h = 20m and \u03b2 = 15 \u2022 resulting in y = 110m and x = 25m.", "This means, that the swimmer is 110m in front and 25m to the right of the UAV as measured on the ground.", "Note that we also estimate the horizon line as indicated by the blue line."], "text_after_citation": ["In fact, the best two VOD models of the last VisDrone-VID challenge were single-frame methods, entirely ignoring the temporal domain #OTHEREFR .", "Furthermore, all common video object detectors on UAVs ignore the underlying 3D geometry of the scene and only operate in image space #OTHEREFR .", "While there is much research focusing on geolocation, it is only focused on obtaining world coordinates of objects for downstream tasks, such as following a target #OTHEREFR .", "Similarly, occupancy networks and other mapping approaches aim at obtaining a map for mapping or scene understanding #OTHEREFR .", "As opposed to these works, we aim to leverage metadata to build a memory map in GPS space that improves the video object detection performance."], "citing_paper_content": {"title": "Memory Maps For Video Object Detection And Tracking On Uavs", "abstract": "This paper introduces a novel approach to video object detection detection and tracking on Unmanned Aerial Vehicles (UAVs). By incorporating metadata, the proposed approach creates a memory map of object locations in actual world coordinates, providing a more robust and interpretable representation of object locations in both, image space and the real world. We use this representation to boost confidences, resulting in improved performance for several temporal computer vision tasks, such as video object detection, short and long-term single and multi-object tracking, and video anomaly detection. These findings confirm the benefits of metadata in enhancing the capabilities of UAVs in the field of temporal computer vision and pave the way for further advancements in this area."}, "cited_paper_content": {"title": "Detect Or Track: Towards Cost-Effective Video Object Detection/Tracking", "abstract": "State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised \u2013 how to improve the accuracy of video object detection/tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping \u2013 detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection/tracking."}, "keywords": ["single-frame object detection"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To improve the accuracy of video object detection/tracking within a given time budget, a scheduler network has been proposed as a generalization of Siamese trackers, which determines to detect or track at a certain frame, and has been shown to outperform the frame skipping baselines and flow-based approaches on the ImageNet VID dataset #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the accuracy of video object detection/tracking while utilizing existing detectors and trackers within a given time budget, a scheduler network has been proposed as a generalization of Siamese trackers, which determines to detect or track at a certain frame, and has been shown to outperform frame skipping baselines and flow-based approaches on the ImageNet VID dataset #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Single-frame object detection can be limited in its benefits, as shown in previous studies #REFR, and thus alternative approaches are needed to improve the performance of video object detection and tracking on UAVs.", "given_citation_intent": "background", "given_keywords": ["single-frame object detection"]}]}
{"citing_id": "2304.00573v1", "cited_id": "1906.00949", "section_title": "Background 28", "citation": "Therefore, #REFR argues for constraining the learnt policy to take actions in the support of the behaviour policy.", "text_before_citation": ["this can be highly restrictive and result in suboptimal behaviour."], "text_after_citation": ["However, support constraints generally require approximations to implement #OTHEREFR .", "Therefore, subsequent works have shown that other techniques for ensuring that the policy does not leave the data distribution work better in practice #OTHEREFR .", "Another approach is to incorporate conservatism into the value function update to prevent overestimation of the value of state-action pairs that are not present in the dataset #OTHEREFR .", "For example, the value function update for CQL #OTHEREFR penalises the Q-values of state-action pairs that are not contained in the dataset.", "Another line of work uses uncertainty quantification to obtain more robust value estimates #OTHEREFR ."], "citing_paper_content": {"title": "Risk-Sensitive And Robust Model-Based Reinforcement Learning And Planning", "abstract": "Goal-Oriented Long-Lived Systems Group (GOALS) Department of Engineering Science I'm deeply grateful to my supervisors Nick and Bruno. Thank you for being fantastic mentors, and for always being generous with your time. This thesis would not have been possible without your guidance, knowledge, enthusiasm, and support. I greatly appreciate the insightful feedback of my examiners, Jakob Foerster and Aviv Tamar, which has helped to significantly improve the final version of this thesis. Many thanks to Benjamin Morrell, KC Wong, Rob Reid, and David Willson, for mentoring me when I first became interested in research. Thank you for having the patience and generosity to assist an enthusiastic undergraduate student. I would like to extend my thanks to the members of the GOALS group that have been here throughout my DPhil: Charlie,"}, "cited_paper_content": {"title": "Stabilizing Off-Policy Q-Learning Via Bootstrapping Error Reduction", "abstract": "Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks."}, "keywords": ["learnt policy"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Off-policy reinforcement learning methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data, which can be highly restrictive and result in suboptimal behaviour. To address this issue, the authors of #REFR propose a practical algorithm, bootstrapping error accumulation reduction (BEAR), which is able to learn robustly from different off-policy distributions.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data, which can result in suboptimal behaviour #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Off-policy reinforcement learning methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, which can result in suboptimal behavior of the learnt policy #REFR.", "given_citation_intent": "background", "given_keywords": ["learnt policy"]}]}
{"citing_id": "2303.07868v1", "cited_id": "1612.03144", "section_title": "Dual-Level Fpn", "citation": "Original image-level FPN (i-FPN) #REFR introduces a topdown path to propagate contextual semantic information from higher layers to lower ones.", "text_before_citation": [], "text_after_citation": ["Actually, the lower-level features contain more fine-grained details than higher-level ones, which are beneficial for high-quality segmentation, especially on boundary regions, but these information is not fully explored in Mask R-CNN #OTHEREFR .", "In this work, we propose a region-level FPN (r-FPN) to integrate more detailed information from lower layers of i-FPN into region-wise feature hierarchies.", "The information flows from each level of i-FPN to r-FPN are shown in Fig. 2 .", "Region-Level FPN.", "We follow the original i-FPN to define the layers producing feature maps of the same resolution as one network stage corresponding to one feature level."], "citing_paper_content": {"title": "Dynamask: Dynamic Mask Selection For Instance Segmentation", "abstract": "The representative instance segmentation methods mostly segment different object instances with a mask of the fixed resolution, e.g., 28 \u00d7 28 grid. However, a lowresolution mask loses rich details, while a high-resolution mask incurs quadratic computation overhead. It is a challenging task to predict the optimal binary mask for each instance. In this paper, we propose to dynamically select suitable masks for different object proposals. First, a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation is developed to gradually increase the mask grid resolution, ensuring high-quality segmentation of objects. Specifically, an efficient region-level top-down path (r-FPN) is introduced to incorporate complementary contextual and detailed information from different stages of image-level FPN (i-FPN). Then, to alleviate the increase of computation and memory costs caused by using large masks, we develop a Mask Switch Module (MSM) with negligible computational cost to select the most suitable mask resolution for each instance, achieving high efficiency while maintaining high segmentation accuracy. Without bells and whistles, the proposed method, namely DynaMask, brings consistent and noticeable performance improvements over other state-ofthe-arts at a moderate computation overhead. The source code: https://github.com/lslrh/DynaMask."}, "cited_paper_content": {"title": "Feature Pyramid Networks For Object Detection", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."}, "keywords": ["Original image-level FPN"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To gradually increase the mask grid resolution, the authors developed a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation, which incorporates complementary contextual and detailed information from different stages of image-level FPN (i-FPN) through an efficient region-level top-down path (r-FPN) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of predicting the optimal binary mask for each instance, the authors propose to dynamically select suitable masks for different object proposals using a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation and a Mask Switch Module (MSM) that selects the most suitable mask resolution for each instance, achieving high efficiency while maintaining high segmentation accuracy (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To incorporate complementary contextual and detailed information from different stages of the original image-level FPN, the authors developed an efficient region-level top-down path (r-FPN) in their proposed DynaMask method, as described in #REFR.", "given_citation_intent": "background", "given_keywords": ["Original image-level FPN"]}]}
{"citing_id": "2304.03145v1", "cited_id": "1611.01603", "section_title": "Related Works", "citation": "They retrained the BIDAF model #REFR on these generated adversarial examples to test its robustness.", "text_before_citation": ["The use of adversarial examples to evaluate and improve the robustness of machine learning models has a long-standing history #OTHEREFR .", "In the field of NLP, one of the early works by #OTHEREFR showed that despite existing neural network QA systems proving their success when evaluated on standard metrics, they perform poorly when evaluated on adversarial examples.", "In their work, they propose the creation of adversarial examples for SQuAD v1.1 using the AddSent and AddAny algorithms.", "In AddSent, a distractor sentence is appended at the end of each context.", "For the AddAny algorithm, a random sequence of grammatical and ungrammatical words are appended to each context."], "text_after_citation": ["AddSent algorithm swaps namedentities and numbers in the question with the nearest word in GloVe word vector space #OTHEREFR . Our method, EntSwap, slightly differs from AddSent. We replace named-entities, while leaving numbers unchanged.", "Unlike AddSent, which replaces entities in the question, appends distracting sentences to the context, and leaves the answers unchanged, EntSwap replaces all detected named-entities for the questions, context, and answers to create an altered SQuAD2.0 dev set.", "Although the BIDAF model #OTHEREFR was retrained on adversarial examples, there is no guarantee of its robustness when evaluated on adversarial examples generated differently.", "Wang and Bansal #OTHEREFR ) generated slightly different adversarial examples for SQuAD using the AddSentMod algorithm by prepending the distractor sentences to the context instead of appending them and also used a different set of fake answers from ADD-SENT.", "The authors show that the pre-trained BiDAF model #OTHEREFR is not robust to this set of adversarial examples as the model's F1-score drops by 30%."], "citing_paper_content": {"title": "Evaluating The Robustness Of Machine Read-Ing Comprehension Models To Low Resource Entity Renaming", "abstract": "Question answering (QA) models have shown compelling results in the task of Machine Reading Comprehension (MRC). Recently these systems have proved to perform better than humans on held-out test sets of datasets e.g. SQuAD, but their robustness is not guaranteed. The QA model's brittleness is exposed when evaluated on adversarial generated examples by a performance drop. In this study, we explore the robustness of MRC models to entity renaming, with entities from low resource regions such as Africa. We propose EntSwap, a method for test-time perturbations, to create a test set whose entities have been renamed. In particular, we rename entities of type: country, person, nationality, location, organization and city, to create AfriSQuAD2. Using the perturbed test set, we evaluate the robustness of three popular MRC models. We find that compared to base models, large models perform well comparatively on novel entities. Furthermore, our analysis indicate that person, as an entity type, highly challenges the model performance."}, "cited_paper_content": {"title": "Bidirectional Attention Flow For Machine Comprehension", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test."}, "keywords": ["generated adversarial examples", "BIDAF model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the field of NLP, one of the early works by #REFR showed that despite existing neural network QA systems proving their success when evaluated on standard metrics, they perform poorly when evaluated on adversarial examples.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In the field of NLP, attention mechanisms have been successfully extended to machine comprehension (MC) tasks, with recent methods using attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In the field of NLP, adversarial examples have been used to evaluate the robustness of machine learning models, as shown by #OTHEREFR. Specifically, they proposed the creation of generated adversarial examples for SQuAD v1.1 using the AddSent and AddAny algorithms. Moreover, the BIDAF model introduced in #REFR has been successfully used to obtain a query-aware context representation without early summarization.", "given_citation_intent": "method", "given_keywords": ["generated adversarial examples", "BIDAF model"]}]}
{"citing_id": "2303.08789v1", "cited_id": "1910.10897", "section_title": "Benchmarks And Training Data", "citation": "Meta-World: Meta-World #REFR is a collection of 50 tasks, each set in its own environment involving a Sawyer arm. Meta-World provides several task splits.", "text_before_citation": [], "text_after_citation": ["We consider the ML50 split, which consists of 45 training tasks and 5 target tasks: door-lock, door-unlock, hand-insert, bin-picking, and box-close.", "These 5 target tasks serve for evaluating various PLEX models.", "We use Meta-World's v2 version with image observations, which is described in more detail in Appendix C.", "Meta-World comes with high-quality scripted policies for all tasks.", "To get video demonstration data (D mtvd ), we use these scripted policies to generate 100 successful video-only demonstrations for each of the 45 pretraining tasks. Therefore, |D mtvd | = 4500 trajectories."], "citing_paper_content": {"title": "Plex: Making The Most Of The Available Data For Robotic Manipulation Pretraining", "abstract": "A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which comes in the form of expert visuomotor demonstrations for a variety of annotated tasks, is scarce. In this work we propose PLEX, a transformer-based architecture that learns from task-agnostic visuomotor trajectories accompanied by a much larger amount of taskconditioned object manipulation videos-a type of robotics-relevant data available in quantity. The key insight behind PLEX is that the trajectories with observations and actions help induce a latent feature space and train a robot to execute task-agnostic manipulation routines, while a diverse set of video-only demonstrations can efficiently teach the robot how to plan in this feature space for a wide variety of tasks. In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task policy, not just an observational representation. We also show that using relative positional encoding in PLEX's transformers further increases its data efficiency when learning from human-collected demonstrations. Experiments showcase PLEX's generalization on Meta-World benchmark and establish state-of-the-art performance in challenging Robosuite environments."}, "cited_paper_content": {"title": "Meta-World: A Benchmark And Evaluation For Multi-Task And Meta Reinforcement Learning", "abstract": "Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods."}, "keywords": ["50 tasks", "several task splits"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In contrast to most works on robotic manipulation pretraining, PLEX learns a generalizable sensorimotor multi-task policy, not just an observational representation. We also show that using relative positional encoding in PLEX's transformers further increases its data efficiency when learning from human-collected demonstrations. Experiments showcase PLEX's generalization on Meta-World benchmark #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable meaningful generalization in multi-task learning and meta-learning, it is crucial to evaluate algorithms on task distributions that are sufficiently broad, as highlighted in the Meta-World benchmark proposed by Yu et al. (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the generalization ability of their proposed PLEX architecture, the authors compare their results with state-of-the-art meta-reinforcement learning and multi-task learning algorithms on #REFR, which consists of 50 distinct robotic manipulation tasks and several task splits.", "given_citation_intent": "background", "given_keywords": ["50 tasks", "several task splits"]}]}
{"citing_id": "2303.04729v2", "cited_id": "1706.03762", "section_title": "Background", "citation": "More recently, #REFR introduces an attention-based mechanism, called Transformers, to replace the former RNN-based models.", "text_before_citation": ["In this section, we will review some basic concepts from language modeling.", "Many architectures have been proposed to be used as LMs in various applications.", "#OTHEREFR introduced Long Short-Term Memory recurrent neural networks to generate real-valued sequences with long-range structures to predict the next token.", "#OTHEREFR introduced an encoding-decoding approach using an attention mechanism for machine translation systems."], "text_after_citation": ["All the large-scale NLP algorithms are using transformers, like BERT #OTHEREFR , T5 #OTHEREFR , GPT2 #OTHEREFR , GPT3 #OTHEREFR .", "We will skip the details of the architectures and refer you to the mentioned works."], "citing_paper_content": {"title": "On The Risks Of Stealing The Decoding Algorithms Of Language Models", "abstract": "A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["former RNN-based models", "attention-based mechanism"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section, we will review some basic concepts from language modeling. Many architectures have been proposed to be used as LMs in various applications. #OTHEREFR introduced Long Short-Term Memory recurrent neural networks to generate real-valued sequences with long-range structures to predict the next token. Another approach proposed by #REFR is the Transformer architecture, which is based solely on attention mechanisms and has shown superior performance in machine translation tasks while being more parallelizable and requiring significantly less time to train.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Another approach that has gained popularity in recent years is the Transformer architecture proposed by Vaswani et al. (#REFR), which relies solely on attention mechanisms and has shown superior performance in machine translation tasks while being more parallelizable and requiring less training time.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Another approach to language modeling is the use of attention-based mechanisms, which have shown to outperform former RNN-based models in various tasks such as machine translation, as demonstrated in the paper \"Attention Is All You Need\" (#REFR).", "given_citation_intent": "background", "given_keywords": ["former RNN-based models", "attention-based mechanism"]}]}
{"citing_id": "2303.11470v2", "cited_id": "1910.01108", "section_title": "Effectiveness", "citation": "For text data, in addition to BERT-base, we also consider two BERT variants: RoBERTa and Distill-BERT #REFR .", "text_before_citation": ["Defenders can modify the certainty value \u03b1 to adjust the recall and precision rate of the detection results.", "Transferability.", "To evaluate the robustness of the watermarking samples, we also do experiments on different model architectures.", "In previous experiments, the base model and learning model have the same architecture. Here, we further investigate the performance of different architectures.", "Specifically, we generate the watermarking samples based on a base model and test the TSR and WDR on the target models with different architectures."], "text_after_citation": ["For image datasets besides ResNet, we select two commonly used models: .", "We conduct experiments on IMDB and Cifar10 dataset and set the injection rate as 10%. Results are shown in Tab. 5.", "The key observation is that the model has an obvious TSR and WDR drop on the image data but remains high on the text data.", "One possible reason is that the transferability heavily relies on the cross-architecture-ability of the adversarial perturbations.", "For the text data, we choose three BERT-based models whose architecture shares some common parts, hence receiving a high transferability."], "citing_paper_content": {"title": "Did You Train On My Dataset? Towards Public Dataset Protection With Clean-Label Backdoor Watermarking", "abstract": "The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding publicavailable data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoor watermarking framework that uses imperceptible perturbations to replace mislabeled samples. As a result, the watermarking samples remain consistent with the original labels, making them difficult to detect. Our experiments on text, image, and audio datasets demonstrate that the proposed framework effectively safeguards datasets with minimal impact on original task performance. We also show that adding just 1% of watermarking samples can inject a traceable watermarking function and that our watermarking samples are stealthy and look benign upon visual inspection."}, "cited_paper_content": {"title": "Distilbert, A Distilled Version Of Bert: Smaller, Faster, Cheaper And Lighter", "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."}, "keywords": ["Distill-BERT"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To reduce the size of a BERT model while retaining its language understanding capabilities, the authors of this paper propose a method of pre-training a smaller general-purpose language representation model called DistilBERT, which can be fine-tuned for a wide range of tasks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train a smaller general-purpose language representation model, the authors propose a method called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and can reduce the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train a smaller general-purpose language representation model, the authors propose a method called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, as shown in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["Distill-BERT"]}]}
{"citing_id": "2303.05834v1", "cited_id": "1404.5278", "section_title": "Relative Clauses.", "citation": "The advantage of this underhanded construction is that now we can translate the empty word as the relative pronoun \"who\" or \"that.\" This ties in perfectly with the Frobenius algebra approach of #REFR .", "text_before_citation": ["The relative pronoun \"who\" acts as a bridge that \"copies\" the subject into the relative clause and then transfers it back into the main clause.", "We translate the following relative clause. We assign types in a less straightforward way.", "We first insert an empty word between the modifier \"t\u014dky\u014d kara untensita\" and the head \"onna.\" We assign the following types: \"t\u014dky\u014d\" and \"onna\" are both type n, the ablative particle \"kara\" has type n r o 7 , \"ky\u014d\" has type t (temporal adverb), the verb \"untensita\" has type o r 7 t r so \u2113 1 and the empty word acts like a phantom relative pronoun with type o 1 s r nn \u2113 . ky\u014d t\u014dky\u014d kara untensita", "\u2205 onna t n n r o 7 o r 7 t r s o \u2113 1 o 1 s r n n \u2113 n", "This construction generates a noun phrase and it can be translated using a straightforward anti-homomorphism."], "text_after_citation": ["In this example we modelled our relative clause as what the authors of the reference call a subject relative clause. 4.5. Coordinate sentences.", "The simplest way of coordinating sentences is by connecting them with the particle \"ga\" (\"and\") to which we assign the type s r ss \u2113 . We translate the following sentence where subjects are omitted.", "We decorate the pregroup with braces and assign the following type", "n \u2022 n r o 5 \u2022 o r 5 s s r ss \u2113 n \u2022 n r o 2 \u2022 o r 2 s .", "Extending the morphism \u03a8 from Example 7 to monoids with 3-braces, we obtain"], "citing_paper_content": {"title": "An Algebraic Approach To Translating Japanese", "abstract": "We use Lambek's pregroups and the framework of compositional distributional models of language (\"DisCoCat\") to study translations from Japanese to English as pairs of functors. Adding decorations to pregroups we show how to handle word order changes between languages."}, "cited_paper_content": {"title": "The Frobenius Anatomy Of Word Meanings I: Subject And Object Relative Pronouns", "abstract": "This paper develops a compositional vector-based semantics of subject and object relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. We develop two instantiations of the abstract semantics, one based on a truth-theoretic approach and one based on corpus statistics."}, "keywords": ["relative pronoun"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors develop a compositional vector-based semantics of subject and object relative pronouns within a categorical framework, using Frobenius algebras to formalize the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the modified noun phrase, as well as copying, combining, and discarding parts of the relative clause. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' use of Frobenius algebras to formalize the operations required to model the semantics of relative pronouns, including copying, combining, and discarding parts of the relative clause, is a notable contribution to the field of compositional vector-based semantics. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of Frobenius algebras to formalize the operations required to model the semantics of relative pronouns, including copying, combining, and discarding parts of the relative clause, has been previously explored (The Frobenius Anatomy Of Word Meanings I: Subject And Object Relative Pronouns). #REFR", "given_citation_intent": "background", "given_keywords": ["relative pronoun"]}]}
{"citing_id": "2303.14404v1", "cited_id": "1906.03842", "section_title": "Bpc: Train-Time Calibration Loss For Detection", "citation": "The remaining detections after populating t AC and t AN are false positives (inaccurate). Similar to Eq. #REFR and Eq. 7, we categorize them based on their confidence scores.", "text_before_citation": ["In principle, we need accurate detections to be more confident and inaccurate ones to be less confident, so we define the following objective that should be maximized:", "EQUATION", "In object detection, the obtained predictions are either accurate or inaccurate.", "Given the predicted class label, bounding boxes, 1 as an indicator function, and th is the threshold on score, we define the following:", "t AC = i 1[IoU(b i , b i ) \u2265 \u03c1]1[\u0233 i = y i ] |s i \u2265 th (6) t AN = i 1[IoU(b i , b i ) \u2265 \u03c1]1[\u0233 i = y i ] |s i < th (7) t IC & t IN :"], "text_after_citation": ["In our loss formulation, we consider precision since it includes true positives and false positives, for which we have confidence scores.", "Whereas false negatives cannot be considered as they do not have confidence scores because of no detections. Since Eq.", "(5) is not differentiable owing to the indicator functions for t AC , t AN , t IC and t IN , we formulate its differentiable version to approximate these quantities.", "Let t AC , t AN , t IC and t IN be the approximations to t AC , t AN , t IC and t IN , respectively. We express the differentiable formulation as:", "t AC = i\u2208 Ki=1 & si\u2265th s i tanh(s i ) t AN = i\u2208 Ki=1 & si<th s i (1 \u2212 tanh(s i )) t IC = i\u2208 Ki=0 & si\u2265th (1 \u2212s i ) tanh(s i ) t IN = i\u2208 Ki=0 & si<th (1 \u2212s i ) (1 \u2212 tanh(s i ))"], "citing_paper_content": {"title": "Bridging Precision And Confidence: A Train-Time Loss For Calibrating Object Detection", "abstract": "Deep neural networks (DNNs) have enabled astounding progress in several vision-based problems. Despite showing high predictive accuracy, recently, several works have revealed that they tend to provide overconfident predictions and thus are poorly calibrated. The majority of the works addressing the miscalibration of DNNs fall under the scope of classification and consider only in-domain predictions. However, there is little to no progress in studying the calibration of DNN-based object detection models, which are central to many vision-based safety-critical applications. In this paper, inspired by the train-time calibration methods, we propose a novel auxiliary loss formulation that explicitly aims to align the class confidence of bounding boxes with the accurateness of predictions (i.e. precision). Since the original formulation of our loss depends on the counts of true positives and false positives in a minibatch, we develop a differentiable proxy of our loss that can be used during training with other application-specific loss functions. We perform extensive experiments on challenging in-domain and out-domain scenarios with six benchmark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. Our results reveal that our train-time loss surpasses strong calibration baselines in reducing calibration error for both in and out-domain scenarios. Our source code and pre-trained models are available at https:// github.com/akhtarvision/bpc_calibration"}, "cited_paper_content": {"title": "Analyzing The Role Of Model Uncertainty For Electronic Health Records", "abstract": "In medicine, both ethical and monetary costs of incorrect predictions can be significant, and the complexity of the problems often necessitates increasingly complex models. Recent work has shown that changing just the random seed is enough for otherwise well-tuned deep neural networks to vary in their individual predicted probabilities. In light of this, we investigate the role of model uncertainty methods in the medical domain. Using RNN ensembles and various Bayesian RNNs, we show that population-level metrics, such as AUC-PR, AUC-ROC, log-likelihood, and calibration error, do not capture model uncertainty. Meanwhile, the presence of significant variability in patient-specific predictions and optimal decisions motivates the need for capturing model uncertainty. Understanding the uncertainty for individual patients is an area with clear clinical impact, such as determining when a model decision is likely to be brittle. We further show that RNNs with only Bayesian embeddings can be a more efficient way to capture model uncertainty compared to ensembles, and we analyze how model uncertainty is impacted across individual input features and patient subgroups."}, "keywords": ["remaining detections", "confidence scores"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In object detection, the obtained predictions are either accurate or inaccurate, and recent work has shown that changing just the random seed is enough for otherwise well-tuned deep neural networks to vary in their individual predicted probabilities. Therefore, it is crucial to calibrate the confidence of the predictions, and the authors propose a novel auxiliary loss formulation that explicitly aims to align the class confidence of bounding boxes with the accurateness of predictions. The proposed method outperforms strong calibration baselines in reducing calibration error for both in and out-domain scenarios, as demonstrated in extensive experiments on benchmark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To capture model uncertainty in object detection, the authors propose a novel auxiliary loss formulation that aligns the class confidence of bounding boxes with the accurateness of predictions, inspired by the train-time calibration methods used in the medical domain #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of overconfidence in object detection models, the authors propose a novel auxiliary loss formulation that aims to align the confidence scores of accurate and remaining detections with the precision of predictions, which is inspired by the train-time calibration methods #REFR.", "given_citation_intent": "method", "given_keywords": ["remaining detections", "confidence scores"]}]}
{"citing_id": "2304.05470v1", "cited_id": "1211.7343", "section_title": "Related Work", "citation": "Clauset and Eagle introduced topological overlap as a similarity measure between two networks (the two networks can be consecutive snapshots of the same network) #REFR . Buttner et al.", "text_before_citation": ["Many metrics have been used with various temporal networks in analyzing network properties. Zhao et al.", "used temporal centrality on a stock market based network as a portfolio selection tool #OTHEREFR .", "They emphasized the importance of temporal attributes of the network for real-world applications. Prado et.", "al determined the most relevant characters in a story by using eigenvector centrality over a temporal network of literary text #OTHEREFR . Cunha et al.", "analyzed contact duration and inter-contact times in vehicular networks to compare original and calibrated mobility traces of GPS-equipped taxis #OTHEREFR ."], "text_after_citation": ["adapted the metric for edge cases and used it on a real-world animal trade network #OTHEREFR . Navarro et al.", "used topological overlap to measure embeddedness of a link to show correlation with tie persistence. #OTHEREFR .", "Utilizing the methodical information gained from the aforementioned studies, we used a network of cities connected via linkages of multinational firms.", "These linkages allow us to study stable and changing cities from the perspective of economic growth.", "We use a temporal metric to examine networks of cities from 2010 to 2019 and use this data to identify the top stable and changing cities over the time period."], "citing_paper_content": {"title": "World City Networks And Multinational Firms: An Analysis Of Economic Ties Over A Decade", "abstract": "One perspective to view the economic development of cities is through the presence of multinational firms; how subsidiaries of various organizations are set up throughout the globe and how cities are connected to each other through these networks of multinational firms. Analysis of these networks can reveal interesting economical and spatial trends, as well as help us understand the importance of cities in national and regional economic development. This paper aims to study networks of cities formed due to the linkages of multinational firms over a decade (from 2010 to 2019). More specifically we are interested in analyzing the growth and stability of various cities in terms of the connections they form with other cities over time. Our results can be summarized into two key findings: First, we ascertain the central position of several cities due to their economically stable connections; Second, we successfully identify cities that have evolved over the past decade as the presence of multinational firms has increased in these cities."}, "cited_paper_content": {"title": "Persistence And Periodicity In A Dynamic Proximity Network", "abstract": "The topology of social networks can be understood as being inherently dynamic, with edges having a distinct position in time. Most characterizations of dynamic networks discretize time by converting temporal information into a sequence of network \"snapshots\" for further analysis. Here we study a highly resolved data set of a dynamic proximity network of 66 individuals. We show that the topology of this network evolves over a very broad distribution of time scales, that its behavior is characterized by strong periodicities driven by external calendar cycles, and that the conversion of inherently continuous-time data into a sequence of snapshots can produce highly biased estimates of network structure. We suggest that dynamic social networks exhibit a natural time scale \\Delta_{nat}, and that the best conversion of such dynamic data to a discrete sequence of networks is done at this natural rate."}, "keywords": ["two networks", "network"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Many metrics have been used with various temporal networks in analyzing network properties. Zhao et al. used temporal centrality on a stock market based network as a portfolio selection tool #OTHEREFR . They emphasized the importance of temporal attributes of the network for real-world applications. Prado et. al determined the most relevant characters in a story by using eigenvector centrality over a temporal network of literary text #OTHEREFR . Cunha et al. analyzed contact duration and inter-contact times in vehicular networks to compare original and calibrated mobility traces of GPS-equipped taxis #OTHEREFR . The authors should also consider the work of Holme et al. who studied the topology of social networks and showed that the conversion of inherently continuous-time data into a sequence of snapshots can produce highly biased estimates of network structure #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The importance of considering temporal attributes of a network for real-world applications has been emphasized in previous studies #REFR, which have shown that the conversion of inherently continuous-time data into a sequence of snapshots can produce highly biased estimates of network structure.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The importance of considering temporal attributes of networks for real-world applications has been emphasized in previous studies, such as the analysis of a dynamic proximity network by Holme and Saram\u00e4ki, which showed that converting continuous-time data into a sequence of snapshots can produce biased estimates of network structure #REFR.", "given_citation_intent": "background", "given_keywords": ["two networks", "network"]}]}
{"citing_id": "2303.06580v1", "cited_id": "1810.04805", "section_title": "Introduction", "citation": "Large language models (LLM) are the prototype of foundation models, which are trained on a large corpus collected from the internet with unsupervised tasks such as masked language modeling (MLM) #REFR .", "text_before_citation": ["Recently, foundation models have gained a lot of popularity in deep learning.", "People use these huge pre-trained models with billions or even trillion parameters to solve downstream specific tasks such as machine translation and document classification #OTHEREFR ."], "text_after_citation": ["Foundation models have been prevalent in natural language processing fields for a while in recent years, but it took years for this trend to deep vision learn-ing and its related fields.", "Compared to LLMs, foundation models for vision tasks have two possible pre-training approaches.", "The first approach is to reconstruct the input image using masked auto-encoders (MAE) #OTHEREFR directly, and the second is to introduce the pairing text descriptions of images as weak supervising labels.", "The first approach has shown its extraordinary imaging understanding capability in many downstream vision tasks #OTHEREFR .", "However, based on some recent research, the MAE method may not have strong domain generalization capability compared to the second approach."], "citing_paper_content": {"title": "Towards General Purpose Medical Ai: Continual Learning Medical Foundation Model", "abstract": "* Equal contribution \u2020 Corresponding author 1 work in progress. Hippocampus (ADNI) Figure 1. Results on three different paradigms for implementing universal medical foundation models: specialized learning, joint learning, and continual learning. Continual learning may be a practical and efficient paradigm for the medical foundation model."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["Large language models"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, foundation models have gained a lot of popularity in deep learning. People use these huge pre-trained models with billions or even trillion parameters to solve downstream specific tasks such as machine translation and document classification #REFR. The authors of the reference paper introduced BERT, a new language representation model that can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, including question answering and language inference.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recently, foundation models have gained a lot of popularity in deep learning. People use these huge pre-trained models with billions or even trillion parameters to solve downstream specific tasks such as machine translation and document classification. One such model is BERT, which is a bidirectional encoder representation from transformers that can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recently, foundation models have gained a lot of popularity in deep learning, particularly the use of large language models for pre-training representations from unlabeled text #OTHEREFR. This approach has shown to be effective in various natural language processing tasks, and the Bidirectional Encoder Representations from Transformers (BERT) model is a notable example of this method #REFR.", "given_citation_intent": "method", "given_keywords": ["Large language models"]}]}
{"citing_id": "2304.12455v1", "cited_id": "1911.11130", "section_title": "Introduction", "citation": "E.g., human faces are roughly symmetric, and exploiting this as a prior yields considerable improvements #REFR .", "text_before_citation": ["This task is difficult, however, due to various forms of underdetermination, including depth ambiguity and occlusion.", "The straighforward resolution to these problems is applying supervised learning with 2D/3D ground truth information, keypoints #OTHEREFR or depth maps #OTHEREFR .", "However, it is hard and sometimes even impossible to collect such supervisory signals, as this requires considerable human effort.", "Hence, recent techniques #OTHEREFR exploit unsupervised learning to leverage human prior knowledge about the natural objects in the world.", "This alleviates the need for annotated data, but the prior has to be chosen wisely for each object class."], "text_after_citation": ["Besides reconstructing the 3D structure, one may be also interested in altering its style to one that is different from that of the original input image.", "Consider, e.g., supporting -or even replacing -CG artists by automatically changing a 3D human model's gender, haircut, skin color or age.", "Interestingly, Unsupervised 2D to Explicit 3D Style Transfer (UE3DST) is a less discussed problem in the literature.", "By explicit, we mean that the method extracts a set of features, which allow recovering the 3D shape via using an explicit rendering function #OTHEREFR .", "Unfortunately, existing methods are either (i) implicit 3D, or (ii) supervised."], "citing_paper_content": {"title": "Unsupervised Style-Based Explicit 3D Face Reconstruction From Single Image", "abstract": "self style albedo depth shade norm 3D reconstruction Figure 1. Given a single 2D image, we reconstruct the 3D object (top) by predicting the features of the rendering process: albedo, depth, shading and surface normals. Then the method can synthesize images from an arbitrary viewpoint with a new style, potentially including changes in both shape and appearance (bottom)."}, "cited_paper_content": {"title": "Unsupervised Learning Of Probably Symmetric Deformable 3D Objects From Images In The Wild", "abstract": "We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences."}, "keywords": ["human faces"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recent techniques have exploited unsupervised learning to leverage human prior knowledge about natural objects in the world, alleviating the need for annotated data. For instance, the authors of #REFR proposed a method to learn 3D deformable object categories from raw single-view images, without external supervision, by disentangling depth, albedo, viewpoint, and illumination components using an autoencoder.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of underdetermination in 3D object reconstruction, recent techniques exploit unsupervised learning to leverage human prior knowledge about natural objects in the world, such as exploiting the underlying object symmetry even if the appearance is not symmetric due to shading #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Unsupervised learning has been used to leverage human prior knowledge about natural objects in the world, which alleviates the need for annotated data, but the prior has to be chosen wisely for each object class #REFR. The reference paper presents a method that can recover very accurately the 3D shape of human faces from single-view images, without any supervision or a prior shape model.", "given_citation_intent": "background", "given_keywords": ["human faces"]}]}
{"citing_id": "2304.01519v1", "cited_id": "1912.04838", "section_title": "Grid Sampling", "citation": "Compared to Waymo #REFR , nuScenes includes more object categories, covering different types of vehicles such as cars, trucks, buses, and trailers.", "text_before_citation": ["There are two difficulties: levels 1 and 2, where the bounding boxes contain at least five and one LiDAR points, respectively.", "We use official metrics, including the mean Average Precision (mAP) and the mean Average Precision weighted by Heading (mAPH).", "We benchmark multi-class 3D object detection using only single-frame LiDAR point clouds.", "The IoU thresholds for vehicles, pedestrians, and cyclists are 0.7, 0.5, and 0.5.", "nuScenes #OTHEREFR is another popular benchmark dataset with a total of 1,000 driving sequences."], "text_after_citation": ["We use the mean Average Precision (mAP) and the nuScenes Detection Score (NDS) to compare different baseline approaches."], "citing_paper_content": {"title": "Lidar-Based 3D Object Detection Via Hybrid 2D Semantic Scene Generation", "abstract": "Figure 1: (a) We propose to integrate the 2D semantic scene representation into LiDAR-based 3D object detectors, which can be generated by either an explicit network (left) or an implicit network (middle) using the projection of 3D bounding boxes into the Bird's Eye View (right) as supervision. (b) The proposed 2D semantic scene representation can be easily integrated into most existing detectors (top) as a 2D semantic scene generation module (bottom) with minimal engineering efforts."}, "cited_paper_content": {"title": "Scalability In Perception For Autonomous Driving: Waymo Open Dataset", "abstract": "The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open."}, "keywords": ["object categories", "vehicles"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To benchmark our multi-class 3D object detection approach, we use official metrics such as mean Average Precision (mAP) and mean Average Precision weighted by Heading (mAPH) with IoU thresholds of 0.7, 0.5, and 0.5 for vehicles, pedestrians, and cyclists, respectively, and compare our results with those on the nuScenes #OTHEREFR dataset as well as the state-of-the-art Waymo Open Dataset, which provides a large-scale, diverse dataset with exhaustive annotations for 2D and 3D bounding boxes and strong baselines for 2D and 3D detection and tracking tasks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limited scale and variation of existing self-driving datasets, Waymo Open Dataset provides a diverse dataset with well-synchronized and calibrated high-quality LiDAR and camera data, which can be used for 3D object detection tasks with single-frame LiDAR point clouds, as shown in the manuscript. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Existing self-driving datasets are limited in the scale and variation of the environments they capture, which is crucial for generalization within and between operating regions, especially for multi-class 3D object detection of #KEYWORD1# such as vehicles, pedestrians, and cyclists, as shown in the benchmark dataset nuScenes #OTHEREFR#. Therefore, the proposed large scale, high quality, diverse dataset in the #REFR# could be a valuable resource for autonomous driving research.", "given_citation_intent": "background", "given_keywords": ["object categories", "vehicles"]}]}
{"citing_id": "2303.08561v1", "cited_id": "2002.05709", "section_title": "Approach", "citation": "We incorporate the proposed ASG mechanism into existing contrastive learning framework, i.e., SimCLR #REFR , to further enhance the discriminative power of the learned audio representation, as shown in Fig. 3 .", "text_before_citation": [], "text_after_citation": ["Unlike the previous methods that only generate positive pairs, we divide the data transformation strategies into two groups: (1) Positive sample generation, which modifies the audio stream while retain its semantic.", "(2) Adversarial sample generation, which employs positional transformations on spectrogram to generate negative samples.", "Then, the NT-Xent Loss proposed in SimCLR is employed to optimize the network.", "The detailed descriptions of each part are given in the following sections."], "citing_paper_content": {"title": "Enhancing Unsupervised Audio Representation Learning Via Adversarial Sample Generation", "abstract": "Existing audio analysis methods generally first transform the audio stream to spectrogram, and then feed it into CNN for further analysis. A standard CNN recognizes specific visual patterns over feature map, then pools for high-level representation, which overlooks the positional information of recognized patterns. However, unlike natural image, the semantic of an audio spectrogram is sensitive to positional change, as its vertical and horizontal axes indicate the frequency and temporal information of the audio, instead of naive rectangular coordinates. Thus, the insensitivity of CNN to positional change plays a negative role on audio spectrogram encoding. To address this issue, this paper proposes a new self-supervised learning mechanism, which enhances the audio representation by first generating adversarial samples (i.e., negative samples), then driving CNN to distinguish the embeddings of negative pairs in the latent space. Extensive experiments show that the proposed approach achieves best or competitive results on 9 downstream datasets compared with previous methods, which verifies its effectiveness on audio representation learning."}, "cited_paper_content": {"title": "A Simple Framework For Contrastive Learning Of Visual Representations", "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."}, "keywords": ["learned audio representation", "existing contrastive learning"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the insensitivity of CNN to positional change in audio spectrogram encoding, the proposed self-supervised learning mechanism enhances the audio representation by generating adversarial samples and driving CNN to distinguish the embeddings of negative pairs in the latent space (SimCLR) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the audio representation, the authors propose a self-supervised learning mechanism that generates adversarial samples and drives CNN to distinguish the embeddings of negative pairs in the latent space (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the learned audio representation, the proposed self-supervised learning mechanism generates adversarial samples and distinguishes the embeddings of negative pairs in the latent space using existing contrastive learning #REFR.", "given_citation_intent": "method", "given_keywords": ["learned audio representation", "existing contrastive learning"]}]}
{"citing_id": "2304.12556v1", "cited_id": "1603.09382", "section_title": "Model", "citation": "During training, incorporating 10% stochastic depth #REFR lead to a 0.102 dB improvement in PSNR.", "text_before_citation": ["Due to the limitation of the GPU resources, we do not further enlarge the window size and training patch size.", "This shows that the rectangular training patch and larger local window size indeed can help improve the feature extraction ability across stereo images. Stochastic Depth.", "As per the research conducted by #OTHEREFR , a deeper stochastic depth can improve the performance of stereoSR.", "Therefore, we employ SwinFSR-L to examine how stochastic depth affects our Swin Transformer based model.", "Our results based on the validation set of Flickr1024 #OTHEREFR are presented in Table 6 ."], "text_after_citation": ["When using 20% stochastic depth, the performance of SwinFSR-L improves slightly by 0.1014 dB.", "However, setting the stochastic depth to 30% results in a performance decrease of 0.121 dB, but it still outperforms the baseline.", "This suggests that larger models have a tendency to overfit the Flickr1024 training data.", "However, incorporating stochastic depth can help enhance the overall performance and generalization ability of the networks."], "citing_paper_content": {"title": "Swinfsr: Stereo Image Super-Resolution Using Swinir And Frequency Domain Knowledge", "abstract": "Stereo Image Super-Resolution (stereoSR) has attracted significant attention in recent years due to the extensive deployment of dual cameras in mobile phones, autonomous vehicles and robots. In this work, we propose a new StereoSR method, named SwinFSR, based on an extension of SwinIR, originally designed for single image restoration, and the frequency domain knowledge obtained by the Fast Fourier Convolution (FFC). Specifically, to effectively gather global information, we modify the Residual Swin Transformer blocks (RSTBs) in SwinIR by explicitly incorporating the frequency domain knowledge using the FFC and employing the resulting residual Swin Fourier Transformer blocks (RSFTBs) for feature extraction. Besides, for the efficient and accurate fusion of stereo views, we propose a new cross-attention module referred to as RCAM, which achieves highly competitive performance while requiring less computational cost than the state-of-the-art cross-attention modules. Extensive experimental results and ablation studies demonstrate the effectiveness and efficiency of our proposed SwinFSR."}, "cited_paper_content": {"title": "Deep Networks With Stochastic Depth", "abstract": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10)."}, "keywords": ["stochastic depth"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As per the research conducted by Huang et al. (2016), a deeper stochastic depth can improve the performance of stereoSR. Therefore, we employ SwinFSR-L to examine how stochastic depth affects our Swin Transformer based model. Our results based on the validation set of Flickr1024 (Huang et al., 2016) are presented in Table 6.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "As per the research conducted by Zhang et al. (2016), a deeper stochastic depth can improve the performance of stereoSR. Therefore, we employ SwinFSR-L to examine how stochastic depth affects our Swin Transformer based model.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "As per the research conducted by He et al. (2016), a deeper stochastic depth can improve the performance of stereoSR. Therefore, we employ SwinFSR-L to examine how stochastic depth affects our Swin Transformer based model.", "given_citation_intent": "background", "given_keywords": ["stochastic depth"]}]}
{"citing_id": "2304.01974v1", "cited_id": "1910.14424", "section_title": "Main Results", "citation": "Among the neural baselines, the transformer-based cross-encoder outperforms the bi-encoder, which is consistent with previous findings #REFR .", "text_before_citation": ["Our main results are summarized in Table 2 .", "All neural re-ranking models outperform the baseline Expert System in both metrics, suggesting that re-ranking does up-rank the more relevant historytaking questions."], "text_after_citation": ["Surprisingly, the LongT5 autoregressive re-ranker, despite having more parameters (220M parameters), also performs worse than the crossencoder (\u223c110M parameters).", "The best performance is achieved by the global re-ranker for both transformer and S4 architectures, regardless of the loss functions chosen.", "Among the various loss functions, the pointwise binary cross-entropy (BCE) performs the best.", "Our hypothesis is that since our ground truth relevance scores are binary rather than continuous, the current task does not make full use of the listwise loss functions.", "The effectiveness of the global re-ranker lies in the fact that it attends to the semantic interactions not only between the dialogue and the candidate questions but also between the candidate questions themselves."], "citing_paper_content": {"title": "Dialogue-Contextualized Re-Ranking For Medical History-Taking", "abstract": "AI-driven medical history-taking is an important component in symptom checking, automated patient intake, triage, and other AI virtual care applications. As historytaking is extremely varied, machine learning models require a significant amount of data to train. To overcome this challenge, existing systems are developed using indirect data or expert knowledge. This leads to a training-inference gap as models are trained on different kinds of data than what they observe at inference time. In this work, we present a two-stage re-ranking approach that helps close the training-inference gap by re-ranking the first-stage question candidates using a dialogue-contextualized model. For this, we propose a new model, global re-ranker, which cross-encodes the dialogue with all questions simultaneously, and compare it with several existing neural baselines. We test both transformer and S4-based language model backbones. We find that relative to the expert system, the best performance is achieved by our proposed global re-ranker with a transformer backbone, resulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77% higher mean average precision (mAP). As part of this work, we also release pre-trained checkpoints for bi-directional and autoregressive S4 models trained on Wikipedia and PubMed data."}, "cited_paper_content": {"title": "Multi-Stage Document Ranking With Bert", "abstract": "The advent of deep neural networks pre-trained via language modeling tasks has spurred a number of successful applications in natural language processing. This work explores one such popular model, BERT, in the context of document ranking. We propose two variants, called monoBERT and duoBERT, that formulate the ranking problem as pointwise and pairwise classification, respectively. These two models are arranged in a multi-stage ranking architecture to form an end-to-end search system. One major advantage of this design is the ability to trade off quality against latency by controlling the admission of candidates into each pipeline stage, and by doing so, we are able to find operating points that offer a good balance between these two competing metrics. On two large-scale datasets, MS MARCO and TREC CAR, experiments show that our model produces results that are either at or comparable to the state of the art. Ablation studies show the contributions of each component and characterize the latency/quality tradeoff space."}, "keywords": ["neural baselines"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "All neural re-ranking models outperform the baseline Expert System in both metrics, suggesting that re-ranking does up-rank the more relevant historytaking questions. This is consistent with the findings in the reference paper #REFR, which proposes a multi-stage ranking architecture using BERT models for document ranking and achieves state-of-the-art results on large-scale datasets.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "All neural re-ranking models outperform the baseline Expert System in both metrics, suggesting that re-ranking does up-rank the more relevant historytaking questions. This is consistent with the findings in the reference paper \"#REFR\", where the authors propose a multi-stage ranking architecture using BERT models and show that their approach produces results that are either at or comparable to the state of the art on large-scale datasets.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "All neural re-ranking models outperform the baseline Expert System in both metrics, suggesting that re-ranking does up-rank the more relevant historytaking questions. This is consistent with the findings reported in the reference paper #REFR, which proposes two variants of BERT for document ranking and shows that their multi-stage ranking architecture produces results that are either at or comparable to the state of the art on large-scale datasets.", "given_citation_intent": "result", "given_keywords": ["neural baselines"]}]}
{"citing_id": "2304.07444v1", "cited_id": "2003.06957", "section_title": "Overview", "citation": "Please visit the publication #REFR or (Wu et al., 2019) for more details on other parameters.", "text_before_citation": ["Our MTFA #OTHEREFR baseline is implemented using Detectron2 framework (Wu et al., 2019) . Our backbone is", "ResNet-101 #OTHEREFR with Feature Pyramid Network #OTHEREFR .", "Each experiment is set up with a single GPU GeForce RTX 2080Ti with a batch size of 2 images.", "The novel phase has a learning rate of 0.00125 inferred from the MTFA configuration.", "We set the balance parameters \u03b1 = 1e \u22121 and \u03b2 = 1e \u22122 when we train the model with instance triplet and instance memory loss function, respectively."], "text_after_citation": [], "citing_paper_content": {"title": "Computer Vision And Image Understanding", "abstract": "Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data of camouflaged objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. We then propose a novel method to efficiently detect and segment the camouflaged objects in the images. In particular, we introduce the instance triplet loss and the instance memory storage. The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset."}, "cited_paper_content": {"title": "Frustratingly Simple Few-Shot Object Detection", "abstract": "Detecting rare objects from a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2~20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection."}, "keywords": ["details"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently detect and segment camouflaged objects in images, we propose a novel method that utilizes the instance triplet loss and instance memory storage (Li et al., 2021). Our proposed method achieves state-of-the-art performance on the newly collected CAMO-FS dataset, outperforming prior works by a significant margin. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of few-shot learning for camouflaged object detection and segmentation, the authors propose a novel method that includes the instance triplet loss and the instance memory storage ( #REFR ).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of few-shot learning for camouflaged object detection and segmentation, the authors propose a novel method that includes the instance triplet loss and the instance memory storage (Fang et al., 2021 #REFR).", "given_citation_intent": "background", "given_keywords": ["details"]}]}
{"citing_id": "2303.11611v1", "cited_id": "2003.01690", "section_title": "Assumption 3.3", "citation": "The results in the figure are clean accuracy in DFKD and robust accuracy under AA attack #REFR in DFARD.", "text_before_citation": ["According to #OTHEREFR , proper smoothing factor for KD helps to better results.", "Different degrees of softness indicates the difficulty of different learning objectives.", "In this part, we use different temperatures #OTHEREFR to construct various soft labels as shown in Figure 2 .", "At the same time, we carry out experiments in both DFKD and DFARD tasks to obtain comprehensive results. The experimental results are shown in Figure 3 .", "(a) and (b) show the student's learning effectiveness of learning objectives with different difficulties, which are adjusted by different distillation temperatures. We distill all student models for 50 epochs."], "text_after_citation": ["An obvious conclusion is that, in both DFKD and DFARD tasks, a too-difficult learning target (low distillation temperature) or a too-simple target (high distillation temperature) leads to a decline in the student's performance.", "Besides, we also show that maintaining a fixed temperature throughout the training process may affect the student's performance. The results can be found in Table 3 ."], "citing_paper_content": {"title": "Model Robustness Meets Data Privacy: Adversarial Robustness Distillation Without Original Data", "abstract": "Large-scale deep learning models have achieved great performance based on large-scale datasets. Moreover, the existing Adversarial Training (AT) can further improve the robustness of these large models. However, these large models are difficult to deploy to mobile devices, and the effect of AT on small models is very limited. In addition, the data privacy issue (e.g., face data and diagnosis report) may lead to the original data being unavailable, which relies on data-free knowledge distillation technology for training. To tackle these issues, we propose a challenging novel task called Data-Free Adversarial Robustness Distillation (DFARD), which tries to train small, easily deployable, robust models without relying on the original data. We find the combination of existing techniques resulted in degraded model performance due to fixed training objectives and scarce information content. First, an interactive strategy is designed for more efficient knowledge transfer to find more suitable training objectives at each epoch. Then, we explore an adaptive balance method to suppress information loss and obtain more data information than previous methods. Experiments show that our method improves baseline performance on the novel task."}, "cited_paper_content": {"title": "Reliable Evaluation Of Adversarial Robustness With An Ensemble Of Diverse Parameter-Free Attacks", "abstract": "The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 40 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\\%$, identifying several broken defenses."}, "keywords": ["robust accuracy"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "According to previous work on evaluating adversarial defenses, improper tuning of hyperparameters of the attacks and gradient obfuscation or masking are frequent pitfalls in the evaluation process #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors achieved lower robust test accuracy than reported in the papers they tested, often by more than $10\\%$, identifying several broken defenses #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript compare their results with those in #REFR, showing that their proposed method achieves lower robust accuracy in all but one of the cases, often by more than 10%.", "given_citation_intent": "result", "given_keywords": ["robust accuracy"]}]}
{"citing_id": "2303.08816v1", "cited_id": "1703.00048", "section_title": "B.2 Proof Of Theorem 8", "citation": "The following lemma characterizes the non-asymptotic behavior of the MLE estimator. It is extracted from #REFR .", "text_before_citation": [], "text_after_citation": ["Lemma 14 (Theorem 1, #OTHEREFR .", "Define V s = s t=1 \u03c6 it,jt \u03c6 it,jt , and \u03b8 s as the MLE estimator (3) at round s.", "EQUATION", "then for any fixed", "x \u2208 R d , with probability at least 1 \u2212 \u03b4, | \u03b8 s \u2212 \u03b8 * , x | \u2264 3 \u03ba x 2 V \u22121 s log(3/\u03b4)."], "citing_paper_content": {"title": "Borda Regret Minimization For Generalized Linear Dueling Bandits", "abstract": "Dueling bandits are widely used to model preferential feedback that is prevalent in machine learning applications such as recommendation systems and ranking. In this paper, we study the Borda regret minimization problem for dueling bandits, which aims to identify the item with the highest Borda score while minimizing the cumulative regret. We propose a new and highly expressive generalized linear dueling bandits model, which covers many existing models. Surprisingly, the Borda regret minimization problem turns out to be difficult, as we prove a regret lower bound of order \u2126(d 2/3 T 2/3), where d is the dimension of contextual vectors and T is the time horizon. To attain the lower bound, we propose an explore-then-commit type algorithm, which has a nearly matching regret upper bound O(d 2/3 T 2/3). When the number of items/arms K is small, our algorithm can achieve a smaller regret O((d log K) 1/3 T 2/3) with proper choices of hyperparameters. We also conduct empirical experiments on both synthetic data and a simulated real-world environment, which corroborate our theoretical analysis."}, "cited_paper_content": {"title": "Provable Optimal Algorithms For Generalized Linear Contextual Bandits", "abstract": "Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an $\\tilde{O}(\\sqrt{dT})$ regret over $T$ rounds with $d$ dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a $\\sqrt{d}$ factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases."}, "keywords": ["MLE estimator"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In their work on Borda regret minimization for dueling bandits, the authors propose a new generalized linear dueling bandits model that covers many existing models and prove a regret lower bound of order \u2126(d 2/3 T 2/3). To attain the lower bound, they propose an explore-then-commit type algorithm, which has a nearly matching regret upper bound O(d 2/3 T 2/3) and improves on the best previous result by a sqrt(d) factor, assuming the number of arms is fixed (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Contextual bandits have been widely used in various machine learning applications, and the upper confidence bound based algorithm proposed in the reference paper #REFR for generalized linear contextual bandits has achieved a regret that matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a sqrt(d) factor, assuming the number of arms is fixed, which may be of interest for the Borda regret minimization problem studied in this paper.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a new algorithm for generalized linear contextual bandits, achieving an $\\tilde{O}(\\sqrt{dT})$ regret over $T$ rounds with $d$ dimensional feature vectors, and establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest, including the use of MLE estimator. #REFR", "given_citation_intent": "background", "given_keywords": ["MLE estimator"]}]}
{"citing_id": "2303.09861v1", "cited_id": "1709.06283", "section_title": "Manipulating Various Objects", "citation": "Unlike existing multimodal grasping mechanisms (31, #REFR 37) , the same spiral robot with the same strategy can successfully grasp and manipulate a wide variety of objects in size and weight ( Fig. 6A-D, movie S5 ).", "text_before_citation": ["To demonstrate the ability to grasp and manipulate a variety of objects, we first designed and fabricated a 2-cable SpiRob (45 cm in length) by 3D printing using soft filaments ( Fig.", "S2A ; another origami-based fabrication method is described in detail with Text S1).", "This robot was used to demonstrate the potential of the proposed spiral-shaped design principle and operating strategy.", "We tested for numerous objects varying in size, weight, and material."], "text_after_citation": ["Even in a confined space, the packed body can navigate through obstacles without relative sliding ( Fig. 6 E,  F, S6 , and movie S6).", "Interestingly, we observed that the robot takes advantage of its interaction with the obstacles to reach the target instead of trying to avoid it.", "Since visual perception and control were not the focus in these experiments, we manually controlled the two cables like a puppet (implementation of a remote-control scheme with a joystick is detailed in Fig. S2, movie S7 , and text S3).", "Evidently, given that the base of the robot is fixed in our experiments, objects need to be positioned in a suitable range."], "citing_paper_content": {"title": "", "abstract": "Across various species and different scales, certain organisms use their appendages to grasp objects not through clamping but through wrapping. This pattern of movement is found in octopus tentacles, elephant trunks, and chameleon prehensile tails, demonstrating a great versatility to grasp a wide range of objects of various sizes and weights as well as dynamically manipulate them in the 3D space. We observed that the structures of these appendages follow a common pattern-a logarithmic spiral-which is especially challenging for existing robot designs to reproduce. This paper reports the design, fabrication, and operation of a class of cable-driven soft robots that morphologically replicate spiral-shaped wrapping. This amounts to substantially curling in length while actively controlling the curling direction as enabled by two principles: a) the parametric design based on the logarithmic spiral makes it possible to tightly pack to grasp objects that vary in size by more than two orders of magnitude and up to 260 times self-weight and b) asymmetric cable forces allow the swift control of the curling direction for conducting object manipulation. We demonstrate the ability to dynamically operate objects at a sub-second level by exploiting passive compliance. We believe that our study constitutes a step towards engineered systems that wrap to grasp and manipulate, and further sheds some insights into understanding the efficacy of biological spiral-shaped appendages. One-Sentence Summary: Design, fabrication, and operation of spiral soft robots at variable scales that can manipulate objects through wrapping. Main Text: INTRODUCTION Wrapping as a paradigm for grasping and manipulation (1), which are two key objectives in robotics (2, 3), is found in the prehensile tail of chameleons and seahorses with length scales as small as a few millimeters (4), as well as in the tentacles of octopuses and the trunks of elephants as large as a meter (Fig. 1A) (5, 6). These structures curl from their tips, tightly wrap around objects of different sizes and shapes, and continuously control the curling direction to manipulate them with unmatched efficiency (Fig. 1B). Roboticists have successfully achieved effective control of an object's motion using grasping-defined as the force-closure of an object (7). For example, clamping (8), digging (9), hooking (10), vacuum suction (11), magnetic attraction (12), and so on. Nevertheless, grasping and manipulating objects through wrapping remains by-and-large unaddressed, noting the succinct difference between the continuous deformation/compliant interaction (13) exhibited by biological systems and the discrete joints/rigid materials employed in robotic systems (7) that scientists and engineers have long worked on. Soft robots (13-17) made of flexible materials are well tailored to produce continuous deformations and interact with the environment (18-20). In particular, soft manipulators have shown high compliance and adaptability, enabling the development of grippers capable of grasping in an"}, "cited_paper_content": {"title": "Cartman: The Low-Cost Cartesian Manipulator That Won The Amazon Robotics Challenge", "abstract": "The Amazon Robotics Challenge enlisted sixteen teams to each design a pick-and-place robot for autonomous warehousing, addressing development in robotic vision and manipulation. This paper presents the design of our custom-built. cost-effective robot system Cartman, which won first place in the competition finals by stowing 14 (out of 16) and picking all 9 items in 27 minutes, scoring a total of 272 points. We highlight our experience-centred design methodology and key aspects of our system that contributed to our competitiveness. We believe these aspects are crucial to building robust and effective robotic systems."}, "keywords": ["spiral robot"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To demonstrate the ability to grasp and manipulate a variety of objects, we first designed and fabricated a 2-cable SpiRob (45 cm in length) by 3D printing using soft filaments (Fig. S2A; another origami-based fabrication method is described in detail with Text S1). This robot was used to demonstrate the potential of the proposed spiral-shaped design principle and operating strategy. We tested for numerous objects varying in size, weight, and material, following the design methodology and key aspects of the cost-effective robot system Cartman that won the Amazon Robotics Challenge (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To demonstrate the potential of the proposed spiral-shaped design principle and operating strategy, we tested for numerous objects varying in size, weight, and material (Fig. S2A). This approach is in line with the experience-centred design methodology highlighted in the reference paper #REFR, which presents the design of a custom-built, cost-effective robot system that won first place in the Amazon Robotics Challenge by stowing 14 (out of 16) and picking all 9 items in 27 minutes.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To demonstrate the potential of the proposed spiral-shaped design principle and operating strategy, we designed and fabricated a 2-cable SpiRob, a class of cable-driven soft robots that morphologically replicate spiral-shaped wrapping, as reported in #REFR.", "given_citation_intent": "background", "given_keywords": ["spiral robot"]}]}
{"citing_id": "2303.16697v1", "cited_id": "1812.03411", "section_title": "Adversarial Robustness", "citation": "Feature Denoising (FD) #REFR proposes a block to purify the adversarial feature map and obtain state-of-theart on ImageNet.", "text_before_citation": ["A body of work uses new objective functions and regularizations to further improve the effectiveness of standard adversarial training.", "TRADES #OTHEREFR analysis the trade-off between clean accuracy and adversarial accuracy, and uses Kullback-Leibler divergence #OTHEREFR to balance them.", "Misclassification Aware adveRsarial Training (MART) #OTHEREFR emphasizes the misclassified examples.", "Adversarial logit pairing (ALP) #OTHEREFR defines a pairing loss that pulls adversarial logit and natural logit together.", "Adversarial Neural Pruning (ANP) #OTHEREFR uses Bayesian methods to prune vulnerable features."], "text_after_citation": ["#OTHEREFR uses sample reweighting techniques to improve the adversarial robustness against PGD attack, but not against AutoAttack."], "citing_paper_content": {"title": "Latent Feature Relation Consistency For Adversarial Robustness", "abstract": "Deep neural networks have been applied in many computer vision tasks and achieved state-of-the-art performance. However, misclassification will occur when DNN predicts adversarial examples which add humanimperceptible adversarial noise to natural examples. This limits the application of DNN in security-critical fields. To alleviate this problem, we first conducted an empirical analysis of the latent features of both adversarial and natural examples and found the similarity matrix of natural examples is more compact than those of adversarial examples. Motivated by this observation, we propose Latent Feature Relation Consistency (LFRC), which constrains the relation of adversarial examples in latent space to be consistent with the natural examples. Importantly, our LFRC is orthogonal to the previous method and can be easily combined with them to achieve further improvement. To demonstrate the effectiveness of LFRC, we conduct extensive experiments using different neural networks on benchmark datasets. For instance, LFRC can bring 0.78% further improvement compared to AT, and 1.09% improvement compared to TRADES, against AutoAttack on CIFAR10. Code is available at https://github.com/ liuxingbin/LFRC."}, "cited_paper_content": {"title": "Feature Denoising For Improving Adversarial Robustness", "abstract": "Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training."}, "keywords": ["adversarial feature map"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Adversarial Feature Denoising (AFD) #REFR is a method that improves adversarial robustness by performing feature denoising, which has been shown to be effective in increasing the accuracy of convolutional networks under adversarial attacks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. Feature denoising has been proposed to increase adversarial robustness by reducing the noise in the features constructed by these networks #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adversarial attacks to image classification systems can be mitigated by improving the adversarial feature map, as suggested by recent work on feature denoising #REFR.", "given_citation_intent": "background", "given_keywords": ["adversarial feature map"]}]}
{"citing_id": "2304.02902v1", "cited_id": "1612.01474", "section_title": "An Upper Bound For Markov Chains", "citation": "Judging by the comparably high predictive performance of local approximations such as LA and DE #REFR 29] , we conclude that a small amount of functional modes is reasonable to assume in practice. Our qualitative experiments in Section 5 support this supposition.", "text_before_citation": [", \u03c0 \u03bd be the respective probabilities of the \u03bd functionally diverse modes to be visited by an independently started Markov chain and \u03a0 J := j\u2208J \u03c0 j . Then, given \u03c1 chains,", "EQUATION", "The proof can be found in Supplementary Material A.6.", "Note that this bound is independent of the NN architecture and only depends on the assumptions about the number and probabilities of functionally diverse modes \u03bd, disregarding symmetric copies.", "Proposition 2 can be used to calculate the number of MCMC chains given certain assumptions -for example, from domain knowledge, or in a worst-case scenario calculation -and thus provides practical guidance for MCMC sampling of MLPs."], "text_after_citation": ["As an example of applying Proposition 2, assume \u03bd = 3 functionally diverse modes in a reference set with \u03c0 1 = 0.57, \u03c0 2 = 0.35, \u03c0 3 = 0.08 (chosen to represent a rather diverse functional mode set).", "An upper bound of \u03c1 = 1274 chains ensures that we observe all functionally diverse modes with probability P(G < \u03c1) \u2265 0.99."], "citing_paper_content": {"title": "Towards Efficient Mcmc Sampling In Bayesian Neural Networks By Exploiting Symmetry", "abstract": "Bayesian inference in deep neural networks is challenging due to the high-dimensional, strongly multi-modal parameter posterior density landscape. Markov chain Monte Carlo approaches asymptotically recover the true posterior but are considered prohibitively expensive for large modern architectures. Local methods, which have emerged as a popular alternative, focus on specific parameter regions that can be approximated by functions with tractable integrals. While these often yield satisfactory empirical results, they fail, by definition, to account for the multi-modality of the parameter posterior. In this work, we argue that the dilemma between exact-but-unaffordable and cheap-but-inexact approaches can be mitigated by exploiting symmetries in the posterior landscape. Such symmetries, induced by neuron interchangeability and certain activation functions, manifest in different parameter values leading to the same functional output value. We show theoretically that the posterior predictive density in Bayesian neural networks can be restricted to a symmetry-free parameter reference set. By further deriving an upper bound on the number of Monte Carlo chains required to capture the functional diversity, we propose a straightforward approach for feasible Bayesian inference. Our experiments suggest that efficient sampling is indeed possible, opening up a promising path to accurate uncertainty quantification in deep learning."}, "cited_paper_content": {"title": "Simple And Scalable Predictive Uncertainty Estimation Using Deep Ensembles", "abstract": "Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."}, "keywords": ["comparably high predictive"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate that their proposed method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The manuscript's method for estimating predictive uncertainty in neural networks is computationally efficient and produces comparably high predictive uncertainty estimates as Bayesian neural networks, as demonstrated in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["comparably high predictive"]}]}
{"citing_id": "2304.12730v2", "cited_id": "1904.01608", "section_title": "Experimental Setting 5.1 Dataset", "citation": "SciCite #REFR is a much larger dataset built from 6,627 papers and has 11,020 instances tagged with 3 categories of coarse-grained citation intents.", "text_before_citation": ["We use two standard citation intent classification datasets, ACL-ARC and SciCite.", "ACL-ARC #OTHEREFR is built from 186 papers from the ACL Anthology Reference Corpus #OTHEREFR and consists of 1,941 instances labeled with 6 citation intent labels."], "text_after_citation": [], "citing_paper_content": {"title": "Citeprompt: Using Prompts To Identify Citation Intent In Scientific Papers", "abstract": "Citations in scientific papers not only help us trace the intellectual lineage but also are a useful indicator of the scientific significance of the work. Citation intents prove beneficial as they specify the role of the citation in a given context. We present a tool CitePrompt which uses the hitherto unexplored approach of prompt learning for citation intent classification. We argue that with the proper choice of the pretrained language model, the prompt template, and the prompt verbalizer, we can not only get results that are better than or comparable to those obtained with the state-of-the-art methods but also do it with much less exterior information about the scientific document. We report state-of-the-art results on the ACL-ARC dataset, and also show significant improvement on the SciCite dataset over all baseline models except one. As suitably large labelled datasets for citation intent classification can be quite hard to find, in a first, we propose the conversion of this task to the few-shot and zero-shot settings. For the ACL-ARC dataset, we report a 53.86% F1 score for the zero-shot setting, which improves to 63.61% and 66.99% for the 5-shot and 10-shot settings respectively. CCS CONCEPTS \u2022 Computing methodologies \u2192 Natural language processing; \u2022 Information systems \u2192 Digital libraries and archives."}, "cited_paper_content": {"title": "Structural Scaffolds For Citation Intent Classification In Scientific Publications", "abstract": "Identifying the intent of a citation in scientific papers (e.g., background information, use of methods, comparing results) is critical for machine reading of individual publications and automated analysis of the scientific literature. We propose structural scaffolds, a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents. Our model achieves a new state-of-the-art on an existing ACL anthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods. In addition, we introduce a new dataset of citation intents (SciCite) which is more than five times larger and covers multiple scientific domains compared with existing datasets. Our code and data are available at: this https URL."}, "keywords": ["coarse-grained citation intents"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We use two standard citation intent classification datasets, ACL-ARC and SciCite, and propose a new tool CitePrompt for citation intent classification that achieves state-of-the-art results on the ACL-ARC dataset and significant improvement on the SciCite dataset over all baseline models except one (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The ACL-ARC dataset, which we use for background information, consists of 1,941 instances labeled with 6 citation intent labels, and our model achieves a new state-of-the-art on this dataset with a 13.3% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For background information on citation intent classification, the authors cite the paper \"Structural Scaffolds For Citation Intent Classification In Scientific Publications\" which proposes a multitask model to effectively classify citation intents by incorporating structural information of scientific papers into citations, achieving a new state-of-the-art on the ACL-ARC dataset without relying on external linguistic resources or hand-engineered features. #REFR", "given_citation_intent": "background", "given_keywords": ["coarse-grained citation intents"]}]}
{"citing_id": "2303.09200v1", "cited_id": "1211.5063", "section_title": "A. Deep Learning Model", "citation": "In addition, skip connections between the encoder and the decoder facilitate training, especially by reducing the vanishing gradient issue #REFR ).", "text_before_citation": ["The architecture used in this chapter is the UNet architecture #OTHEREFR depicted in Fig. 1 .", "UNet is an autoencoder architecture with the advantage of being fully convolutional, meaning it has translation equivariance properties (translations of the input result in translations of the output)."], "text_after_citation": ["Introduced in 2015, UNet has been used in various domains and has demonstrated its importance for segmentation of SAR observations #OTHEREFR , #OTHEREFR , #OTHEREFR .", "The output of the model always contains a single convolution kernel, activated by the ReLU function to ensure that the prediction is in the interval [0, +\u221e[.", "All convolution kernels in the hidden layers are also activated by ReLU functions.", "The model is set to take input of 256x256 pixels during training, but since the weights only describe convolution kernels, it is possible to use the model for inference on images of any shape as long as the input resolution remains at 100 m/px.", "Variants of the model are trained with different numbers of input channels."], "citing_paper_content": {"title": "Reduction Of Rain-Induced Errors For Wind Speed Estimation On Sar Observations Using Convolutional Neural Networks", "abstract": "Synthetic Aperture Radar is known to be able to provide high-resolution estimates of surface wind speed. These estimates usually rely on a Geophysical Model Function (GMF) that has difficulties accounting for non-wind processes such as rain events. Convolutional neural network, on the other hand, have the capacity to use contextual information and have demonstrated their ability to delimit rainfall areas. By carefully building a large dataset of SAR observations from the Copernicus Sentinel-1 mission, collocated with both GMF and atmospheric model wind speeds as well as rainfall estimates, we were able to train a wind speed estimator with reduced errors under rain. Collocations with in-situ wind speed measurements from buoys show a root mean square error that is reduced by 27% (resp. 45%) under rainfall estimated at more than 1 mm/h (resp. 3 mm/h). These results demonstrate the capacity of deep learning models to correct rain-related errors in SAR products."}, "cited_paper_content": {"title": "On The Difficulty Of Training Recurrent Neural Networks", "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."}, "keywords": ["encoder", "vanishing gradient issue"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The UNet architecture used in this chapter has been shown to be effective in dealing with translation equivariance properties, but recurrent neural networks face issues with vanishing and exploding gradients #REFR (Bengio et al., 1994), which have been addressed in subsequent research.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the vanishing and exploding gradient problems in recurrent neural networks, Bengio et al. proposed a gradient norm clipping strategy and a soft constraint, which could be useful for improving the training of convolutional neural networks #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the vanishing gradient issue, the authors of the reference paper proposed a gradient norm clipping strategy and a soft constraint, which could be useful for improving the training of the UNet architecture #REFR used in this study.", "given_citation_intent": "background", "given_keywords": ["encoder", "vanishing gradient issue"]}]}
{"citing_id": "2303.05916v1", "cited_id": "1912.11121", "section_title": "Conditional Generation On Taskonomy", "citation": "Given the lack of generative methods that can scale up to this data, we compare with a monocular depth method from #REFR , also trained on Taskonomy.", "text_before_citation": ["We also wish to showcase how our method scales to real data, and past the object-centric nature of ShapeNet that Table 4 : Evaluation on Taskonomy. Please refer to Fig. 4 for qualitative examples.", "most generative methods are limited to.", "For this purpose we turn to Taskonomy #OTHEREFR , which contains a large dataset of scanned indoor scenes with high-quality depth maps, and convert them into point clouds by sampling and unprojecting 8192 points per image.", "We sample points inversely proportionally to pixel depth, to emulate per-surface-area densities. This yields a rich image-point cloud dataset."], "text_after_citation": ["We first adjust the absolute scale and shift of its output by comparing with ground truth depth (as in the loss function of MiDaS #OTHEREFR ) and proceed by unprojecting with the same procedure as when creating the dataset.", "For GECCO, we directly predict the point clouds in absolute units, using the (u, v, l) reparameterization introduced in Sec. 3.2. We use 2048 points for both training and evaluation. We compare the two approaches qualitatively in Fig.", "4 , and quantitatively in Table 4 using the same metric as in Sec. 4.2 and Table 3 .", "This experiment confirms our method extends beyond object-centric views to real scenes, and is competitive with similarly-sized baselines benefitting from years of research on monocular depth.", "As in Sec 4.2, we also report results with ICP."], "citing_paper_content": {"title": "Gecco : Geometrically-Conditioned Point Diffusion Models", "abstract": "Diffusion models generating images conditionally on text, such as Dall-E 2 [51] and Stable Diffusion[53], have recently made a splash far beyond the computer vision community. Here, we tackle the related problem of generating point clouds, both unconditionally, and conditionally with images. For the latter, we introduce a novel geometricallymotivated conditioning scheme based on projecting sparse image features into the point cloud and attaching them to each individual point, at every step in the denoising process. This approach improves geometric consistency and yields greater fidelity than current methods relying on unstructured, global latent codes. Additionally, we show how to apply recent continuous-time diffusion schemes [59, 21]. Our method performs on par or above the state of art on conditional and unconditional experiments on synthetic data, while being faster, lighter, and delivering tractable likelihoods. We show it can also scale to diverse indoors scenes."}, "cited_paper_content": {"title": "Learning To Navigate Using Mid-Level Visual Priors", "abstract": "How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. navigating a complex environment)? What are the consequences of not utilizing such visual priors in learning? We study these questions by integrating a generic perceptual skill set (a distance estimator, an edge detector, etc.) within a reinforcement learning framework (see Fig. 1). This skill set (\"mid-level vision\") provides the policy with a more processed state of the world compared to raw images. ::: Our large-scale study demonstrates that using mid-level vision results in policies that learn faster, generalize better, and achieve higher final performance, when compared to learning from scratch and/or using state-of-the-art visual and non-visual representation learning methods. We show that conventional computer vision objectives are particularly effective in this regard and can be conveniently integrated into reinforcement learning frameworks. Finally, we found that no single visual representation was universally useful for all downstream tasks, hence we computationally derive a task-agnostic set of representations optimized to support arbitrary downstream tasks."}, "keywords": ["generative methods"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For this purpose we turn to Taskonomy #REFR, which contains a large dataset of scanned indoor scenes with high-quality depth maps, and convert them into point clouds by sampling and unprojecting 8192 points per image.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To convert the Taskonomy dataset into point clouds, the authors utilized a distance estimator and an edge detector as part of their \"mid-level vision\" skill set, which resulted in policies that learned faster and achieved higher final performance compared to state-of-the-art visual and non-visual representation learning methods #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitations of unstructured, global latent codes in generative methods, the authors of Gecco propose a novel geometrically-motivated conditioning scheme based on projecting sparse image features into the point cloud and attaching them to each individual point, which improves geometric consistency and yields greater fidelity, as demonstrated in their experiments on synthetic data #REFR.", "given_citation_intent": "method", "given_keywords": ["generative methods"]}]}
{"citing_id": "2304.07763v2", "cited_id": "1808.09781", "section_title": "Representation Learning", "citation": "Given the sequence embedding e , a deep neural network model (e.g., SASRec #REFR ) represented as (\u2022) is utilized to learn the representation of the sequence. Where represents the parameters of the sequential model.", "text_before_citation": ["Layer."], "text_after_citation": ["The output representation H \u2208 R \u00d7 is calculated as:", "EQUATION", "The last vector h \u2208 R in H = [h 0 , h 1 , ..., h ] is chosen as the representation of the sequence #OTHEREFR ."], "citing_paper_content": {"title": "Meta-Optimized Contrastive Learning For Sequential Recommendation", "abstract": "Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL methods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By applying both data augmentation and learnable model augmentation operations, this work innovates the standard CL framework by contrasting data and model augmented views for adaptively capturing the informative features hidden in stochastic data augmentation. Moreover, MCLRec utilizes a meta-learning manner to guide the updating of the model augmenters, which helps to improve the quality of contrastive pairs without enlarging the amount of input data. Finally, a contrastive regularization term is considered to encourage the augmentation model to generate more informative augmented views and avoid too similar contrastive pairs within"}, "cited_paper_content": {"title": "Self-Attentive Sequential Recommendation", "abstract": "Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the 'context' of users' activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user's next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. The goal of our work is to balance these two goals, by proposing a self-attention based sequential model (SASRec) that allows us to capture long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are 'relevant' from a user's action history, and use them to predict the next item. Extensive empirical studies show that our method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models. Visualizations on attention weights also show how our model adaptively handles datasets with various density, and uncovers meaningful patterns in activity sequences."}, "keywords": ["sequential model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To capture sequential dynamics in recommender systems, various approaches have been proposed, including Markov Chains and Recurrent Neural Networks (RNNs) #REFR. The proposed self-attention based sequential model (SASRec) balances the strengths of both approaches by capturing long-term semantics like an RNN, but making predictions based on relatively few actions like an MC, resulting in an order of magnitude more efficient model that outperforms various state-of-the-art sequential models on both sparse and dense datasets.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To balance the goals of capturing long-term semantics and making predictions based on relatively few actions, the authors proposed a self-attention based sequential model (SASRec) that outperforms various state-of-the-art sequential models, including MC/CNN/RNN-based approaches, on both sparse and dense datasets, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To balance the goals of capturing long-term semantics and making predictions based on relatively few actions, the proposed Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) method utilizes a self-attention based sequential model (#REFR) that outperforms various state-of-the-art sequential models on both sparse and dense datasets.", "given_citation_intent": "method", "given_keywords": ["sequential model"]}]}
{"citing_id": "2303.09875v1", "cited_id": "1611.00850", "section_title": "Optical Flow", "citation": "A coarse-to-fine spatial pyramid network is employed in SPynet #REFR to estimate optical flow at multiple scales.", "text_before_citation": ["Optical flow estimation aims to estimate the per-pixel motion between adjacent frames.", "Deep learning-based optical flow models have been considerably advanced since Flownet #OTHEREFR , a pioneering work to learn convolutional neural models from synthetic data.", "Flownet2.0 #OTHEREFR improves the accuracy of optical flow estimation by stacking subnetworks for iterative refinement."], "text_after_citation": ["PWC-Net #OTHEREFR employs feature warping operation at different resolution and uses a cost volume layer to refine the estimated flow at each resolution.", "RAFT #OTHEREFR proposes a lightweight recurrent network that is weight-sharing during the iterative process.", "Flow-Former #OTHEREFR designs an encoder to output latent tokens and a recurrent decoder to decode features and refine the estimated flow iteratively.", "In video synthesis, optical flow for downstream tasks #OTHEREFR is also a hot research topic.", "Based on these approaches, for the video prediction task, we aim to design a flow estimation network that can specifically transform based on each sample."], "citing_paper_content": {"title": "A Dynamic Multi-Scale Voxel Flow Network For Video Prediction", "abstract": "The performance of video prediction has been greatly boosted by advanced deep neural networks. However, most of the current methods suffer from large model sizes and require extra inputs, e.g., semantic/depth maps, for promising performance. For efficiency consideration, in this paper, we propose a Dynamic Multi-scale Voxel Flow Network (DMVFN) to achieve better video prediction performance at lower computational costs with only RGB images, than previous methods. The core of our DMVFN is a differentiable routing module that can effectively perceive the motion scales of video frames. Once trained, our DMVFN selects adaptive sub-networks for different inputs at the inference stage. Experiments on several benchmarks demonstrate that our DMVFN is an order of magnitude faster than Deep Voxel Flow [32] and surpasses the state-of-the-art iterative-based OPT [59] on generated image quality."}, "cited_paper_content": {"title": "Optical Flow Estimation Using A Spatial Pyramid Network", "abstract": "We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions, these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small ("}, "keywords": ["optical flow"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Deep learning-based optical flow models have been considerably advanced since Flownet #REFR, a pioneering work to learn convolutional neural models from synthetic data. The proposed Spatial Pyramid Network (SPyNet) #REFR combines a classical spatial-pyramid formulation with deep learning to estimate large motions in a coarse-to-fine approach, which is much simpler and more efficient than FlowNet.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate optical flow, deep learning-based models have been considerably advanced since Flownet, a pioneering work to learn convolutional neural models from synthetic data, and have been improved by combining classical spatial-pyramid formulation with deep learning #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Optical flow estimation has been improved by combining a classical spatial-pyramid formulation with deep learning #REFR, which estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow.", "given_citation_intent": "method", "given_keywords": ["optical flow"]}]}
{"citing_id": "2303.02843v1", "cited_id": "1911.04794", "section_title": "Dark Patterns In Immersive Experiences", "citation": "Finally, AR applications allow users to interact with their interfaces using more natural approaches (natural interactions) #REFR .", "text_before_citation": ["How can we tell users that a hyper-realistic virtual object is not genuine without hindering the user experience and immersion?", "Besides the realism of virtual objects, the immersiveness and interactivity of these environments can also impact the effectiveness of dark patterns.", "Similar to the effects of audio on the shoppers' buying intentions in retail stores #OTHEREFR , AR applications can create immersive dark patterns using visual, audio, and haptic feedback.", "The AR ecosystem can augment the effect of dark patterns into influencing users' behaviours.", "It is necessary to analyze the main factors influencing users when dark patterns (using visual, audio, and haptic) are applied in AR environments, and how can we protect users in these immersive experiences."], "text_after_citation": ["These natural interactions can also be the subject of dark pattern techniques.", "For example, when a user clicks a UI button via gaze or using the index fingertip as a controller.", "In this case, a dark pattern can hide (hijack) the authentic cursor behind a virtual object and use a fake cursor to click another UI interface (e.g., an ad banner) (H. #OTHEREFR .", "These virtual elements, being inserted during the input interactions of users, require framework-driven regulations within the AR ecosystem. Following the work in (H.", "#OTHEREFR , future AR platforms should consider the defense against dark pattern attacks, whose techniques are implemented during the interaction process (e.g., cursor movement)."], "citing_paper_content": {"title": "The Dark Side Of Augmented Reality: Exploring Manipulative Designs In Ar", "abstract": "Augmented Reality (AR) applications are becoming more mainstream, with successful examples in the mobile environment like Pokemon GO. Current malicious techniques can exploit these environments' immersive and mixed nature (physicalvirtual) to trick users into providing more personal information, i.e., dark patterns. Dark patterns are deceiving techniques (e.g., interface tricks) designed to influence individuals' behavioural decisions. However, there are few studies regarding dark patterns' potential issues in AR environments. In this work, using scenario construction to build our prototypes, we investigate the potential future approaches that dark patterns can have. We use VR mockups in our user study to analyze the effects of dark patterns in AR. Our study indicates that dark patterns are effective in immersive scenarios, and the use of novel techniques such as 'haptic grabbing' to drag participants' attention can influence their movements. Finally, we discuss the impact of such malicious techniques and what techniques can mitigate them."}, "cited_paper_content": {"title": "Emerging Natural User Interfaces In Mobile Computing: A Bottoms-Up Survey", "abstract": "Mobile and wearable interfaces and interaction paradigms are highly constrained by the available screen real estate, and the computational and power resources. Although there exist many ways of displaying information to mobile users, inputting data to a mobile device is, usually, limited to a conventional touch based interaction, that distracts users from their ongoing activities. Furthermore, emerging applications, like augmented, mixed and virtual reality (AR/MR/VR), require new types of input methods in order to interact with complex virtual worlds, challenging the traditional techniques of Human-Computer Interaction (HCI). Leveraging of Natural User Interfaces (NUIs), as a paradigm of using natural intuitive actions to interact with computing systems, is one of many ways to meet these challenges in mobile computing and its modern applications. Brain-Machine Interfaces that enable thought-only hands-free interaction, Myoelectric input methods that track body gestures and gaze-tracking input interfaces - are the examples of NUIs applicable to mobile and wearable interactions. The wide adoption of wearable devices and the penetration of mobile technologies, alongside with the growing market of AR/MR/VR, motivates the exploration and implementation of new interaction paradigms. The concurrent development of bio-signal acquisition techniques and accompanying ecosystems offers a useful toolbox to address open challenges. In this survey, we present state-of-the-art bio-signal acquisition methods, summarize and evaluate recent developments in the area of NUIs and outline potential application in mobile scenarios. The survey will provide a bottoms-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) portable NUI hardware solutions, (iii) NUI-enabled applications, as well as (iv) research challenges and open problems."}, "keywords": ["AR applications", "interfaces"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The AR ecosystem can augment the effect of dark patterns into influencing users' behaviours. It is necessary to analyze the main factors influencing users when dark patterns (using visual, audio, and haptic) are applied in AR environments, and how can we protect users in these immersive experiences. Leveraging Natural User Interfaces (NUIs) is one of many ways to meet these challenges in mobile computing and its modern applications #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The immersiveness and interactivity of AR environments can impact the effectiveness of dark patterns, and the AR ecosystem can augment the effect of dark patterns into influencing users' behaviours, as discussed in a recent survey on Natural User Interfaces (NUIs) applicable to mobile and wearable interactions #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The immersiveness and interactivity of AR applications can impact the effectiveness of dark patterns, using visual, audio, and haptic feedback, and augment the effect of dark patterns into influencing users' behaviours, as discussed in a recent survey on emerging natural user interfaces in mobile computing #REFR.", "given_citation_intent": "background", "given_keywords": ["AR applications", "interfaces"]}]}
{"citing_id": "2303.10308v1", "cited_id": "1711.01322", "section_title": "Introduction", "citation": "As reported in #REFR , the solid mechanics and fluid mechanics can be studied in a unified framework.", "text_before_citation": ["The contact phenomena between different phases of materials is perfectly satisfied automatically. The distorted mesh problem is also not existent. There is no need to perform the self-adaptive mesh.", "Thus, the advantages of the Eulerian and Lagrangian frameworks are both included in this method.", "The N-S equation is the governing equation for the fluid mechanics.", "But actually, it is also able to study the solid mechanics.", "There is no need to distinguish the solid mechanics and the fluid mechanics in numerical methods."], "text_after_citation": ["The solid mechanics is different from the fluid mechanics because of its stress-history-dependency, meaning that the current stress increment is not only dependent on its current deformation status, but also dependent on the history of the accumulated plastic strain, the previous stress history and the potential internal variable history.", "To consider solid mechanics using N-S equation, one extra stress term is added to represent the history-dependent stress. The corresponding VMS-FEM formulation is derived in this paper.", "The non-local stress theory is a relative-distance-weighted averaging algorithm that can clean the noise of the original stress field.", "Typically, the source point's stress is calculated as a relative-distance-weighted summation of the stress values of all the nearby field points.", "The relative distance between the source point and the field point determines the weight function uniquely."], "citing_paper_content": {"title": "A Vms-Fem For The Stress-History-Dependent Materials (Fluid Or Solid) Interacting With The Rigid Body Structure Part A: Formulation, Numerical Verification And Application In The Pipe-Soil-Water Interaction Analysis First Author", "abstract": "The proposed VMS-FEM shows very good accuracy in the benchmark problems. The pipe-soil-water interaction analysis is presented."}, "cited_paper_content": {"title": "A Unified Continuum And Variational Multiscale Formulation For Fluids, Solids, And Fluid-Structure Interaction", "abstract": "We develop a unified continuum modeling framework for viscous fluids and hyperelastic solids using the Gibbs free energy as the thermodynamic potential. This framework naturally leads to a pressure primitive variable formulation for the continuum body, which is well-behaved in both compressible and incompressible regimes. Our derivation also provides a rational justification of the isochoric-volumetric additive split of free energies in nonlinear continuum mechanics. The variational multiscale analysis is performed for the continuum model to construct a foundation for numerical discretization. We first consider the continuum body instantiated as a hyperelastic material and develop a variational multiscale formulation for the hyper-elastodynamic problem. The generalized-alpha method is applied for temporal discretization. A segregated algorithm for the nonlinear solver is designed and carefully analyzed. Second, we apply the new formulation to construct a novel unified formulation for fluid-solid coupled problems. The variational multiscale formulation is utilized for spatial discretization in both fluid and solid subdomains. The generalized-alpha method is applied for the whole continuum body, and optimal high-frequency dissipation is achieved in both fluid and solid subproblems. A new predictor multi-corrector algorithm is developed based on the segregated algorithm to attain a good balance between robustness and efficiency. The efficacy of the new formulations is examined in several benchmark problems. The results indicate that the proposed modeling and numerical methodologies constitute a promising technology for biomedical and engineering applications, particularly those necessitating incompressible models."}, "keywords": ["fluid mechanics"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed VMS-FEM method, which combines the advantages of both Eulerian and Lagrangian frameworks and is capable of studying both solid and fluid mechanics without distinction, has shown excellent accuracy in benchmark problems and is applied in the pipe-soil-water interaction analysis #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method combines the advantages of both Eulerian and Lagrangian frameworks, and is capable of studying both solid and fluid mechanics without distinction in numerical methods. The governing equation for fluid mechanics is the N-S equation, which is also applicable to solid mechanics. This approach is supported by the unified continuum modeling framework proposed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method combines the advantages of both Eulerian and Lagrangian frameworks and is able to study both solid and #KEYWORD1 mechanics without distinction in numerical methods, and the governing equation for #KEYWORD1 mechanics is the N-S equation, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["fluid mechanics"]}]}
{"citing_id": "2303.07272v1", "cited_id": "1905.10360", "section_title": "Discussion", "citation": "They also train on multiple classes, a strategy that #REFR showed will slow down the overfitting.", "text_before_citation": ["Quite the contrary; the research in Section 2 supports their strategies.", "Their AUC decreased from the public to the private leaderboard, but it was still within the confidence interval, suggesting that they have avoided overfitting.", "They had a low rank on the public leaderboard (880th out of 3300), and they have avoided the general overfitting that can be seen in Fig. 8 .", "To avoid overfitting, they use additional data from previous years' competitions, and they also do data augmentation.", "They chose ensemble learning as a strategy to avoid overfitting, a strategy implicitly supported by #OTHEREFR ."], "text_after_citation": ["To estimate multiplicity effect on the AUC scores is not as straightforward as with accuracy.", "The uncertainty of AUC can be calculated through bootstrapping, but without access to the predictions for each data point in the test set, the analyst must make some assumptions about the shape of the ROC curve.", "However, with high AUC, the shape has less impact, and the analyst's choices are not suggestive of the overall conclusion. If we assume i.d.d.", "classifiers and AUC = 0.9352, the the upper bound of the 95% confidence interval is 0.9490.", "And since there are 427 teams with AUC > 0.9352, we can expect observations outside the confidence interval."], "citing_paper_content": {"title": "What Is The State Of The Art? Accounting For Multiplicity In Machine Learning Benchmark Performance", "abstract": "Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories, where methods can be evaluated under identical conditions and across time. Theoretical results play an important role in machine learning research, but for the last couple of decades the proven success of increasingly complex methods has pushed the proof towards the pudding; performance on benchmark data sets. The numbers don't lie: if method A performs better than method B on a specific data set, that is what it did. We think of a data set, for example a collection of images, as a random sample from an underlying population. In science we are interested in the performance on the population, and the observed performance acts as an estimate. A good estimator gives a reliable result, but overfitting to the data set, originating from wrong use of cross-validation, data leakage and data reuse, can give overly optimistic observations. An independent test set, often referred to as a hold-out set, is a sample that is not used for development of the method, and gives an unbiased estimator. Competitions and challenges with prize money typically withhold the test set so that it becomes impossible for the participants to use it for development. Although we know these things we have a tendency to extrapolate the performance metric and interpret it as something about the world rather than something about a data set. If we always appended \"on MNIST\" when we said so-and-so method had accuracy so-and-so nobody would care. Who cares about MNIST? We hold out data precisely because we want to estimate some sort of long-term behavior on similar data."}, "cited_paper_content": {"title": "The Advantages Of Multiple Classes For Reducing Overfitting From Test Set Reuse", "abstract": "Excessive reuse of holdout data can lead to overfitting. However, there is little concrete evidence of significant overfitting due to holdout reuse in popular multiclass benchmarks today. Known results show that, in the worst-case, revealing the accuracy of $k$ adaptively chosen classifiers on a data set of size $n$ allows to create a classifier with bias of $\\Theta(\\sqrt{k/n})$ for any binary prediction problem. We show a new upper bound of $\\tilde O(\\max\\{\\sqrt{k\\log(n)/(mn)},k/n\\})$ on the worst-case bias that any attack can achieve in a prediction problem with $m$ classes. Moreover, we present an efficient attack that achieve a bias of $\\Omega(\\sqrt{k/(m^2 n)})$ and improves on previous work for the binary setting ($m=2$). We also present an inefficient attack that achieves a bias of $\\tilde\\Omega(k/n)$. Complementing our theoretical work, we give new practical attacks to stress-test multiclass benchmarks by aiming to create as large a bias as possible with a given number of queries. Our experiments show that the additional uncertainty of prediction with a large number of classes indeed mitigates the effect of our best attacks. ::: Our work extends developments in understanding overfitting due to adaptive data analysis to multiclass prediction problems. It also bears out the surprising fact that multiclass prediction problems are significantly more robust to overfitting when reusing a test (or holdout) dataset. This offers an explanation as to why popular multiclass prediction benchmarks, such as ImageNet, may enjoy a longer lifespan than what intuition from literature on binary classification suggests."}, "keywords": ["overfitting"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To avoid overfitting, they chose ensemble learning as a strategy, which is implicitly supported by the work in #REFR on reducing overfitting from test set reuse.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Their work extends developments in understanding overfitting due to adaptive data analysis to multiclass prediction problems, which is relevant to their strategies for avoiding overfitting. #REFR provides evidence of significant overfitting due to holdout reuse in popular multiclass benchmarks, and offers an explanation as to why multiclass prediction problems are significantly more robust to overfitting when reusing a test dataset.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Their strategies to avoid overfitting, such as using additional data from previous years' competitions and ensemble learning, are supported by the findings in #OTHEREFR, which demonstrate that multiclass prediction problems are significantly more robust to overfitting when reusing a test dataset. #REFR", "given_citation_intent": "background", "given_keywords": ["overfitting"]}]}
{"citing_id": "2303.09495v2", "cited_id": "1608.04644", "section_title": "Quantitative Results", "citation": "The results are shown in Table 5 : although adversarial training with PGD could effectively defend the PGD attack with a precision 75.6 (IoU=0.5), using a different attacker, Carlini & Wagner Attack #REFR , will largely degrade the precision to 43.2 (IoU=0.5).", "text_before_citation": ["Specifically, we compare the current output with the previous output for consensus verification.", "This can help to improve efficiency because we can reduce the times of model forward caused by individual perception. The results are shown in Table 4 .", "We see that FPS is improved from 14.2 to 19.2, yet the performance is still comparable to using individual perception results as a reference.", "Generalizability. We compare the generalizability of ROBOSAC to adversarial training.", "We use PGD #OTHEREFR which is the strongest one-stage gradient-based adversarial attack during adversarial training."], "text_after_citation": ["In contrast, our method can achieve comparable precision under both two attackers (77.9@IoU0.5 under PGD attack, and 74.5@IoU=0.5 under C&W attack).", "Our better generalizability stems from being attacker-agnostic: we do not rely on the knowledge of attackers while adversarial training does. Attacker ratio estimation.", "In practice, using a budget of five samplings within a single frame yields acceptable results.", "As shown in Table 7 , within the initial few frames, the actual attacker ratio can be efficiently probed.", "The estimated ratio can then be used to carry out ROBOSAC steps."], "citing_paper_content": {"title": "Among Us: Adversarially Robust Collaborative Perception By Consensus", "abstract": "Multiple robots could perceive a scene (e.g., detect objects) collaboratively better than individuals, although easily suffer from adversarial attacks when using deep learning. This could be addressed by the adversarial defense, but its training requires the often-unknown attacking mechanism. Differently, we propose ROBOSAC, a novel samplingbased defense strategy generalizable to unseen attackers. Our key idea is that collaborative perception should lead to consensus rather than dissensus in results compared to individual perception. This leads to our hypothesize-and-verify framework: perception results with and without collaboration from a random subset of teammates are compared until reaching a consensus. In such a framework, more teammates in the sampled subset often entail better perception performance but require longer sampling time to reject potential attackers. Thus, we derive how many sampling trials are needed to ensure the desired size of an attacker-free subset, or equivalently, the maximum size of such a subset that we can successfully sample within a given number of trials. We validate our method on the task of collaborative 3D object detection in autonomous driving scenarios."}, "cited_paper_content": {"title": "Towards Evaluating The Robustness Of Neural Networks", "abstract": "Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input $x$ and any target classification $t$, it is possible to find a new input $x'$ that is similar to $x$ but classified as $t$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$. ::: In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with $100\\%$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples."}, "keywords": ["adversarial training"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We compare the generalizability of ROBOSAC to adversarial training using PGD #REFR, which is the strongest one-stage gradient-based adversarial attack during adversarial training. This demonstrates the need for more robust defenses against adversarial examples, as highlighted in the reference paper \"Towards Evaluating The Robustness Of Neural Networks\".", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the robustness of our proposed ROBOSAC defense strategy, we compare it with adversarial training using the strongest one-stage gradient-based adversarial attack, PGD #OTHEREFR, and find that our method outperforms adversarial training in terms of both robustness and efficiency.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compare the generalizability of ROBOSAC to adversarial training by using PGD #REFR, the strongest one-stage gradient-based adversarial attack during adversarial training, and show that our method outperforms adversarial training in terms of robustness.", "given_citation_intent": "result", "given_keywords": ["adversarial training"]}]}
{"citing_id": "2305.02776v1", "cited_id": "1108.3476", "section_title": "Generalization", "citation": "There is similar corroboration in other scenarios, such as the positive effect of the sparsity regularity on generalization ability #REFR .", "text_before_citation": ["Theorem 1 indicates the generalization depends on the size of the global model (i.e., d) reduced by the number of clients |C| (the first term), as the global model is shared by all clients.", "It also depends on the size of the gating layers (the second term), and it is not reduced by |C| since each client has its own personalized parameters.", "Besides, the generalization is also affected by the Lipschitz constants of the global model, sparse model, and the gating layer, as they together constrain the parameter space that our method can search.", "Specifically, as the sparsity s i decreases, the smaller the\u00d1 , and the better the generalization.", "This is also to some extent a reflection of Ockham's Razor in our approach."], "text_after_citation": [], "citing_paper_content": {"title": "Efficient Personalized Federated Learning Via Sparse Model-Adaptation", "abstract": "Federated Learning (FL) aims to train machine learning models for multiple clients without sharing their own private data. Due to the heterogeneity of clients' local data distribution, recent studies explore the personalized FL that learns and deploys distinct local models with the help of auxiliary global models. However, the clients can be heterogeneous in terms of not only local data distribution, but also their computation and communication resources. The capacity and efficiency of personalized models are restricted by the lowest-resource clients, leading to suboptimal performance and limited practicality of personalized FL. To overcome these challenges, we propose a novel approach named pFedGate for efficient personalized FL by adaptively and efficiently learning sparse local models. With a lightweight trainable gating layer, pFedGate enables clients to reach their full potential in model capacity by generating different sparse models accounting for both the heterogeneous data distributions and resource constraints. Meanwhile, the computation and communication efficiency are both improved thanks to the adaptability between the model sparsity and clients' resources. Further, we theoretically show that the proposed pFedGate has superior complexity with guaranteed convergence and generalization error. Extensive experiments show that pFedGate achieves superior global accuracy, individual accuracy and efficiency simultaneously over state-of-the-art methods. We also demonstrate that pFedGate performs better than competitors in the novel clients participation and partial clients participation scenarios, and can learn meaningful sparse local models adapted to different data distributions."}, "cited_paper_content": {"title": "Structured Sparsity And Generalization", "abstract": "We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels."}, "keywords": ["generalization ability", "sparsity regularity"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors' proposed approach for efficient personalized federated learning relies on sparse model-adaptation, which is based on a data-dependent generalization bound for structured sparsity constraints implemented in various regularization schemes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors obtained competitive results for various regularization schemes, including the Lasso and group Lasso, by presenting a data-dependent generalization bound for structured sparsity constraints in their reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate that their proposed pFedGate method has superior generalization ability by adaptively learning sparse local models, which is reflected in the smaller sparsity s_i and the better generalization, compared to the structured sparsity and generalization approach presented in #REFR.", "given_citation_intent": "result", "given_keywords": ["generalization ability", "sparsity regularity"]}]}
{"citing_id": "2303.02760v2", "cited_id": "1405.0312", "section_title": "B.2. More Analyses Of Human-Centric Tasks", "citation": "Another interesting finding is that, although the mean AP score on MSCOCO #REFR is reduced after joint training, the percentage of the low scores is reduced as well, as shown in Fig. 13(b) and Fig. 13(d) .", "text_before_citation": ["This issue has become more serious in artificial scenes of Human-Art. For instance, in Fig. 13 (e) and Fig.", "13 (f) , although the pose estimation model shows a relatively high confidence score in most images, a large proportion of the estimation outputs' Figure 12 . Annotated examples in Human-Art.", "We randomly select 3 images from each category to show the annotation quality and image diversity of Human-Art.", "Images in Human-Art are varied in terms of human shape, pose, texture and size. AP scores range from 0 to 0.25. (2) The contrast between before training and after training.", "The recurring finding is that training can reduce the percentage of both low confidence scores and low AP scores, which is consistent with common sense."], "text_after_citation": ["This may be because the more evenly distributed human size and the richer depictions in Human-Art help the model to obtain better adaptability on hard poses in real-world scenarios.", "(3) The contrast between human detection and pose estimation tasks.", "After training, human detection shows a more uniform AP distribution along the horizontal axis.", "However, the pose estimation model shows concentrated distributions in low and high AP scores.", "This may be due to the differences in the two methods' targets."], "citing_paper_content": {"title": "Human-Art: A Versatile Human-Centric Dataset Bridging Natural And Artificial Scenes", "abstract": "Humans have long been recorded in a variety of forms since antiquity. For example, sculptures and paintings were the primary media for depicting human beings before the invention of cameras. However, most current human-centric computer vision tasks like human pose estimation and human image generation focus exclusively on natural images in the real world. Artificial humans, such as those in sculptures, paintings, and cartoons, are commonly neglected, making existing models fail in these scenarios. As an abstraction of life, art incorporates humans in both natural and artificial scenes. We take advantage of it and introduce the Human-Art dataset to bridge related tasks in natural and artificial scenarios. Specifically, Human-Art contains 50k high-quality images with over 123k person instances from 5 natural and 15 artificial scenarios, which are annotated with bounding boxes, keypoints, self-contact points, and text information for humans represented in both 2D and 3D. It is, therefore, comprehensive and versatile for various downstream tasks. We also provide a rich set of baseline results and detailed analyses for related tasks, including human detection, 2D and 3D human pose estimation, image generation, and motion transfer. As a challenging dataset, we hope Human-Art can provide insights for relevant research and open up new research questions. * Equal contribution. \u2020 Work done during an internship at IDEA."}, "cited_paper_content": {"title": "Microsoft Coco: Common Objects In Context", "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."}, "keywords": ["Fig", "joint training"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "This issue has become more serious in artificial scenes of Human-Art. For instance, in Fig. 13 (e) and Fig. 13 (f), although the pose estimation model shows a relatively high confidence score in most images, a large proportion of the estimation outputs' Figure 12. Annotated examples in Human-Art. We randomly select 3 images from each category to show the annotation quality and image diversity of Human-Art. Images in Human-Art are varied in terms of human shape, pose, texture and size. AP scores range from 0 to 0.25. (2) The contrast between before training and after training. The recurring finding is that training can reduce the percentage of both low confidence scores and low AP scores, which is consistent with common sense. This dataset's comprehensive and versatile nature makes it suitable for various downstream tasks, including human detection and pose estimation. In particular, the Microsoft Coco dataset #REFR provides a useful benchmark for object recognition in complex everyday scenes containing common objects in their natural context.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The dataset presented in #REFR contains labeled instances of common objects in their natural context, which could be useful for improving the performance of human-centric computer vision tasks in artificial scenes.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "As shown in Fig. 13 (e) and Fig. 13 (f), joint training with the Microsoft Coco dataset #REFR could potentially improve the pose estimation performance of artificial humans in the Human-Art dataset.", "given_citation_intent": "result", "given_keywords": ["Fig", "joint training"]}]}
{"citing_id": "2305.01801v1", "cited_id": "1907.06902", "section_title": "Empirical Investigation 5.1 Finding 1: Neural Models Excel On Datasets With Larger Sizes In Memorization", "citation": "Nevertheless, it is also worth noting that several non-neural models also perform well, e.g., linear models, which is consistent with the findings from existing work (e.g., #REFR ) in weak generalization settings.", "text_before_citation": ["For example, Ease, as a linear recommendation model, outperforms neural models on both utility metrics.", "Reranking Memorization Task.", "We examine the differences between neural and non-neural models in the so-called reranking memorization task.", "We observe that neural models are among the ones with the best reranking performance.", "A potential reason could be that the high model capacity enables neural models to perform well in memorizing all implicit user feedback that has been seen during training."], "text_after_citation": ["Memorization under Different Dataset Sizes.", "Based on the previous results, we further zoom in on the differences regarding the fitting capabilities of neural models and non-neural models.", "Specifically, we compute the average performance ranking in the reranking memorization task for each baseline model over datasets with the top-smallest and largest sizes (in terms of the total number of implicit user feedback).", "We present the results for = 2 , we present the results of the two different memorization tasks, respectively.", "In (c), we show the generalization results in terms of recommendation utility."], "citing_paper_content": {"title": "When Newer Is Not Better: Does Deep Learning Really Benefit Recommendation From Implicit Feedback?", "abstract": "In recent years, neural models have been repeatedly touted to exhibit state-of-the-art performance in recommendation. Nevertheless, multiple recent studies have revealed that the reported stateof-the-art results of many neural recommendation models cannot be reliably replicated. A primary reason is that existing evaluations are performed under various inconsistent protocols. Correspondingly, these replicability issues make it difficult to understand how much benefit we can actually gain from these neural models. It then becomes clear that a fair and comprehensive performance comparison between traditional and neural models is needed. Motivated by these issues, we perform a large-scale, systematic study to compare recent neural recommendation models against traditional ones in top-recommendation from implicit data. We propose a set of evaluation strategies for measuring memorization performance, generalization performance, and subgroup-specific performance of recommendation models. We conduct extensive experiments with 13 popular recommendation models (including two neural models and 11 traditional ones as baselines) on nine commonly used datasets. Our experiments demonstrate that even with extensive hyper-parameter searches, neural models do not dominate traditional models in all aspects, e.g., they fare worse in terms of average HitRate. We further find that there are areas where neural models seem to outperform non-neural models, for example, in recommendation diversity and robustness between different subgroups of users and items. Our work illuminates the relative advantages and disadvantages of neural models in recommendation and is therefore an important step towards building better recommender systems."}, "cited_paper_content": {"title": "Are We Really Making Much Progress? A Worrying Analysis Of Recent Neural Recommendation Approaches", "abstract": "Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area."}, "keywords": ["weak generalization settings"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "According to a recent analysis of algorithmic proposals for top-n recommendation tasks, only a few neural models can be reproduced with reasonable effort, and comparably simple heuristic methods can often outperform them (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to a recent analysis of algorithmic proposals for top-n recommendation tasks, only 7 out of 18 algorithms presented at top-level research conferences in the last years could be reproduced with reasonable effort, and for these methods, it turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our findings are consistent with those of #REFR, which reported problems in today's research practice in applied machine learning, particularly in terms of the reproducibility of the results or the choice of the baselines when proposing new models, especially in weak generalization settings.", "given_citation_intent": "result", "given_keywords": ["weak generalization settings"]}]}
{"citing_id": "2303.01140v1", "cited_id": "1810.00826", "section_title": "Gnce Architecture", "citation": "This is in line with the theoretical findings from #REFR , which states that GINECONV is strictly more expressive than all other Message Passing Layers.", "text_before_citation": ["Possible explanations for this are that 1.) the query graphs are rather small so 2 steps of message-passing are sufficient, and 2.) fewer layers avoid the oversmoothing problem #OTHEREFR , where node representations become increasingly similar, which deteriorates the model performance.", "! , | ! | # , | # | \u2026 $ , | $ | min \u2212 log ( ) Training !\"# $ $%! Prediction New", "Our custom Message Passing function, which we term TPN Message Passing, is a modulation of the GINECONV #OTHEREFR message passing function defined as", "EQUATION", "We initially evaluated several different Message Passing Layers but found GINECONV to be superior to the others."], "text_after_citation": ["It also aligns with results from LSS [Zhao et al., 2021] , which also uses GINECONV as the Message Passing Function.", "In Eq. (8), each h", "(k)", "\u03b8 is a multilayer perception with a ReLU Activation function.", "N (i) are all neighboring nodes of node i with corresponding edges e j,i ."], "citing_paper_content": {"title": "Cardinality Estimation Over Knowledge Graphs With Embeddings And Graph Neural Networks", "abstract": "Cardinality Estimation over Knowledge Graphs (KG) is crucial for query optimization, yet remains a challenging task due to the semi-structured nature and complex correlations of typical Knowledge Graphs. In this work, we propose GNCE, a novel approach that leverages knowledge graph embeddings and Graph Neural Networks (GNN) to accurately predict the cardinality of conjunctive queries. GNCE first creates semantically meaningful embeddings for all entities in the KG, which are then integrated into the given query, which is processed by a GNN to estimate the cardinality of the query. We evaluate GNCE on several KGs in terms of q-Error and demonstrate that it outperforms state-of-the-art approaches based on sampling, summaries, and (machine) learning in terms of estimation accuracy while also having lower execution time and less parameters. Additionally, we show that GNCE can inductively generalize to unseen entities, making it suitable for use in dynamic query processing scenarios. Our proposed approach has the potential to significantly improve query optimization and related applications that rely on accurate cardinality estimates of conjunctive queries."}, "cited_paper_content": {"title": "How Powerful Are Graph Neural Networks?", "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance."}, "keywords": ["Message Passing Layers"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Possible citation sentence: \n\nTo ensure the expressive power of Graph Neural Networks (GNNs) in capturing different graph structures, recent research has presented a theoretical framework for analyzing their discriminative power #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our results are consistent with the findings in the reference paper #REFR, which also showed that popular GNN variants such as Graph Convolutional Networks and GraphSAGE have limited expressive power and cannot learn to distinguish certain simple graph structures.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We initially evaluated several different Message Passing Layers but found GINECONV to be superior to the others, and our custom Message Passing function, which we term TPN Message Passing, is a modulation of the GINECONV #REFR message passing function defined as EQUATION, demonstrating the effectiveness of GNNs in capturing graph structures.", "given_citation_intent": "result", "given_keywords": ["Message Passing Layers"]}]}
{"citing_id": "2303.09404v1", "cited_id": "1706.03762", "section_title": "Implementation", "citation": "The two decoders also follows the general line of the original Transformer #REFR (see Fig.", "text_before_citation": ["We only retain the RNN output at the last time index T , since it contains all information on the complete sequence s1:T .", "The sampled value of w is then replicated T times and concatenated to s1:T to produce the input of the Transformer encoder.", "This latter follows the line of the original Transformer #OTHEREFR (see Fig.", "1 ): input embedding followed by positional encoding, followed by a Multi-Head Attention (MHA) module, normalization layer (NL) with skip connection with the MHA input, feed-forward (FF) layer and another NL with skip connection with the FF layer input.", "Finally, a linear layer produces the parameters of (6), that is \u00b5 \u03c6z,t and the logarithm of v \u03c6z,t ."], "text_after_citation": ["1 ; here we do not detail the different modules as we did for the encoder, because of room limitation; see #OTHEREFR for details).", "The sampled value of z1:T is concatenated with the replicated sampled value of w to produce the \"main\" input, which is the source information for the query of the second MHA module.", "In parallel, s1:T is used as the second input, i.e.", "the source information for the key and value of the second MHA module.", "A causal mask in the MHA module ensures that the dependency at time t is on s1:t\u22121."], "citing_paper_content": {"title": "Speech Modeling With A Hierarchical Transformer Dynamical Vae", "abstract": "The dynamical variational autoencoders (DVAEs) are a family of latent-variable deep generative models that extends the VAE to model a sequence of observed data and a corresponding sequence of latent vectors. In almost all the DVAEs of the literature, the temporal dependencies within each sequence and across the two sequences are modeled with recurrent neural networks. In this paper, we propose to model speech signals with the Hierarchical Transformer DVAE (HiT-DVAE), which is a DVAE with two levels of latent variable (sequence-wise and frame-wise) and in which the temporal dependencies are implemented with the Transformer architecture. We show that HiT-DVAE outperforms several other DVAEs for speech spectrogram modeling, while enabling a simpler training procedure, revealing its high potential for downstream low-level speech processing tasks such as speech enhancement."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["two decoders"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The Transformer architecture proposed in #REFR is used to implement the temporal dependencies in our Hierarchical Transformer DVAE model for speech signal modeling, as described in the previous paragraph.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Transformer architecture proposed in #REFR, which is based solely on attention mechanisms, is used to implement the temporal dependencies in the Hierarchical Transformer DVAE for speech signal modeling, resulting in improved performance and a simpler training procedure.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed HiT-DVAE model implements temporal dependencies with the Transformer architecture, which has been shown to outperform RNN-based models in various sequence transduction tasks #REFR, including machine translation and English constituency parsing.", "given_citation_intent": "background", "given_keywords": ["two decoders"]}]}
{"citing_id": "2304.05934v1", "cited_id": "1705.07750", "section_title": "Novel Benchmarks", "citation": "We trained our appearance-based I3D model #REFR on the ASL Citizen dataset to establish an initial baseline, which achieves a top-1 accuracy (recall-at-1) of 62.10%, with a DCG of 0.784 and MRR of 0.725 (see the third row in Table 3 ).", "text_before_citation": [], "text_after_citation": ["4 This accuracy is notable, given the di culty of the problem -our dataset has completely unseen users, and spans one of the largest vocabulary sizes in ISLR to date (2,731 signs).", "The pose-based ST-GCN model performs similarly, but consistently worse by a few percentage points on all metrics, but still substantially better than any previous reported results on datasets of similar size and complexity.", "For comparison, due to the number of classes, random guessing would only yield 0.04% expected accuracy.", "In previous work, appearance-based and pose-based models have generally shown competitive performance.", "Pose-based methods reduce information and potentially introduce errors at the keypoint extraction step, at the bene t of making features relevant for signs more accessible and standardized compared to raw pixels."], "citing_paper_content": {"title": "Asl Citizen: A Community-Sourced Dataset For Advancing Isolated Sign Language Recognition", "abstract": "Sign languages are used as a primary language by approximately 70 million D/deaf people worldwide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the largest Isolated Sign Language Recognition (ISLR) dataset to date, collected with consent and containing 83,912 videos for 2,731 distinct signs lmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their own webcam with the aim of retrieving matching signs from a dictionary. We show that training supervised machine learning classi ers with our dataset greatly advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%, evaluated entirely on videos of users who are not present in the training or validation sets. Preprint. Under review."}, "cited_paper_content": {"title": "Quo Vadis, Action Recognition? A New Model And The Kinetics Dataset", "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. ::: We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101."}, "keywords": ["ASL Citizen", "top-1 accuracy"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose the use of their ASL Citizen dataset for sign language dictionary retrieval and demonstrate the effectiveness of supervised machine learning classifiers trained on the dataset. To improve the state-of-the-art, they suggest pre-training on the Kinetics Human Action Video dataset, which contains significantly more data and provides better evaluation metrics. Specifically, they recommend the use of the Two-Stream Inflated 3D ConvNet (I3D) from #REFR for spatio-temporal feature extraction from video.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of supervised machine learning classifiers for sign language recognition, the authors propose pre-training on the Kinetics Human Action Video dataset, which contains a large number of human action classes and clips, and show that their new Two-Stream Inflated 3D ConvNet (I3D) model considerably outperforms the state-of-the-art on action classification, achieving 98.0% accuracy on UCF-101 and 80.9% on HMDB-51 after pre-training on Kinetics (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve a top-1 accuracy of 62% on the ASL Citizen dataset, the authors trained supervised machine learning classifiers using a specific method from the reference paper, namely the Two-Stream Inflated 3D ConvNet (I3D) that was pre-trained on Kinetics and considerably improved upon the state-of-the-art in action classification, as reported in #REFR.", "given_citation_intent": "method", "given_keywords": ["ASL Citizen", "top-1 accuracy"]}]}
{"citing_id": "2303.11935v1", "cited_id": "1610.02357", "section_title": "Experimental Setup", "citation": "Similarly, the depth-separable regular convolutions of the XceptionNet architecture #REFR are put to the test.", "text_before_citation": ["We show that the proposed network model provides higher sensitivity and interpretability than the current COVID-Net #OTHEREFR and COVID-NET-S #OTHEREFR .", "We also employ ResNet50, a ResNet variant developed by Kaiming He et al.", "#OTHEREFR , with 50 layers, where we replace the output layer with a regression head with two outputs. We also tested the Swin transformer #OTHEREFR .", "This is a hierarchical transformer architecture whose representation is generated by shifted windows.", "It can serve as the main structural support for a regression task performed for evaluation."], "text_after_citation": ["We also tested the InceptionNet architecture, #OTHEREFR which emphasizes parallel processing and concurrent feature extraction.", "Moreover, we tested the model proposed by Cohen et al.", "#OTHEREFR which was trained using a large dataset as a feature extractor and allows score predictions.", "In addition, we tested MobileNetV3, a convolutional neural network tailored to cell phone CPUs through a combination of hardware-aware network architecture search (NAS) and the NetAdapt algorithm #OTHEREFR .", "The output of this model was also modified to predict the score through a regression head."], "citing_paper_content": {"title": "Vision Transformer-Based Model For Severity Quantification Of Lung Pneumonia Using Chest X-Ray Images", "abstract": "To develop generic and reliable approaches for diagnosing and assessing the severity of COVID-19 from chest X-rays (CXR), a large number of well-maintained COVID-19 datasets are needed. Existing severity quantification architectures require expensive training calculations to achieve the best results. For healthcare professionals to quickly and automatically identify COVID-19 patients and predict associated severity indicators, computer utilities are needed. In this work, we propose a Vision Transformer (ViT)-based neural network model that relies on a small number of trainable parameters to quantify the severity of COVID-19 and other lung diseases. We present a feasible approach to quantify the severity of CXR, called Vision Transformer Regressor Infection Prediction (ViTReg-IP), derived from a ViT and a regression head. We investigate the generalization potential of our model using a variety of additional test chest radiograph datasets from different open sources. In this context, we performed a comparative study with several competing deep learning analysis methods. The experimental results show that our model can provide peak performance in quantifying severity with high generalizability at a relatively low computational cost. The source codes used in our work are publicly available at https://github.com/ bouthainas/ViTReg-IP."}, "cited_paper_content": {"title": "Xception: Deep Learning With Depthwise Separable Convolutions", "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."}, "keywords": ["XceptionNet architecture", "depth-separable regular convolutions"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We also tested the Swin transformer, a hierarchical transformer architecture whose representation is generated by shifted windows, which can serve as the main structural support for a regression task performed for evaluation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We also compared our proposed ViTReg-IP model with the Xception architecture proposed by Chollet et al. #REFR, which uses depthwise separable convolutions to achieve high performance on image classification tasks.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We also experimented with the XceptionNet architecture, which utilizes depthwise separable convolutions, and found it to be a promising approach for severity quantification of lung pneumonia from chest X-ray images #REFR.", "given_citation_intent": "method", "given_keywords": ["XceptionNet architecture", "depth-separable regular convolutions"]}]}
{"citing_id": "2304.04934v1", "cited_id": "1803.03635", "section_title": "Iv. Sparsity-Aided Machine Unlearning", "citation": "Similar to IMP in #REFR , we adopt a progressive learning process, where each round prunes p 1/k % of the weights on top of the previous round (p = 20% by default).", "text_before_citation": ["Yet, different from IMP, we replace the optimization step of retraining non-zero weights with unlearning on non-zero model weights.", "We term the resulting method 'AO-sparse MU' and summarize its pipeline (S1)-(S3) below.", "(S1) Initialize model \u03b8 = \u03b8 o and a pruning mask m = 1. (S2) Prune p% non-zero parameters in \u03b8 per magnitude. Then, update m to a sparser mask.", "(S3) Unlearn D f on the sparse model m \u03b8 using an approximate unlearning method (e.g., FT) to update the non-zero model parameters and go back to (S2).", "The above procedure (S1)\u2192(S2) (S3) repeatedly prunes and unlearns the model over multiple rounds (assuming k rounds)."], "text_after_citation": ["However, different from IMP, AO-sparse MU is much lighter in computation since approximate unlearning in (S2) takes fewer computation overheads than retraining non-zero model weights in IMP."], "citing_paper_content": {"title": "Model Sparsification Can Simplify Machine Unlearning", "abstract": "Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed 'prune first, then unlearn' and 'sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios including class-wise data scrubbing, random data scrubbing, and backdoor data forgetting. One highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) in the proposed sparsity-aware unlearning paradigm. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse."}, "cited_paper_content": {"title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks", "abstract": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard technique for pruning weights naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the\"lottery ticket hypothesis\": dense, randomly-initialized feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - arrive at comparable test accuracy in a comparable number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Furthermore, the winning tickets we find above that size learn faster than the original network and exhibit higher test accuracy."}, "keywords": ["progressive learning process"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We adopt weight pruning to sparsify the model and improve the multi-criteria unlearning performance of an approximate unlearner, as suggested by the \"lottery ticket hypothesis\" proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve model sparsity, the authors propose a weight pruning technique that can reduce the parameter counts of trained networks by over 90% while maintaining accuracy (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To facilitate the progressive learning process, the authors propose a novel method called 'AO-sparse MU' that repeatedly prunes and unlearns the model over multiple rounds, where the pruning step is based on the weight magnitude and the unlearning step is conducted on non-zero model weights using an approximate unlearning method, as described in detail in #REFR.", "given_citation_intent": "method", "given_keywords": ["progressive learning process"]}]}
{"citing_id": "2304.10026v1", "cited_id": "1711.03789", "section_title": "Theorem 4.3", "citation": "For the terms z i , we will prove the following by the steps (1) and #REFR below.", "text_before_citation": ["Proof.", "By the discrete Helmholtz decomposition in the last equation in (4.9), we decompose u h in (4.9) as", "u h = curlw 0 + z 0 + N i=1 (curlw i + z i ).", "For the terms curlw i of u h , by Section 3 in this paper, we have w 0 \u2208 N D H,0 and w i \u2208", "V i , which satisfy N i=0 a div (curlw i , curlw i ) \u2264 C(1 + H \u03b4 )a div (curlw \u22a5 h , curlw \u22a5 h ). (4.11)"], "text_after_citation": ["EQUATION", "(1) For the coarse component z 0 , by (4.9), (4.8), (4.1) and (4.7), we have", "||divz 0 || 0 = ||div Q H P h z \u22a5 h || 0 \u2264 C| P h z \u22a5 h | 1 \u2264 C||div P h z \u22a5 h || 0 = C||divz \u22a5 h || 0 .", "By (4.9), (4.8), (4.7) and the definition \u0398 \u22a5 , we get", "||z 0 || 0 = || Q H P h z \u22a5 h || 0 \u2264 || P h z \u22a5 h || 0 = || \u0398 \u22a5 z \u22a5 h || 0 \u2264 ||z \u22a5 h || 0 ."], "citing_paper_content": {"title": "On A Sharp Estimate Of Overlapping Schwarz Methods In H(Curl; \u2126) And H(Div; \u2126)", "abstract": "The previous proved-bound is C(1 + H 2 \u03b4 2) for the condition number of the overlapping domain decomposition H(curl; \u2126) and H(div; \u2126) methods, where H and \u03b4 are the sizes of subdomains and overlaps respectively. But all numerical results indicate that the best bound is C(1 + H \u03b4). In this work, we solve this long-standing open problem by proving that C(1 + H \u03b4) is indeed the best bound."}, "cited_paper_content": {"title": "Domain Decomposition Preconditioning For The High-Frequency Time-Harmonic Maxwell Equations With Absorption", "abstract": "This paper rigorously analyses preconditioners for the time-harmonic Maxwell equations with absorption, where the PDE is discretised using curl-conforming finite-element methods of fixed, arbitrary order and the preconditioner is constructed using additive Schwarz domain decomposition methods. The theory developed here shows that if the absorption is large enough, and if the subdomain and coarse mesh diameters and overlap are chosen appropriately, then the classical two-level overlapping additive Schwarz preconditioner (with PEC boundary conditions on the subdomains) performs optimally--in the sense that GMRES converges in a wavenumber-independent number of iterations--for the problem with absorption. An important feature of the theory is that it allows the coarse space to be built from low-order elements even if the PDE is discretised using high-order elements. It also shows that additive methods with minimal overlap can be robust. Numerical experiments are given that illustrate the theory and its dependence on various parameters. These experiments motivate some extensions of the preconditioners which have better robustness for problems with less absorption, including the propagative case. At the end of the paper we illustrate the performance of these on two substantial applications; the first (a problem with absorption arising from medical imaging) shows the empirical robustness of the preconditioner against heterogeneity, and the second (scattering by a COBRA cavity) shows good scalability of the preconditioner with up to 3,000 processors."}, "keywords": ["terms"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the terms curlw i of u h , by Section 3 in this paper, we have w 0 \u2208 N D H,0 and w i \u2208 V i , which satisfy N i=0 a div (curlw i , curlw i ) \u2264 C(1 + H \u03b4 )a div (curlw \u22a5 h , curlw \u22a5 h ) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For the terms curlw_i of u_h, the authors in Section 3 of this manuscript have shown that w_0 \u2208 N D H,0 and w_i \u2208 V_i, which satisfy N_i=0 a div(curlw_i, curlw_i) \u2264 C(1 + H\u03b4)a div(curlw\u22a5_h, curlw\u22a5_h), where \"Section 3\" refers to the method proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For the terms curlw_i of u_h, the authors used the method of additive Schwarz domain decomposition with appropriate subdomain and coarse mesh diameters and overlap, as rigorously analyzed in #REFR, to achieve optimal performance for the problem with absorption.", "given_citation_intent": "method", "given_keywords": ["terms"]}]}
{"citing_id": "2303.01980v1", "cited_id": "1906.02243", "section_title": "Introduction", "citation": "Model training comes at a high environmental cost, as energy is required to run it on hardware for weeks or months at a time #REFR .", "text_before_citation": ["They provide superior performance in these tasks compared to traditional models, and there is a trend for developing even larger and more powerful DNNs #OTHEREFR .", "However, these models generally require large amounts of data and high computing power, which is associated with high energy consumption incurring high financial and environmental costs #OTHEREFR .", "Furthermore, for emerging applications such as autonomous driving and the Internet of Things, models must be run on low-power devices #OTHEREFR .", "Therefore, energy-efficient DL is crucial from an economic, environmental, and application perspective.", "The efficiency of DL can be divided into training efficiency and inference efficiency #OTHEREFR ."], "text_after_citation": ["However, training a DL model is only the beginning of the lifecycle.", "Once the model is trained, it will be implemented and used. This process, called inference, also consumes enormous energy.", "Inference does not last weeks or months, but unlike training, it is not a one-time event.", "It takes place continuously and can therefore exceed the energy consumption of the training after a certain number of inference events #OTHEREFR .", "Various researchers have presented approaches, methods, and techniques such as mixed-precision training, pruning or knowledge distillation that can accelerate training and inference time and reduce the energy consumption of the models."], "citing_paper_content": {"title": "Towards Energy-Efficient Deep Learning: An Overview Of Energyefficient Approaches Along The Deep Learning Lifecycle", "abstract": "Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption."}, "cited_paper_content": {"title": "Energy And Policy Considerations For Deep Learning In Nlp", "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."}, "keywords": ["high environmental cost"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "They provide superior performance in these tasks compared to traditional models, and there is a trend for developing even larger and more powerful DNNs #OTHEREFR . However, these models generally require large amounts of data and high computing power, which is associated with high energy consumption incurring high financial and environmental costs #OTHEREFR . Furthermore, for emerging applications such as autonomous driving and the Internet of Things, models must be run on low-power devices #OTHEREFR . Therefore, energy-efficient DL is crucial from an economic, environmental, and application perspective. The efficiency of DL can be divided into training efficiency and inference efficiency, and recent studies have quantified the approximate financial and environmental costs of training neural network models for NLP #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to recent research, the accuracy improvements of large neural network models in NLP tasks come at a high cost, both financially and environmentally, due to the large amounts of data and high computing power required for training #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The development of larger and more powerful DNNs has led to superior performance in various tasks, but their high energy consumption incurs high financial and #OTHEREFR high environmental costs. Therefore, energy-efficient DL is crucial from an economic, environmental, and application perspective, and the proposed recommendations in #REFR can help reduce these costs.", "given_citation_intent": "background", "given_keywords": ["high environmental cost"]}]}
{"citing_id": "2304.11801v1", "cited_id": "1906.07372", "section_title": "B. One-Shot Imitation Learning", "citation": "One solution is to incorporate imitation learning with RL to improve the robustness of the policy with fine-tuning. #REFR integrates a task-specific inverse dynamic model into RL.", "text_before_citation": ["One-shot imitation learning means robots are able to learn from a single demonstration of a given task and then generalize to new situations of the same task #OTHEREFR . However, it is very challenging due to data scarcity."], "text_after_citation": ["#OTHEREFR extracts the tool trajectory from the video and aligns the simulated environment with the video to initialize a policy for RL to learn.", "There are two main drawbacks of this kind of method: (1) training robots in the real world is challenging due to sample efficiency and safety concerns.", "(2) learning in simulation necessitates a high level of verisimilitude with the actual world and additional techniques to deal with the sim-to-real gap.", "Other researchers #OTHEREFR , #OTHEREFR formulate this problem as visual servoing.", "#OTHEREFR iterates the pose aligning between the robot and the human hand in the video; #OTHEREFR obtains the approaching policy in a self-supervised manner through backward learning from the goal pose. However, the above methods hold strong taskrelated assumptions (e.g."], "citing_paper_content": {"title": "Efficient Robot Skill Learning With Imitation From A Single Video For Contact-Rich Fabric Manipulation", "abstract": "Classical policy search algorithms for robotics typically require performing extensive explorations, which are timeconsuming and expensive to implement with real physical platforms. To facilitate the efficient learning of robot manipulation skills, in this work, we propose a new approach comprised of three modules: (1) learning of general prior knowledge with random explorations in simulation, including state representations, dynamic models, and the constrained action space of the task; (2) extraction of a state alignment-based reward function from a single demonstration video; (3) real-time optimization of the imitation policy under systematic safety constraints with sampling-based model predictive control. This solution results in an efficient one-shot imitation-from-video strategy that simplifies the learning and execution of robot skills in real applications. Specifically, we learn priors in a scene of a task family and then deploy the policy in a novel scene immediately following a single demonstration, preventing time-consuming and risky explorations in the environment. As we do not make a strong assumption of dynamic consistency between the scenes, learning priors can be conducted in simulation to avoid collecting data in real-world circumstances. We evaluate the effectiveness of our approach in the context of contact-rich fabric manipulation, which is a common scenario in industrial and domestic tasks. Detailed numerical simulations and real-world hardware experiments reveal that our method can achieve rapid skill acquisition for challenging manipulation tasks."}, "cited_paper_content": {"title": "Ridm: Reinforced Inverse Dynamics Modeling For Learning From A Single Observed Demonstration", "abstract": "Imitation learning has long been an approach to alleviate the tractability issues that arise in reinforcement learning. However, most literature makes several assumptions such as access to the expert's actions, availability of many expert demonstrations, and injection of task-specific domain knowledge into the learning process. We propose reinforced inverse dynamics modeling (RIDM), a method of combining reinforcement learning and imitation from observation (IfO) to perform imitation using a single expert demonstration, with no access to the expert's actions, and with little task-specific domain knowledge. Given only a single set of the expert's raw states, such as joint angles in a robot control task, at each time-step, we learn an inverse dynamics model to produce the necessary low-level actions, such as torques, to transition from one state to the next such that the reward from the environment is maximized. We demonstrate that RIDM outperforms other techniques when we apply the same constraints on the other methods on six domains of the MuJoCo simulator and for two different robot soccer tasks for two experts from the RoboCup 3D simulation league on the SimSpark simulator."}, "keywords": ["task-specific inverse dynamic"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One-shot imitation learning means robots are able to learn from a single demonstration of a given task and then generalize to new situations of the same task #OTHEREFR. However, it is very challenging due to data scarcity. Reinforced inverse dynamics modeling (RIDM) is a method that combines reinforcement learning and imitation from observation to perform imitation using a single expert demonstration, with no access to the expert's actions, and with little task-specific domain knowledge #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of one-shot imitation learning, the authors propose a method called reinforced inverse dynamics modeling (RIDM) that combines reinforcement learning and imitation from observation (IfO) to perform imitation using a single expert demonstration, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the challenge of data scarcity in one-shot imitation learning, the authors propose a method called reinforced inverse dynamics modeling (RIDM) that can perform imitation using a single expert demonstration and with little #OTHEREFR task-specific domain knowledge by learning an inverse dynamics model to produce the necessary low-level actions, such as torques, to transition from one state to the next such that the reward from the environment is maximized.", "given_citation_intent": "method", "given_keywords": ["task-specific inverse dynamic"]}]}
{"citing_id": "2304.09790v1", "cited_id": "2003.12039", "section_title": "Multi-Field Refinement", "citation": "On the one hand, our average flow is generally consistent with the flow estimated from RAFT #REFR , which approximates to the ground truth displacements.", "text_before_citation": ["(4) but estimates the residual part in the final interpolation result. As shown in Tab.", "2f, the results of this variant underperform the original setting, which indicates we need to compensate details for each frame candidate separately. Besides, if we replace the convolution operators in Eqn.", "#OTHEREFR with an average operation, the performance will be degraded (see Tab. 2f).", "This indicates that it is important for our AMT to perform an adaptive fusion and refinement. Discussion.", "For further discussion, we visualize the mean and deviation of three estimated flow pairs. The results are shown in Fig. 6 ."], "text_after_citation": ["On the other hand, we observe that the major diversities of flows are at the motion boundaries and in the regions with rich textures.", "This indicates that these regions need to involve more potential pixel candidates for reconstruction.", "Through these visualizations, we see that our method generate promising task-oriented flows, generally consistent with the ground truth optical flows but diverse in local details."], "citing_paper_content": {"title": "Amt: All-Pairs Multi-Field Transforms For Efficient Frame Interpolation", "abstract": "We present All-Pairs Multi-Field Transforms (AMT), a new network architecture for video frame interpolation. It is based on two essential designs. First, we build bidirectional correlation volumes for all pairs of pixels, and use the predicted bilateral flows to retrieve correlations for updating both flows and the interpolated content feature. Second, we derive multiple groups of fine-grained flow fields from one pair of updated coarse flows for performing backward warping on the input frames separately. Combining these two designs enables us to generate promising taskoriented flows and reduce the difficulties in modeling large motions and handling occluded areas during frame interpolation. These qualities promote our model to achieve stateof-the-art performance on various benchmarks with high efficiency. Moreover, our convolution-based model competes favorably compared to Transformer-based models in terms of accuracy and efficiency. Our code is available at https://github.com/MCG-NKU/AMT."}, "cited_paper_content": {"title": "Raft: Recurrent All-Pairs Field Transforms For Optical Flow", "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance, with strong cross-dataset generalization and high efficiency in inference time, training speed, and parameter count. Code is available \\url{https://github.com/princeton-vl/RAFT}."}, "keywords": ["flow"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our AMT architecture is based on bidirectional correlation volumes for all pairs of pixels, which are used to retrieve correlations for updating both flows and the interpolated content feature, as well as multiple groups of fine-grained flow fields for performing backward warping on the input frames separately. This approach enables us to generate promising task-oriented flows and reduce the difficulties in modeling large motions and handling occluded areas during frame interpolation, as demonstrated by our state-of-the-art performance on various benchmarks with high efficiency. To achieve this, we draw inspiration from the Recurrent All-Pairs Field Transforms (RAFT) architecture proposed by Teed and Deng #REFR, which extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the results in Tab. 2f, our AMT outperforms the previous state-of-the-art method RAFT in terms of efficiency and accuracy on various benchmarks, indicating the superiority of our bidirectional correlation volumes and fine-grained flow fields for frame interpolation. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed AMT model builds bidirectional correlation volumes for all pairs of pixels and uses predicted bilateral flows to retrieve correlations for updating both flows and the interpolated content feature, which is similar to the Recurrent All-Pairs Field Transforms (RAFT) method for optical #KEYWORD(flow) estimation #REFR.", "given_citation_intent": "result", "given_keywords": ["flow"]}]}
{"citing_id": "2303.02700v2", "cited_id": "1904.01786", "section_title": "Extraction Of Strand Map", "citation": "For the synthetic data, we can easily obtain strand maps and depth maps from 3D strand models assisted by mature rendering techniques #REFR .", "text_before_citation": ["To enable learning-based single-view 3D hair modeling, HairStep needs to be firstly extracted from both synthetic 3D hair data and real images for training and testing."], "text_after_citation": ["But it is infeasible to extract strand maps from real images via existing approaches.", "Thus, we use a learning-based approach and annotate a dataset HiSa to provide supervision.", "HiSa dataset.", "We collect 1,250 clear portrait images with various hairstyles from the Internet.", "The statistics of the hairstyles, gender and race are given in the supplementary material."], "citing_paper_content": {"title": "Hairstep: Transfer Synthetic To Real Using Strand And Depth Maps For Single-View 3D Hair Modeling", "abstract": "Our experiments show that HairStep narrows the domain gap between synthetic and real and achieves state-of-theart performance on single-view 3D hair reconstruction."}, "cited_paper_content": {"title": "Soft Rasterizer: A Differentiable Renderer For Image-Based 3D Reasoning", "abstract": "Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers."}, "keywords": ["3D strand models", "depth maps"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To enable learning-based single-view 3D hair modeling, HairStep needs to be firstly extracted from both synthetic 3D hair data and real images for training and testing. Our proposed approach can handle challenging tasks in image-based shape fitting, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable learning-based single-view 3D hair modeling, HairStep needs to be firstly extracted from both synthetic 3D hair data and real images for training and testing. Our proposed method can benefit from the differentiable rendering framework proposed in Soft Rasterizer, which enables efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading, and color images. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable learning-based single-view 3D hair modeling using 3D strand models and depth maps, the authors utilized a differentiable rendering framework proposed in Soft Rasterizer#REFR, which allows for efficient supervision signals to be back-propagated to mesh vertices and their attributes from various forms of image representations.", "given_citation_intent": "method", "given_keywords": ["3D strand models", "depth maps"]}]}
{"citing_id": "2304.02867v1", "cited_id": "1903.02428", "section_title": "Sparse Fusion Layer", "citation": "Accordingly, non-empty voxels with the same horizontal location could project to the corresponding pillars with sparse pooling #REFR .", "text_before_citation": ["= (l j , w j ) \u2208 N 2 } Np j=1 and {f p j \u2208 R Dp } Np j=1", "for pillar indices with their corresponding features (N v \u2265 N p due to many-to-one voxel-pillar relationship).", "With the consistent occupancy in BEV guaranteed, for a certain X-Y coordinate containing a non-empty pillar, we could find the corresponding voxels by comparing the horizontal vector of voxel indices (l i , w i ) with pillar indice p j = (l j , w j ):", "EQUATION", "The collection of c j forms the voxel-to-pillar index {c j } Np j=1 ."], "text_after_citation": ["Conversely, non-empty pillars could also scatter to the matching voxels with the inverse index, which we represent as sparse broadcasting. Sparse Voxel-Pillar Fusion.", "Equipped with sparse pooling and broadcasting, we deploy the SFL. As illustrated in Fig.", "2 , given the pair of sparse voxel and pillar features in a stage, SFL first calculates the voxel-to-pillar index {c j } Np j=1 . Then, sparse features are interacted by two pathways.", "For the voxel-to-pillar connection, voxel features are aggregated in the vertical direction by sparse maxpooling to form the pillar-wise features in shape of (N p , D v ).", "These pillar-wise features are spatially consistent with original sparse features, while distinct in feature dimension."], "citing_paper_content": {"title": "Vpfusion: Towards Robust Vertical Representation Learning For 3D Object Detection", "abstract": "Efficient point cloud representation is a fundamental element of Lidar-based 3D object detection. Recent gridbased detectors usually divide point clouds into voxels or pillars and construct single-stream networks in Bird's Eye View. However, these point cloud encoding paradigms underestimate the point representation in the vertical direction, which cause the loss of semantic or fine-grained information, especially for vertical sensitive objects like pedestrian and cyclists. In this paper, we propose an explicit vertical multi-scale representation learning framework, VPFusion, to combine the complementary information from both voxel and pillar streams. Specifically, VPFusion first builds upon a sparse voxel-pillar-based backbone. The backbone divides point clouds into voxels and pillars, then encodes features with 3D and 2D sparse convolution simultaneously. Next, we introduce the Sparse Fusion Layer (SFL), which establishes a bidirectional pathway for sparse voxel and pillar features to enable the interaction between them. Additionally, we present the Dense Fusion Neck (DFN) to effectively combine the dense feature maps from voxel and pillar branches with multi-scale. Extensive experiments on the large-scale Waymo Open Dataset and nuScenes Dataset demonstrate that VPFusion surpasses the singlestream baselines by a large margin and achieves state-ofthe-art performance with real-time inference speed."}, "cited_paper_content": {"title": "Fast Graph Representation Learning With Pytorch Geometric", "abstract": "We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios."}, "keywords": ["non-empty voxels", "sparse"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently encode point clouds, recent grid-based detectors usually divide them into voxels or pillars and construct single-stream networks in Bird's Eye View. However, this paradigm underestimates the point representation in the vertical direction, which causes the loss of semantic or fine-grained information, especially for vertical sensitive objects like pedestrians and cyclists. To address this issue, VPFusion proposes an explicit vertical multi-scale representation learning framework that combines the complementary information from both voxel and pillar streams #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently handle input examples of different sizes, PyTorch Geometric leverages sparse GPU acceleration and provides dedicated CUDA kernels, as well as introducing efficient mini-batch handling, as described in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure consistent occupancy in BEV, the authors compared the horizontal vector of voxel indices with pillar indices to find the corresponding voxels for non-empty pillars, as described in #REFR. This approach allowed them to collect the voxel-to-pillar index {c_j} Np j=1 and effectively handle sparse non-empty voxels.", "given_citation_intent": "background", "given_keywords": ["non-empty voxels", "sparse"]}]}
{"citing_id": "2304.11073v1", "cited_id": "1907.01669", "section_title": "Introduction", "citation": "Figure 1: DSTC11 Track 3 introduced a spoken version of MultiWOZ 2.1 #REFR with user utterances voiced by crowdworkers.", "text_before_citation": ["However, state-of-the-art systems trained on chats face robustness issues when dealing with spoken inputs #OTHEREFR .", "In a TOD system, the role of DST is to predict at each turn and based on the dialogue history the current belief state, i.e. a condensed and updated representation of the user needs.", "DST plays a central role as the system relies on the belief state to decide which action to take next.", "The belief state is typically frame-based and represented as a list of <slot, value> pairs. 1 Our code is made available at https://github.com/ Orange-OpenSource/olisia-dstc11.", "* Equal contribution."], "text_after_citation": ["While both cascade and end-to-end approaches have been well studied for Spoken Language Understanding (SLU, Serdyuk et al.", "(2018) ), there has been little recent work on spoken DST.", "Considering the entire dialogue context, as opposed to only the current turn, requires tricky strategies for end-to-end systems #OTHEREFR .", "In order to leverage state-of-the-art models, a cascade approach with separate ASR and DST components is thus preferred.", "However, these two components do not benefit from joint optimization and often lack integration."], "citing_paper_content": {"title": "Olisia: A Cascade System For Spoken Dialogue State Tracking", "abstract": "Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language. In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations. With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations. 1"}, "cited_paper_content": {"title": "Multiwoz 2.1: A Consolidated Multi-Domain Dialogue Dataset With State Corrections And State Tracking Baselines", "abstract": "MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there is substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32% of state annotations across 40% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future."}, "keywords": ["user utterances"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of noisy state annotations and dialogue utterances in spoken dialogue systems, MultiWOZ 2.1 provides a consolidated multi-domain dialogue dataset with state corrections and state tracking baselines #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the robustness issues of state-of-the-art systems trained on chats when dealing with spoken inputs, recent work has proposed OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a Dialogue State Tracking (DST) model, and achieved state-of-the-art performance on spoken DST benchmark #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of state-tracking models on spoken conversations, recent work has focused on adapting the DST inputs through data augmentation and normalizing the ASR outputs #OTHEREFR, as demonstrated in Multiwoz 2.1 dataset #REFR.", "given_citation_intent": "background", "given_keywords": ["user utterances"]}]}
{"citing_id": "2303.08808v1", "cited_id": "1803.04758", "section_title": "Loss Functions", "citation": "Unlike previous works (e.g., #REFR ), we observe that imposing a low-deformation loss reduces performance (Sec 4.4).", "text_before_citation": ["From the mesh M i , we project keypoint\u015d k i and encourage their proximity to target 2D keypoints k i via the loss:", "EQUATION", "We run HRNet #OTHEREFR on each frame to produce k i for peoplesnapshot and use the given keypoints for ZJU Mocap.", "Mesh regularization losses. We add some regularization to the mesh representation.", "This is especially important for the free-form per-vertex offsets D."], "text_after_citation": ["This is because loose clothing need not necessarily correspond to a low deformation from the underlying SMPL model.", "We only encourage normal consistency of adjacent faces in the mesh.", "Let M D be the mesh generated from the SMPL parameters (\u03b2, 0, D, I, 0) and let M 0 be generated from the SMPL parameters (\u03b2, 0, 0, I, 0).", "Let f j be the j th face of M D and f j be the j th face of M 0 .", "With some abuse of notation, for two faces f j and f k , we denote |f j \u2229 f k | as the number of vertices that are shared between both faces. The normal consistency loss is then given as:"], "citing_paper_content": {"title": "Mesh Strikes Back: Fast And Efficient Human Reconstruction From Rgb Videos", "abstract": "Human reconstruction and synthesis from monocular RGB videos is a challenging problem due to clothing, occlusion, texture discontinuities and sharpness, and framespecific pose changes. Many methods employ deferred rendering, NeRFs and implicit methods to represent clothed humans, on the premise that mesh-based representations cannot capture complex clothing and textures from RGB, silhouettes, and keypoints alone. We provide a counter viewpoint to this fundamental premise by optimizing a SMPL+D mesh and an efficient, multi-resolution texture representation using only RGB images, binary silhouettes and sparse 2D keypoints. Experimental results demonstrate that our approach is more capable of capturing geometric details compared to visual hull, mesh-based methods. We show competitive novel view synthesis and improvements in novel pose synthesis compared to NeRF-based methods, which introduce noticeable, unwanted artifacts. By restricting the solution space to the SMPL+D model combined with differentiable rendering, we obtain dramatic speedups in compute, training times (up to 24x) and inference times (up to 192x). Our method therefore can be used as is or as a fast initialization to NeRF-based methods."}, "cited_paper_content": {"title": "Video Based Reconstruction Of 3D People Models", "abstract": "This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping."}, "keywords": ["low-deformation loss"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We add some regularization to the mesh representation, especially for the free-form per-vertex offsets D, following the method proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors compare their mesh-based method with visual hull, mesh-based methods and NeRF-based methods, demonstrating that their approach is more capable of capturing geometric details, resulting in improved results in novel view and pose synthesis, and dramatic speedups in compute, training times, and inference times, as reported in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our method achieves accurate 3D model fits with 5mm accuracy for clothed people, which is comparable to the results reported in the reference paper #REFR, while also incorporating a low-deformation loss to improve mesh regularization.", "given_citation_intent": "result", "given_keywords": ["low-deformation loss"]}]}
{"citing_id": "2304.08911v1", "cited_id": "1706.03762", "section_title": "S 2 P-Cpd Framework", "citation": "We refer #REFR for more details about the internal architecture of Transformer model. Encoder for Table and Persona.", "text_before_citation": ["S2S module is utilized to encode original tabular data X i into a latent representation z S2S (X i ), and AE module is used to encode its aligned context y i and unpaired persona information u i into latent representation z AE (y i ) and z AE (u i ) respectively.", "Then, we utilize a latent space fusion technique to distill persona information into z f use .", "Besides, a contrastive-based discriminator is employed to guarantee the style consistency.", "Both S2S and AE modules are initially parameterized by the Transformer-based model #OTHEREFR .", "We focus on the interaction among latent variables, and omit the details inside encoder and decoder."], "text_after_citation": ["The encoder modules in autoencoders are used to encode y i and u i into latent representations z AE (y i ) and z AE (u i ) separately.", "Note that although the autoencoders for y i and u i have a shared decoder, their encoders are separate.", "The auto-encoder module empowers the model with the ability of generating personalized content, especially in the situation that strong supervision towards personalized context is absent.", "It can help force the S 2 P-CPD model to memorize the persona information.", "Therefore, we combine z AE (y i ) and z AE (u i ), defined as follows:"], "citing_paper_content": {"title": "Towards Zero-Shot Personalized Table-To-Text Generation With Contrastive Persona Distillation", "abstract": "Existing neural methods have shown great potentials towards generating informative text from structured tabular data as well as maintaining high content fidelity. However, few of them shed light on generating personalized expressions, which often requires well-aligned persona-table-text datasets that are difficult to obtain. To overcome these obstacles, we explore personalized table-to-text generation under a zeroshot setting, by assuming no well-aligned persona-table-text triples are required during training. To this end, we firstly collect a set of unpaired persona information and then propose a semi-supervised approach with contrastive persona distillation (S 2 P-CPD) to generate personalized context. Specifically, tabular data and persona information are firstly represented as latent variables separately. Then, we devise a latent space fusion technique to distill persona information into the table representation. Besides, a contrastive-based discriminator is employed to guarantee the style consistency between the generated context and its corresponding persona. Experimental results on two benchmarks demonstrate S 2 P-CPD's ability on keeping both content fidelity and personalized expressions."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["Table", "Encoder"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Both S2S and AE modules are initially parameterized by the Transformer-based model (Vaswani et al., 2017). We focus on the interaction among latent variables, and omit the details inside encoder and decoder.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide additional context, we cite the Transformer-based model proposed in \"Attention Is All You Need\" #REFR, which has been used to parameterize both S2S and AE modules in our approach.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To encode the original tabular data and aligned context, the authors utilized an S2S module and an AE module, both initially parameterized by the Transformer-based model proposed in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["Table", "Encoder"]}]}
{"citing_id": "2304.04497v1", "cited_id": "cond-mat/0112110", "section_title": "D. Network Inference", "citation": "Assumption 2 indicates that all communities have approximately the same size, which can be realized by the Girvan-Newman algorithm #REFR .", "text_before_citation": ["EQUATION", "where N max and N min are the maximum and minimum numbers of nodes in each community, respectively, and \u03b5 > 0 is an arbitrarily small constant.", "3) We partition the nodes into K subsets according to the number of communities to which they belong. Let", "EQUATION", "where D is the degree distribution of nodes and A i is the event such that a certain node v belongs to i communities."], "text_after_citation": ["Assumption 3 enforces nodes not to belong overly to either only a single community or multiple (\u2265 2) communities.", "Note that many real-world networks (e.g., Ego-Facebook #OTHEREFR and citation networks #OTHEREFR ) tend to almost follow Assumption 3.", "Now, we are ready to show the following two theorems.", "EQUATION", "where"], "citing_paper_content": {"title": "Graph Neural Network-Aided Exploratory Learning For Community Detection With Unknown Topology", "abstract": "In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often unknown, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a novel end-to-end solution for detecting overlapping communities in networks with unknown topology via exploratory learning aided by easy-to-collect node metadata. Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via communityaffiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through comprehensive evaluations using five real-world datasets, we demonstrate that META-CODE exhibits (a) its superiority over benchmark community detection methods, (b) empirical evaluations as well as theoretical findings to see the effectiveness of our node query, (c) the influence of each module, and (d) its computational efficiency."}, "cited_paper_content": {"title": "Community Structure In Social And Biological Networks", "abstract": "A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known\u2014a collaboration network and a food web\u2014and find that it detects significant and informative community divisions in both cases."}, "keywords": ["communities"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose a method for detecting community structure in networks, which involves using centrality indices to find community boundaries, and demonstrate its effectiveness on both computer-generated and real-world graphs. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect communities in networks, the authors propose a method that uses centrality indices to find community boundaries, which has been shown to have high sensitivity and reliability on both computer-generated and real-world graphs in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect tightly knit groups of nodes in networks, the authors propose a method based on centrality indices, which is effective in detecting known community structures and informative community divisions in networks with unknown community structures #REFR.", "given_citation_intent": "method", "given_keywords": ["communities"]}]}
{"citing_id": "2304.01658v1", "cited_id": "1911.07249", "section_title": "Ii. Related Work", "citation": "Similar to our work, most of these prior works have focused on next-day predictions. However, there are examples of hourly predictions #REFR .", "text_before_citation": ["#OTHEREFR modeled river segments using an LSTM network with graph convolutions.", "One segment in the river network corresponds to a distance that the water flows during approximately one day.", "Input features include daily average precipitation, daily average air temperature, date of the year, solar radiation, shade fraction, potential evapotranspiration, elevation, length, slope, and width of each segment.", "Models were trained using a physics-informed setup where a traditional flow model acted as a teacher for the machine learning model.", "LSTM networks have also been used for post-processing the output from the NWM #OTHEREFR ."], "text_after_citation": ["Others have also employed convolutional neural networks (CNNs) for stream forecasting #OTHEREFR , #OTHEREFR , #OTHEREFR .", "However, in contrast to us, these works do not incorporate spatial data from satellites or GIS, but instead model only the much lower-dimensional data (in single coordinates or very small neighborhoods, not entire areas as in our setup) provided as a feature vector for each time step, similar to the models using LSTMs.", "More broadly, deep learning has been used for many related tasks, such as groundwater level estimation #OTHEREFR , water quality estimation #OTHEREFR , and rainfall-runoff #OTHEREFR .", "While some of the above mentioned works on stream flow estimation include information about the near environment (such as elevation and slope), none of them use detailed spatial information inputs as is proposed in our work.", "The use of fully convolutional neural networks to encode this information, in combination with traditional inputs such as rainfall and temperature, has the potential of representing more complex relationships and can result in a more detailed view of the near environment."], "citing_paper_content": {"title": "Fully Convolutional Networks For Dense Water Flow Intensity Prediction In Swedish Catchment Areas", "abstract": "Intensifying climate change will lead to more extreme weather events, including heavy rainfall and drought. Accurate stream flow prediction models which are adaptable and robust to new circumstances in a changing climate will be an important source of information for decisions on climate adaptation efforts, especially regarding mitigation of the risks of and damages associated with flooding. In this work we propose a machine learning-based approach for predicting water flow intensities in inland watercourses based on the physical characteristics of the catchment areas, obtained from geospatial data (including elevation and soil maps, as well as satellite imagery), in addition to temporal information about past rainfall quantities and temperature variations. We target the one-day-ahead regime, where a fully convolutional neural network model receives spatio-temporal inputs and predicts the water flow intensity in every coordinate of the spatial input for the subsequent day. To the best of our knowledge, we are the first to tackle the task of dense water flow intensity prediction; earlier works have considered predicting flow intensities at a sparse set of locations at a time. An extensive set of model evaluations and ablations are performed, which empirically justify our various design choices. Code and preprocessed data have been made publicly available at https://github.com/ aleksispi/fcn-water-flow."}, "cited_paper_content": {"title": "The Proper Care And Feeding Of Camels: How Limited Training Data Affects Streamflow Prediction", "abstract": "Accurate streamflow prediction largely relies on historical records of both meteorological data and streamflow measurements. For many regions around the world, however, such data are only scarcely or not at all available. To select an appropriate model for a region with a given amount of historical data, it is therefore indispensable to know a model's sensitivity to limited training data, both in terms of geographic diversity and different spans of time. In this study, we provide decision support for tree- and LSTM-based models. We feed the models meteorological measurements from the CAMELS dataset, and individually restrict the training period length and the number of basins used in training. Our findings show that tree-based models provide more accurate predictions on small datasets, while LSTMs are superior given sufficient training data. This is perhaps not surprising, as neural networks are known to be data-hungry; however, we are able to characterize each model's strengths under different conditions, including the \"breakeven point\" when LSTMs begin to overtake tree-based models."}, "keywords": ["next-day predictions"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "LSTM networks have been used for predicting streamflow with limited training data, as shown in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Accurate streamflow prediction largely relies on historical records of both meteorological data and streamflow measurements, and for many regions around the world, such data are only scarcely or not at all available #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve next-day predictions of water flow intensity, it is important to consider the sensitivity of models to limited training data, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["next-day predictions"]}]}
{"citing_id": "2303.01494v1", "cited_id": "1703.06870", "section_title": "Context Clusters Pipeline", "citation": "For downstream dense prediction tasks like detection and segmentation, we need to rearrange the output points by position after each stage to satisfy the needs of most detection and segmentation heads (e.g., Mask-RCNN #REFR ).", "text_before_citation": ["To reduce the points number, we evenly select some anchors in space, and the nearest k points are concatenated and fused by a linear projection.", "Note that this reduction can be achieved by a convolutional operation if all points are arranged in order and k is properly set (i.e., 4 and 9), like in ViT #OTHEREFR .", "For clarity on the centers and anchors stated previously, we strongly suggest the readers check appendix \u00a7 B.", "Task-Specific Applications.", "For classification, we average all points of the last block's output and use a FC layer for classification."], "text_after_citation": ["That is, Context Cluster offers remarkable flexibility in classification, but is limited to a compromise between dense prediction tasks' requirements and our model configurations.", "We expect innovative detection & segmentation heads (like DETR #OTHEREFR ) can seamlessly integrate with our method."], "citing_paper_content": {"title": "Image As Set Of Points", "abstract": "What is an image and how to extract latent features? Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution-and attention-free, and only rely on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of clustering process. Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better results than ConvNets or ViTs on several benchmarks. Codes are available at: https://github.com/ma-xu/Context-Cluster."}, "cited_paper_content": {"title": "Mask R-Cnn", "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron ."}, "keywords": ["(e.g., Mask-RCNN"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To reduce the points number, we evenly select some anchors in space, and the nearest k points are concatenated and fused by a linear projection. Note that this reduction can be achieved by a convolutional operation if all points are arranged in order and k is properly set (i.e., 4 and 9), like in ViT #REFR. For clarity on the centers and anchors stated previously, we strongly suggest the readers check appendix \u00a7 B. Task-Specific Applications. For classification, we average all points of the last block's output and use a FC layer for classification.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce the number of points, a simplified clustering algorithm is employed, which is similar to the approach used in Mask R-CNN #REFR for predicting an object mask in parallel with bounding box recognition.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To extract object masks from an image, Mask R-CNN #REFR extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.", "given_citation_intent": "background", "given_keywords": ["(e.g., Mask-RCNN"]}]}
{"citing_id": "2303.04456v1", "cited_id": "1609.05158", "section_title": "Ablation Study", "citation": "Thanks to the residual upsampling, depth edges are less dispersed comparing to the results using conventional upsampling #REFR as demonstrated in Fig. 9 . A RMU consists of modulation and update parts.", "text_before_citation": ["All the results are evaluated on their testing splits and are capped at 80m per standard practice. RMU and Residual Upsampling.", "As shown in Table 4 , the full model outperforms the baseline by a large margin. The Gordon et al.", "#OTHEREFR Distilled Semantics #OTHEREFR Lite-HR-Depth #OTHEREFR Monodepth2 #OTHEREFR Lee et al.", "#OTHEREFR HR-Depth #OTHEREFR PackNet #OTHEREFR RM-Depth proposed components are effective in improving the depth accuracy.", "By removing either the residual upsampling or RMU, the depth error is increased."], "text_after_citation": ["When the modulation part is removed, the depth error is increased.", "This indicates that the depth improvement is largely benefited by the modulation since it adaptively modifies the feed-forward behavior of the encoder. Object Motion.", "The full model performs the best among Figure 9 . Depth map predictions using different upsamplings. all the variants as summarized in Table 5 .", "The proposed components are effective in improving depth accuracy on non-rigid scenes.", "When warping is disabled, the source images are not warped towards the target image."], "citing_paper_content": {"title": "Rm-Depth: Unsupervised Learning Of Recurrent Monocular Depth In Dynamic Scenes *", "abstract": "Unsupervised methods have showed promising results on monocular depth estimation. However, the training data must be captured in scenes without moving objects. To push the envelope of accuracy, recent methods tend to increase their model parameters. In this paper, an unsupervised learning framework is proposed to jointly predict monocular depth and complete 3D motion including the motions of moving objects and camera. (1) Recurrent modulation units are used to adaptively and iteratively fuse encoder and decoder features. This not only improves the singleimage depth inference but also does not overspend model parameters. (2) Instead of using a single set of filters for upsampling, multiple sets of filters are devised for the residual upsampling. This facilitates the learning of edgepreserving filters and leads to the improved performance. (3) A warping-based network is used to estimate a motion field of moving objects without using semantic priors. This breaks down the requirement of scene rigidity and allows to use general videos for the unsupervised learning. The motion field is further regularized by an outlier-aware training loss. Despite the depth model just uses a single image in test time and 2.97M parameters, it achieves state-of-the-art results on the KITTI and Cityscapes benchmarks. * This research work is not for commercial use unless a prior arrangement has been made with the author. 1 The words, ego-motion, camera motion and pose, are interchangeably used throughout the paper."}, "cited_paper_content": {"title": "Real-Time Single Image And Video Super-Resolution Using An Efficient Sub-Pixel Convolutional Neural Network", "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."}, "keywords": ["depth edges", "conventional upsampling"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed RM-Depth model achieves state-of-the-art results on the KITTI and Cityscapes benchmarks by jointly predicting monocular depth and complete 3D motion, and the full model outperforms the baseline by a large margin (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed approach in this manuscript outperforms previous CNN-based methods by a significant margin, as shown in Table 4, achieving a performance improvement of +0.15dB on Images and +0.39dB on Videos, and is an order of magnitude faster, which is consistent with the findings in the #REFR paper.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed RM-Depth model outperforms the baseline by a large margin, demonstrating the effectiveness of its components in improving depth accuracy, including the use of multiple sets of filters for residual upsampling and the introduction of an efficient sub-pixel convolution layer to replace the conventional upsampling filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, as shown in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["depth edges", "conventional upsampling"]}]}
{"citing_id": "2303.13935v1", "cited_id": "1705.05363", "section_title": "Vi. Conclusions And Future Work", "citation": "This flexible framework incorporates well with the reward-shaping techniques, such as entropy regularization, curiosity #REFR , etc.", "text_before_citation": ["Our work unifies the SF-GPI and value composition to the continuous concurrent composition framework and allows reconstructing task policy from a set of primitives.", "The proposed method was extended to composition at the action component level.", "We demonstrate in the Pointmass environment that our multi-task agents can reconstruct the task policy from a set of primitives in real time and transfer the skills to solve unseen tasks while the single-task performance is competitive with SAC."], "text_after_citation": ["In addition, the task-agnostic property should benefit the autotelic framework #OTHEREFR where agents can set goals and curriculum for themselves #OTHEREFR .", "However, the primary concern at this stage is whether the proposed approach can scale to higher dimensional problems. Additionally, two important topics are left as future works. First, look for the corresponding value composition for DAC.", "A good starting point might be thinking of the MSF composition with weights evaluated by GPE. Second, the optimality of each composition method.", "One might start with bounding the loss incurred by the policy and value composition.", "Max Planck Institute for Intelligent Systems, 72076 T\u00fcbingen, Germany.2 Institute of Flight Mechanics and Controls, University of Stuttgart, 70569 Stuttgart, Germany. yutang.liu@tuebingen.mpg.de, aamir.ahmad@ifr.uni-stuttgart.de"], "citing_paper_content": {"title": "Multi-Task Reinforcement Learning In Continuous Control With Successor Feature-Based Concurrent Composition", "abstract": "Deep reinforcement learning (DRL) frameworks are increasingly used to solve high-dimensional continuouscontrol tasks in robotics. However, due to the lack of sample efficiency, applying DRL for online learning is still practically infeasible in the robotics domain. One reason is that DRL agents do not leverage the solution of previous tasks for new tasks. Recent work on multi-tasking DRL agents based on successor features has proven to be quite promising in increasing sample efficiency. In this work, we present a new approach that unifies two prior multi-task RL frameworks, SF-GPI and value composition, for the continuous control domain. We exploit compositional properties of successor features to compose a policy distribution from a set of primitives without training any new policy. Lastly, to demonstrate the multi-tasking mechanism, we present a new benchmark for multi-task continuous control environment based on Raisim. This also facilitates large-scale parallelization to accelerate the experiments. Our experimental results in the Pointmass environment show that our multi-task agent has single task performance on par with soft actor critic (SAC) and the agent can successfully transfer to new unseen tasks where SAC fails. We provide our code as open-source at https://github.com/robot-perception-group/ concurrent_composition for the benefit of the community."}, "cited_paper_content": {"title": "Curiosity-Driven Exploration By Self-Supervised Prediction", "abstract": "In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at this https URL"}, "keywords": ["reward-shaping techniques"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our work unifies the SF-GPI and value composition to the continuous concurrent composition framework and allows reconstructing task policy from a set of primitives. The proposed method was extended to composition at the action component level. We demonstrate in the Pointmass environment that our multi-task agents can reconstruct the task policy from a set of primitives in real time and transfer the skills to solve unseen tasks while the single-task performance is competitive with SAC. Our approach is based on successor features and value composition, which have been shown to be promising in increasing sample efficiency in multi-tasking DRL agents. Specifically, we leverage the compositional properties of successor features to compose a policy distribution from a set of primitives without training any new policy #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed method for multi-task reinforcement learning based on successor features and value composition is inspired by the self-supervised prediction approach presented in the reference paper #REFR, which formulates curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed method, which leverages compositional properties of successor features to compose a policy distribution from a set of primitives without training any new policy, is inspired by reward-shaping techniques and is similar to the approach presented in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["reward-shaping techniques"]}]}
{"citing_id": "2304.05379v1", "cited_id": "1706.05347", "section_title": "7)", "citation": "Therefore, for this case, the improvement in the information rate supported by 3-Group NOMA transmissions and 2-Group NOMA transmissions is given by #REFR and (22), respectively.", "text_before_citation": ["10) CASE X (l n = l m > l f ) : In this case, a total of l n transmissions will be required by 3-Group IC-NOMA scheme, with l f 3-Group NOMA transmissions and l m \u2212 l f 2-Group NOMA transmissions. The transmitted messages are", "S IC\u2212N OMA k = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u221a \u03b1P sn k + \u221a \u03b2P sm k + \u221a \u03b3P s f k , k \u2208 [l f ] and \u221a \u03b1 1 P sn k + (1 \u2212 \u03b1 1 )P sm k , k = {l f + 1, \u2022 \u2022 \u2022 ln}", "with \u03b1 < \u03b2 < \u03b3, \u03b1 + \u03b2 + \u03b3=1 and \u03b1 1 < 0.5", "The information rate for 3-Group NOMA transmissions will be the same as #OTHEREFR .", "The information rate for 2-Group NOMA transmissions will be the same as (21)."], "text_after_citation": ["11) CASE XI ( l m > l f = l n ) : In this case, a total of l m transmissions will be required by 3-Group IC-NOMA scheme, with l n 3-Group NOMA transmissions and l m \u2212l f index-coded transmissions. The transmitted messages are", "S IC\u2212N OMA k = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u221a \u03b1P sn k + \u221a \u03b2P sm k + \u221a \u03b3P s f k , k \u2208 [ln]", "and", "\u221a P sm k , k = {ln + 1, \u2022 \u2022 \u2022 lm}", "with \u03b1 < \u03b2 < \u03b3 and \u03b1 + \u03b2 + \u03b3=1."], "citing_paper_content": {"title": "Design And Analysis Of Index Codes For 3-Group Noma In Vehicular Adhoc Networks", "abstract": "Index coding (IC) is a source coding technique employed to improve spectral utilisation, where the source node aims to satisfy users' demands by making minimum transmissions. Non-orthogonal multiple access (NOMA) is integral to the radio access technique used in 5G networks. Index-coded NOMA (IC-NOMA) transmission scheme in Vehicular Adhoc Networks (VANETs) involves applying NOMA principles on index-coded data to avoid network congestion and to improve spectral efficiency compared to conventional IC systems. In this work, a spectral efficient transmission scheme called 3-Group IC-NOMA is proposed, and an innovative index code design that fits with NOMA decoding principles to obtain improved spectral efficiency is developed. Through exhaustive analytical studies, we demonstrate that the proposed transmission scheme always supports higher rates than the conventional IC systems and requires less power to achieve an information rate at least as good as conventional IC systems."}, "cited_paper_content": {"title": "A Survey On Non-Orthogonal Multiple Access For 5G Networks: Research Challenges And Future Trends", "abstract": "Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed."}, "keywords": ["3-Group NOMA transmissions", "2-Group NOMA transmissions"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The information rate for 3-Group NOMA transmissions will be the same as the one reported in \"A Survey On Non-Orthogonal Multiple Access For 5G Networks: Research Challenges And Future Trends\" #REFR. The information rate for 2-Group NOMA transmissions will be the same as (21).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Non-orthogonal multiple access (NOMA) is a key technology for 5G networks, enabling low latency, high reliability, massive connectivity, improved fairness, and high throughput, as highlighted in the survey by D. Dardari et al. (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Non-orthogonal multiple access (NOMA) is a key enabling technology for 5G networks, and the proposed 3-Group IC-NOMA scheme in Vehicular Adhoc Networks (VANETs) requires a total of l n transmissions, with l f 3-Group NOMA transmissions and l m \u2212 l f 2-Group NOMA transmissions, achieving higher rates than conventional IC systems and requiring less power, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["3-Group NOMA transmissions", "2-Group NOMA transmissions"]}]}
{"citing_id": "2303.11592v1", "cited_id": "1703.06211", "section_title": "Introduction", "citation": "In contrast to prior works that require complex structures, the proposed method requires only simple convolutional neural networks (CNNs) for restoration and a single deformable convolution #REFR to align the features of the current and reference frame.", "text_before_citation": ["However, the first step has the limitation that the restoration is based on general datasets and does not utilize any video-specific information.", "Hence, we further enhance the video with the guidance of a reference image.", "The reference image is an uncompressed frame, which drives the enhancement process to be video-specific, leading to improved restoration for both background and textures.", "To better utilize the reference image, we let the network derive a confidence map indicating which pixels to use for the enhancement.", "Through the confidence map, we can efficiently exclude parts of the reference image that are uncorrelated to the current frame, i.e., components that may rather increase the distortion due to the misguided information."], "text_after_citation": ["Experiments show that the proposed method achieves comparable performance for highresolution benchmarks while requiring practical decoding times when the HEVC is employed in Fig. 1 .", "For the case of using the VVC, we still obtain significant coding gains over the VVC, leading to higher performance compared to other neural codecs. In summary, the main contributions are as follows:", "\u2022 We propose a novel hybrid video compression method, consisting of a conventional video codec, a lossless image codec, and a reference-guided restoration network.", "\u2022 The compressed video is enhanced in a two-step procedure, where the first step restores the details lost by conventional video encoding, and the second step uses video-specific information from the reference image for further enhancement.", "\u2022 Our method requires only 38 ms for decoding a 2K frame on NVIDIA 1080 Ti while achieving comparable or better performance than top-tier methods."], "citing_paper_content": {"title": "Lightweight Hybrid Video Compression Framework Using Reference-Guided Restoration Network", "abstract": "Recent deep-learning-based video compression methods brought coding gains over conventional codecs such as AVC and HEVC. However, learning-based codecs generally require considerable computation time and model complexity. In this paper, we propose a new lightweight hybrid video codec consisting of a conventional video codec (HEVC or VVC), a lossless image codec, and our new restoration network. Precisely, our encoder consists of a conventional video encoder and a lossless image encoder, transmitting a lossy-compressed video bitstream along with a losslessly-compressed reference frame. The decoder is constructed with corresponding video/image decoders and a new restoration network, which enhances the compressed video in two-step processes. In the first step, a network trained with a video dataset restores the details lost by the conventional encoder. Then, we further boost the video quality with the guidance of a reference image, which is a losslessly compressed video frame. The reference image provides video-specific information, which can be utilized to better restore the details of a compressed video. Experimental results show that when the HEVC is employed as a baseline in our framework, the overall coding gain is comparable to recent top-tier neural codecs while requiring much less encoding time and lower complexity. When combined with the VVC, our method brings significant gain over the VVC, thus achieving state-of-the-art coding performance."}, "cited_paper_content": {"title": "Deformable Convolutional Networks", "abstract": "Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets."}, "keywords": ["simple convolutional neural", "single deformable convolution"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To better utilize the reference image, we let the network derive a confidence map indicating which pixels to use for the enhancement (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the restoration process, the authors propose a new restoration network that utilizes a reference image to provide video-specific information, leading to improved restoration for both background and textures, and the network derives a confidence map indicating which pixels to use for the enhancement, excluding uncorrelated parts of the reference image that may increase the distortion due to the misguided information (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the restoration process, the authors propose a new restoration network that utilizes a reference image to provide video-specific information, leading to improved restoration for both background and textures. The network derives a confidence map indicating which pixels to use for the enhancement, efficiently excluding parts of the reference image that are uncorrelated to the current frame. The proposed method achieves state-of-the-art coding performance with much less encoding time and lower complexity compared to recent top-tier neural codecs, such as those based on simple convolutional neural networks. (#REFR)", "given_citation_intent": "method", "given_keywords": ["simple convolutional neural", "single deformable convolution"]}]}
{"citing_id": "2305.00400v1", "cited_id": "1501.04131", "section_title": "C. Illustrative Example", "citation": "It is known #REFR that the (i, j) th element of matrix A \u22121 is +1 if branch j is directed along path from i to slack bus, -1 if it is directed against, and 0 otherwise.", "text_before_citation": ["Lemma 2 (Norm values of r j , s j and t j ). We have, for all branches j,", "r j 2 = |H j \u2229 N g |, s j 2 = |H j \u2229 N l |, t j 2 = |H j \u2229 N l |.", "Proof.", "Recall that branch j shares its index with its downstream bus j.", "We use the properties of matrix F in this proof. From Lemma 1, F = A \u2212 ."], "text_after_citation": ["By the construction of G, A \u22121 only has entries -1 and 0.", "Thus, A \u2212 has (i, j) th element -1 if branch i is falls in the path from bus j to the origin and 0 otherwise.", "Therefore, the i th row of A \u2212 collects all buses in H i .", "The result follows by separating generator buses and placing their coefficients in r j , and load buses in s j and t j ."], "citing_paper_content": {"title": "On Lindistflow Model Congestion Pricing: Bounding The Changes In Power Tariffs", "abstract": "The optimal power flow (OPF) problem is an important mathematical program that aims at obtaining the best operating point of an electric power grid. The optimization problem typically minimizes the total generation cost subject to certain physical constraints of the system. The so-called linearized distribution flow (LinDistFlow) model leverages a set of linear equations to approximate the nonlinear AC power flows. In this paper, we consider an OPF problem based on the LinDistFlow model for a single-phase radial power network. We derive closed-form solutions to the marginal values of both real and reactive power demands. We also derive upper bounds on the congestion price (a.k.a. 'shadow price'), which denotes the change in marginal demand prices when the apparent power flow limits of certain lines are binding at optimum. Various cases of our result are discussed while simulations are carried out on a 141-bus radial power network."}, "cited_paper_content": {"title": "Structure Learning In Power Distribution Networks", "abstract": "Traditionally power distribution networks are either not observable or only partially observable. This complicates development and implementation of new smart grid technologies, such as those related to demand response, outage detection and management, and improved load-monitoring. Here, inspired by proliferation of the metering technology, we discuss statistical estimation problems in structurally loopy but operationally radial distribution grids consisting in learning operational layout of the network from measurements, e.g. voltage data, which are either already available or can be made available with a relatively minor investment. Our newly suggested algorithms apply to a wide range of realistic scenarios. The algorithms are also computationally efficient -- polynomial in time -- which is proven theoretically and illustrated computationally on a number of test cases. The technique developed can be applied to detect line failures in real time as well as to understand the scope of possible adversarial attacks on the grid."}, "keywords": ["bus", "matrix"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors proposed algorithms for learning the operational layout of power distribution networks from measurements, which can be applied to detect line failures in real time and understand the scope of possible adversarial attacks on the grid #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' newly suggested algorithms for learning the operational layout of power distribution networks from measurements, such as voltage data, can be found in the reference paper \"Structure Learning In Power Distribution Networks\" and apply to a wide range of realistic scenarios. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The matrix F used in the proof of Lemma 2 is related to the statistical estimation problems in power distribution networks, which can be solved efficiently with polynomial time algorithms proposed in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["bus", "matrix"]}]}
{"citing_id": "2303.00738v1", "cited_id": "2003.13922", "section_title": "Experimental Design", "citation": "We also have two control explanations to which we compare our experimental explanations: one where there are no privacy protections (deterministic control) and another that includes a high-level explanation of DP that does not mention , which is adapted from Xiong et al. #REFR (Xiong et al. control).", "text_before_citation": ["We use a 3 \u00d7 4 \u00d7 2 between-subjects study design where each respondent sees one explanation, computed using a particular explanation method (ODDS-TEXT, ODDS-VIS, SAMPLE REPORTS) and privacy budget ( \u2208 {.1, .5, 2, 4}), in a given scenario type (optional, mandatory)."], "text_after_citation": ["Deterministic Control: The worst-case situation for a hypothetical employee in our vignette is that no privacy protection is applied.", "In this case, the risk of the negative consequence in the scenario is deterministic (i.e., respondents can expect deterministic outcomes in their manager's beliefs): If the respondent participates/answers NO, their manager will believe they responded NO with probability 1 and if they do the opposite, their manager will believe they responded NO with probability 0.", "Not only is this \"explanation\" a worst case, but prior work #OTHEREFR on risk communication also suggests that it will be the simplest for respondents to understand (objective risk comprehension) and may give them the greatest self-efficacy because of its determinism, in contrast to our probabilistic explanations.", "Hence, we use this deterministic setting as a control: to do so, we include the same scenario text as in the experimental conditions but omit the stylized description of DP and explanation of privacy guarantees."], "citing_paper_content": {"title": "What Are The Chances? Explaining The Epsilon Parameter In Differential Privacy", "abstract": "Differential privacy (DP) is a mathematical privacy notion increasingly deployed across government and industry. With DP, privacy protections are probabilistic: they are bounded by the privacy budget parameter,. Prior work in health and computational science finds that people struggle to reason about probabilistic risks. Yet, communicating the implications of to people contributing their data is vital to avoiding privacy theater-presenting meaningless privacy protection as meaningful-and empowering more informed data-sharing decisions. Drawing on best practices in risk communication and usability, we develop three methods to convey probabilistic DP guarantees to end users: two that communicate odds and one offering concrete examples of DP outputs. We quantitatively evaluate these explanation methods in a vignette survey study (n = 963) via three metrics: objective risk comprehension, subjective privacy understanding of DP guarantees, and self-efficacy. We find that odds-based explanation methods are more effective than (1) output-based methods and (2) state-of-the-art approaches that gloss over information about. Further, when offered information about , respondents are more willing to share their data than when presented with a state-of-the-art DP explanation; this willingness to share is sensitive to values: as privacy protections weaken, respondents are less likely to share data. Keywords differential privacy \u2022 usable security & privacy \u2022 privacy budgets * The author conducted part of this work while visiting Columbia University. \u2020 The author conducted part of this work while visiting the Simons Institute for the Theory of Computing at UC Berkeley. \u2021 The author contributed equally to advising this work."}, "cited_paper_content": {"title": "Towards Effective Differential Privacy Communication For Users' Data Sharing Decision And Comprehension", "abstract": "Differential privacy protects an individual's privacy by perturbing data on an aggregated level (DP) or individual level (LDP). We report four online human-subject experiments investigating the effects of using different approaches to communicate differential privacy techniques to laypersons in a health app data collection setting. Experiments 1 and 2 investigated participants' data disclosure decisions for low-sensitive and high-sensitive personal information when given different DP or LDP descriptions. Experiments 3 and 4 uncovered reasons behind participants' data sharing decisions, and examined participants' subjective and objective comprehensions of these DP or LDP descriptions. When shown descriptions that explain the implications instead of the definition/processes of DP or LDP technique, participants demonstrated better comprehension and showed more willingness to share information with LDP than with DP, indicating their understanding of LDP's stronger privacy guarantee compared with DP."}, "keywords": ["privacy protections"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We use a 3 \u00d7 4 \u00d7 2 between-subjects study design where each respondent sees one explanation, computed using a particular explanation method (ODDS-TEXT, ODDS-VIS, SAMPLE REPORTS) and privacy budget ( \u2208 {.1, .5, 2, 4}), in a given scenario type (optional, mandatory) #REFR. The reference paper provides insights into the effects of using different approaches to communicate differential privacy techniques to laypersons in a health app data collection setting, which could be relevant to our study design.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To communicate differential privacy techniques to laypersons, the authors of this study drew on best practices in risk communication and usability, and developed three methods to convey probabilistic DP guarantees to end users, as reported in \"#REFR\".", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To effectively communicate differential privacy techniques to laypersons and improve their comprehension and willingness to share information, the authors of this study draw on best practices in risk communication and usability, and develop three methods to convey probabilistic DP guarantees to end users, as reported in the reference paper #REFR. These methods are crucial for avoiding privacy theater and empowering more informed data-sharing decisions, thereby enhancing privacy protections.", "given_citation_intent": "method", "given_keywords": ["privacy protections"]}]}
{"citing_id": "2303.06213v1", "cited_id": "1807.03748", "section_title": "Contrastive Loss", "citation": "Inspired by InfoNCE loss #REFR , we propose a hyperedge-level contrastive loss to enhance the Fig. 5 (a) ).", "text_before_citation": ["Basic contrastive loss is similar to GCL #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR that contrasts the same data from different views.", "Two used contrastive loss notions are used commonly: node-level contrastive loss #OTHEREFR , #OTHEREFR , #OTHEREFR and clusterlevel contrastive loss #OTHEREFR .", "The former aims to distribute nodes uniformly in the hyperplane, while the latter focuses on clustering nodes with similar categories and separating nodes with different categories.", "Clearly, the latter aligns with the objective of classification, while the former does not.", "Thus, we include cluster-level contrastive loss in the basic contrastive loss but exclude node-level contrastive loss."], "text_after_citation": ["Conversely, the cluster membership probability distributions of different nodes learned from different views are treated as negative samples (e.g., cluster Fig.", "5 (a) ), to differentiate between node clusters rather than nodes.", "Ideally, each node cluster should correspond to nodes of the same class.", "To achieve this, we project node embeddings into a space with dimensionality equal to the number of data classes (i.e., the target number of clusters).", "In particular, the projection is carried out using a two-layer MLP, i.e., the projection head in Fig. 5 (a) ."], "citing_paper_content": {"title": "Chgnn: A Semi-Supervised Contrastive Hypergraph Learning Network", "abstract": "Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also includes basic and cross-validation contrastive losses, associated with an enhanced contrastive loss training process. Experimental results on nine real datasets offer insight into the effectiveness of CHGNN, showing that it outperforms 13 competitors in terms of classification accuracy consistently."}, "cited_paper_content": {"title": "Representation Learning With Contrastive Predictive Coding", "abstract": "While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments."}, "keywords": ["hyperedge-level contrastive loss"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The basic contrastive loss is similar to GCL that contrasts the same data from different views. Two used contrastive loss notions are used commonly: node-level contrastive loss and cluster-level contrastive loss. The former aims to distribute nodes uniformly in the hyperplane, while the latter focuses on clustering nodes with similar categories and separating nodes with different categories. Clearly, the latter aligns with the objective of classification, while the former does not. Thus, we include cluster-level contrastive loss in the basic contrastive loss but exclude node-level contrastive loss, as proposed in the Contrastive Predictive Coding paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The basic contrastive loss used in our proposed CHGNN is similar to that used in GCL, which contrasts the same data from different views #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The cluster-level contrastive loss used in our basic contrastive loss, which aims to cluster nodes with similar categories and separate nodes with different categories, aligns with the objective of classification, as shown in #REFR's use of hyperedge-level contrastive loss.", "given_citation_intent": "background", "given_keywords": ["hyperedge-level contrastive loss"]}]}
{"citing_id": "2303.01350v1", "cited_id": "1903.01237", "section_title": "The Essence Of The Mio Dijkstra Monad", "citation": "When we write computations, we will work directly with the pre-and post-condition, and not with the predicate transformer, but these two approaches are equivalent #REFR .", "text_before_citation": ["For simplicity, we define operations for file management and for socket communication, but these account only for one possible instantiation of the monad.", "Our approach can be extended to other IO operations as needed.", "Moreover, the free monad can be used to implement other effects such as exceptions, state and non-determinism by extending the signature of effects #OTHEREFR , and it is future work to integrate these effects in our account.", "The MIO monadic effect uses a specification monad that captures the behavior of a computation as a predicate transformer, i.e., a function that given a post-condition, it returns a pre-condition that is strong enough to guarantee the post-condition after execution of the computation.", "As explained in the introduction, in our setting a pre-condition is a property of the current trace, and a post-condition is a property of the result and the new trace."], "text_after_citation": ["Finally, we need to define a monad morphism from the computation monad to the specification monad.", "This is done using a translation from computations written in the free monad to a predicate transformer written in the specification monad, following a recipe widely employed in F \u2605 .", "Up to this point, we focused on defining a Dijkstra monad that allows statically verifying IO programs via specifications, but we have not mentioned any mechanism to support dynamic checks.", "To enable dynamic verification, we include a new silent operation called get_trace that is meant to return the trace computed until the point it is called.", "This operation allows to write computations that reflect on the past events performed, and which allows to write dynamic checks based on that."], "citing_paper_content": {"title": "Securely Compiling Verified F \u2605 Programs With Io", "abstract": "We propose a secure compilation chain for statically verified partial programs with input-output (IO). The source language is an F \u2605 subset in which a verified IO-performing program interacts with its IO-performing context via a higher-order interface that includes refinement types as well as pre-and post-conditions about past IO events. The target language is a smaller F \u2605 subset in which the compiled program is linked with an adversarial context via an interface without refinement types or pre-and post-conditions. To bridge this interface gap and make compilation and linking secure we propose a novel combination of higher-order contracts and reference monitoring for recording and controlling IO operations. During compilation we use contracts to convert the logical assumptions the program makes about the context into dynamic checks on each context-program boundary crossing. These boundary checks can depend on information about past IO events stored in the monitor's state, yet these checks cannot stop the adversarial target context before it performs dangerous IO operations. So, additionally, our linking forces the context to perform all IO via a secure IO library that uses reference monitoring to dynamically enforce an access control policy before each IO operation. We propose a novel way to model in F \u2605 that the context cannot directly access the IO operations and the monitor's internal state, based on F \u2605 's recent support for flag-based effect polymorphism. We prove in F \u2605 that enforcing the access control policy on the context in combination with static verification of the program soundly enforces a global trace property. Moreover, we prove in F \u2605 that our secure compilation chain satisfies by construction Robust Relational Hyperproperty Preservation, a very strong secure compilation criterion. Finally, we illustrate our secure compilation chain at work on a simple web server example."}, "cited_paper_content": {"title": "Dijkstra Monads For All", "abstract": "This paper proposes a general semantic framework for verifying programs with arbitrary monadic side-effects using Dijkstra monads, which we define as monad-like structures indexed by a specification monad. We prove that any monad morphism between a computational monad and a specification monad gives rise to a Dijkstra monad, which provides great flexibility for obtaining Dijkstra monads tailored to the verification task at hand. We moreover show that a large variety of specification monads can be obtained by applying monad transformers to various base specification monads, including predicate transformers and Hoare-style pre- and postconditions. For defining correct monad transformers, we propose a language inspired by Moggi's monadic metalanguage that is parameterized by a dependent type theory. We also develop a notion of algebraic operations for Dijkstra monads, and start to investigate two ways of also accommodating effect handlers. We implement our framework in both Coq and F*, and illustrate that it supports a wide variety of verification styles for effects such as exceptions, nondeterminism, state, input-output, and general recursion."}, "keywords": ["computations", "predicate transformer"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The free monad can be extended to implement other effects such as exceptions, state, and non-determinism by extending the signature of effects #REFR, and it is future work to integrate these effects in our account.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The free monad can be extended to implement other effects such as exceptions, state, and non-determinism by extending the signature of effects #OTHEREFR, as proposed in the Dijkstra Monads For All paper.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The MIO monadic effect uses a specification monad that captures the behavior of computations as a predicate transformer, which can be obtained by applying monad transformers to various base specification monads #REFR.", "given_citation_intent": "background", "given_keywords": ["computations", "predicate transformer"]}]}
{"citing_id": "2304.11832v1", "cited_id": "1612.03144", "section_title": "D.1 Method", "citation": "The reason are twofold: 1) whereas the propagation of image classification models is unidirectional, intermediate features calculated by detector's backbone model may be revisited long after its first usage due to the existence of FPN #REFR , region proposal networks (RPN), and ROI heads.", "text_before_citation": ["On object detection, we do not match features by feed the transformed student (teacher) backbone features to the later part of the teacher (student) backbone model."], "text_after_citation": ["Therefore, directly matching intermediate features calculated by detectors' backbone models w.r.t.", "function will make the subsequent propagation process complicated, and could involve great extra overhead; 2) In existing works, the common practice for conducting feature distillation on detectors is to match the features output by FPN #OTHEREFR , rather than by the backbone.", "When considering how to functionally align FPN features, the situation becomes clearer and easier.", "Therefore, the detection version of FCFD is designed to match FPN features.", "Denote the list of teacher features output by FPN as F t = { F k t } K k=1 , and the list of student features output by FPN as"], "citing_paper_content": {"title": "Function-Consistent Feature Distillation", "abstract": "Feature distillation makes the student mimic the intermediate features of the teacher. Nearly all existing feature-distillation methods use L2 distance or its slight variants as the distance metric between teacher and student features. However, while L2 distance is isotropic w.r.t. all dimensions, the neural network's operation on different dimensions is usually anisotropic, i.e., perturbations with the same 2-norm but in different dimensions of intermediate features lead to changes in the final output with largely different magnitude. Considering this, we argue that the similarity between teacher and student features should not be measured merely based on their appearance (i.e., L2 distance), but should, more importantly, be measured by their difference in function, namely how later layers of the network will read, decode, and process them. Therefore, we propose Function-Consistent Feature Distillation (FCFD), which explicitly optimizes the functional similarity between teacher and student features. The core idea of FCFD is to make teacher and student features not only numerically similar, but more importantly produce similar outputs when fed to the later part of the same network. With FCFD, the student mimics the teacher more faithfully and learns more from the teacher. Extensive experiments on image classification and object detection demonstrate the superiority of FCFD to existing methods. Furthermore, we can combine FCFD with many existing methods to obtain even higher accuracy. Our codes are available at https://github.com/LiuDongyang6/FCFD."}, "cited_paper_content": {"title": "Feature Pyramid Networks For Object Detection", "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."}, "keywords": ["unidirectional, intermediate features", "region proposal networks"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "On object detection, we do not match features by feed the transformed student (teacher) backbone features to the later part of the teacher (student) backbone model. To address this issue, Feature Pyramid Networks (FPN) have been proposed as a generic feature extractor in several applications, which constructs feature pyramids with marginal extra cost by exploiting the inherent multi-scale, pyramidal hierarchy of deep convolutional networks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of matching features, recent object detectors have avoided pyramid representations, which are a basic component in recognition systems for detecting objects at different scales #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To match features unidirectionally, the authors of \"Function-Consistent Feature Distillation\" proposed Function-Consistent Feature Distillation (FCFD), which optimizes the functional similarity between teacher and student features, not only numerically but also in terms of producing similar outputs when fed to the later part of the same network, and achieved superior results in image classification and object detection compared to existing methods. This approach is different from the region proposal networks that avoid pyramid representations due to their slow computation and memory intensity, as described in the reference paper #REFR on feature pyramid networks for object detection.", "given_citation_intent": "background", "given_keywords": ["unidirectional, intermediate features", "region proposal networks"]}]}
{"citing_id": "2303.02557v1", "cited_id": "1805.00909", "section_title": "Background", "citation": "This objective has since been generalized for the setting of entropy-regularized RL [Ziebart, 2010 #REFR , which augments the standard RL objective in Eq.", "text_before_citation": ["In this work, we analyze the case of finite, discrete state and action spaces, with the Markov Decision Process (MDP) model #OTHEREFR .", "The MDP is represented as a tuple T = S, A, p, \u00b5, r, \u03b3 where S is the set of available states; A is the set of possible actions; p : S \u00d7 A \u2192 S is a transition function describing the system dynamics; \u00b5 is the initial state distribution; r : S \u00d7 A \u2192 R is a (bounded) reward function which associates a reward (or cost) with each state-action pair; and \u03b3 \u2208 (0, 1] is a discount factor which discounts future rewards and guarantees the convergence of total reward for infinitely long trajectories (T \u2192 \u221e).", "In \"standard\" (un-regularized) RL, the agent maximizes an objective function which is the expected future reward:", "EQUATION"], "text_after_citation": ["(1) by appending an entropic regularization term for the policy:", "EQUATION", "where \u03c0 0 (a|s) is the fixed prior policy.", "The additional entropic control cost discourages the agent from choosing policies that deviate too much from the prior policy.", "Importantly, entropy-regularized MDPs lead to stochastic optimal policies that are provably robust to perturbations of rewards and dynamics #OTHEREFR ; making them a more suitable choice for real-world problems."], "citing_paper_content": {"title": "Bounding The Optimal Value Function In Compositional Reinforcement Learning", "abstract": "In the field of reinforcement learning (RL), agents are often tasked with solving a variety of problems differing only in their reward functions. In order to quickly obtain solutions to unseen problems with new reward functions, a popular approach involves functional composition of previously solved tasks. However, previous work using such functional composition has primarily focused on specific instances of composition functions whose limiting assumptions allow for exact zero-shot composition. Our work unifies these examples and provides a more general framework for compositionality in both standard and entropy-regularized RL. We find that, for a broad class of functions, the optimal solution for the composite task of interest can be related to the known primitive task solutions. Specifically, we present double-sided inequalities relating the optimal composite value function to the value functions for the primitive tasks. We also show that the regret of using a zero-shot policy can be bounded for this class of functions. The derived bounds can be used to develop clipping approaches for reducing uncertainty during training, allowing agents to quickly adapt to new tasks."}, "cited_paper_content": {"title": "Reinforcement Learning And Control As Probabilistic Inference: Tutorial And Review", "abstract": "The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research."}, "keywords": ["entropy-regularized RL"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In \"standard\" (un-regularized) RL, the agent maximizes an objective function which is the expected future reward (REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Reinforcement learning provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable, and has been shown to be equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Entropy-regularized RL has been widely used in reinforcement learning and control, and has been shown to be equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics #REFR.", "given_citation_intent": "background", "given_keywords": ["entropy-regularized RL"]}]}
{"citing_id": "2304.04319v1", "cited_id": "1812.07032", "section_title": "Conclusion And Future Works", "citation": "Interestingly (and ironically, in a way), the results of this paper point to a previously unseen relationship between Dice loss and Boundary loss #REFR .", "text_before_citation": ["We have shown, both analytically and empirically, that it is possible to mimic the dice loss with a simple negative of the ground truth.", "This indicates that supervision comes mostly from the gradient sign, and much less than its magnitude.", "This interesting finding should help to interpret and diagnose results when tuning losses for segmentation networks, and guide researchers when evaluating or designing novel segmentation losses.", "Those initial findings also point to many future and exciting new works.", "For instance, the simpler mime might be more straightforward to adapt for imbalanced tasks #OTHEREFR ."], "text_after_citation": ["Another area to investigate is the behavior with respect to sub-patching.", "While computable, the definition of Dice loss makes less semantic sense when applied to only a sub-patch of an image-while potentially causing divide by 0 errors on empty patches-, but this is not the case with a mime loss."], "citing_paper_content": {"title": "On The Dice Loss Gradient And The Ways To Mimic It", "abstract": "In the past few years, in the context of fully-supervised semantic segmentation, several losses-such as cross-entropy and dicehave emerged as de facto standards to supervise neural networks. The Dice loss is an interesting case, as it comes from the relaxation of the popular Dice coefficient; one of the main evaluation metric in medical imaging applications. In this paper, we first study theoretically the gradient of the dice loss, showing that concretely it is a weighted negative of the ground truth, with a very small dynamic range. This enables us, in the second part of this paper, to mimic the supervision of the dice loss, through a simple element-wise multiplication of the network output with a negative of the ground truth. This rather surprising result sheds light on the practical supervision performed by the dice loss during gradient descent. This can help the practitioner to understand and interpret results while guiding researchers when designing new losses."}, "cited_paper_content": {"title": "Boundary Loss For Highly Unbalanced Segmentation", "abstract": "Widely used loss functions for convolutional neural network (CNN) segmentation, e.g., Dice or cross-entropy, are based on integrals (summations) over the segmentation regions. Unfortunately, it is quite common in medical image analysis to have highly unbalanced segmentations, where standard losses contain regional terms with values that differ considerably --typically of several orders of magnitude-- across segmentation classes, which may affect training performance and stability. The purpose of this study is to build a boundary loss, which takes the form of a distance metric on the space of contours, not regions. We argue that a boundary loss can mitigate the difficulties of regional losses in the context of highly unbalanced segmentation problems because it uses integrals over the boundary between regions instead of unbalanced integrals over regions. Furthermore, a boundary loss provides information that is complementary to regional losses. Unfortunately, it is not straightforward to represent the boundary points corresponding to the regional softmax outputs of a CNN. Our boundary loss is inspired by discrete (graph-based) optimization techniques for computing gradient flows of curve evolution. Following an integral approach for computing boundary variations, we express a non-symmetric L2 distance on the space of shapes as a regional integral, which avoids completely local differential computations involving contour points. Our boundary loss is the sum of linear functions of the regional softmax probability outputs of the network. Therefore, it can easily be combined with standard regional losses and implemented with any existing deep network architecture for N-D segmentation. Our boundary loss has been validated on two benchmark datasets corresponding to difficult, highly unbalanced segmentation problems: the ischemic stroke lesion (ISLES) and white matter hyperintensities (WMH)."}, "keywords": ["Boundary loss"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our findings suggest that the boundary loss can mitigate the difficulties of regional losses in the context of highly unbalanced segmentation problems, as it uses integrals over the boundary between regions instead of unbalanced integrals over regions #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' proposed boundary loss function could potentially address the issue of highly unbalanced segmentations in medical image analysis, as it uses integrals over the boundary between regions instead of unbalanced integrals over regions, and has been validated on benchmark datasets for difficult segmentation problems such as ischemic stroke lesion and white matter hyperintensities #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of the boundary loss proposed in the reference paper #REFR could potentially address the issue of highly unbalanced segmentations, as it uses integrals over the boundary between regions instead of unbalanced integrals over regions.", "given_citation_intent": "result", "given_keywords": ["Boundary loss"]}]}
{"citing_id": "2304.08178v1", "cited_id": "1807.11546", "section_title": "Iii. Prediction And Explanation Model", "citation": "In order to achieve higher-quality textual action descriptions and explanations, we modified the language generator in #REFR with the aim of generating better sentence grammar.", "text_before_citation": ["In the training, vehicle acceleration and vehicle course values are fed to the model along with corresponding ground truth natural language annotations.", "Structurally, the pipeline consists of a CNNbased visual feature encoder that extracts visual information which represent high-level object descriptions.", "Afterwards, an LSTM-based vehicle controller with implemented spatial attention predicts vehicle acceleration and change of course for a series of image frames.", "In the end, an LSTM-based language generator uses the controller's spatial attention together with temporal attention over multiple video frames.", "The textual action descriptions and explanations are generated by outputting per-word softmax probabilities."], "text_after_citation": ["Further, we integrated part of speech prediction in the generator and included special token penalties in the loss function."], "citing_paper_content": {"title": "Textual Explanations For Automated Commentary Driving", "abstract": "The vehicle stops because the traffic light turns red\" Neural Network (Black Box) Fig. 1. On a high level, the approach takes a series of image frames as input into an opaque neural network and outputs a natural language explanation."}, "cited_paper_content": {"title": "Textual Explanations For Self-Driving Vehicles", "abstract": "Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller's output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e., acceleration and change of course. The controller's attention identifies image regions that potentially influence the network's output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving."}, "keywords": ["higher-quality textual action", "explanations"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The vehicle controller's attention identifies image regions that potentially influence the network's output, and an attention-based video-to-text model produces textual explanations of model actions, as proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The vehicle controller's attention identifies image regions that potentially influence the network's output, and an attention-based video-to-text model is used to produce textual explanations of model actions, as proposed in the reference paper \"Textual Explanations For Self-Driving Vehicles\" by Kim et al. #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The approach proposed in this manuscript for generating higher-quality textual action descriptions and explanations is based on an attention-based video-to-text model, which is aligned with the controller's attention maps to ground the explanations in the parts of the scene that mattered to the controller (#REFR).", "given_citation_intent": "method", "given_keywords": ["higher-quality textual action", "explanations"]}]}
{"citing_id": "2303.10275v1", "cited_id": "1607.08128", "section_title": "Implementation Details 4.1. Mesh Fitting", "citation": "On the contrary, results of iterative non-linear optimization #REFR better correspond 2D RGB images but are less plausible or unstable in 3D, since the fitting process relies on 2D image keypoints, which are poorly constrained along the depth dimension.", "text_before_citation": ["For both our system and the standard DNR model to perform well, precise and time-consistent SMPL-X mesh fitting to the training video frames (and at test time) is important.", "As for RGB photos, the convolutional regressionbased techniques #OTHEREFR produce somewhat accurate but inconsistent results that frequently mismatch the silhouettes."], "text_after_citation": ["Regressionbased and optimization-based methods are unable to handle complex cases such as poses with self contact.", "Below we describe existing enhancements to SMPL-X optimizationbased fitting that increase the stability of DNR-based avatar systems such as ours.", "We later validate the individual impact of these adjustments through ablation study.", "Shared shape A person's identity does not change between frames of a training video, thus the shape-specific parameters of SMPL-X model can be shared.", "Since those shape parameters are also suitable for inference, the overhead of SMPL-X inference on mobile can be reduced."], "citing_paper_content": {"title": "Morf: Mobile Realistic Fullbody Avatars From A Monocular Video", "abstract": "We present a new approach for learning Mobile Realistic Fullbody (MoRF) avatars. MoRF avatars can be rendered in real-time on mobile phones, have high realism, and can be learned from monocular videos. As in previous works, we use a combination of neural textures and the mesh-based body geometry modeling SMPL-X. We improve on prior work, by learning per-frame warping fields in the neural texture space, allowing to better align the training signal between different frames. We also apply existing SMPL-X fitting procedure refinements for videos to improve overall avatar quality. In the comparisons to other monocular video-based avatar systems, MoRF avatars achieve higher image sharpness and temporal consistency. Participants of our user study also preferred avatars generated by MoRF."}, "cited_paper_content": {"title": "Keep It Smpl: Automatic Estimation Of 3D Human Pose And Shape From A Single Image", "abstract": "We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art."}, "keywords": ["3D"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For both our system and the standard DNR model to perform well, precise and time-consistent SMPL-X mesh fitting to the training video frames (and at test time) is important. As for RGB photos, the convolutional regression-based techniques #REFR produce somewhat accurate but inconsistent results that frequently mismatch the silhouettes. \n\nMethod: To achieve precise and time-consistent SMPL-X mesh fitting, the authors of this manuscript used the fitting procedure refinements from the reference paper, \"Keep It Smpl: Automatic Estimation Of 3D Human Pose And Shape From A Single Image.\" #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To accurately estimate the 3D pose and shape of the human body from a single image, previous works have utilized the SMPL model, which captures correlations in human shape across the population, as well as a CNN-based method to predict 2D body joint locations, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To accurately estimate the 3D pose and shape of the human body from a single image, previous works have utilized the SMPL model, which captures correlations in human shape across the population, as well as a CNN-based method to predict 2D body joint locations, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["3D"]}]}
{"citing_id": "2304.07134v1", "cited_id": "1709.02753", "section_title": "Introduction", "citation": "Soon after the technology was deployed, it was found that Apple's implementation uses \u03b5 = 4 when collecting emoji usage data and \u03b5 = 8 when collecting web domain data #REFR .", "text_before_citation": ["One of these mechanisms is Count Mean Sketch (CMS).", "CMS is used on iOS and Mac OS devices to report both emojis used and websites visited to Apple.", "Featured in Apple's keynote, local differential privacy allows the company to \"help discover the usage patterns of a large number of users without compromising individual privacy\" #OTHEREFR .", "This implementation, and in particular Apple's choice of the parameter \u03b5, has come under criticism from privacy researchers.", "It is generally believed that \u03b5 -which controls the privacy loss incurred by the user -should typically not exceed ln(3) (\u2248 1.10) #OTHEREFR ."], "text_after_citation": ["Apple's choice to only consider the privacy loss per submission -once a day for both the web domain and emoji data -instead of a total privacy loss \u03b5 tot (after which objects would no longer be collected from the user #OTHEREFR ) has similarly raised concerns on theoretical ground.", "While Apple states that they remove user identifiers and IP addresses after the obfuscated objects are received by their server #OTHEREFR , this is a measure that relies on trust and hence conflicts with local differential privacy's purpose of protecting against an untrusted curator #OTHEREFR .", "It is indeed well-known that the mathematical guarantees offered by local differential privacy degrade as multiple objects are collected from the same user, something that can be quantified with an upper bound using the Composition Theorem #OTHEREFR (\u03b5 tot \u2264 \u03b5 1 + . . . + \u03b5 n ).", "Regardless of how revealing the user's original data may be, a low \u03b5 tot would guarantee that the obfuscated data will never leak much information.", "However, \u03b5 tot is a worst-case theoretical measure: it is unclear the extent to which collecting multiple objects and using a large \u03b5 for each object open the door to attacks in practice. Pool inference attack."], "citing_paper_content": {"title": "Pool Inference Attacks On Local Differential Privacy: Quantifying The Privacy Guarantees Of Apple'S Count Mean Sketch In Practice", "abstract": "Figure 1: Example of pools defined on a universe \u2126 consisting of emojis, when the adversary is interested in determining the skin tone that is most often selected by the user. In this case, Usr's preferred pool is the one containing medium-light skin tone emojis."}, "cited_paper_content": {"title": "Privacy Loss In Apple'S Implementation Of Differential Privacy On Macos 10.12", "abstract": "In June 2016, Apple announced that it will deploy differential privacy for some user data collection in order to ensure privacy of user data, even from Apple. The details of Apple's approach remained sparse. Although several patents have since appeared hinting at the algorithms that may be used to achieve differential privacy, they did not include a precise explanation of the approach taken to privacy parameter choice. Such choice and the overall approach to privacy budget use and management are key questions for understanding the privacy protections provided by any deployment of differential privacy. ::: In this work, through a combination of experiments, static and dynamic code analysis of macOS Sierra (Version 10.12) implementation, we shed light on the choices Apple made for privacy budget management. We discover and describe Apple's set-up for differentially private data processing, including the overall data pipeline, the parameters used for differentially private perturbation of each piece of data, and the frequency with which such data is sent to Apple's servers. ::: We find that although Apple's deployment ensures that the (differential) privacy loss per each datum submitted to its servers is $1$ or $2$, the overall privacy loss permitted by the system is significantly higher, as high as $16$ per day for the four initially announced applications of Emojis, New words, Deeplinks and Lookup Hints. Furthermore, Apple renews the privacy budget available every day, which leads to a possible privacy loss of 16 times the number of days since user opt-in to differentially private data collection for those four applications. ::: We advocate that in order to claim the full benefits of differentially private data collection, Apple must give full transparency of its implementation, enable user choice in areas related to privacy loss, and set meaningful defaults on the privacy loss permitted."}, "keywords": ["emoji usage data", "Apple's implementation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Apple's implementation of differential privacy on macOS 10.12 has been scrutinized for its privacy budget management, and it has been found that the overall privacy loss permitted by the system is significantly higher than the differential privacy loss per datum submitted to its servers #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Apple's implementation of differential privacy on MacOS 10.12 has been shown to have a significantly higher overall privacy loss than the (differential) privacy loss per each datum submitted to its servers, which raises concerns about the privacy protections provided by any deployment of differential privacy #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Apple's implementation of local differential privacy for collecting emoji usage data has been criticized for its choice of privacy parameter \u03b5, which has been shown to result in significantly higher privacy loss than expected, as demonstrated in #REFR.", "given_citation_intent": "background", "given_keywords": ["emoji usage data", "Apple's implementation"]}]}
{"citing_id": "2304.14835v1", "cited_id": "1206.0038", "section_title": "Iii. Main Results", "citation": "Hence, even if a very small \u03b2 is selected -so that (12) holds with practical certainty -the number of scenarios to be sampled remains manageable, see also #REFR .", "text_before_citation": ["Conversely, if the safety constraints (10b) are overly restrictive, the scenario problem (11) may become unfeasible; if this were the case, however, the original problem (9) would also certainly be unfeasible, and one would need to consider broader classes of policies, or to relax the safety requirements, e.g., by introducing slack variables in #OTHEREFR . \u03b8 , it holds that:", "P \u03b8 max w 2 \u22641 R(\u03a6 u , \u03a8 u (\u03b8), w, \u03b8) \u2264R N ,", "and (x, u) \u2208 S(\u03b8) , \u2200w \u2208 W(\u03b8) \u2265 1 \u2212 , #OTHEREFR that is, given a parameter realization \u03b8 \u223c P \u03b8 , the probability that the optimal policy \u03a6 u (\u03a8 u (\u03b8), D) computed solving (11) both incurs regret of at mostR N (\u03a8 u (\u03b8), D) and complies with the safety constraints is lower bounded by 1 \u2212 .", "Theorem 1 presents an explicit sample complexity bound that, given a priori specified confidence and violation levels, ensures that our safety and regret guarantees extend to all but at most a fraction of unseen dynamics \u03b8 \u2208 \u0398 with arbitrarily high probability 1 \u2212 \u03b2.", "As well-known in the literature on scenario optimization, the minimum number of scenarios N ( , \u03b2) required to fulfill the conditions of Theorem 1 grows at most logarithmically with \u03b2 \u22121 ."], "text_after_citation": ["Further, we note that the condition on the number N of uncertainty samples given in Theorem 1 is tight for fully-supported problems #OTHEREFR ; a simpler, albeit not tight, sufficient condition on N is given by #OTHEREFR :", "EQUATION", "We remark that following a scenario approach allows us to explicitly compute the clairvoyant optimal policy \u03c8(w, \u03b8) by replacing the uncertain system dynamics with their sampled counterparts.", "Regret bounds relative to the instance-wise optimal benchmark \u03c8(w, \u03b8) are attractive, as they yield upper bounds on the closed-loop cost that adapt to the realized dynamics \u03b8 and perturbation w.", "To illustrate this point more thoroughly, let us consider an alternative design based on a classical worst-case H \u221e objective:"], "citing_paper_content": {"title": "Regret Optimal Control For Uncertain Stochastic Systems", "abstract": "We consider control of uncertain linear timevarying stochastic systems from the perspective of regret minimization. Specifically, we focus on the problem of designing a feedback controller that minimizes the loss relative to a clairvoyant optimal policy that has foreknowledge of the system dynamics and the exogenous disturbances. In this competitive framework, establishing robustness guarantees proves challenging as, differently from the case where the model is known, the benchmark policy is not only inapplicable, but also impossible to compute without knowledge of the system parameters. To overcome this issue, we embrace a scenario optimization approach, and we propose minimizing regret robustly over a finite set of randomly sampled system parameters. We prove that this policy optimization problem can be efficiently solved through semidefinite programming, and that the corresponding solution retains strong probabilistic out-of-sample regret guarantees in face of the uncertain dynamics. Our method naturally extends to include satisfaction of safety constraints with high probability. We validate our theoretical results and showcase the potential of our approach by means of numerical simulations."}, "cited_paper_content": {"title": "Robust Model Predictive Control Via Scenario Optimization", "abstract": "This paper discusses a novel probabilistic approach for the design of robust model predictive control (MPC) laws for discrete-time linear systems affected by parametric uncertainty and additive disturbances. The proposed technique is based on the iterated solution, at each step, of a finite-horizon optimal control problem (FHOCP) that takes into account a suitable number of randomly extracted scenarios of uncertainty and disturbances, followed by a specific command selection rule implemented in a receding horizon fashion. The scenario FHOCP is always convex, also when the uncertain parameters and disturbance belong to nonconvex sets, and irrespective of how the model uncertainty influences the system's matrices. Moreover, the computational complexity of the proposed approach does not depend on the uncertainty/disturbance dimensions, and scales quadratically with the control horizon. The main result in this work is related to the analysis of the closed loop system under receding-horizon implementation of the scenario FHOCP, and essentially states that the devised control law guarantees constraint satisfaction at each step with some a priori assigned probability p, while the system's state reaches the target set either asymptotically, or in finite time with probability at least p. The proposed method may be a valid alternative when other existing techniques, either deterministic or stochastic, are not directly usable due to excessive conservatism or to numerical intractability caused by lack of convexity of the robust or chance-constrained optimization problem."}, "keywords": ["practical certainty"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To overcome the issue of establishing robustness guarantees in the face of uncertain dynamics, the authors propose a scenario optimization approach, as discussed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors proposed a scenario optimization approach for the design of robust model predictive control laws for discrete-time linear systems affected by parametric uncertainty and additive disturbances, which is based on the iterated solution of a finite-horizon optimal control problem that takes into account a suitable number of randomly extracted scenarios of uncertainty and disturbances, followed by a specific command selection rule implemented in a receding horizon fashion #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed scenario optimization approach in this work provides a practical certainty for the design of robust model predictive control laws for discrete-time linear systems affected by parametric uncertainty and additive disturbances, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": ["practical certainty"]}]}
{"citing_id": "2303.14302v1", "cited_id": "1908.11310", "section_title": "Vision-Language Pretraining For Aesthetics", "citation": "For aesthetic pretraining, we use the AVA-Captions dataset #REFR which is currently the largest available dataset for aesthetic comments.", "text_before_citation": ["Many of these methods are trained on large proprietary datasets #OTHEREFR with image-text pairs crawled from the web.", "While this general pretraining strategy has proven useful for tasks such as image classification and retrieval, it is limited in its ability to represent aestheticrelated information due to the under-representation of such information on the web.", "Consequently, the aesthetic information gets diluted in the vast amount of pretraining data.", "To address this limitation, we propose a two-stage pretraining approach that involves initializing the model with a generally pretrained image-text model and then further pretraining it on aesthetic image-comment pairs.", "For general pretraining, we use a 650M filtered subset of the openly available LAION-5B-English #OTHEREFR dataset."], "text_after_citation": ["Each image in AVA-Captions is associated with one or more user comments that provide informative insights into different aesthetic aspects of the image.", "We randomly sample one comment for each image to construct imagecomment pairs during training.", "In contrast to traditional supervised learning with predefined labels or categories, vision-language pretraining enables learning of open-set aesthetic concepts through noisy image-comment pairs.", "This results in visual and textual representations that encompass a wider range of aesthetic concepts, enhancing transferability to downstream tasks."], "citing_paper_content": {"title": "Vila: Learning Image Aesthetics From User Comments With Vision-Language Pretraining", "abstract": "Assessing the aesthetics of an image is challenging, as it is influenced by multiple factors including composition, color, style, and high-level semantics. Existing image aesthetic assessment (IAA) methods primarily rely on humanlabeled rating scores, which oversimplify the visual aesthetic information that humans perceive. Conversely, user comments offer more comprehensive information and are a more natural way to express human opinions and preferences regarding image aesthetics. In light of this, we propose learning image aesthetics from user comments, and exploring vision-language pretraining methods to learn multimodal aesthetic representations. Specifically, we pretrain an image-text encoder-decoder model with image-comment pairs, using contrastive and generative objectives to learn rich and generic aesthetic semantics without human labels. To efficiently adapt the pretrained model for downstream IAA tasks, we further propose a lightweight rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept. Our results show that our pretrained aesthetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and it has powerful zero-shot capability for aesthetic tasks such as zero-shot style classification and zero-shot IAA, surpassing many supervised baselines. With only minimal finetuning parameters using the proposed adapter module, our model achieves state-of-the-art IAA performance over the AVA dataset."}, "cited_paper_content": {"title": "Aesthetic Image Captioning From Weakly-Labelled Photographs", "abstract": "Aesthetic image captioning (AIC) refers to the multi-modal task of generating critical textual feedbacks for photographs. While in natural image captioning (NIC), deep models are trained in an end-to-end manner using large curated datasets such as MS-COCO, no such large-scale, clean dataset exists for AIC. Towards this goal, we propose an automatic cleaning strategy to create a benchmarking AIC dataset, by exploiting the images and noisy comments easily available from photography websites. We propose a probabilistic caption-filtering method for cleaning the noisy web-data, and compile a large-scale, clean dataset \"AVA-Captions\", (230, 000 images with 5 captions per image). Additionally, by exploiting the latent associations between aesthetic attributes, we propose a strategy for training the convolutional neural network (CNN) based visual feature extractor, the first component of the AIC framework. The strategy is weakly supervised and can be effectively used to learn rich aesthetic representations, without requiring expensive ground-truth annotations. We finally show-case a thorough analysis of the proposed contributions using automatic metrics and subjective evaluations."}, "keywords": ["aesthetic comments"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Many of these methods are trained on large proprietary datasets with image-text pairs crawled from the web. While this general pretraining strategy has proven useful for tasks such as image classification and retrieval, it is limited in its ability to represent aesthetic-related information due to the under-representation of such information on the web. Consequently, the aesthetic information gets diluted in the vast amount of pretraining data. To address this limitation, we propose a two-stage pretraining approach that involves initializing the model with a generally pretrained image-text model and then further pretraining it on aesthetic image-comment pairs. For general pretraining, we use a 650M filtered subset of the openly available LAION-5B-English dataset (REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitation of under-representation of aesthetic-related information in general pretraining, the authors propose a two-stage pretraining approach that involves initializing the model with a generally pretrained image-text model and then further pretraining it on aesthetic image-comment pairs, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To further pretrain the model on aesthetic image-comment pairs, the authors propose a probabilistic caption-filtering method for cleaning the noisy web-data and compile a large-scale, clean dataset \"AVA-Captions\" consisting of 230,000 images with 5 #REFR captions per image.", "given_citation_intent": "method", "given_keywords": ["aesthetic comments"]}]}
{"citing_id": "2304.09560v1", "cited_id": "1906.01824", "section_title": "Estimating Conditional Mutual Information With The Logging Policy (Cmip)", "citation": "Given this divergence-based formulation of CMI, we employ a two-step approach suggested in #REFR .", "text_before_citation": ["In this section, we cover how to estimate the CMIP metric to quantify debiasedness.", "First, we note that CMI can be expressed as the Kullback-Leibler divergence between two distributions:", "EQUATION", "which is a pseudo-distance between the joint distribution of all three variables occurring together and the distribution in which the predicted relevance scores\u02dcand the relevance of the logging policies are independent, conditional on .", "If the divergence between both distributions is zero, the joint distribution (which we actually observe) is equivalent to the distribution on which conditional independence holds."], "text_after_citation": ["First, we obtain samples from the marginal distribution on which conditional independence holds.", "Second, we estimate the KL-divergence between the observed dataset and the generated samples, which is the estimate of our CMIP metric. 4.3.1 Sampling from the marginal distribution .", "How can we obtain samples from the conditional independence distribution given our observational dataset ? For a proof that this methodology actually approximates , we refer to #OTHEREFR Theorem 1] .", "We use a knn-based approach suggested in #OTHEREFR ; its simplicity and computational speed make it suitable for an evaluation metric.", "Given a dataset of observed relevance labels for each document, R = {(\u02dc( ), ( ), ( ))} \u2208 D , we split the data into two equal parts R and R ."], "citing_paper_content": {"title": "An Offline Metric For The Debiasedness Of Click Models", "abstract": "A well-known problem when learning from user clicks are inherent biases prevalent in the data, such as position or trust bias. Click models are a common method for extracting information from user clicks, such as document relevance in web search, or to estimate click biases for downstream applications such as counterfactual learning-to-rank, ad placement, or fair ranking. Recent work shows that the current evaluation practices in the community fail to guarantee that a well-performing click model generalizes well to downstream tasks in which the ranking distribution differs from the training distribution, i.e., under covariate shift. In this work, we propose an evaluation metric based on conditional independence testing to detect a lack of robustness to covariate shift in click models. We introduce the concept of debiasedness and a metric for measuring it. We prove that debiasedness is a necessary condition for recovering unbiased and consistent relevance scores and for the invariance of click prediction under covariate shift. In extensive semi-synthetic experiments, we show that our proposed metric helps to predict the downstream performance of click models under covariate shift and is useful in an off-policy model selection setting. CCS CONCEPTS \u2022 Information systems \u2192 Query log analysis."}, "cited_paper_content": {"title": "Ccmi : Classifier Based Conditional Mutual Information Estimation", "abstract": "Conditional Mutual Information (CMI) is a measure of conditional dependence between random variables X and Y, given another random variable Z. It can be used to quantify conditional dependence among variables in many data-driven inference problems such as graphical models, causal learning, feature selection and time-series analysis. While k-nearest neighbor (kNN) based estimators as well as kernel-based methods have been widely used for CMI estimation, they suffer severely from the curse of dimensionality. In this paper, we leverage advances in classifiers and generative models to design methods for CMI estimation. Specifically, we introduce an estimator for KL-Divergence based on the likelihood ratio by training a classifier to distinguish the observed joint distribution from the product distribution. We then show how to construct several CMI estimators using this basic divergence estimator by drawing ideas from conditional generative models. We demonstrate that the estimates from our proposed approaches do not degrade in performance with increasing dimension and obtain significant improvement over the widely used KSG estimator. Finally, as an application of accurate CMI estimation, we use our best estimator for conditional independence testing and achieve superior performance than the state-of-the-art tester on both simulated and real data-sets."}, "keywords": ["divergence-based formulation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We leverage advances in classifiers and generative models to design methods for CMI estimation, as proposed in #REFR, which can be used to quantify conditional dependence among variables in many data-driven inference problems such as graphical models, causal learning, feature selection and time-series analysis.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate the CMIP metric for quantifying debiasedness, the authors propose using an estimator for KL-Divergence based on the likelihood ratio by training a classifier to distinguish the observed joint distribution from the product distribution, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate the CMIP metric for quantifying debiasedness, the authors leverage advances in classifiers and generative models to design methods for CMI estimation, including a divergence-based formulation that draws ideas from conditional generative models #REFR.", "given_citation_intent": "method", "given_keywords": ["divergence-based formulation"]}]}
{"citing_id": "2304.13971v1", "cited_id": "1701.01430", "section_title": "Synthetic Seismic Data", "citation": "The second problem we apply our scale separation algorithm to is a seismic problem where waves, or primary waves, are fast and S waves, or secondary waves are slow. In #REFR , Vogl et al.", "text_before_citation": [], "text_after_citation": ["present a method for a high-resolution seismic model of a fault slippage under the ocean.", "In the Figure 4 : The DMD recreations of the fast and slow components of the toy problem right-hand side.", "These recreations are fed into ARKODE as the fast and slow functions.", "work they present a linear hyperbolic system of equations that model the 2D plane-strain case of isotropic linear elasticity:", "EQUATION"], "citing_paper_content": {"title": "Data-Driven Time-Scale Separation Of Ode Right-Hand Sides Using Dynamic Mode Decomposition And Time Delay Embedding", "abstract": "Multi-physics simulation often involve multiple different scales. The ARKODE ODE solver package in the SUNDIALS library addresses multi-scale problems with a multi-rate time-integrator that can work with a right-hand side that has fast scale and slow scale components. In this report, we use dynamic mode decomposition and time delay embedding to extract the fast and and slow components of the right-hand sides of a simple ODE from data. We then use the extracted components to solve the ODE with ARKODE. Finally, to move towards a real-world use case, we attempt to extract fast and slow scale dynamics from synthetic seismic modeling data."}, "cited_paper_content": {"title": "A High-Resolution Finite Volume Seismic Model To Generate Seafloor Deformation For Tsunami Modeling", "abstract": "A high-resolution finite volume method approach to incorporating time-dependent slip across rectangular subfaults when modeling general fault geometry is presented. The fault slip is induced by a modification of the Riemann problem to the linear elasticity equations across cell interfaces aligned with the subfaults. This is illustrated in the context of the high-resolution wave-propagation algorithms that are implemented in the open source Clawpack software (www.clawpack.org), but this approach could be easily incorporated into other Riemann solver based numerical methods. Surface deformation results are obtained in both two and three dimensions and compared to those given by the steady-state, homogeneous half-space Okada solution."}, "keywords": ["seismic problem"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address multi-scale problems in seismic modeling, the authors attempt to extract fast and slow scale dynamics from data using dynamic mode decomposition and time delay embedding, and then solve the resulting ODE with ARKODE, a multi-rate time-integrator that can work with a right-hand side that has fast scale and slow scale components #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model seafloor deformation for tsunami modeling, the authors of this report use a high-resolution finite volume method approach that induces fault slip by modifying the Riemann problem to the linear elasticity equations across cell interfaces aligned with the subfaults, as presented in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To accurately model the fast and slow scale components of the right-hand side of the ODE in seismic problems, the authors of this report used dynamic mode decomposition and time delay embedding, and then solved the ODE with ARKODE, a multi-rate time-integrator that incorporates a method from the reference paper for incorporating time-dependent slip across rectangular subfaults when modeling general fault geometry #REFR.", "given_citation_intent": "method", "given_keywords": ["seismic problem"]}]}
{"citing_id": "2304.11846v1", "cited_id": "1811.11286", "section_title": "Ablation Study", "citation": "And the improvement is more evident in the comparison between the third and forth rows, it proves that our midpoint interpolation result provides a better initial position, which benefits the update process under the same number of iterations. P3DConv vs. EdgeConv #REFR .", "text_before_citation": ["We employ the midpoint interpolation result P I to serve as both input of P2PNet and initial point cloud to be updated.", "For validating the effectiveness of midpoint interpolation, we replace the input of P2PNet with the low-res point cloud P L .", "And we also sample points from the Gaussian distribution N (p, \u03c3 2 ) to get another initial point cloud, denoted as P S .", "Finally, we combine different network inputs and initial point clouds to conduct the experiments, holding all other parameters the same.", "From the second and forth rows of Tab 7, we conclude that using the denser interpolated point cloud P I as network input can achieve better performance, since it contains more geometrical information and benefits the feature extraction process."], "text_after_citation": ["The EdgeConv based feature extractor #OTHEREFR is widely used by previous work #OTHEREFR .", "However, EdgeConv #OTHEREFR utilizes most of parameters to refine each individual feature.", "While our P3DConv focuses on the feature aggregation achieved by generated convolution kernels, thus benefits the extraction of local and global features.", "For verifying the superior performance of our P3DConv, we replace the dense block in our P2PNet with EdgeConv in #OTHEREFR , and we also fine-tune the number of EdgeConv layers to achieve the comparable network parameters, as Tab 8 shows. Table 8 .", "4\u00d7 comparative results on the PU1K dataset with different convolution layers."], "citing_paper_content": {"title": "Grad-Pu: Arbitrary-Scale Point Cloud Upsampling Via Gradient Descent With Learned Distance Functions", "abstract": "Most existing point cloud upsampling methods have roughly three steps: feature extraction, feature expansion and 3D coordinate prediction. However, they usually suffer from two critical issues: (1) fixed upsampling rate after one-time training, since the feature expansion unit is customized for each upsampling rate; (2) outliers or shrinkage artifact caused by the difficulty of precisely predicting 3D coordinates or residuals of upsampled points. To adress them, we propose a new framework for accurate point cloud upsampling that supports arbitrary upsampling rates. Our method first interpolates the low-res point cloud according to a given upsampling rate. And then refine the positions of the interpolated points with an iterative optimization process, guided by a trained model estimating the difference between the current point cloud and the high-res target. Extensive quantitative and qualitative results on benchmarks and downstream tasks demonstrate that our method achieves the state-of-the-art accuracy and efficiency."}, "cited_paper_content": {"title": "Patch-Based Progressive 3D Point Set Upsampling", "abstract": "We present a detail-driven deep neural network for point set upsampling. A high-resolution point set is essential for point-based rendering and surface reconstruction. Inspired by the recent success of neural image super-resolution techniques, we progressively train a cascade of patch-based upsampling networks on different levels of detail end-to-end. We propose a series of architectural design contributions that lead to a substantial performance boost. The effect of each technical contribution is demonstrated in an ablation study. Qualitative and quantitative experiments show that our method significantly outperforms the state-of-the-art learning-based and optimazation-based approaches, both in terms of handling low-resolution inputs and revealing high-fidelity details."}, "keywords": ["midpoint interpolation result"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Using the denser interpolated point cloud P I as network input can achieve better performance, since it contains more geometrical information and benefits the feature extraction process. This is consistent with the findings in the reference paper on patch-based progressive 3D point set upsampling, which demonstrated a significant performance boost through a series of architectural design contributions and outperformed the state-of-the-art learning-based and optimization-based approaches in handling low-resolution inputs and revealing high-fidelity details. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the experiments conducted in this study, using the denser interpolated point cloud as network input can achieve better performance, since it contains more geometrical information and benefits the feature extraction process (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The denser interpolated point cloud P I obtained from the midpoint interpolation result is shown to improve the performance of the feature extraction process, as demonstrated in the experiments conducted in this study #REFR.", "given_citation_intent": "result", "given_keywords": ["midpoint interpolation result"]}]}
{"citing_id": "2303.13846v1", "cited_id": "2003.08866", "section_title": "Conclusion", "citation": "From this observation, we can see that too small \u03c4 will degenerate the Gumbel softmax into binary sampling and make the mask become a binary mask, which could result in no gradients or improper training #REFR .", "text_before_citation": ["This is because they do not fully recapture the discriminative cues underlying in non-robust activations.", "Our method with Separation leads to the highest robustness, showing that FSR well identifies the non-robust activations and recaptures discriminative cues from them. Hyperparameter Study.", "We also compare the robustness as we vary the temperature \u03c4 (Eq. 2) that controls how \"discrete\" the mask is.", "For low temperature values, the output mask becomes more discrete (i.e., most values are close to either 0 or 1), and for high temperature values, it becomes more uniform (i.e., most values are far away from 0 or 1) #OTHEREFR . As shown in Fig.", "A1a , we achieve the highest robustness when \u03c4 = 0.1."], "text_after_citation": ["In contrast, too large \u03c4 will make the mask become more uniformly distributed and reduce the gap between the mask values applied on robust or non-robust activations, thus making our goal of disentanglement less feasible. In Fig. A1b and Fig.", "A1c , we visualize the trends of model robustness as we vary the weights on our proposed loss functions L sep (Eq. 3) and L rec (Eq. 4).", "Higher value of \u03bb sep generally improves robustness under all attacks with the best performance achieved when \u03bb sep = 1, showing that our proposed objectives help the model learn more robust feature representations.", "Similar trends can also be observed for \u03bb rec ; higher value of \u03bb rec generally improves robustness with the best performance achieved when \u03bb rec = 1.", "Setting \u03bb sep and \u03bb rec to be too high, however, tends to degrade robustness."], "citing_paper_content": {"title": "Feature Separation And Recalibration For Adversarial Robustness", "abstract": "Deep neural networks are susceptible to adversarial attacks due to the accumulation of perturbations in the feature level, and numerous works have boosted model robustness by deactivating the non-robust feature activations that cause model mispredictions. However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional useful information for correct model predictions. To this end, we propose a novel, easy-to-plugin approach named Feature Separation and Recalibration (FSR) that recalibrates the malicious, non-robust activations for more robust feature maps through Separation and Recalibration. The Separation part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activations that are responsible for model mispredictions upon adversarial attack. The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions. Extensive experiments verify the superiority of FSR compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57% with small computational overhead. Codes are available at https://github.com/wkim97/FSR."}, "cited_paper_content": {"title": "Spatially Adaptive Inference With Stochastic Feature Sampling And Interpolation", "abstract": "In the feature maps of CNNs, there commonly exists considerable spatial redundancy that leads to much repetitive processing. Towards reducing this superfluous computation, we propose to compute features only at sparsely sampled locations, which are probabilistically chosen according to activation responses, and then densely reconstruct the feature map with an efficient interpolation procedure. With this sampling-interpolation scheme, our network avoids expending computation on spatial locations that can be effectively interpolated, while being robust to activation prediction errors through broadly distributed sampling. A technical challenge of this sampling-based approach is that the binary decision variables for representing discrete sampling locations are non-differentiable, making them incompatible with backpropagation. To circumvent this issue, we make use of a reparameterization trick based on the Gumbel-Softmax distribution, with which backpropagation can iterate these variables towards binary values. The presented network is experimentally shown to save substantial computation while maintaining accuracy over a variety of computer vision tasks."}, "keywords": ["improper training", "binary sampling"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The presented network is experimentally shown to save substantial computation while maintaining accuracy over a variety of computer vision tasks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce the superfluous computation in feature maps of CNNs, the authors of #REFR proposed a sampling-interpolation scheme that computes features only at sparsely sampled locations, which are probabilistically chosen according to activation responses, and then densely reconstructs the feature map with an efficient interpolation procedure.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Improper training can lead to non-robust activations that are responsible for model mispredictions upon adversarial attack. To address this issue, Feature Separation and Recalibration (FSR) recalibrates the non-robust activations by identifying them through binary sampling and restoring the potentially useful cues for model predictions #REFR.", "given_citation_intent": "background", "given_keywords": ["improper training", "binary sampling"]}]}
{"citing_id": "2305.01281v1", "cited_id": "1412.6980", "section_title": "D.2 Architectures And Training Setup", "citation": "We train the class prediction models for 50 epochs and the domain classifier for 80 epochs with learning rate 0.001, weight decay 0.0001 and batchsize 128 using the Adam optimizer #REFR .", "text_before_citation": ["Our base architectures are based on the AdaTime benchmark suite, which is a large-scale evaluation of domain adaptation algorithms on time-series data.", "We extended the benchmark suite to support 11 state-of-the-art model architectures on multiple dataset types ranging from language, image to time-series data, addressed by Transformed Moons, Amazon Reviews, MiniDomainNet and the four time-series datasets (UCI-HAR, WISDM, HHAR, and Sleep-EDF) spanning in total 38 cross-domain real-world scenarios.", "Transformed Moons For the Transformed Moons dataset we use two sequential blocks with fullyconnected layers, 1D-BatchNorm, ReLU activation functions and Dropout.", "The full architecture specification can be found in Table 3 .", "The domain classifier (density ratio estimator) uses the same architecture."], "text_after_citation": ["We share the same base architecture and training setup across every domain adaption method (e.g., DANN, HoMM, CMD). Additional hyper-parameters are reported in Table 9 . Table 7 and Table 8 for details.", "Amazon Reviews For the Amazon Reviews dataset we use two sequential blocks with fullyconnected layers, 1D-BatchNorm, ReLU activation function and Dropout, analogous to the setup for Transformed Moons.", "We also use the same architecture for the domain classifier.", "We train the class prediction models for 50 epochs and the domain classifier for 80 epochs with learning rate 0.001, weight decay 0.0001 and batchsize 128 using the Adam optimizer #OTHEREFR .", "We share the same base architecture and training setup across every domain adaption method (e.g., DANN, HoMM, CMD). Additional hyper-parameters are reported in Table 9 . Table 7 and Table 8 for details."], "citing_paper_content": {"title": "Addressing Parameter Choice Issues In Unsu-Pervised Domain Adaptation By Aggregation", "abstract": "We study the problem of choosing algorithm hyper-parameters in unsupervised domain adaptation, i.e., with labeled data in a source domain and unlabeled data in a target domain, drawn from a different input distribution. We follow the strategy to compute several models using different hyper-parameters, and, to subsequently compute a linear aggregation of the models. While several heuristics exist that follow this strategy, methods are still missing that rely on thorough theories for bounding the target error. In this turn, we propose a method that extends weighted least squares to vector-valued functions, e.g., deep neural networks. We show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. We also perform a large scale empirical comparative study on several datasets, including text, images, electroencephalogram, body sensor signals and signals from mobile phones. Our method 1 outperforms deep embedded validation (DEV) and importance weighted validation (IWV) on all datasets, setting a new state-of-the-art performance for solving parameter choice issues in unsupervised domain adaptation with theoretical error guarantees. We further study several competitive heuristics, all outperforming IWV and DEV on at least five datasets. However, our method outperforms each heuristic on at least five of seven datasets."}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["domain classifier", "Adam optimizer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the base architectures, we used the AdaTime benchmark suite, which evaluates domain adaptation algorithms on time-series data, and extended it to support 11 state-of-the-art model architectures on multiple dataset types. The method we used for optimization was Adam, which is computationally efficient and well-suited for problems that are large in terms of data and/or parameters #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We implemented the Adam algorithm for stochastic optimization, which is computationally efficient and well-suited for problems with large data and/or parameters, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To build the domain classifier, we used the Adam optimizer #REFR, a first-order gradient-based optimization algorithm that is well suited for problems with very noisy and/or sparse gradients.", "given_citation_intent": "method", "given_keywords": ["domain classifier", "Adam optimizer"]}]}
{"citing_id": "2304.01517v1", "cited_id": "1810.04105", "section_title": "A. Background And Motivations", "citation": "The JCS system also benefits from mutual sharing of sensing information for improved reliability performance, e.g., using the range and Doppler knowledge to assist beamforming and channel prediction #REFR .", "text_before_citation": ["By 2030, the 6G MTC system intends to fulfill tremendous IoT connections for intelligent precise control applications to support the future IoT, such as advanced driving automation (from L4 to L5) and accurate vehicle swarm control in the industrial scenarios #OTHEREFR .", "These applications aim to conduct customized and precise control missions in mixed sensing/actuation/haptics scenarios, and require sensing accuracy of 0.1 m indoors or 1 m outdoors and communication reliability of 99.9999% for the critical applications #OTHEREFR .", "The tremendous connections and high function requirements will result in great spectrum congestion, and demand high spectrum and energy utilization efficiency #OTHEREFR - #OTHEREFR , which can not be fully addressed in 5G cellular MTC scenarios #OTHEREFR .", "The state-of-the-art joint communication and sensing (JCS) technique is a promising technique to confront the aforementioned challenges.", "The JCS system can share the unified transceiver, the same spectrum and digital signal processing hardware to exploit the reflected echo of communication beam to achieve additional radar sensing function #OTHEREFR , and achieve immediate improvement in both the spectrum and energy efficiency, compared with the conventional communication system where the echo signal is not well used for sensing #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "Code-Division Ofdm Joint Communication And Sensing System For 6G Machine-Type Communication", "abstract": "The joint communication and sensing (JCS) system can provide higher spectrum efficiency and load-saving for 6G machine-type communication (MTC) applications by merging necessary communication and sensing abilities with unified spectrum and transceivers. In order to suppress the mutual interference between the communication and radar sensing signals to improve the communication reliability and radar sensing accuracy, we propose a novel code-division orthogonal frequency division multiplex (CD-OFDM) JCS MTC system, where MTC users can simultaneously and continuously conduct communication and sensing with each other. We propose a novel CD-OFDM JCS signal and corresponding successive-interferencecancellation (SIC) based signal processing technique that obtains code-division multiplex (CDM) gain, which is compatible with the prevalent orthogonal frequency division multiplex (OFDM) communication system. To model the unified JCS signal transmission and reception process, we propose a novel unified JCS channel model. Finally, the simulation and numerical results are shown to verify the feasibility of the CD-OFDM JCS MTC system and the error propagation performance. We show that the CD-OFDM JCS MTC system can achieve not only more reliable communication but also comparably robust radar sensing compared with the precedent OFDM JCS system, especially in low signal-to-interference-and-noise ratio (SINR) regime."}, "cited_paper_content": {"title": "Multibeam For Joint Communication And Radar Sensing Using Steerable Analog Antenna Arrays", "abstract": "Beamforming has a great potential for joint communication and radar sensing (JCAS), which is becoming a demanding feature on many emerging platforms, such as unmanned aerial vehicles and smart cars. Although beamforming has been extensively studied for communication and radar sensing respectively, its application in the joint system is not straightforward due to different beamforming requirements by communication and sensing. In this paper, we propose a novel multibeam framework using steerable analog antenna arrays, which allows seamless integration of communication and sensing. Different to conventional JCAS schemes that support JCAS using a single beam, our framework is based on the key innovation of multibeam technology: providing fixed subbeam for communication and packet-varying scanning subbeam for sensing, simultaneously from a single transmitting array. We provide a system architecture and protocols for the proposed framework, complying well with modern packet communication systems with multicarrier modulation. We also propose low-complexity and effective multibeam design and generation methods, which offer great flexibility in meeting different communication and sensing requirements. We further develop sensing parameter estimation algorithms using conventional digital Fourier transform and one-dimensional compressive sensing techniques, matching well with the multibeam framework. Simulation results are provided and validate the effectiveness of our proposed framework, beamforming design methods, and the sensing algorithms."}, "keywords": ["Doppler knowledge"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed CD-OFDM JCS MTC system achieves more reliable communication and comparably robust radar sensing, especially in low SINR regime, compared with the precedent OFDM JCS system #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of multibeam technology with steerable analog antenna arrays proposed in #REFR allows for seamless integration of communication and sensing, which is a promising technique to confront the challenges of the 6G machine-type communication system, such as spectrum congestion and high spectrum and energy utilization efficiency.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed multibeam framework using steerable analog antenna arrays in #REFR provides a novel solution for joint communication and radar sensing, allowing for fixed subbeams for communication and packet-varying scanning subbeams for sensing, which can greatly improve the spectrum and energy efficiency of the 6G machine-type communication system while maintaining high Doppler knowledge.", "given_citation_intent": "background", "given_keywords": ["Doppler knowledge"]}]}
{"citing_id": "2303.09594v1", "cited_id": "1203.4580", "section_title": "I. Introduction", "citation": "In the past two decades, sparsity-based processing methods have been attracting a growing interest in statistical signal processing applications #REFR .", "text_before_citation": [], "text_after_citation": ["Quadratic compressed sensing (QCS) is a widely used formulation in sparse signal recovery;", "examples include when imaging a sparse object using partially and spatially incoherent illumination #OTHEREFR , or phase retrieval for sparse signals #OTHEREFR .", "To approach the global optimum, the QCS problem was relaxed as a semidefinite programming (SDP) problem, which involves minimizing the rank of a lifted matrix while satisfying both the recovery constraints and the row sparsity constraints on the signal #OTHEREFR .", "To retrieve the sparse solution, an iterative thresholding algorithm was proposed that leverages a sequence of SDPs.", "This approach is similar to the recent developments in the field of phase retrieval, where similar semidefinite programming-based ideas have been utilized #OTHEREFR ."], "citing_paper_content": {"title": "One-Bit Quadratic Compressed Sensing: From Sample Abundance To Linear Feasibility", "abstract": "One-bit quantization with time-varying sampling thresholds has recently found significant utilization potential in statistical signal processing applications due to its relatively low power consumption and low implementation cost. In addition to such advantages, an attractive feature of one-bit analog-to-digital converters (ADCs) is their superior sampling rates as compared to their conventional multi-bit counterparts. This characteristic endows one-bit signal processing frameworks with what we refer to as sample abundance. On the other hand, many signal recovery and optimization problems are formulated as (possibly non-convex) quadratic programs with linear feasibility constraints in the one-bit sampling regime. We demonstrate, with a particular focus on quadratic compressed sensing, that the sample abundance paradigm allows for the transformation of such quadratic problems to merely a linear feasibility problem by forming a large-scale overdetermined linear system; thus removing the need for costly optimization constraints and objectives. To efficiently tackle the emerging overdetermined linear feasibility problem, we further propose an enhanced randomized Kaczmarz algorithm, called Block SKM. Several numerical results are presented to illustrate the effectiveness of the proposed methodologies."}, "cited_paper_content": {"title": "Sparsity Constrained Nonlinear Optimization: Optimality Conditions And Algorithms", "abstract": "This paper treats the problem of minimizing a general continuously differentiable function subject to sparsity constraints. We present and analyze several different optimality criteria which are based on the notions of stationarity and coordinate-wise optimality. These conditions are then used to derive three numerical algorithms aimed at finding points satisfying the resulting optimality criteria: the iterative hard thresholding method and the greedy and partial sparse-simplex methods. The first algorithm is essentially a gradient projection method while the remaining two algorithms are of coordinate descent type. The theoretical convergence of these methods and their relations to the derived optimality conditions are studied. The algorithms and results are illustrated by several numerical examples."}, "keywords": ["sparsity-based processing methods"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently solve the quadratic compressed sensing problem with linear feasibility constraints in the one-bit sampling regime, the authors propose an enhanced randomized Kaczmarz algorithm, called Block SKM, and demonstrate the transformation of such quadratic problems to merely a linear feasibility problem by forming a large-scale overdetermined linear system, removing the need for costly optimization constraints and objectives. The proposed methodologies are illustrated by several numerical results. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of minimizing a general continuously differentiable function subject to sparsity constraints, several optimality criteria have been proposed based on the notions of stationarity and coordinate-wise optimality, and three numerical algorithms have been developed, including the iterative hard thresholding method and the greedy and partial sparse-simplex methods, as presented in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sparsity-based processing methods have been widely used in optimization problems, and the reference paper #REFR provides several different optimality criteria and numerical algorithms for minimizing a general continuously differentiable function subject to sparsity constraints.", "given_citation_intent": "background", "given_keywords": ["sparsity-based processing methods"]}]}
{"citing_id": "2304.09498v1", "cited_id": "1810.04805", "section_title": "Masked Multimodal Modeling Strategy", "citation": "Following the default settings in the field of natural language processing task #REFR , here we also randomly mask a certain fraction (e.g.", "text_before_citation": ["In fact, most of the previous visual-language modeling approaches focus on the masked language modeling of the texts in multimodal input by reconstructing the mask labeling unit, while neglects the mask feature learning at the pixel level for single image modality.", "In this work, we introduce a Masked Multimodal Modeling strategy to perform pretraining based on the MMET model, which can effectively achieve the goal of performing mask loss calculations on image and text modality. More details are depicted in Fig. 3 .", "More specifically, we mainly mask the pedestrian region instead of whole part of input images, since the human part contains more discriminative feature for representation learning, and it has no effect on the prediction performance of the model when the masked region mainly focus on the background of input samples.", "In this work, masked multimodal modeling is a dynamic strategy which can be applied to pretraining tasks on text encoders for monolingual datasets."], "text_after_citation": ["masking ratio: 15%) of image blocks and text labeling units respectively.", "Then we adopt the classifier to reconstruct from other image blocks and tokenized units on the hidden state output h M of the image-text modality.", "In contrast to denoising auto-encoders #OTHEREFR , we only predict the masked image and words rather than reconstructing the entire text inputs.", "During the pretraining, masked image modeling is applied into the image encoder over unpaired image data, while masked language modeling is applied into the text encoder over unpaired text data.", "Specifically, given a image and text data, we perform masking on image encoder and text encoder respectively."], "citing_paper_content": {"title": "Learning Robust Visual-Semantic Embedding For Generalizable Person Re-Identification", "abstract": "Generalizable person re-identification (Re-ID) is a very hot research topic in machine learning and computer vision, which plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. However, previous methods mainly focus on the visual representation learning, while neglect to explore the potential of semantic features during training, which easily leads to poor generalization capability when adapted to the new domain. In this paper, we propose a Multi-Modal Equivalent Transformer called MMET for more robust visual-semantic embedding learning on visual, textual and visualtextual tasks respectively. To further enhance the robust feature learning in the context of transformer, a dynamic masking mechanism called Masked Multimodal Modeling strategy (MMM) is introduced to mask both the image patches and the text tokens, which can jointly works on multimodal or unimodal data and significantly boost the performance of generalizable person Re-ID. Extensive experiments on benchmark datasets demonstrate the competitive performance of our method over previous approaches. We hope this method could advance the research towards visual-semantic representation learning."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["natural language processing"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this work, we introduce a Masked Multimodal Modeling strategy to perform pretraining based on the MMET model, which can effectively achieve the goal of performing mask loss calculations on image and text modality (Fig. 3). To the best of our knowledge, this is the first work that proposes such a strategy for multimodal representation learning. The proposed strategy is inspired by the Bidirectional Encoder Representations from Transformers (BERT) model #REFR, which has achieved state-of-the-art results on various natural language processing tasks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To perform pretraining tasks on text encoders for monolingual datasets, the authors introduced a dynamic strategy called Masked Multimodal Modeling, which can effectively achieve the goal of performing mask loss calculations on image and text modality (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To perform pretraining tasks on text encoders for monolingual datasets, the authors introduced a dynamic strategy called masked multimodal modeling, which can effectively achieve the goal of performing mask loss calculations on image and text modality (#REFR). This method could be useful for natural language processing tasks that require robust feature learning.", "given_citation_intent": "method", "given_keywords": ["natural language processing"]}]}
{"citing_id": "2303.16048v1", "cited_id": "1912.13477", "section_title": "Amortized Analysis", "citation": "We note a similarity to monad-comonad interaction laws #REFR , here adjusted for call-by-push-value.", "text_before_citation": ["(return : A) + (enqueue : P \u00d7 E) + (dequeue : U ((E + 1) \u21d2 F P ))", "An element of program E (A) is a finite sequence of queue instructions terminated by returning a value of type A.", "We may evaluate a program on a queue, by induction on the program:", "\u03c8 : \u2212 program E (A) \u00d7 U (queue E ) \u21d2 F A", "This expresses the usual notion of running a sequence of operations on a data structure; the code is in Listing 6."], "text_after_citation": [], "citing_paper_content": {"title": "Amortized Analysis Via Coinduction", "abstract": "Amortized analysis is a program cost analysis technique for data structures in which the cost of operations is specified in aggregate, under the assumption of continued sequential use. Typically, amortized analyses are presented inductively, in terms of finite sequences of operations; we demonstrate that coinduction provides an equivalent but more natural characterization. We describe a classic amortized data structure, the batched queue, and outline a coinductive proof of its amortized efficiency in calf , a type theory for cost analysis."}, "cited_paper_content": {"title": "Interaction Laws Of Monads And Comonads", "abstract": "We introduce and study functor-functor and monad-comonad interaction laws as mathematical objects to describe interaction of effectful computations with behaviors of effect-performing machines. Monad-comonad interaction laws are monoid objects of the monoidal category of functor-functor interaction laws. We show that, for suitable generalizations of the concepts of dual and Sweedler dual, the greatest functor resp. monad interacting with a given functor or comonad is its dual while the greatest comonad interacting with a given monad is its Sweedler dual. We relate monad-comonad interaction laws to stateful runners. We show that functor-functor interaction laws are Chu spaces over the category of endofunctors taken with the Day convolution monoidal structure. Hasegawa's glueing endows the category of these Chu spaces with a monoidal structure whose monoid objects are monad-comonad interaction laws."}, "keywords": ["monad-comonad interaction laws"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We may evaluate a program on a queue, by induction on the program: \u03c8 : \u2212 program E (A) \u00d7 U (queue E ) \u21d2 F A This expresses the usual notion of running a sequence of operations on a data structure; the code is in Listing 6. The concept of monad-comonad interaction laws introduced in #REFR can be used to describe the interaction of effectful computations with behaviors of effect-performing machines, which may provide a useful framework for analyzing the efficiency of data structures.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' use of monad-comonad interaction laws to describe the interaction of effectful computations with behaviors of effect-performing machines provides valuable background information for understanding the stateful runners in the program analyzed in this manuscript. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Monad-comonad interaction laws are mathematical objects that can be used to describe the interaction of effectful computations with the behaviors of effect-performing machines, as introduced and studied in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["monad-comonad interaction laws"]}]}
{"citing_id": "2305.00623v1", "cited_id": "1502.03167", "section_title": "Postprocessing", "citation": "BN was initially proposed to alleviate internal covariate shift, defined as the shift in the distribution of network activations caused by the change in network parameters during training #REFR .", "text_before_citation": ["The latter showed that adding a 2-layers MLPs with a non-linear activation function to the ResNet #OTHEREFR outperformed other choices such as MLPs with a linear activation function or the one without postprocessing for vision tasks #OTHEREFR .", "(2) Column-wise Postprocessing -and more specifically, column-wise standardization, in which case each dimension of the raw embeddings is centered and rescaled.", "Therefore, each node embedding is processed along with others in a batch, and no longer independently.", "This type of processing mechanism is referred to as \"batch normalization\" (BN) in the deep learning literature #OTHEREFR .", "When training deep neural networks (DNNs), BN is typically used before the activation function between layers to standardize inputs."], "text_after_citation": ["BN was subsequently found to improve speed and stability of the model's training and generalization accuracy by smoothing the optimization landscape #OTHEREFR and allowing faster learning rates #OTHEREFR .", "Here, we propose a new way of using BN as a postprocessing step in self-supervised contrastive learning.", "We posit indeed that the row-wise MLPs postprocessing used in usual methods overly complexifies the architecture of the algorithm with no clear gain in performance.", "Rather, in Sections 3 and 4, we will show that the column-wise BN postprocessing results in better representations and this result is supported by recent findings in the contrastive learning literature of alignment and uniformity paradigm #OTHEREFR .", "We refer to the contrastive learning framework with this choice of postprocessing as Contrastive Learning method for Node Representations (CLNR). The final node embeddings can be written as follows:"], "citing_paper_content": {"title": "A Simplified Framework For Contrastive Learning For Node Representations", "abstract": "Contrastive learning has recently established itself as a powerful self-supervised learning framework for extracting rich and versatile data representations. Broadly speaking, contrastive learning relies on a data augmentation scheme to generate two versions of the input data and learns low-dimensional representations by maximizing a normalized temperature-scaled cross entropy loss (NT-Xent) to identify augmented samples corresponding to the same original entity (Chen et al., 2020). In this paper, we investigate the potential of deploying contrastive learning in combination with Graph Neural Networks (Scarselli et al., 2008) for embedding nodes in a graph. Specifically, we show that the quality of the resulting embeddings and training time can be significantly improved by a simple column-wise postprocessing of the embedding matrix, instead of the row-wise postprocessing via multilayer perceptrons (MLPs) that is adopted by the majority of peer methods. This modification yields improvements in downstream classification tasks of up to 1.5% and even beats existing state-of-the-art approaches on 6 out of 8 different benchmarks. We justify our choices of postprocessing by revisiting the \"alignment vs. uniformity paradigm\" of Wang and Isola (2020), and show that column-wise post-processing improves both \"alignment\" and \"uniformity\" of the embeddings."}, "cited_paper_content": {"title": "Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate Shift", "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}, "keywords": ["network activations", "internal covariate shift"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "When training deep neural networks, batch normalization (BN) is typically used before the activation function between layers to standardize inputs (Ioffe & Szegedy, 2015). The authors of this manuscript cite the reference paper #REFR, which introduced BN as a method to address internal covariate shift and improve training efficiency by normalizing layer inputs.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "When training deep neural networks, batch normalization is typically used before the activation function between layers to standardize inputs, which draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the internal covariate shift problem in training deep neural networks, batch normalization is typically used before the activation function between layers to standardize inputs, as described in the reference paper by Ioffe and Szegedy (2015) #REFR, which has become a popular technique for stabilizing network activations.", "given_citation_intent": "background", "given_keywords": ["network activations", "internal covariate shift"]}]}
{"citing_id": "2303.13051v1", "cited_id": "1712.09867", "section_title": "Datasets And Evaluation Metrics", "citation": "ShanghaiTech #REFR is a challenging multi-scene dataset containing 13 campus scenes with various light conditions and camera angles.", "text_before_citation": ["We evaluate the proposed method on three public datasets: UCSD Ped2 #OTHEREFR , Avenue #OTHEREFR , and Shang-haiTech #OTHEREFR .", "UCSD Ped2 #OTHEREFR is a single-scene dataset collected from pedestrian walkways, including anomalies such as bikers, skaters, small carts across a walkway. Avenue #OTHEREFR is a single-scene dataset as well.", "It is captured in CUHK campus avenue, containing anomalies like running, bicycling, etc. It also contains some rare normal patterns #OTHEREFR ."], "text_after_citation": ["The statistics of these datasets are summarized in Table 1 .", "However, these three datasets contain very few scenedependent anomalies.", "And as far as we know, there is no public scene-dependent anomaly dataset available.", "In order to investigate the performance of our method on scenedependent anomaly detection, we additionally create three mixture datasets based on ShanghaiTech.", "The mixture set [01, 02] consists of videos taken from scenes 01 and 02."], "citing_paper_content": {"title": "Hierarchical Semantic Contrast For Scene-Aware Video Anomaly Detection", "abstract": "Increasing scene-awareness is a key challenge in video anomaly detection (VAD). In this work, we propose a hierarchical semantic contrast (HSC) method to learn a sceneaware VAD model from normal videos. We first incorporate foreground object and background scene features with highlevel semantics by taking advantage of pre-trained video parsing models. Then, building upon the autoencoderbased reconstruction framework, we introduce both scenelevel and object-level contrastive learning to enforce the encoded latent features to be compact within the same semantic classes while being separable across different classes. This hierarchical semantic contrast strategy helps to deal with the diversity of normal patterns and also increases their discrimination ability. Moreover, for the sake of tackling rare normal activities, we design a skeleton-based motion augmentation to increase samples and refine the model further. Extensive experiments on three public datasets and scene-dependent mixture datasets validate the effectiveness of our proposed method."}, "cited_paper_content": {"title": "Future Frame Prediction For Anomaly Detection - A New Baseline", "abstract": "Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events"}, "keywords": ["challenging multi-scene dataset"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To predict a future frame with higher quality for normal events, the authors propose a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, which is introduced for the first time in the video prediction task, and such spatial and motion constraints facilitate the future frame prediction for normal events, as well as the identification of abnormal events that do not conform to the expectation #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Anomaly detection in videos is a challenging multi-scene dataset, and the proposed method in this manuscript leverages the difference between a predicted future frame and its ground truth to detect an abnormal event, which is a novel approach compared to existing methods that minimize the reconstruction errors of training data #REFR.", "given_citation_intent": "background", "given_keywords": ["challenging multi-scene dataset"]}]}
{"citing_id": "2303.02341v1", "cited_id": "2002.00995", "section_title": "Rolnip", "citation": "Thus, overall, the proposed approach RoLNiP outperforms the state-of-the-art LCNiP #REFR on different datasets and various noise rates.", "text_before_citation": ["\u2022 RoLNiP Versus LCNiP: We observe that with actual as well as estimated noise rates, RoLNiP outperforms LCNiP for Adult, Breast Cancer, Phishing, and W8a datasets for all combinations of noise rates.", "For the Ionosphere dataset, RoLNiP outperforms LCNiP for all noise combinations except the (20%,10%) case.", "For the (20%,10%) case, the performance of RoLNiP is comparable to LCNiP.", "For the CIFAR-10 dataset, RolNiP and LCNiP perform comparably to each other except for the (30%,25%) noise case.", "\u2022 RoLNiP with Sigmoid Loss, Ramp Loss and Absolute Error Loss: We observe that sigmoid loss, ramp loss and absolute error loss show robust behavior with SD conditional SD label noise."], "text_after_citation": [], "citing_paper_content": {"title": "Rolnip: Robust Learning Using Noisy Pairwise Comparisons", "abstract": "This paper presents a robust approach for learning from noisy pairwise comparisons. We propose sufficient conditions on the loss function under which the risk minimization framework becomes robust to noise in the pairwise similar dissimilar data. Our approach does not require the knowledge of noise rate in the uniform noise case. In the case of conditional noise, the proposed method depends on the noise rates. For such cases, we offer a provably correct approach for estimating the noise rates. Thus, we propose an end-to-end approach to learning robust classifiers in this setting. We experimentally show that the proposed approach RoLNiP outperforms the robust state-of-the-art methods for learning with noisy pairwise comparisons."}, "cited_paper_content": {"title": "Learning From Noisy Similar And Dissimilar Data", "abstract": "With the widespread use of machine learning for classification, it becomes increasingly important to be able to use weaker kinds of supervision for tasks in which it is hard to obtain standard labeled data. One such kind of supervision is provided pairwise---in the form of Similar (S) pairs (if two examples belong to the same class) and Dissimilar (D) pairs (if two examples belong to different classes). This kind of supervision is realistic in privacy-sensitive domains. Although this problem has been looked at recently, it is unclear how to learn from such supervision under label noise, which is very common when the supervision is crowd-sourced. In this paper, we close this gap and demonstrate how to learn a classifier from noisy S and D labeled data. We perform a detailed investigation of this problem under two realistic noise models and propose two algorithms to learn from noisy S-D data. We also show important connections between learning from such pairwise supervision data and learning from ordinary class-labeled data. Finally, we perform experiments on synthetic and real world datasets and show our noise-informed algorithms outperform noise-blind baselines in learning from noisy pairwise data."}, "keywords": ["different datasets"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We observe that with actual as well as estimated noise rates, RoLNiP outperforms LCNiP for various datasets and noise combinations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the experimental results, RoLNiP outperforms noise-blind baselines in learning from noisy pairwise data, as demonstrated in the reference paper \"Learning From Noisy Similar And Dissimilar Data\" (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the experimental results on different datasets, #REFR outperforms the robust state-of-the-art methods for learning with noisy pairwise comparisons.", "given_citation_intent": "result", "given_keywords": ["different datasets"]}]}
{"citing_id": "2303.09101v1", "cited_id": "1903.09766", "section_title": "Datasets", "citation": "The unlabeled images are sampled from the unpaired data in the EUVP benchmark #REFR , which cover a variety of underwater scenes, water types and lighting conditions.", "text_before_citation": ["Our training set contains 1600 labeled image pairs and 1600 unlabeled images.", "The labeled image pairs are randomly sampled from #OTHEREFR and UIEB #OTHEREFR with a ratio of 1:1.", "#OTHEREFR provides a synthesized underwater image dataset in indoor scene.", "The UIEB #OTHEREFR dataset contains 890 real underwater images with corresponding ground truths."], "text_after_citation": ["Test set is built with full-reference and non-reference benchmarks.", "Full-reference test set includes 110 pairs from #OTHEREFR and 90 pairs from UIEB #OTHEREFR , namely testS and testR.", "Non-reference test set includes nearly 700 real world underwater images without ground truths from benchmarks such as UIEB, EUVP, RUIE #OTHEREFR and Seathru #OTHEREFR ."], "citing_paper_content": {"title": "Contrastive Semi-Supervised Learning For Underwater Image Restoration Via Reliable Bank", "abstract": "Despite the remarkable achievement of recent underwater image restoration techniques, the lack of labeled data has become a major hurdle for further progress. In this work, we propose a mean-teacher based Semi-supervised Underwater Image Restoration (Semi-UIR) framework to incorporate the unlabeled data into network training. However, the naive mean-teacher method suffers from two main problems: (1) The consistency loss used in training might become ineffective when the teacher's prediction is wrong. (2) Using L1 distance may cause the network to overfit wrong labels, resulting in confirmation bias. To address the above problems, we first introduce a reliable bank to store the \"best-ever\" outputs as pseudo ground truth. To assess the quality of outputs, we conduct an empirical analysis based on the monotonicity property to select the most trustworthy NR-IQA method. Besides, in view of the confirmation bias problem, we incorporate contrastive regularization to prevent the overfitting on wrong labels. Experimental results on both full-reference and nonreference underwater benchmarks demonstrate that our algorithm has obvious improvement over SOTA methods quantitatively and qualitatively. Code has been released at https://github.com/Huang-ShiRui/Semi-UIR."}, "cited_paper_content": {"title": "Fast Underwater Image Enhancement For Improved Visual Perception", "abstract": "In this paper, we present a conditional generative adversarial network-based model for underwater image enhancement in real-time. In order to supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, and local style information. In addition, we present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of poor and good quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. Furthermore, we perform a number of qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance the quality of underwater images from both paired and unpaired training. More importantly, the enhanced images provide improved performances for several standard models for underwater object detection and human pose estimation; hence, the proposed model can be used as an image processing pipeline by visually-guided underwater robots in real-time applications."}, "keywords": ["underwater scenes"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our training set contains 1600 labeled image pairs and 1600 unlabeled images. The labeled image pairs are randomly sampled from #OTHEREFR and UIEB #OTHEREFR with a ratio of 1:1. #OTHEREFR provides a synthesized underwater image dataset in indoor scene. The UIEB #OTHEREFR dataset contains 890 real underwater images with corresponding ground truths. To enhance the quality of the underwater images, the authors of this work propose a conditional generative adversarial network-based model for underwater image enhancement in real-time, which is trained on a large-scale dataset of paired and unpaired underwater images, as presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To augment the labeled data, the authors utilized the EUVP dataset, which contains a large-scale collection of paired and unpaired underwater images captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments, and demonstrated that their proposed method outperforms the state-of-the-art techniques on both full-reference and non-reference underwater benchmarks. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate high-quality underwater scenes, the authors of this manuscript utilized a conditional generative adversarial network-based model for underwater image enhancement, which was proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["underwater scenes"]}]}
