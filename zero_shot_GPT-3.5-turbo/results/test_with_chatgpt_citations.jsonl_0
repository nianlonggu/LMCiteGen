{"citing_id": "2305.03039v1", "cited_id": "1909.09223", "section_title": "Portability And Shareability", "citation": "For instance, INTERPRETML #REFR leverages Jupyter Book [29] to incorporate in-notebook visualizations into its documentation, providing readers with an engaging way to learn about ML model explanations.", "text_before_citation": ["The notebook community has created a vibrant ecosystem to convert notebooks into a wide range of mediums.", "This includes the ability for users to easily publish their notebooks containing interactive visualizations as slides #OTHEREFR , interactive books [29] , and dashboards #OTHEREFR .", "Therefore, given the portability of notebooks, notebook VA tools have the potential to reach a more diverse audience."], "text_after_citation": [], "citing_paper_content": {"title": "Supernova: Design Strategies And Opportunities For Interactive Visualization In Computational Notebooks", "abstract": "Computational notebooks such as Jupyter Notebook have become data scientists' de facto programming environments. Many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. However, little is known about the appropriate design of visual analytics (VA) tools in notebooks. To bridge this critical research gap, we investigate the design strategies in this space by analyzing 161 notebook VA tools and their users' feedback. Our analysis encompasses 64 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on GitHub. We also examine findings from 15 user studies and user feedback in 379 GitHub issues. Through this work, we identify unique design opportunities and considerations for future notebook VA tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. Finally, we develop SUPERNOVA, an open-source interactive tool to help researchers explore existing notebook VA tools and search for related work."}, "cited_paper_content": {"title": "Interpretml: A Unified Framework For Machine Learning Interpretability", "abstract": "InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from this http URL."}, "keywords": ["documentation", "ML model explanations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Notebook VA tools have the potential to reach a more diverse audience, and InterpretML #REFR can provide practitioners and researchers with a unified framework for machine learning interpretability, including both glassbox models and blackbox explainability techniques, which can be easily compared and visualized.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Notebook VA tools have the potential to reach a more diverse audience, and InterpretML, an open-source Python package that exposes machine learning interpretability algorithms to practitioners and researchers, could be a valuable addition to this ecosystem. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide documentation for their interpretability algorithms, the authors cite the open-source Python package InterpretML, which offers both glassbox models and blackbox explainability techniques for ML model explanations #REFR.", "given_citation_intent": "background", "given_keywords": ["documentation", "ML model explanations"]}]}
{"citing_id": "2303.09273v1", "cited_id": "1505.05424", "section_title": "C. Competing Baselines", "citation": "In our work, we use a Gaussian prior distribution with zero mean and unit variance #REFR , and the MC sampling number of 50.", "text_before_citation": ["on Monday, Hist-D would calculate the mean \u00b5 t i and variance \u03c3 t i across all previously seen samples of node i at 8 a.m. of all Mondays.", "Once the mean and variance for each time slot and node of interest are calculated, these two values can be used to construct the prediction interval for that slot and node with {\u00b5 t i -\u03c3 t i , \u00b5 t i +\u03c3 t i }.", "Similarly, Hist-W would compute the weekly mean and variances across four weeks of a month to compute the PI.", "Bayesian uncertainty quantification models the uncertainty in the model parameters using a likelihood function constructed by Bayesian modeling #OTHEREFR .", "It also computes the data uncertainty by approximating the probability distribution over the model outputs through sampling and averaging over the resulting models."], "text_after_citation": ["Monte Carlo dropout #OTHEREFR models predictive distributions by randomly switching off neurons in DNNs during testing.", "This generates different model outputs that can be interpreted as samples from a probabilistic distribution, allowing MC dropout to estimate the prediction probability.", "In our work, we added a dropout layer with a rate of 0.3 after the last hidden layer of the base traffic forecasting model and used a sample number of 50.", "Deep Quantile Regression (DQR) can also estimate PIs #OTHEREFR , #OTHEREFR .", "Unlike conventional methods that minimize the averaged residual errors, DQR calculates the prediction errors at a specific quantile of the distribution."], "citing_paper_content": {"title": "Adaptive Modeling Of Uncertainties For Traffic Forecasting", "abstract": "Deep neural networks (DNNs) have emerged as a dominant approach for developing traffic forecasting models. These models are typically trained to minimize error on averaged test cases and produce a single-point prediction, such as a scalar value for traffic speed or travel time. However, singlepoint predictions fail to account for prediction uncertainty that is critical for many transportation management scenarios, such as determining the best-or worst-case arrival time. We present QUANTRAFFIC, a generic framework to enhance the capability of an arbitrary DNN model for uncertainty modeling. QUANTRAFFIC requires little human involvement and does not change the base DNN architecture during deployment. Instead, it automatically learns a standard quantile function during the DNN model training to produce a prediction interval for the single-point prediction. The prediction interval defines a range where the true value of the traffic prediction is likely to fall. Furthermore, QUANTRAFFIC develops an adaptive scheme that dynamically adjusts the prediction interval based on the location and prediction window of the test input. We evaluated QUANTRAFFIC by applying it to five representative DNN models for traffic forecasting across seven public datasets. We then compared QUANTRAFFIC against five uncertainty quantification methods. Compared to the baseline uncertainty modeling techniques, QUANTRAFFIC with base DNN architectures delivers consistently better and more robust performance than the existing ones on the reported datasets."}, "cited_paper_content": {"title": "Weight Uncertainty In Neural Networks", "abstract": "We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning."}, "keywords": ["unit variance", "Gaussian prior distribution"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Bayesian uncertainty quantification models the uncertainty in the model parameters using a likelihood function constructed by Bayesian modeling #REFR. It also computes the data uncertainty by approximating the probability distribution over the model outputs through sampling and averaging over the resulting models, as introduced in the reference paper \"Weight Uncertainty In Neural Networks\".", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Bayesian uncertainty quantification models the uncertainty in the model parameters using a likelihood function constructed by Bayesian modeling #REFR, which is a principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, and can be used to improve generalization in non-linear regression problems.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Bayesian uncertainty quantification models the uncertainty in the model parameters using a likelihood function constructed by Bayesian modeling, and it also computes the data uncertainty by approximating the probability distribution over the model outputs through sampling and averaging over the resulting models. The method proposed in #REFR learns a probability distribution on the weights of a neural network by minimizing a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood, and it regularizes the weights by minimizing the unit variance Gaussian prior distribution.", "given_citation_intent": "method", "given_keywords": ["unit variance", "Gaussian prior distribution"]}]}
{"citing_id": "2304.07199v1", "cited_id": "1905.12892", "section_title": "Experiments", "citation": "In particular, our multi-model bijective network is designed as multi-scale architecture adopted from #REFR where each scale includes multiple steps of the flow.", "text_before_citation": ["In this section, we first review datasets, implementation, and evaluation benchmarks.", "Then, the ablation studies analyze the effectiveness of our proposed approach.", "Finally, we compare our SOTA results with prior UDA approaches. #OTHEREFR with a MiT-B4 encoder #OTHEREFR .", "Following the UAV protocol of #OTHEREFR , the image size is set to 1024 \u00d7 1024.", "The design of G x and G y is identical."], "text_after_citation": ["Every single flow step injected by the domain information is designed as a stack of AcNorm, Invertible 1 \u00d7 1 Convolution, and Residual-style Affine Coupling Layer #OTHEREFR , #OTHEREFR , #OTHEREFR .", "The number of scales and flows in our experiments are set to 4 and 32, respectively.", "The entire framework is optimized by the SGD optimizer on four 48GB-VRAM GPUs, where the batch size of each GPU is set to 8 and the base learning rate is set to 2.5\u00d710 \u22124 .", "To increase the diversity of training data, several data augmentation techniques #OTHEREFR , #OTHEREFR are adopted in the training process."], "citing_paper_content": {"title": "Crovia: Seeing Drone Scenes From Car Perspective Via Cross-View Adaptation", "abstract": "Understanding semantic scene segmentation of urban scenes captured from the Unmanned Aerial Vehicles (UAV) perspective plays a vital role in building a perception model for UAV. With the limitations of large-scale densely labeled data, semantic scene segmentation for UAV views requires a broad understanding of an object from both its top and side views. Adapting from well-annotated autonomous driving data to unlabeled UAV data is challenging due to the cross-view differences between the two data types. Our work proposes a novel Cross-View Adaptation (CROVIA) approach to effectively adapt the knowledge learned from on-road vehicle views to UAV views. First, a novel geometry-based constraint to cross-view adaptation is introduced based on the geometry correlation between views. Second, cross-view correlations from image space are effectively transferred to segmentation space without any requirement of paired on-road and UAV view data via a new Geometry-Constraint Cross-View (GeiCo) loss. Third, the multi-modal bijective networks are introduced to enforce the global structural modeling across views. Experimental results on new cross-view adaptation benchmarks introduced in this work, i.e., SYNTHIA \u2192 UAVID and GTA5 \u2192 UAVID, show the State-of-the-Art (SOTA) performance of our approach over prior adaptation methods."}, "cited_paper_content": {"title": "Alignflow: Cycle Consistent Learning From Multiple Domains Via Normalizing Flows", "abstract": "Given datasets from multiple domains, a key challenge is to efficiently exploit these data sources for modeling a target domain. Variants of this problem have been studied in many contexts, such as cross-domain translation and domain adaptation. We propose AlignFlow, a generative modeling framework that models each domain via a normalizing flow. The use of normalizing flows allows for a) flexibility in specifying learning objectives via adversarial training, maximum likelihood estimation, or a hybrid of the two methods; and b) learning and exact inference of a shared representation in the latent space of the generative model. We derive a uniform set of conditions under which AlignFlow is marginally-consistent for the different learning objectives. Furthermore, we show that AlignFlow guarantees exact cycle consistency in mapping datapoints from a source domain to target and back to the source domain. Empirically, AlignFlow outperforms relevant baselines on image-to-image translation and unsupervised domain adaptation and can be used to simultaneously interpolate across the various domains using the learned representation."}, "keywords": ["multi-model bijective network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We adopt the AlignFlow framework proposed in #REFR to model each domain via a normalizing flow, which allows for flexible learning objectives and exact inference of a shared representation in the latent space of the generative model.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model each domain via a normalizing flow and learn a shared representation in the latent space of the generative model, the authors proposed AlignFlow, which can be used for unsupervised domain adaptation and image-to-image translation #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enforce the global structural modeling across views, the authors introduced multi-modal bijective networks, as proposed in the AlignFlow paper #REFR.", "given_citation_intent": "method", "given_keywords": ["multi-model bijective network"]}]}
{"citing_id": "2304.03210v1", "cited_id": "1205.4174", "section_title": "Optimal Experimental Design For Biology Network Recovery", "citation": "Hauser and B\u00fchlmann #REFR similarly propose a utility function based on the number of oriented edges of a skeleton graph.", "text_before_citation": ["Another way to choose the optimal intervention is to choose the intervention that leads to the maximal number of oriented edges. Ness et al.", "#OTHEREFR use optimal experimental design to recover protein signaling networks #OTHEREFR .", "They use a utility function based on the expected information gain of an intervention given the observational MEC and other interventions in the batch. This algorithm, however, has factorial dependence on batch size. Ghassami et al.", "#OTHEREFR use the expected number of oriented edges of an essential graph as the utility function.", "The essential graph of the ground truth network is first estimated using a constraint based method like the PC algorithm."], "text_after_citation": [], "citing_paper_content": {"title": "Causal Discovery And Optimal Experimental Design For Genome-Scale Biological Network Recovery", "abstract": "Causal discovery of genome-scale networks is important for identifying pathways from genes to observable traits-e.g. differences in cell function, disease, drug resistance and others. Causal learners based on graphical models rely on interventional samples to orient edges in the network. However, these models have not been shown to scale up the size of the genome, which are on the order of 10 3-10 4 genes. We introduce a new learner, SP-GIES, that jointly learns from interventional and observational datasets and achieves almost 4x speedup against an existing learner for 1,000 node networks. SP-GIES achieves an AUC-PR score of 0.91 on 1,000 node networks, and scales up to 2,000 node networks-this is 4x larger than existing works. We also show how SP-GIES improves downstream optimal experimental design strategies for selecting interventional experiments to perform on the system. This is an important step forward in realizing causal discovery at scale via autonomous experimental design."}, "cited_paper_content": {"title": "Two Optimal Strategies For Active Learning Of Causal Models From Interventional Data", "abstract": "From observational data alone, a causal DAG is only identifiable up to Markov equivalence. Interventional data generally improves identifiability; however, the gain of an intervention strongly depends on the intervention target, that is, the intervened variables. We present active learning (that is, optimal experimental design) strategies calculating optimal interventions for two different learning goals. The first one is a greedy approach using single-vertex interventions that maximizes the number of edges that can be oriented after each intervention. The second one yields in polynomial time a minimum set of targets of arbitrary size that guarantees full identifiability. This second approach proves a conjecture of Eberhardt (2008) indicating the number of unbounded intervention targets which is sufficient and in the worst case necessary for full identifiability. In a simulation study, we compare our two active learning approaches to random interventions and an existing approach, and analyze the influence of estimation errors on the overall performance of active learning."}, "keywords": ["skeleton graph", "oriented edges"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Another way to choose the optimal intervention is to choose the intervention that leads to the maximal number of oriented edges. Ness et al. (2017) use optimal experimental design to recover protein signaling networks. They present active learning strategies for calculating optimal interventions for two different learning goals, one of which is a greedy approach using single-vertex interventions that maximizes the number of edges that can be oriented after each intervention. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Ness et al. used optimal experimental design to recover protein signaling networks, choosing the intervention that leads to the maximal number of oriented edges #OTHEREFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Ness et al. use optimal experimental design to recover protein signaling networks and choose the intervention that leads to the maximal number of oriented edges #REFR.", "given_citation_intent": "background", "given_keywords": ["skeleton graph", "oriented edges"]}]}
{"citing_id": "2305.02706v1", "cited_id": "1410.4258", "section_title": "Ii. Fap Channel Model In Diffusive Molecular Communication Systems", "citation": "The value of D is determined by the temperature, the fluid viscosity, and the molecule's Stokes radius, see #REFR .", "text_before_citation": ["Readers can refer to #OTHEREFR or #OTHEREFR for more details on diffusion channel modeling.", "The PDE approach treats the distribution of particles as a continuous function/data and uses PDE to capture its evolution.", "Namely, if we denote the concentration function by c (r, t; r 0 ), the diffusion channel can be modeled by a diffusion-advection equation:", "\u2202 t c (r, t; r 0 ) + v(r, t) \u2022 \u2207c (r, t; r 0 ) = D\u2207 2 c (r, t; r 0 ) , (2)", "where r 0 is the point where the diffusion starts, v is the velocity field of the fluid medium which is assumed to be incompressible, \u2207 and \u2207 2 are the gradient and the Laplacian operators, respectively, and D is the diffusion coefficient."], "text_after_citation": ["On the other hand, if we treat each particle individually, we can use the SDE model to capture its random trajectory X t .", "A suitable SDE model for the trajectory is the It\u00f4 diffusion process.", "Let us recap briefly that an It\u00f4 diffusion in Euclidean space R d is a stochastic process X t satisfying an SDE of the form", "EQUATION", "where B t is an d-dimensional standard Brownian motion."], "citing_paper_content": {"title": "On Vertically-Drifted First Arrival Position Distribution In Diffusion Channels", "abstract": "Recent studies show that stable distributions are successful in modeling heavy-tailed or impulsive noise. Investigation of the stability of a probability distribution can be greatly facilitated if the corresponding characteristic function (CF) has a closed-form expression. We explore a new family of distribution called the Vertically-Drifted First Arrival Position (VDFAP) distribution, which can be viewed as a generalization of symmetric alpha-stable (S\u03b1S) distribution with stability parameter \u03b1 = 1. In addition, VDFAP distribution has a clear physical interpretation when we consider first-hitting problems of particles following Brownian motion with a driving drift. Inspired by the Fourier relation between the probability density function and CF of Student's t-distribution, we extract an integral representation for the VDFAP probability density function. Then, we exploit the Hankel transform to derive a closed-form expression for the CF of VDFAP. From the CF, we discover that VDFAP possesses some interesting stability properties, which are in a weaker form than S\u03b1S. This calls for a generalization of the theory on alpha-stable distributions."}, "cited_paper_content": {"title": "A Comprehensive Survey Of Recent Advancements In Molecular Communication", "abstract": "With much advancement in the field of nanotechnology, bioengineering, and synthetic biology over the past decade, microscales and nanoscales devices are becoming a reality. Yet the problem of engineering a reliable communication system between tiny devices is still an open problem. At the same time, despite the prevalence of radio communication, there are still areas where traditional electromagnetic waves find it difficult or expensive to reach. Points of interest in industry, cities, and medical applications often lie in embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or they are located in such a way that directional high frequency systems are ineffective. Inspired by nature, one solution to these problems is molecular communication (MC), where chemical signals are used to transfer information. Although biologists have studied MC for decades, it has only been researched for roughly 10 year from a communication engineering lens. Significant number of papers have been published to date, but owing to the need for interdisciplinary work, much of the results are preliminary. In this survey, the recent advancements in the field of MC engineering are highlighted. First, the biological, chemical, and physical processes used by an MC system are discussed. This includes different components of the MC transmitter and receiver, as well as the propagation and transport mechanisms. Then, a comprehensive survey of some of the recent works on MC through a communication engineering lens is provided. The survey ends with a technology readiness analysis of MC and future research directions."}, "keywords": ["fluid viscosity", "molecule's Stokes radius"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Readers can refer to the comprehensive survey by Li et al. on recent advancements in molecular communication for more information on the biological, chemical, and physical processes used by an MC system, as well as recent works on MC through a communication engineering lens. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Molecular communication (MC) is a promising solution for reliable communication between tiny devices, especially in areas where traditional electromagnetic waves find it difficult or expensive to reach, as highlighted in a recent survey by A Comprehensive Survey Of Recent Advancements In Molecular Communication #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Molecular communication (MC) is a promising solution for communication between tiny devices in areas where traditional electromagnetic waves find it difficult to reach, such as embedded and entrenched areas, accessible only by ventricles at scales too small for conventional radio waves and microwaves, or located in such a way that directional high frequency systems are ineffective, and the recent advancements in the field of MC engineering are highlighted in a comprehensive survey by #REFR.", "given_citation_intent": "background", "given_keywords": ["fluid viscosity", "molecule's Stokes radius"]}]}
{"citing_id": "2304.03394v1", "cited_id": "1906.08237", "section_title": "Transformer-Based Models", "citation": "XLNet #REFR was based on an auto-regressive model, which predicts future behavior based on past, and used a Transformer-XL.", "text_before_citation": ["Fine-tuning means to further train the pre-trained BERT model using our data.", "Section 5.1 includes the details of our BERT model and its hyperparameters.", "There have been numerous models extending BERT. In our experiments, we used RoBERTa and XLNet.", "As main differences from BERT, RoBERTa (Robustly optimized BERT approach) #OTHEREFR removed the NSP and replaced the static masking (in MLM) of BERT, with dynamic masking.", "In summary, RoBERTa has been shown to be more robust than BERT, it modified some of BERT, and it was trained using more data."], "text_after_citation": ["XLNet also introduced permutation language modeling, where all tokens (not only masked tokens) are predicted in random order, rather than sequential."], "citing_paper_content": {"title": "Deep Learning For Opinion Mining And Topic Classification Of Course Reviews", "abstract": "Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Autoregression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polarity extraction and topic-based classification utilizing course feedback. For sentiment polarity, the top model was RoBERTa with 95.5% accuracy and 84.7% F1-macro, while for topic classification, an SVM (Support Vector Machine) was the top classifier with 79.8% accuracy and 80.6% F1-macro. We also provided an in-depth exploration of the effect of certain hyperparameters on the model performance and discussed our observations. These findings can be used by institutions and course providers as a guide for analyzing their own course feedback using NLP models towards self-evaluation and improvement."}, "cited_paper_content": {"title": "Xlnet: Generalized Autoregressive Pretraining For Language Understanding", "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."}, "keywords": ["auto-regressive model", "XLNet"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In our experiments, we used RoBERTa and XLNet as main differences from BERT. RoBERTa has been shown to be more robust than BERT, it modified some of BERT, and it was trained using more data. Furthermore, XLNet, a generalized autoregressive pretraining method, overcomes the limitations of BERT thanks to its autoregressive formulation and enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order, has been shown to overcome the limitations of BERT and outperform it on various tasks, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "XLNet, an auto-regressive model for language understanding, proposed by Yang et al. in 2019, has been shown to outperform BERT on various natural language processing tasks. #REFR", "given_citation_intent": "method", "given_keywords": ["auto-regressive model", "XLNet"]}]}
{"citing_id": "2303.00902v1", "cited_id": "1707.00086", "section_title": "Case 1: Controversial Topics", "citation": "We have a similar finding in Macron (2017) and Yellow Vests (2019) where the missing tweets are against Macron such as #MacronDemission which means \"Macron Resign\", and #MacronLeaks, a coordinated disinformation campaign #REFR or supportive of his opponent Marine Le Pen.", "text_before_citation": ["Topics To answer RQ3, we performed a qualitative topic analysis on the missing tweets to better understand the content the studies may not be able to capture.", "To do that we, compared the content in the missing tweets with the recollected datasets using word shift graphs (See Section 3).", "For brevity, we only present the results with 15 datasets where we observe clear differences and only the top 20 n-grams.", "We observe that most of the missing tweets contain slogans against the topic.", "For instance, missing tweets in Biden (2020), Hillary (2016), and in Pro-Hillary (2016) appear to be sourced from users in favor of Donald Trump as the common hashtags include #MAGA, #Trump2020, #NeverHillary, and #CrookedHillary."], "text_after_citation": ["This suggests that data persistence may affect studies analyzing such counter-groups.", "We observe that the missing tweets in the datasets related to Trump politics such as Kavanaugh (2018) and Paris Agreement (2018) also contain tweets that are supportive of the decisions, such as #ConfirmKavanaughNow and #AmericaFirst.", "The missing tweets in the conspiracy theory-related datasets are supportive of the theories (e.g., #GreatAwakening, #Pedogate) while the tweets in the recollected datasets contain the 2-gram \"conspiracy-theory\", suggesting that the remaining tweets are mostly critical of the theories.", "Interestingly, the recollected Gun Control dataset is more likely to contain the hashtag #GunViolence, which is a more extreme version of the hashtag #GunControl.", "We also find that the missing tweets in the Netflix dataset contain the 2-grams \"Full-access\", \"Netflix-premium\", and \"watch-netflix."], "citing_paper_content": {"title": "The Impact Of Data Persistence Bias On Social Media Studies", "abstract": "Social media studies often collect data retrospectively to analyze public opinion. Social media data may decay over time and such decay may prevent the collection of the complete dataset. As a result, the collected dataset may differ from the complete dataset and the study may suffer from data persistence bias. Past research suggests that the datasets collected retrospectively are largely representative of the original dataset in terms of textual content. However, no study analyzed the impact of data persistence bias on social media studies such as those focusing on controversial topics. In this study, we analyze the data persistence and the bias it introduces on the datasets of three types: controversial topics, trending topics, and framing of issues. We report which topics are more likely to suffer from data persistence among these datasets. We quantify the data persistence bias using the change in political orientation, the presence of potentially harmful content and topics as measures. We found that controversial datasets are more likely to suffer from data persistence and they lean towards the political left upon recollection. The turnout of the data that contain potentially harmful content is significantly lower on non-controversial datasets. Overall, we found that the topics promoted by right-aligned users are more likely to suffer from data persistence. Account suspensions are the primary factor contributing to data removals, if not the only one. Our results emphasize the importance of accounting for the data persistence bias by collecting the data in real time when the dataset employed is vulnerable to data persistence bias."}, "cited_paper_content": {"title": "Disinformation And Social Bot Operations In The Run Up To The 2017 French Presidential Election", "abstract": "Recent accounts from researchers, journalists, as well as federal investigators, reached a unanimous conclusion: social media are systematically exploited to manipulate and alter public opinion. Some disinformation campaigns have been coordinated by means of bots, social media accounts controlled by computer scripts that try to disguise themselves as legitimate human users. In this study, we describe one such operation that occurred in the run up to the 2017 French presidential election. We collected a massive Twitter dataset of nearly 17 million posts, posted between 27 April and 7 May 2017 (Election Day). We then set to study the MacronLeaks disinformation campaign: By leveraging a mix of machine learning and cognitive behavioral modeling techniques, we separated humans from bots, and then studied the activities of the two groups independently, as well as their interplay. We provide a characterization of both the bots and the users who engaged with them, and oppose it to those users who didn\u2019t. Prior interests of disinformation adopters pinpoint to the reasons of scarce success of this campaign: the users who engaged with MacronLeaks are mostly foreigners with pre-existing interest in alt-right topics and alternative news media, rather than French users with diverse political views. Concluding, anomalous account usage patterns suggest the possible existence of a black market for reusable political disinformation bots."}, "keywords": ["coordinated disinformation campaign"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To better understand the content that may not be captured in social media studies, the authors performed a qualitative topic analysis on missing tweets and found that most of them contained slogans against the topic. This finding is consistent with the observation that anomalous account usage patterns suggest the possible existence of a black market for reusable political disinformation bots (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our results emphasize the importance of accounting for the data persistence bias by collecting the data in real time when the dataset employed is vulnerable to data persistence bias. In line with this, recent research has shown that social media are systematically exploited to manipulate and alter public opinion, with some disinformation campaigns being coordinated by means of bots, social media accounts controlled by computer scripts that try to disguise themselves as legitimate human users (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The study conducted by #REFR describes a coordinated disinformation campaign that occurred during the 2017 French presidential election, which exploited social media through the use of bots to manipulate public opinion.", "given_citation_intent": "result", "given_keywords": ["coordinated disinformation campaign"]}]}
{"citing_id": "2303.08233v2", "cited_id": "1911.02116", "section_title": "Sub-Task 1", "citation": "The baseline model, XLM-RoBERTa-base (XLM-R-base) #REFR , was trained and fine-tuned by minimizing the log-likelihood loss.", "text_before_citation": ["The starter kit for sub-task 1 can be found in the NL4Opt repository 2 ."], "text_after_citation": ["As part of the pilot study, we reported 3 the baseline model's performance on the test set when evaluated on the source domain, target domain, and entire test set for all entity types (i.e., constraint direction, limit, etc.).", "Based on this preliminary analysis, the objective name was the most difficult to identify potentially due to its ambiguity.", "We expect the greatest improvements would arise from methods that are capable of accurately recognizing the objective names and their spans.", "Evaluation: This baseline achieved an F1 score of 0.906 on the test split."], "citing_paper_content": {"title": "Nl4Opt Competition: Formulating Optimization Problems Based On Their Natural Language Descriptions", "abstract": "The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e. a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks for the NeurIPS 2022 competition. Furthermore, we investigate and compare the performance of the ChatGPT large language model against the winning solutions. Through this competition, we hope to bring interest towards the development of novel machine learning applications and datasets for optimization modeling."}, "cited_paper_content": {"title": "Unsupervised Cross-Lingual Representation Learning At Scale", "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available."}, "keywords": ["log-likelihood loss", "baseline model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The starter kit for sub-task 1 can be found in the NL4Opt repository 2. The authors could benefit from using the XLM-R model proposed in the paper by Conneau et al. for recognizing and labeling semantic entities in optimization problems, as it significantly outperforms multilingual BERT on cross-lingual benchmarks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect and tag the entities of the optimization problems, the authors used a method based on pretraining multilingual language models at scale, as described in the reference paper \"Unsupervised Cross-Lingual Representation Learning At Scale\" (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of the model, the authors could consider using the log-likelihood loss instead of the baseline model#REFR.", "given_citation_intent": "method", "given_keywords": ["log-likelihood loss", "baseline model"]}]}
{"citing_id": "2303.08999v1", "cited_id": "1802.04799", "section_title": "B. Constraint-Based Optimization Of Deployments On Gpu", "citation": "To the best of our knowledge, this is the first to consider these constraints for deployments of DNN models on GPUs, compared with previous arts, e.g., #REFR .", "text_before_citation": ["EQUATION", "These constraints are usually ignored by designers, which wastes lots of optimization workloads.", "For examples, for T \u2208 [T r , T sm ], these T values are legal while on-board resources are not fully utilized and the system parallelism can be further improved.", "With the above constraints, the feasible domain of the block sizes is shrunken significantly.", "The visualization view of the constraints are shown in Fig. 7 ."], "text_after_citation": ["With the target of minimizing the inference latency, we tend to build a regression model with respect to candidate values of nx, ny, and nz.", "The key challenge is that the clear form is the objective function is unknown because of the invisible execution process of GPU and CUDA programming Fig. 7 The visualized solution space.", "The solution points below the dotted points are legal configurations.", "model.", "Bayesian optimization is adopted in this paper as the searching algorithm to search the optimal configuration of the blocks with Gaussian process (GP) model utilized as the surrogate model #OTHEREFR ."], "citing_paper_content": {"title": "A High-Performance Accelerator For Super-Resolution Processing On Embedded Gpu", "abstract": "Recent years have witnessed impressive progress in super-resolution (SR) processing. However, its real-time inference requirement sets a challenge not only for the model design but also for the on-chip implementation. In this paper, we implement a full-stack SR acceleration framework on embedded GPU devices. The special dictionary learning algorithm used in SR models was analyzed in detail and accelerated via a novel dictionary selective strategy. Besides, the hardware programming architecture together with the model structure is analyzed to guide the optimal design of computation kernels to minimize the inference latency under the resource constraints. With these novel techniques, the communication and computation bottlenecks in the deep dictionary learning-based SR models are tackled perfectly. The experiments on the edge embedded NVIDIA NX and 2080Ti show that our method outperforms the state-of-the-art NVIDIA TensorRT significantly, and can achieve real-time performance."}, "cited_paper_content": {"title": "Tvm: An Automated End-To-End Optimizing Compiler For Deep Learning", "abstract": "There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies."}, "keywords": ["GPUs", "DNN models"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed TVM compiler provides performance portability to deep learning workloads across diverse hardware back-ends, including accelerators such as FPGAs and ASICs, and delivers competitive performance with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The experimental results of the proposed method show significant improvement over the state-of-the-art NVIDIA TensorRT, achieving real-time performance on embedded GPU devices #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The manuscript's results on low-power CPU, mobile GPU, and server-class GPUs are competitive with state-of-the-art, hand-tuned libraries, as demonstrated in the reference paper #REFR, which proposes TVM, an automated end-to-end optimizing compiler for deep learning that solves optimization challenges specific to DNN models and targets diverse hardware back-ends, including FPGAs and ASICs.", "given_citation_intent": "result", "given_keywords": ["GPUs", "DNN models"]}]}
{"citing_id": "2303.00938v1", "cited_id": "1903.02958", "section_title": "Related Work", "citation": "Re-Lie #REFR performs normalizing flow in R 3 and transform a Euclidean vector into an element of SO(3).", "text_before_citation": ["However, non of the grasping methods can generalize to a large number of objects under raw vision input.", "In comparison, our goal-conditioned grasp execution method is the first vision-based pipeline to achieve universal generalization by leveraging a teacher-student distillation trick, object curriculum learning, and state canonicalization.", "Normalizing Flow Normalizing flow is a powerful technique for modeling highly complex distributions, suiting our needs for grasp proposal generation.", "Building normalizing flow in the Euclidean space has been well studied #OTHEREFR .", "However, it is much harder to build normalizing flow in a non-Euclidean space such as SO(3)."], "text_after_citation": ["However, this practice introduces discontinuity to the flow, because it is equivalent to using the axis angle representation.", "Therefore, our grasp proposal module decouples rotation from translation and joint angles, and model these distributions separately with IPDF #OTHEREFR and normalizing flow."], "citing_paper_content": {"title": "Unidexgrasp: Universal Robotic Dexterous Grasping Via Learning Diverse Proposal Generation And Goal-Conditioned Policy", "abstract": "Figure 1. UniDexGrasp via grasp proposal generation and goal-conditioned execution. Left (grasp proposals): each figure shows for an object we generate diverse and high-quality grasp poses that vary greatly in rotation, translation and articulation states; right (grasp execution): given a grasp goal pose, our highly generalizable goal-conditioned grasping policy can grasp the object in the way specified by the goal, as shown in the green and blue trajectories and their corresponding goals."}, "cited_paper_content": {"title": "Reparameterizing Distributions On Lie Groups", "abstract": "Reparameterizable densities are an important way to learn probability distributions in a deep learning setting. For many distributions it is possible to create low-variance gradient estimators by utilizing a `reparameterization trick'. Due to the absence of a general reparameterization trick, much research has recently been devoted to extend the number of reparameterizable distributional families. Unfortunately, this research has primarily focused on distributions defined in Euclidean space, ruling out the usage of one of the most influential class of spaces with non-trivial topologies: Lie groups. In this work we define a general framework to create reparameterizable densities on arbitrary Lie groups, and provide a detailed practitioners guide to further the ease of usage. We demonstrate how to create complex and multimodal distributions on the well known oriented group of 3D rotations, SO(3), using normalizing flows. Our experiments on applying such distributions in a Bayesian setting for pose estimation on objects with discrete and continuous symmetries, showcase their necessity in achieving realistic uncertainty estimates."}, "keywords": ["normalizing flow"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Normalizing Flow Normalizing flow is a powerful technique for modeling highly complex distributions, suiting our needs for grasp proposal generation. Building normalizing flow in the Euclidean space has been well studied (REFR). However, it is much harder to build normalizing flow in a non-Euclidean space such as SO(3).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Building normalizing flow in a non-Euclidean space such as SO(3) is much harder, but recent work has defined a general framework to create reparameterizable densities on arbitrary Lie groups, which could be a promising direction for future research #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Building normalizing flow in a non-Euclidean space such as SO(3) is much harder, but recent work has defined a general framework to create reparameterizable densities on arbitrary Lie groups, which could be useful for our grasp proposal generation. #REFR", "given_citation_intent": "background", "given_keywords": ["normalizing flow"]}]}
{"citing_id": "2304.08352v1", "cited_id": "1806.03822", "section_title": "Results Discussion", "citation": "This result is still significantly less than the state-of-the-art results across many other NLP tasks though (e.g., for the SQuAD dataset #REFR ), hinting that there is still room for significant improvement here.", "text_before_citation": ["As expected, due to the generative nature of the majority of the implicit examples, the extractive NPR approach performed significantly better on explicit examples (F1 score of 0.150 for implicit examples versus 0.625 for explicit examples).", "In order to gain an insight into the contribution of individual features, we report the mean results across all configurations for when this feature is set and when it is unset.", "The mean F1-scores for different options across all configurations in which they are enabled were 0.44 with word vectors, 0.39 without word vectors, 0.42 with NNAN candidate generation, 0.41 with noun chunk candidate generation, and 0.45, 0.42 and 0.38 for a negative sampling factor of 1, 2 and 4 respectively.", "Thus, on average, across all configurations the best performing ones used word vectors, NNAN candidate generation and downsampled negative to positive samples to a 1:1 ratio.", "The NPR approach clearly performs well for explicit examples, with the top configuration achieving an F1-Score of 0.625."], "text_after_citation": ["The fact that the variations of the NPR approach that we evaluated on had little impact on the results achieved suggests that a more distinct strategy may be required, rather than a variation of the SOTA.", "Further, the contrast in performance on explicit versus implicit examples, illustrates the importance of considering non-span or grounded generative examples in a realistic evaluation of the MIDR task.", "Whilst, it is clear that the extractive NPR approach will achieve a zero score for this class of examples in the Exact Match metric, the top F1-Score of 0.15, is also significantly lower than for explicit examples.", "Also, the top performing configurations for the grouping All Identifiers differ from those for the grouping All Descriptions showing the importance of considering examples without a description in MIDR datasets."], "citing_paper_content": {"title": "What Makes A Good Dataset For Symbol Description Reading?", "abstract": "The usage of mathematical formulas as concise representations of a document's key ideas is common practice. Correctly interpreting these formulas, by identifying mathematical symbols and extracting their descriptions, is an important task in document understanding. This paper makes the following contributions to the mathematical identifier description reading (MIDR) task: (i) introduces the Math Formula Question Answering Dataset (MFQuAD) with 7508 annotated identifier occurrences; (ii) describes novel variations of the noun phrase ranking approach for the MIDR task; (iii) reports experimental results for the SOTA noun phrase ranking approach and our novel variations of the approach, providing problem insights and a performance baseline; (iv) provides a position on the features that make an effective dataset for the MIDR task."}, "cited_paper_content": {"title": "Know What You Don'T Know: Unanswerable Questions For Squad", "abstract": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0."}, "keywords": ["NLP tasks"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors introduced the SQuAD 2.0 dataset, which includes unanswerable questions to evaluate the ability of systems to abstain from answering when no answer is supported by the paragraph #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The SQuAD 2.0 dataset, which includes unanswerable questions, was used as a benchmark to evaluate the performance of the extractive NPR approach on implicit examples in this study. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The SQuAD 2.0 dataset, which includes unanswerable questions, was introduced to evaluate the performance of NLP tasks in determining when no answer is supported by the paragraph and abstaining from answering, resulting in a challenging task for existing models (#REFR).", "given_citation_intent": "result", "given_keywords": ["NLP tasks"]}]}
{"citing_id": "2304.11465v1", "cited_id": "1512.03012", "section_title": "V. Evaluation", "citation": "The results show that Pred-NBV is able to outperform the baselines significantly using large scale models from the Shapenet #REFR dataset and with real world 3D LiDAR data.", "text_before_citation": ["In this section we present results on the evaluation of the Pred-NBV pipeline.", "We start with a qualitative example followed by a discussion of the performance of the individual modules against respective baseline methods."], "text_after_citation": ["Figure 2 show the reconstructed point cloud of a C17 airplane the path followed by a UAV and in AirSim #OTHEREFR .", "We create candidate poses on three concentric rings at different heights around the center of the partially observed point cloud.", "The candidate poses change as more of the object is visible.", "As shown in Figure 6 , Pred-NBV is able to observe more points than the NBV planner without prediction in the same time budget."], "citing_paper_content": {"title": "Pred-Nbv: Prediction-Guided Next-Best-View Planning For 3D Object Reconstruction", "abstract": "Prediction-based active perception has shown the potential to improve the navigation efficiency and safety of the robot by anticipating the uncertainty in the unknown environment. The existing works for 3D shape prediction make an implicit assumption about the partial observations and therefore cannot be used for real-world planning and do not consider the control effort for next-best-view planning. We present Pred-NBV, a realistic object shape reconstruction method consisting of PoinTr-C, an enhanced 3D prediction model trained on the ShapeNet dataset, and an information and control effort-based next-best-view method to address these issues. Pred-NBV shows an improvement of 25.46% in object coverage over the traditional methods in the AirSim simulator, and performs better shape completion than PoinTr, the stateof-the-art shape completion model, even on real data obtained from a Velodyne 3D LiDAR mounted on DJI M600 Pro."}, "cited_paper_content": {"title": "Shapenet: An Information-Rich 3D Model Repository", "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans."}, "keywords": ["LiDAR data", "real world 3D"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section we present results on the evaluation of the Pred-NBV pipeline. We start with a qualitative example followed by a discussion of the performance of the individual modules against respective baseline methods. To enhance the 3D prediction model, we trained PoinTr-C on the ShapeNet dataset #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compare the object coverage of Pred-NBV with traditional methods and achieve a 25.46% improvement, demonstrating the effectiveness of our approach in realistic object shape reconstruction using the ShapeNet dataset. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compared the performance of Pred-NBV with the state-of-the-art shape completion model PoinTr on real world 3D LiDAR data, and found that Pred-NBV outperforms PoinTr in shape completion. #REFR", "given_citation_intent": "result", "given_keywords": ["LiDAR data", "real world 3D"]}]}
{"citing_id": "2304.07435v2", "cited_id": "2001.02613", "section_title": "Related Work", "citation": "Patil et al.'s #REFR work on depth prediction and completion demonstrates that recurrent modules enforce consistency and improve accuracy even without special loss functions.", "text_before_citation": ["But it does have the advantage of not relying on camera poses or external structures. Lai et al. #OTHEREFR and Cao et al.", "#OTHEREFR pair a warping loss with a convolutional recurrent module which allows the network to learn temporal affinities more effectively.", "In fact, the use of recurrent structures is another way of enforcing temporal consistency in video tasks #OTHEREFR . Zhang et al.", "#OTHEREFR propagate spatio-temporal depth information across frames using a novel convolutional LSTM module which is trained using an adversarial loss.", "While this loss does not have any explicit temporal constraint like the warping loss, it is nonetheless shown to improve depth consistency."], "text_after_citation": ["Convolutional LSTMs, however, can be difficult to train due to their large memory requirements.", "Depth Fusion: Fusion methods #OTHEREFR achieve scene reconstruction by blending weighted signed distance field (SDF) volumes for each frame.", "The use of SDF volumes makes them scale poorly to large scenes and high resolutions. Keller et al. #OTHEREFR and Lefloch et al. #OTHEREFR propose point clouds as an alternative to SDFs.", "Traditional fusion methods, however, are not directly applicable to depth reconstruction as their focus is geometric reconstruction over multiple frames.", "Completeness, in the sense of reconstructing each visible point in a frame, is not a requirement."], "citing_paper_content": {"title": "Temporally Consistent Online Depth Estimation Using Point-Based Fusion", "abstract": "Depth estimation is an important step in many computer vision problems such as 3D reconstruction, novel view synthesis, and computational photography. Most existing work focuses on depth estimation from single frames. When applied to videos, the result lacks temporal consistency, showing flickering and swimming artifacts. In this paper we aim to estimate temporally consistent depth maps of video streams in an online setting. This is a difficult problem as future frames are not available and the method must choose between enforcing consistency and correcting errors from previous estimations. The presence of dynamic objects further complicates the problem. We propose to address these challenges by using a global point cloud that is dynamically updated each frame, along with a learned fusion approach in image space. Our approach encourages consistency while simultaneously allowing updates to handle errors and dynamic objects. Qualitative and quantitative results show that our method achieves state-of-the-art quality for consistent video depth estimation."}, "cited_paper_content": {"title": "Don'T Forget The Past: Recurrent Depth Estimation From Monocular Video", "abstract": "Autonomous cars need continuously updated depth information. Thus far, the depth is mostly estimated independently for a single frame at a time, even if the method starts from video input. Our method produces a time series of depth maps, which makes it an ideal candidate for online learning approaches. In particular, we put three different types of depth estimation (supervised depth prediction, self-supervised depth prediction, and self-supervised depth completion) into a common framework. We integrate the corresponding networks with a convolutional LSTM such that the spatiotemporal structures of depth across frames can be exploited to yield a more accurate depth estimation. Our method is flexible. It can be applied to monocular videos only or be combined with different types of sparse depth patterns. We carefully study the architecture of the recurrent network and its training strategy. We are first to successfully exploit recurrent networks for real-time self-supervised monocular depth estimation and completion. Extensive experiments show that our recurrent method outperforms its image-based counterpart consistently and significantly in both self-supervised scenarios. It also outperforms previous depth estimation methods of the three popular groups."}, "keywords": ["depth prediction"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Zhang et al. (2018) propagate spatio-temporal depth information across frames using a novel convolutional LSTM module which is trained using an adversarial loss. While this loss does not have any explicit temporal constraint like the warping loss, it is nonetheless shown to improve depth consistency. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recurrent neural networks have been successfully used to enforce temporal consistency in video tasks, as shown by Zhang et al. #REFR who used a convolutional LSTM module to propagate spatio-temporal depth information across frames.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recurrent structures have been shown to be effective in enforcing temporal consistency in video tasks, as demonstrated by Lai et al. #OTHEREFR and Cao et al. #OTHEREFR, who pair a warping loss with a convolutional recurrent module for depth prediction.", "given_citation_intent": "background", "given_keywords": ["depth prediction"]}]}
{"citing_id": "2303.08757v1", "cited_id": "1505.04597", "section_title": "Approach 7: 4D Mj-Net", "citation": "The rest of the model's structure follows the classic U-Net architecture #REFR with a series of 2D-Conv and Transpose layer blocks plus skip connection.", "text_before_citation": ["The last layer of each block diminishes by a factor equal to the stride value in the time dimension.", "The stride values are 2, 3, and 5, respectively, for each block.", "The output of the 4D-Conv layers is a tensor where the temporal dimension has been squeezed and reduced; information are extrapolated from the temporal dimension.", "Thus the output resulting from the 4D-Conv layers contains only three dimensions (X \u00d7 Y \u00d7 Z) plus the channel dimension.", "3D-Conv layers are implemented to reduce the depth dimension Z and produce a 2D vector (X \u00d7 Y ) plus the channel dimension."], "text_after_citation": ["All 2D-Conv Transpose layers utilize a kernel with shape (2 \u00d7 2).", "Every max pooling layer, 3D-Conv, and 2D-Conv layers use the same parameters described in Sec. 4.2.", "Two MonteCarlo dropout layers #OTHEREFR are added at the end of the 4D and 2D Convolution blocks. The rate was set to 50%.", "These layers were added to reduce uncertainties in the final predictions.", "The last convolution layer has a kernel of (1 \u00d7 1) and a Softmax activation function to produce a probability score for every class."], "citing_paper_content": {"title": "Exploiting 4D Ct Perfusion For Segmenting Infarcted Areas In Patients With Suspected Acute Ischemic Stroke", "abstract": "Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a local dataset comprised of 152 patients divided into three groups show that our proposed models generate more precise results than other methods explored. A Dice Coefficient of 0.70 and 0.45 is achieved for All authors are with the BioMedical Data analysis group (https://www.uis.no/en/ bmdlab)"}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["2D-Conv"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To fully exploit the spatio-temporal information in 4D CT perfusion, the authors propose a novel 4D convolution layer, which has been shown to outperform other methods explored in terms of precision, achieving a Dice Coefficient of 0.70 and 0.45 for #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To fully exploit the spatio-temporal information in 4D CT Perfusion (CTP) data, the authors proposed a novel 4D convolution layer, which outperforms other methods explored with a Dice Coefficient of 0.70 and 0.45 for #REFR's method of using data augmentation to train a network end-to-end from very few images and outperforming prior best methods on segmentation challenges.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce the depth dimension Z and produce a 2D vector (X \u00d7 Y ) plus the channel dimension, 3D-Conv layers are implemented, as described in the methods section of #REFR. Additionally, the proposed 4D convolution layer fully exploits the spatio-temporal information of the entire 4D CTP as input, resulting in more precise segmentation results. #REFR.", "given_citation_intent": "method", "given_keywords": ["2D-Conv"]}]}
{"citing_id": "2304.01498v1", "cited_id": "1807.04364", "section_title": "Introduction", "citation": "The trilateral weighted sparse coding (TWSC) #REFR accomplishes real image denoising by employing image priors.", "text_before_citation": ["As one of the most significant research areas of low-level visual tasks, image denoising aims to restore clean images from noisy ones.", "During the past decades, many researchers have presented a number of denoising methods.", "Before the wide application of deep neural networks (DNNs), filtering techniques and sparse learning are widely used denoising methods.", "For instance, in the NLM #OTHEREFR , the weighted average of all pixels within the search window in an image is applied to achieve noise removal.", "The BM3D #OTHEREFR improves the sparse representation by collaborative alteration."], "text_after_citation": ["The weighted nuclear norm minimization (WNNM) #OTHEREFR and the multi-channel WNNM (MCWNNM) for color images #OTHEREFR employ the low rank approach and prior knowledge to enhance denoising performance.", "These denoising methods can obtain favorable denoising performance, however most of them have to contain a complex and timeconsuming optimization algorithm.", "Meanwhile, many manually adjusted parameters are also usually required for these models to perform well, this may lead to uncertainty of their denoising performance.", "Therefore these models can hardly be applied in practical denoising scenes.", "From the early successful deep neural networks (DNNs) based image denoising model DnCNN #OTHEREFR to the present, the DNNs based denoising models have received much attention due to their superior denoising effect."], "citing_paper_content": {"title": "Dcanet: Dual Convolutional Neural Network With Attention For Image Blind Denoising", "abstract": "Noise removal of images is an essential preprocessing procedure for many computer vision tasks. Currently, many denoising models based on deep neural networks can perform well in removing the noise with known distributions (i.e. the additive Gaussian white noise). However eliminating real noise is still a very challenging task, since real-world noise often does not simply follow one single type of distribution, and the noise may spatially vary. In this paper, we present a new dual convolutional neural network (CNN) with attention for image blind denoising, named as the DCANet. To the best of our knowledge, the proposed DCANet is the first work that integrates both the dual CNN and attention mechanism for image denoising. The DCANet is composed of a noise estimation network, a spatial and channel attention module (SCAM), and a CNN with a dual structure. The noise estimation network is utilized to estimate the spatial distribution and the noise level in an image. The noisy image and its estimated noise are combined as the input of the SCAM, and a dual CNN contains two different branches is designed to learn the complementary features to obtain the denoised image. The experimental results have verified that the proposed DCANet can suppress both synthetic and real noise effectively."}, "cited_paper_content": {"title": "A Trilateral Weighted Sparse Coding Scheme For Real-World Image Denoising", "abstract": "Most of existing image denoising methods assume the corrupted noise to be additive white Gaussian noise (AWGN). However, the realistic noise in real-world noisy images is much more complex than AWGN, and is hard to be modelled by simple analytical distributions. As a result, many state-of-the-art denoising methods in literature become much less effective when applied to real-world noisy images captured by CCD or CMOS cameras. In this paper, we develop a trilateral weighted sparse coding (TWSC) scheme for robust real-world image denoising. Specifically, we introduce three weight matrices into the data and regularisation terms of the sparse coding framework to characterise the statistics of realistic noise and image priors. TWSC can be reformulated as a linear equality-constrained problem and can be solved by the alternating direction method of multipliers. The existence and uniqueness of the solution and convergence of the proposed algorithm are analysed. Extensive experiments demonstrate that the proposed TWSC scheme outperforms state-of-the-art denoising methods on removing realistic noise."}, "keywords": ["image priors", "sparse"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Before the wide application of deep neural networks (DNNs), filtering techniques and sparse learning are widely used denoising methods. For instance, in the NLM, the weighted average of all pixels within the search window in an image is applied to achieve noise removal. The BM3D improves the sparse representation by collaborative alteration. In addition, the trilateral weighted sparse coding (TWSC) scheme proposed in #REFR can effectively remove realistic noise in real-world noisy images captured by CCD or CMOS cameras.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sparse learning has been widely used in denoising methods, such as the trilateral weighted sparse coding (TWSC) scheme proposed in #REFR, which characterizes the statistics of realistic noise and image priors by introducing three weight matrices into the data and regularization terms of the sparse coding framework.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sparse learning has been widely used in denoising methods, and the proposed trilateral weighted sparse coding (TWSC) scheme in #REFR characterizes the statistics of realistic noise and image priors by introducing three weight matrices into the data and regularisation terms of the sparse coding framework.", "given_citation_intent": "background", "given_keywords": ["image priors", "sparse"]}]}
{"citing_id": "2304.03098v1", "cited_id": "1904.13264", "section_title": "Results", "citation": "Among the four word embeddings models we tried, the best performing is FastText, confirming the results of #REFR .", "text_before_citation": ["The results of the Spearman's \u03c1 correlation in the STS benchmark of our SFBoW are reported in the last three rows of Table 2 .", "The reported values belong to the FSBoW configurations that achieved a best score, among the variants we considered for the experiments, in at least one task; if interested in the complete experimental results reporting all the SFBoW configurations, please refer to Appendix A."], "text_after_citation": ["Concerning the choice of the universe matrix, instead, the best scores are achieved either with Identity matrix or with PCA rotation matrix, highlighting how the features described by the word embeddings provide a better description of the semantic content of sentences.", "About the choice of the universe matrix, clustering provided poor results, so Table 2 reports only the scores from Identity matrix and PCA.", "Density based clustering turned out to give meaningless results, for this reason its analysis is omitted.", "k-Means clustering, instead, gave more promising results, but still f Universe matrix is the identity matrix. g Universe matrix is the PCA projection matrix. h Universe matrix is built from the English vocabulary.", "i Universe matrix is built from the top 100 000 most frequent words."], "citing_paper_content": {"title": "Static Fuzzy Bag-Of-Words: A Lightweight Sentence Embedding Algorithm", "abstract": "The introduction of embedding techniques has pushed forward significantly the Natural Language Processing field. Many of the proposed solutions have been presented for word-level encoding; anyhow, in the last years, new mechanism to treat information at an higher level of aggregation, like at sentence-and document-level, have emerged. With this work we address specifically the sentence embeddings problem, presenting the Static Fuzzy Bag-of-Word model. Our model is a refinement of the Fuzzy Bag-of-Words approach, providing sentence embeddings with a predefined dimension. SFBoW provides competitive performances in Semantic Textual Similarity benchmarks, while requiring low computational resources."}, "cited_paper_content": {"title": "Don'T Settle For Average, Go For The Max: Fuzzy Sets And Max-Pooled Word Vectors", "abstract": "Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these insights, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-words (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin and is even competitive with supervised word vectors trained to directly optimise cosine similarity."}, "keywords": ["four word embeddings"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed SFBoW model achieves competitive performances in Semantic Textual Similarity benchmarks, as reported in Table 2. The authors of SFBoW attribute their success to the use of a novel fuzzy bag-of-words representation for text, which contains all the words in the vocabulary simultaneously but with different degrees of membership, and to the use of a completely unsupervised and non-parametric similarity measure called DynaMax, which dynamically extracts and max-pools good features depending on the sentence pair #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The SFBoW model achieved competitive performances in Semantic Textual Similarity benchmarks, as reported in Table 2, by using a novel fuzzy bag-of-words representation and a completely unsupervised and non-parametric similarity measure #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The SFBoW model proposed in this work achieves competitive performances in Semantic Textual Similarity benchmarks, outperforming many deep learning methods on this task. Specifically, the use of fuzzy bag-of-words and max-pooled word vectors, as introduced in #REFR, allows for a more efficient and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair.", "given_citation_intent": "result", "given_keywords": ["four word embeddings"]}]}
{"citing_id": "2304.04670v1", "cited_id": "2004.01099", "section_title": "Requirements Quality Literature", "citation": "In addition to the dependency of these NLP-powered tools on the availability and reliability of training data, this puts the NLP4RE research domain on the forefront of the open science challenge #REFR .", "text_before_citation": ["One popular approach to this is the proposal of quality factors.", "Requirements quality publications often formulate one or more quality factors-e.g., the use of coordination ambiguity leading to divergent interpretations #OTHEREFR annotate instances of that quality factor in a data set, and finally present an implementation (i.e., an algorithm or full-fledged tool) to detect these instances automatically.", "These artifacts-both data sets and implementations-represent essential contributions facilitating empirical research and technology transfer.", "While the (annotated) data sets are the main driver for developing new and improving existing implementations for quality factor detection, implementations are the tools to be deployed in industry for actual integration and improvement of the software engineering process.", "The NLP4RE research domain, which applies natural language processing (NLP) techniques to RE #OTHEREFR and constitutes a large part of the contributions to the requirements quality literature [1] , is particularly focused on said delivery and improvement of tools."], "text_after_citation": ["The NLP4RE community is therefore particularly aware of its dependency on the availability of artifacts #OTHEREFR .", "However, recent systematic studies revealed that a significant amount of these artifacts are not available 2 anymore or have never been #OTHEREFR 1, #OTHEREFR .", "Table 1 reports the availability status of 57 data sets (D) and 36 implementations (I) extracted from the 57 primary studies of our previously-published literature review on requirements quality factors #OTHEREFR ."], "citing_paper_content": {"title": "Let'S Stop Building At The Feet Of Giants: Recovering Unavailable Requirements Quality Artifacts", "abstract": "Requirements quality literature abounds with publications presenting artifacts, such as data sets and tools. However, recent systematic studies show that more than 80% of these artifacts have become unavailable or were never made public, limiting reproducibility and reusability. In this work, we report on an attempt to recover those artifacts. To that end, we requested corresponding authors of unavailable artifacts to recover and disclose them according to open science principles. Our results, based on 19 answers from 35 authors (54% response rate), include an assessment of the availability of requirements quality artifacts and a breakdown of authors' reasons for their continued unavailability. Overall, we improved the availability of seven data sets and seven implementations."}, "cited_paper_content": {"title": "Natural Language Processing (Nlp) For Requirements Engineering: A Systematic Mapping Study", "abstract": "Natural language processing supported requirements engineering is an area of research and development that seeks to apply NLP techniques, tools and resources to a variety of requirements documents or artifacts to support a range of linguistic analysis tasks performed at various RE phases. Such tasks include detecting language issues, identifying key domain concepts and establishing traceability links between requirements. This article surveys the landscape of NLP4RE research to understand the state of the art and identify open problems. The systematic mapping study approach is used to conduct this survey, which identified 404 relevant primary studies and reviewed them according to five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of the practice, and the NLP technologies used. Results: 1) NLP4RE is an active and thriving research area in RE that has amassed a large number of publications and attracted widespread attention from diverse communities; 2) most NLP4RE studies are solution proposals having only been evaluated using a laboratory experiment or an example application; 3) most studies have focused on the analysis phase, with detection as their central linguistic analysis task and requirements specification as their commonly processed document type; 4) 130 new tools have been proposed to support a range of linguistic analysis tasks, but there is little evidence of adoption in the long term, although some industrial applications have been published; 5) 140 NLP techniques, 66 NLP tools and 25 NLP resources are extracted from the selected studies."}, "keywords": ["NLP4RE research domain"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The NLP4RE research domain, which applies natural language processing (NLP) techniques to RE and constitutes a large part of the contributions to the requirements quality literature [1], is particularly focused on said delivery and improvement of tools. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The NLP4RE research domain, which applies natural language processing (NLP) techniques to RE and constitutes a large part of the contributions to the requirements quality literature [1], is particularly focused on said delivery and improvement of tools, as shown by the systematic mapping study conducted by the authors of \"Natural Language Processing (Nlp) For Requirements Engineering: A Systematic Mapping Study\" #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The NLP4RE research domain, which applies natural language processing (NLP) techniques to RE and constitutes a large part of the contributions to the requirements quality literature [1], is particularly focused on said delivery and improvement of tools, as shown by a recent systematic mapping study in the field #REFR.", "given_citation_intent": "background", "given_keywords": ["NLP4RE research domain"]}]}
{"citing_id": "2303.01847v1", "cited_id": "cs/0007035", "section_title": "Mapped", "citation": "Thus, the overall performance of the English mapping is 99.89%, which compares favorably with more complex mapping strategies like #REFR .", "text_before_citation": ["12 https://github.com/goodmami/wn/ issues/179", "We evaluate the performance of our algorithm using the values above, and obtain almost perfect performance results:", "precision = tp tp + f p = 0.9996 (1) recall = tp tp + f n = 0.9983 (2) f 1 = 2 * precision * recall precision + recall = 0.9989 (3)"], "text_after_citation": ["Comparing the lost English synsets between the two types of synset identifiers (offsets vs.", "ILIs), we found that 143 were lost using both types, while 62 were only lost with offsets (always due to satellite adjectives becoming standard adjectives), and 89 were only lost with CILI 1.0.", "The respective additions of these losses yield the total loss reported for English in table 1 (205 with offsets vs. 232 with the ILI)."], "citing_paper_content": {"title": "Mapping Wordnets On The Fly With Permanent Sense Keys", "abstract": "Most of the major databases on the semantic web have links to Princeton WordNet (PWN) synonym set (synset) identifiers, which differ for each PWN release, and are thus incompatible between versions. On the other hand, both PWN and the more recent Open English Wordnet (OEWN) provide permanent word sense identifiers (the sense keys), which can solve this interoperability problem. We present an algorithm that runs in linear time, to automatically derive a synset mapping between any pair of Wordnet versions that use PWN sense keys. This allows to update old WordNet links, and seamlessly interoperate with newer English Wordnet versions for which no prior mapping exists. By applying the proposed algorithm on the fly, at load time, we combine the Open Multilingual Wordnet (OMW 1.4, which uses old PWN 3.0 identifiers) with OEWN Edition 2021, and obtain almost perfect precision and recall. We compare the results of our approach using respectively synset offsets, versus the Collaborative InterLingual Index (CILI version 1.0) as synset identifiers, and find that the synset offsets perform better than CILI 1.0 in all cases, except a few ties."}, "cited_paper_content": {"title": "Mapping Wordnets Using Structural Information", "abstract": "We present a robust approach for linking already existing lexical/semantic hierarchies. We used a constraint satisfaction algorithm (relaxation labeling) to select - among a set of candidates- the node in a target taxonomy that bests matches each node in a source taxonomy. In particular, we use it to map the nominal part of WordNet 1.5 onto WordNet 1.6, with a very high precision and a very low remaining ambiguity."}, "keywords": ["English mapping"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We used a constraint satisfaction algorithm to map the nominal part of WordNet 1.5 onto WordNet 1.6 with high precision and low ambiguity (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The algorithm proposed in this study for mapping Wordnets using permanent sense keys achieves almost perfect precision and recall, as demonstrated by the results: precision=0.9996, recall=0.9983, and f1=0.9989 (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors achieved a very high precision and low ambiguity in their English mapping approach by using a constraint satisfaction algorithm to link existing lexical/semantic hierarchies, as demonstrated in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["English mapping"]}]}
{"citing_id": "2304.05989v1", "cited_id": "1602.00134", "section_title": "Datasets", "citation": "To obtain human skeletal data in the scene we compared the output of the Convolutional Pose Moachine (CPM) #REFR model with the kinect skeletal data.", "text_before_citation": ["Table 4 summarizes the characteristics of the datasets employed in this study.", "Moreover, the affordance labels present in the LOAD dataset are indicated in Figure 8 , whereas the labels for both the CAD-120 and Watch-n-Patch datasets are: 'can-contain', 'containable', 'can-support', 'supportable', 'drinkable', and 'holdable'.", "supportable can-support sittable  holdable  pullable  can-cover  rollable  pushable  carriable  kickable  coverable  can-contain containable drinkable  bouncable We used 80% of the CAD-120 dataset to determine the method's parameters and train the graph2vec network, resulting to 18,072 AGraphlets, and then evaluated the proposed approach on the remaining 20% unseen videos (24 videos), which comprise of 5,682 AGraphlets.", "Experiments on the Watch-n-Patch and LOAD datasets were conducted on 24 and 15 handpicked videos, or 6,900 and 1,212 AGraphlets respectively, where the predictions of the object detector, after visual inspection, were sufficient for capturing interacting objects.", "For defining the parameters of the algorithm, we exploited hand-picked videos from the training set for the CAD-120 dataset, and for the LOAD and Watch-n-Patch datasets we hand-picked videos different from the ones used for testing the proposed method."], "text_after_citation": ["Through an empirical study we found that the CPM predictions are more accurate, especially when human-joint occlusion occurs or the human agent stands on their side.", "Object locations and depth information are provided from the predicted objects' masks.", "We employ the state-of-the-art Mask R-CNN framework #OTHEREFR ) trained on the COCO dataset #OTHEREFR , since the training data distribution is similar to the data distribution present in the datasets we use in this work, and it is a generic dataset with more object classes than those included in the target task.", "The box enclosing the object's mask corresponds to the object's bounding box.", "This implementation is based on Mask R-CNN predictions, however the proposed method is not object-specific and any class-agnostic proposal method can be used."], "citing_paper_content": {"title": "Object-Agnostic Affordance Categorization Via Unsupervised Learning Of Graph Embeddings", "abstract": "Acquiring knowledge about object interactions and affordances can facilitate scene understanding and human-robot collaboration tasks. As humans tend to use objects in many different ways depending on the scene and the objects' availability, learning object affordances in everyday-life scenarios is a challenging task, particularly in the presence of an open set of interactions and objects. We address the problem of affordance categorization for class-agnostic objects with an open set of interactions; we achieve this by learning similarities between object interactions in an unsupervised way and thus inducing clusters of object affordances. A novel depth-informed qualitative spatial representation is proposed for the construction of Activity Graphs (AGs), which abstract from the continuous representation of spatio-temporal interactions in RGB-D videos. These AGs are clustered to obtain groups of objects with similar affordances. Our experiments in a real-world scenario demonstrate that our method learns to create object affordance clusters with a high V-measure even in cluttered scenes. The proposed approach handles object occlusions by capturing effectively possible interactions and without imposing any object or scene constraints."}, "cited_paper_content": {"title": "Convolutional Pose Machines", "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets."}, "keywords": ["kinect skeletal data"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed approach for affordance categorization is based on unsupervised learning of graph embeddings and clustering of Activity Graphs (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To induce clusters of object affordances, the authors incorporated convolutional networks into the pose machine framework for learning image features and image-dependent spatial models, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn image features and image-dependent spatial models for pose estimation, the authors of this study incorporated convolutional networks into the pose machine framework, which implicitly models long-range dependencies between variables in structured prediction tasks such as articulated pose estimation, without the need for explicit graphical model-style inference, and achieved state-of-the-art performance on standard benchmarks including the MPII dataset. #REFR", "given_citation_intent": "method", "given_keywords": ["kinect skeletal data"]}]}
{"citing_id": "2303.04895v1", "cited_id": "1502.02298", "section_title": "Belief Revision", "citation": "It is substantially similar to that of the general result given in #REFR (see Theorem 1) except that the result here relates to formulas and not to theories.", "text_before_citation": ["A revision operator \u2022 satisfies the AGM postulates if and only if there exists a FA that maps each formula \u03d5 to a binary relation \u03d5 such that for every formula \u03c8", "Mod(\u03d5 \u2022 \u03c8) = Min(Mod(\u03c8), \u03d5 )", "where", "Min(Mod(\u03c8), \u03d5 ) = {M \u2208 Mod(\u03c8) | \u2200M \u2032 \u2208 Mod(\u03c8), M \u2032 \u227a \u03d5 M}.", "The proof of Proposition 16 is given in Appendix."], "text_after_citation": ["This then required redefining a particular FA in the proof.", "The FA used in #OTHEREFR to prove this representation result was an adaptation of the FA proposed in the original paper #OTHEREFR , but the latter was not adaptable within the framework of modal logic 6 ."], "citing_paper_content": {"title": "Morpho-Logic From A Topos Perspective: Application To Symbolic Ai", "abstract": "Modal logics have proved useful for many reasoning tasks in symbolic artificial intelligence (AI), such as belief revision, spatial reasoning, among others. On the other hand, mathematical morphology (MM) is a theory for non-linear analysis of structures, that was widely developed and applied in image analysis. Its mathematical bases rely on algebra, complete lattices, topology. Strong links have been established between MM and mathematical logics, mostly modal logics. In this paper, we propose to further develop and generalize this link between mathematical morphology and modal logic from a topos perspective, i.e. categorial structures generalizing space, and connecting logics, sets and topology. Furthermore, we rely on the internal language and logic of topos. We define structuring elements, dilations and erosions as morphisms. Then we introduce the notion of structuring neighborhoods, and show that the dilations and erosions based on them lead to a constructive modal logic, for which a sound and complete proof system is proposed. We then show that the modal logic thus defined (called morpho-logic here), is well adapted to define concrete and efficient operators for revision, merging, and abduction of new knowledge, or even spatial reasoning."}, "cited_paper_content": {"title": "Belief Revision, Minimal Change And Relaxation: A General Framework Based On Satisfaction Systems, And Applications To Description Logics", "abstract": "Belief revision of knowledge bases represented by a set of sentences in a given logic has been extensively studied but for specific logics, mainly propositional, and also recently Horn and description logics. Here, we propose to generalize this operation from a model-theoretic point of view, by defining revision in the abstract model theory of satisfaction systems. In this framework, we generalize to any satisfaction system the characterization of the AGM postulates given by Katsuno and Mendelzon for propositional logic in terms of minimal change among interpretations. In this generalization, the constraint on syntax independence is partially relaxed. Moreover, we study how to define revision, satisfying these weakened AGM postulates, from relaxation notions that have been first introduced in description logics to define dissimilarity measures between concepts, and the consequence of which is to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge. We show how the proposed general framework can be instantiated in different logics such as propositional, first-order, description and Horn logics. In particular for description logics, we introduce several concrete relaxation operators tailored for the description logic ALC and its fragments EL and ELU , discuss their properties and provide some illustrative examples."}, "keywords": ["Theorem", "general result"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A revision operator \u2022 satisfies the AGM postulates if and only if there exists a FA that maps each formula \u03d5 to a binary relation \u03d5 such that for every formula \u03c8 Mod(\u03d5 \u2022 \u03c8) = Min(Mod(\u03c8), \u03d5 ) where Min(Mod(\u03c8), \u03d5 ) = {M \u2208 Mod(\u03c8) | \u2200M \u2032 \u2208 Mod(\u03c8), M \u2032 \u227a \u03d5 M}. The proof of Proposition 16 is given in Appendix. This framework can be instantiated in different logics such as propositional, first-order, description and Horn logics, and the proposed general framework can be used to define revision, satisfying the weakened AGM postulates, from relaxation notions that have been first introduced in description logics to define dissimilarity measures between concepts, and the consequence of which is to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors showed that the proposed general framework for belief revision can be instantiated in different logics, including description logics, and introduced several concrete relaxation operators tailored for the description logic ALC and its fragments EL and ELU, which were used to define dissimilarity measures between concepts and to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors prove in Proposition 16 that a revision operator \u2022 satisfies the AGM postulates if and only if there exists a FA that maps each formula \u03d5 to a binary relation \u03d5 such that for every formula \u03c8 Mod(\u03d5 \u2022 \u03c8) = Min(Mod(\u03c8), \u03d5 ) where Min(Mod(\u03c8), \u03d5 ) = {M \u2208 Mod(\u03c8) | \u2200M \u2032 \u2208 Mod(\u03c8), M \u2032 \u227a \u03d5 M}, and the proof is given in Appendix. This general result extends the AGM postulates characterization given by Katsuno and Mendelzon for propositional logic to any satisfaction system, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": ["Theorem", "general result"]}]}
{"citing_id": "2305.00986v1", "cited_id": "1505.04597", "section_title": "Unet With Dense Net", "citation": "The algorithm used to achieve this was UNet #REFR , it uses Double Convolutional layers to identify and extract features from the input image and uses skip connects to reuse these features in a related layer.", "text_before_citation": ["In this case, the pattern of interest is the rot present in an image, while identifying rot is a subjective matter it is entirely possible to map this as an input feature to a model and have its representations capture.", "This model would be able to capture image segments on related images.", "To achieve this, the model typically uses a combination of Double Convolutional Neural Networks with a structure called skip-connections which skip some of the connections in a neural network and feeds the output of one layer as input to the other layers.", "Skip-connections greatly reduce the complexity of loss surfaces, making it easier for optimizers to reduce loss while ensuring that feature representations are reused #OTHEREFR .", "The images for a sample image and prediction are shown below (areas in yellow are rotten areas of the meat as identified by the model)."], "text_after_citation": ["The idea is that each feature set captured in a layer is captured in a layer connected by a skip connection and passed to the next layer to compute the representation segment.", "Since this task outputs a set of image patterns the ideal outcome would be identifying the quality of the outputs in terms of the intersection and the overlap resulting from the predictions and the image masks.", "The loss functions capable of representing this effort are Dice loss and Jaccard loss which broadly look at the ratio of the intersection to the union, so concretely both would have a measure of how well the model can segment the patterns of interest from a given input image.", "The extracted image segmented predictions were passed as input features to a DenseNet model and predictions were output based on the segments captured.", "The segment of interest in this case is the rot present in the image and this was one-hot encoded when it was passed to the DenseNet model."], "citing_paper_content": {"title": "Meat Freshness Prediction", "abstract": "In most retail stores, the number of days since initial processing is used as a proxy for estimating the freshness of perishable foods or freshness is assessed manually by an employee. While the former method can lead to wastage, as some fresh foods might get disposed after a fixed number of days, the latter can be time-consuming, expensive and impractical at scale. This project aims to propose a Machine Learning (ML) based approach that evaluates freshness of food based on live data. For the current scope, it only considers meat as a the subject of analysis and attempts to classify pieces of meat as fresh, half-fresh or spoiled. Finally the model achieved an accuracy of above 90% and relatively high performance in terms of the cost of misclassification. It is expected that the technology will contribute to the optimization of the client's business operation, reducing the risk of selling defective or rotten products that can entail serious monetary, non-monetary and health-based consequences while also achieving higher corporate value as a sustainable company by reducing food wastage through timely sales and disposal. 1. Business Problem Assessing the freshness of perishable food is a significant operational challenge for retailers, as it is time-consuming, and can affect their business performance as well as reputation if a wrong judgment is made. In most retail stores, the number of days since initial processing is used as a proxy for freshness. Regardless of actual freshness, products are judged to be fresh if fewer days have passed and stale if more days have passed. Products that have passed many days since their initial processing are discounted, and if they are still not purchased, they are ultimately disposed"}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["algorithm", "Double Convolutional layers"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve this, the model typically uses a combination of Double Convolutional Neural Networks with a structure called skip-connections which skip some of the connections in a neural network and feeds the output of one layer as input to the other layers. Skip-connections greatly reduce the complexity of loss surfaces, making it easier for optimizers to reduce loss while ensuring that feature representations are reused (Ronneberger et al., 2015) #REFR. The images for a sample image and prediction are shown below (areas in yellow are rotten areas of the meat as identified by the model).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve this, the model typically uses a combination of Double Convolutional Neural Networks with a structure called skip-connections which skip some of the connections in a neural network and feeds the output of one layer as input to the other layers, as demonstrated in the U-Net method #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve this, the model typically uses a combination of Double Convolutional layers with a structure called skip-connections which skip some of the connections in a neural network and feeds the output of one layer as input to the other layers, as presented in U-Net algorithm #REFR.", "given_citation_intent": "method", "given_keywords": ["algorithm", "Double Convolutional layers"]}]}
{"citing_id": "2303.11117v2", "cited_id": "1811.00405", "section_title": "Emotion Recognition In Conversation", "citation": "By distinguishing specific speakers, DialogueRNN #REFR modeled emotions dynamically based on the current speaker, contextual content, and emotional state. Zhong et al.", "text_before_citation": ["Distinct from traditional emotion recognition which treats emotion as a static state, ERC takes full consideration of emotion to be dynamic and flow between speaker interactions. Hazarika et al.", "#OTHEREFR proposed a LSTM-based model to enable current utterance to capture contextual information in historical conversations.", "CMN #OTHEREFR employed a skip attention mechanism to merge contextual information in a historical conversation. Jiao et al.", "#OTHEREFR proposed a hierarchical GRU to address the difficulty of capturing longdistance contextual information effectively."], "text_after_citation": ["#OTHEREFR proposed Knowledge-Enriched Transformer, which dynamically exploited external commonsense knowledge through hierarchical self-attention and context-aware graph attention.", "By building directed graphical structures over the input utterance sequences with speaker information, DialogueGCN #OTHEREFR applied graph convolution network to construct inter-and intra-dependencies among distant utterances.", "COSMIC #OTHEREFR combined different commonsense knowledge and learned the interaction between the interlocutors in the dialogue.", "DialogXL #OTHEREFR modified the memory block in XLNet #OTHEREFR to store longer historical contexts and conversation-aware self-attention to handle multi-party structures. Wang et al.", "#OTHEREFR proposed a relational graph attention network to encode the tree structure for sentiment prediction."], "citing_paper_content": {"title": "Research Paper . Emotionic: Emotional Inertia And Contagion-Driven Dependency Modelling For Emotion Recognition In Conversation", "abstract": "Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. However, previous approaches to modeling global and local context dependencies lost the diversity of dependency information and do not take the context dependency into account at the classification level. In this paper, we propose a novel approach to dependency modeling driven by Emotional Inertia and Contagion (EmotionIC) for conversational emotion recognition at the feature extraction and classification levels. At the feature extraction level, our designed Identity Masked Multi-head Attention (IM-MHA) captures the identity-based long-distant context in the dialogue to contain the diverse influence of different participants and construct the global emotional atmosphere, while the devised Dialogue-based Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of dyadic dialogue is applied to refine the contextual features with interand intra-speaker dependencies. At the classification level, by introducing skip connections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF (SkipCRF) to capture the high-order dependencies within and between speakers, and to emulate the emotional flow of distant participants. Experimental results show that our method can significantly outperform the state-of-the-art models on four benchmark datasets. The ablation studies confirm that our modules can effectively model emotional inertia and contagion."}, "cited_paper_content": {"title": "Dialoguernn: An Attentive Rnn For Emotion Detection In Conversations", "abstract": "Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, and so on. Currently systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the state-of-the-art by a significant margin on two different datasets."}, "keywords": ["DialogueRNN modeled emotions"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Distinct from traditional emotion recognition which treats emotion as a static state, ERC takes full consideration of emotion to be dynamic and flow between speaker interactions. Hazarika et al. proposed a LSTM-based model to enable current utterance to capture contextual information in historical conversations. CMN employed a skip attention mechanism to merge contextual information in a historical conversation. Jiao et al. proposed a hierarchical GRU to address the difficulty of capturing long-distance contextual information effectively. In addition, Dialoguernn, a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification, has outperformed the state-of-the-art by a significant margin on two different datasets. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitation of previous approaches in capturing the emotional tendencies of dyadic dialogue, Hazarika et al. proposed a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Jiao et al. proposed a hierarchical GRU to capture long-distance contextual information effectively, while Hazarika et al., CMN, and the DialogueRNN modeled emotions by keeping track of individual party states throughout the conversation. #REFR", "given_citation_intent": "background", "given_keywords": ["DialogueRNN modeled emotions"]}]}
{"citing_id": "2303.09826v1", "cited_id": "1809.00219", "section_title": "Comparisons With State-Of-The-Art Methods", "citation": "And with the help of powerful SR backbone (RRDB-Net #REFR with 16.70M parameters), their results are quite remarkable.", "text_before_citation": ["We also conduct experiments on BasicVSR #OTHEREFR and PDM #OTHEREFR as two representative methods for noneblind VSR method and blind SISR with implicit degradation modeling.", "Apart from AnimeSR #OTHEREFR , for which we report the result in the original paper, all the others are general SR methods for open-domain images or videos.", "For fair comparisons, we fine-tune their officially released models on animation dataset AVC-Train. Quantitative comparison. As shown in Tab.", "1, we evaluate all the methods on AVC-ReaLQ #OTHEREFR for quantitative comparisons.", "Among them, Real-ESRGAN #OTHEREFR and BSR-GAN #OTHEREFR expand the former explicit degradation models by introducing high-order degradations and random shuffling respectively, which greatly improve the synthetic ability."], "text_after_citation": ["Nonetheless, ignoring the intrinsic characteristics of animation data limits their performance when applied to animation domain.", "Although specialized for animation videos, AnimeSR #OTHEREFR only utilizes a small number of real data (three human-annotated animation videos), which hinders the performance of VSR model in real scenarios.", "Different from them, our VQD-SR considers the intrinsic characteristics of animation videos and leverages the enormous degradation priors contained in rich-content real animation videos.", "VQD-SR also adopts the HR enhancement strategy for more effective SR supervision.", "Due to these advantages, VQD-SR achieves a result of 0.4096 and significantly outperforms the SOTA animation VSR model #OTHEREFR by 0.0264 in MANIQA on AVC-RealLQ #OTHEREFR ."], "citing_paper_content": {"title": "Learning Data-Driven Vector-Quantized Degradation Model For Animation Video Super-Resolution", "abstract": "Existing real-world video super-resolution (VSR) methods focus on designing a general degradation pipeline for open-domain videos while ignoring data intrinsic characteristics which strongly limit their performance when applying to some specific domains (e.g. animation videos). In this paper, we thoroughly explore the characteristics of animation videos and leverage the rich priors in real-world animation data for a more practical animation VSR model. In particular, we propose a multi-scale Vector-Quantized Degradation model for animation video Super-Resolution (VQD-SR) to decompose the local details from global structures and transfer the degradation priors in real-world animation videos to a learned vector-quantized codebook for degradation modeling. A rich-content Real Animation Low-quality (RAL) video dataset is collected for extracting the priors. We further propose a data enhancement strategy for high-resolution (HR) training videos based on our observation that existing HR videos are mostly collected from the Web which contains conspicuous compression artifacts. The proposed strategy is valid to lift the upper bound of animation VSR performance, regardless of the specific VSR model. Experimental results demonstrate the superiority of the proposed VQD-SR over state-of-the-art methods, through extensive quantitative and qualitative evaluations of the latest animation video super-resolution benchmark."}, "cited_paper_content": {"title": "Esrgan: Enhanced Super-Resolution Generative Adversarial Networks", "abstract": "The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at this https URL ."}, "keywords": ["powerful SR backbone"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Among them, Real-ESRGAN and BSR-GAN expand the former explicit degradation models by introducing high-order degradations and random shuffling respectively, which greatly improve the synthetic ability #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Real-ESRGAN #REFR and BSR-GAN #REFR expand the former explicit degradation models by introducing high-order degradations and random shuffling respectively, which greatly improve the synthetic ability. In comparison with these methods, the proposed VQD-SR model in this paper achieves superior results on the latest animation video super-resolution benchmark, demonstrating the effectiveness of leveraging the rich priors in real-world animation data for degradation modeling.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Real-ESRGAN and BSR-GAN expand the former explicit degradation models by introducing high-order degradations and random shuffling respectively, which greatly improve the synthetic ability, but the proposed VQD-SR with a powerful SR backbone outperforms them with a significant margin on the animation dataset AVC-ReaLQ #REFR.", "given_citation_intent": "result", "given_keywords": ["powerful SR backbone"]}]}
{"citing_id": "2303.16099v1", "cited_id": "1810.11654", "section_title": "Review Of Brain Glioma Segmentation Methods", "citation": "To avoid over-fitting problems in 3D voxel-level segmentation on limited training datasets, Myronenko #REFR proposed a 3D CNN with an additional variational autoencoder to regularise the decoder by reconstructing the input image.", "text_before_citation": ["#OTHEREFR ensembled three 2D CNNs on three orthogonal 2D patches.", "To fully make use of 3D contextual information, recent works applied 3D convolutional kernels on original volume data. Kamnitsas et al.", "#OTHEREFR proposed two pathway 3D CNN followed with dense CRF called DeepMedic for brain tumor segmentation.", "Authors of #OTHEREFR further extended the work by using model ensembling #OTHEREFR .", "The proposed system EMMA ensembled models from FCN, U-Net and DeepMedic for processing 3D patches."], "text_after_citation": ["The architecture built in #OTHEREFR is further developed in various recent works. Su et al.", "#OTHEREFR extends the architecture built in #OTHEREFR into two sub-networks to fuse the information learned from different modalities. Jiang et al.", "#OTHEREFR proposed two-stage networks where each stage adopts a similar network in #OTHEREFR .", "The first stage network generates a coarse result and the second stage network refines the segmentation result.", "The final result in #OTHEREFR reaches state-of-the-art by ensemble 12 model instances, which requires huge computational resources."], "citing_paper_content": {"title": "Medical Image Analysis Using Deep Relational Learning", "abstract": "School of Informatics Master of Philosophy Medical Image Analysis using Deep Relational Learning by Zhihua LIU Benefited from deep learning techniques, remarkable progress has been made within the medical image analysis area in recent years. However, it is very challenging to fully utilize the relational information (the relationship between tissues or organs or images) within the deep neural network architecture. Thus in this thesis, we propose two novel solutions to this problem called implicit and explicit deep relational learning. We generalize these two paradigms of deep relational learning into different solutions and evaluate them on various medical image analysis tasks. Automated segmentation of brain glioma in 3D magnetic resonance imaging plays an active role in glioma diagnosis, progression monitoring and surgery planning. In this work, we propose a novel Context-Aware Network that effectively models implicit relation information between features to perform accurate 3D glioma segmentation. We evaluate our proposed method on publicly accessible brain tumor segmentation datasets BRATS2017 and BRATS2018 against several state-of-the-art approaches using different segmentation metrics. The experimental results show that the proposed algorithm has better or competitive performance, compared to the standard approaches. Subsequently, we propose a new hierarchical homography estimation network to achieve accurate medical image mosaicing by learning the explicit spatial relationship between adjacent frames. We use the UCL Fetoscopy Placenta dataset to conduct experiments and our hierarchical homography estimation network outperforms the other state-of-the-art mosaicing methods while generating robust and meaningful mosaicing results on unseen frames. First of all, I want to thank my father Mr. Yuhua Du, my mother Mrs. Zengxia Xiao and my fiancee Miss. Xinyu Wang. Thank them for their support to me and the family. I cannot repay the support from my family, and it is also my biggest motivation to continue scientific research. Secondly, I want to thank my first supervisor, Prof. Huiyu Zhou. Prof. Huiyu Zhou has a rigorous attitude towards science. He also dedicates himself to his work. He is both a good teacher and a good friend. I am grateful to him for his continuous criticism and teaching, which have benefited me for life. At the same time, I would also like to thank my second supervisor, Prof. Yudong Zhang, for his professional guidance on my academic work. Also, I would like to thank all colleagues of Biomedical Image Processing Lab (BIPL), especially Mr."}, "cited_paper_content": {"title": "3D Mri Brain Tumor Segmentation Using Autoencoder Regularization", "abstract": "Automated segmentation of brain tumors from 3D magnetic resonance images (MRIs) is necessary for the diagnosis, monitoring, and treatment planning of the disease. Manual delineation practices require anatomical knowledge, are expensive, time consuming and can be inaccurate due to human error. Here, we describe a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture. Due to a limited training dataset size, a variational auto-encoder branch is added to reconstruct the input image itself in order to regularize the shared decoder and impose additional constraints on its layers. The current approach won 1st place in the BraTS 2018 challenge."}, "keywords": ["3D CNN", "additional variational autoencoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Automated segmentation of brain tumors from 3D magnetic resonance images has been an active research area in recent years. To this end, Kamnitsas et al. proposed a two-pathway 3D CNN followed with dense CRF called DeepMedic for brain tumor segmentation, which was further extended by the authors of #REFR using model ensembling. #REFR presents a semantic segmentation network for tumor subregion segmentation from 3D MRIs based on encoder-decoder architecture, which won 1st place in the BraTS 2018 challenge and could potentially be used as a method for brain tumor segmentation.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limited training dataset size in brain tumor subregion segmentation from 3D MRIs, the authors of this manuscript proposed a semantic segmentation network based on encoder-decoder architecture with a variational auto-encoder branch for regularization, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To further improve the performance of 3D CNNs on limited training datasets, Kamnitsas et al. proposed a variational autoencoder branch to regularize the shared decoder and impose additional constraints on its layers, as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["3D CNN", "additional variational autoencoder"]}]}
{"citing_id": "2303.02902v1", "cited_id": "1909.01513", "section_title": "Reeb Graph And Persistence Diagram", "citation": "To ensure that RG f is the Reeb graph of a Morse function, we first eliminate degenerate critical nodes in RG f by breaking them into nondegenerate critical nodes #REFR .", "text_before_citation": ["ExDg 0 (RG f ) encodes the range of f and ExDg 1 (RG f ) captures the 1-cycles or loops in RG f .", "To compute the persistence diagrams of RG f , we require RG f to be the Reeb graph of a Morse function. If f is a Morse function, i.e.", "all its critical points are non-degenerate and are at different levels, then the critical nodes of RG f have distinct values off and belong to one of the following five-types: (i) a minimum (with down-degree = 0, up-degree = 1), (ii) a maximum (with up-degree = 0, down-degree = 1), (iii) a downfork (with down-degree = 2, up-degree = 1) and (iv) a up-fork (with up-degree = 2, down-degree = 1).", "A regular node has up-degree = 1 and down-degree = 1.", "A down-fork (similarly, up-fork) node is called an essential down-fork node when it contributes to a loop (cycle) of the Reeb graph. Otherwise it is called an ordinary down-fork node."], "text_after_citation": ["After eliminating degenerate critical nodes, we ensure that the critical nodes off are at different levels.", "If two critical nodes are at the same level, then the value of one of the nodes is increased/decreased by a small value .", "After removing degenerate critical nodes and ensuring that critical nodes are at different levels, RG f becomes the Reeb graph of a Morse function.", "The points in Dg 0 (f ) are computed by pairing ordinary down-forks with minima, ordinary up-forks with maxima and the global minimum with global maximum.", "Let u be an ordinary down-fork of RG f ."], "citing_paper_content": {"title": "A Topological Distance Measure Between Multi-Fields For Classification And Analysis Of Shapes And Data", "abstract": "Distance measures play an important role in shape classification and data analysis problems. Topological distances based on Reeb graphs and persistence diagrams have been employed to obtain effective algorithms in shape matching and scalar data analysis. In the current paper, we propose an improved distance measure between two multi-fields by computing a multidimensional Reeb graph (MDRG) each of which captures the topology of a multi-field through a hierarchy of Reeb graphs in different dimensions. A hierarchy of persistence diagrams is then constructed by computing a persistence diagram corresponding to each Reeb graph of the MDRG. Based on this representation, we propose a novel distance measure between two MDRGs by extending the bottleneck distance between two Reeb graphs. We show that the proposed measure satisfies the pseudo-metric and stability properties. We examine the effectiveness of the proposed multi-field topologybased measure on two different applications: (1) shape classification and (2) detection of topological features in a time-varying multi-field data. In the shape classification problem, the performance of the proposed measure is compared with the well-known topology-based measures in shape matching. In the second application, we consider a time-varying volumetric multi-field data from the field of computational chemistry where the goal is to detect the site of stable bond formation between Pt and CO molecules. We demonstrate the ability of the proposed distance in classifying each of the sites as occurring before and after the bond stabilization."}, "cited_paper_content": {"title": "Propagate And Pair: A Single-Pass Approach To Critical Point Pairing In Reeb Graphs", "abstract": "With the popularization of Topological Data Analysis, the Reeb graph has found new applications as a summarization technique in the analysis and visualization of large and complex data, whose usefulness extends beyond just the graph itself. Pairing critical points enables forming topological fingerprints, known as persistence diagrams, that provides insights into the structure and noise in data. Although the body of work addressing the efficient calculation of Reeb graphs is large, the literature on pairing is limited. In this paper, we discuss two algorithmic approaches for pairing critical points in Reeb graphs, first a multipass approach, followed by a new single-pass algorithm, called Propagate and Pair."}, "keywords": ["Reeb graph"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently pair critical points in Reeb graphs, previous work has proposed a multipass approach, but the literature on pairing is limited. Recently, a new single-pass algorithm called Propagate and Pair was introduced in #REFR, which may be a useful tool for computing persistence diagrams.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pair critical points in Reeb graphs, two algorithmic approaches are discussed in the literature, a multipass approach and a new single-pass algorithm called Propagate and Pair #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pair critical points in Reeb graphs, two algorithmic approaches are discussed in the literature: a multipass approach and a new single-pass algorithm called Propagate and Pair, as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["Reeb graph"]}]}
{"citing_id": "2303.16251v2", "cited_id": "1711.00165", "section_title": "I. Introduction", "citation": "Similar results are then achieved in #REFR for deep fully-connected networks as all hidden layer widths go to infinity.", "text_before_citation": ["#OTHEREFR gives a constructive method, but only for target and activation functions in L 1 .", "In #OTHEREFR and #OTHEREFR , they propose constructive methods for a class of target functions with unit step and ReLU activations respectively.", "In #OTHEREFR , functions are approximated using trigonometric polynomial ridge functions, which can then be shown in expectation to be equivalent to randomly initialized ReLU activations.", "There are several interesting and important results in the literature having to do with neural networks with random (typically Gaussian) parameter initialization, sometimes as a consequence of using randomly initialized gradient descent for training the network.", "A classic result by #OTHEREFR shows that the output of a single hidden-layer network with Gaussian randomly initialized parameters goes to a Gaussian Process as the width goes to infinity."], "text_after_citation": ["Also for deep fully-connected networks, in #OTHEREFR the authors define the Neural Tangent Kernel and propose that its limit, as the hidden layer widths go to infinity, can be used to study the timestep evolution and dynamics of the parameters, and the corresponding network output function, in gradient descent.", "In #OTHEREFR , the authors show that single hidden-layer networks cannot achieve the same rates of increase in a measure of curvature produced by the network output, as deep networks can, with parameters Gaussian randomly initialized and bounded activation functions.", "In #OTHEREFR , the authors show that single hidden-layer networks of a sufficient width can use Gaussian randomly initialized gradient descent on values of a target function, and achieve guaranteed generalization to the entire function.", "And in #OTHEREFR , the authors show that deep fully-connected networks, where each hidden layer width meets a sufficient size, can be trained with Gaussian randomly initialized gradient descent and be guaranteed to reach the global minimum at a linear rate.", "Our primary contributions in this paper are: developing a novel method for bridging convex combinations of activation functions (eg."], "citing_paper_content": {"title": "Function Approximation With Randomly Initialized Neural Networks For Approximate Model Reference Adaptive Control", "abstract": "Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs, high accuracy can be achieved via linear combinations of randomly initialized activations. These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. This paper defines mollified integral representations, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known. The new construction enables approximation guarantees for randomly initialized networks using any activation for which there exists an established base approximation which may not be constructive. We extend the results to the supremum norm and show how this enables application to an extended, approximate version of (linear) model reference adaptive control."}, "cited_paper_content": {"title": "Deep Neural Networks As Gaussian Processes", "abstract": "A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hidden-layer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks."}, "keywords": ["deep fully-connected networks"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A classic result by #REFR shows that the output of a single hidden-layer network with Gaussian randomly initialized parameters goes to a Gaussian Process as the width goes to infinity.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "A classic result by #REFR shows that the output of a single hidden-layer network with Gaussian randomly initialized parameters goes to a Gaussian Process as the width goes to infinity, which is relevant to the findings in this manuscript.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "A classic result by #REFR shows that deep fully-connected neural networks with Gaussian randomly initialized parameters converge to a Gaussian Process as the width goes to infinity, which is relevant to the findings in this manuscript.", "given_citation_intent": "result", "given_keywords": ["deep fully-connected networks"]}]}
{"citing_id": "2303.14711v1", "cited_id": "1805.03278", "section_title": "Results", "citation": "Per pixel and per instance evaluation for the detection of small hyperreflective specks and foci in comparison to pixel-wise results for larger conventional hyperreflective foci reported by Schlegl et al. #REFR for data acquired on a Cirrus/Spectralis OCT scanner.", "text_before_citation": ["However, on their figures, they appear to be much larger, corresponding to conventional HRF.", "In addition, they also include features in different retinal layers.", "Due to the difference in sizes and scale, the performance of their algorithm is not directly comparable to our results, yet their results show the performance of a different algorithm on a similar task.", "A more detailed comparison of our algorithm and the ground truth yields the following observations (Fig. 2) .", "Labeled features with slightly higher intensity values than the background are difficult to detect and the reader agreement on these is not high. Tab. 1."], "text_after_citation": ["This also applies to boundary regions of the features because of the smooth transition to background intensities and a missing consensus of where to delineate feature boundaries.", "Another situation of disagreement can occur on much higher intensity values on the ELM compared to the rest of the ELM.", "They are usually detected by the algorithm, but usually not annotated by the human experts.", "In addition, for features that are close to IS/OS and OPL, it is sometimes unclear, if they are detached from these layers.", "It occurs that only either the algorithm or the readers include or exclude them, which has a high impact on the scores as the regions are comparably large."], "citing_paper_content": {"title": "Unsupervised Detection Of Small Hyperreflective Features In Ultrahigh Resolution Optical Coherence Tomography", "abstract": "Recent advances in optical coherence tomography such as the development of high speed ultrahigh resolution scanners and corresponding signal processing techniques may reveal new potential biomarkers in retinal diseases. Newly visible features are, for example, small hyperreflective specks in age-related macular degeneration. Identifying these new markers is crucial to investigate potential association with disease progression and treatment outcomes. Therefore, it is necessary to reliably detect these features in 3D volumetric scans. Because manual labeling of entire volumes is infeasible a need for automatic detection arises. Labeled datasets are often not publicly available and there are usually large variations in scan protocols and scanner types. Thus, this work focuses on an unsupervised approach that is based on local peak-detection and random walker segmentation to detect small features on each B-scan of the volume."}, "cited_paper_content": {"title": "Fully Automated Segmentation Of Hyperreflective Foci In Optical Coherence Tomography Images", "abstract": "The automatic detection of disease related entities in retinal imaging data is relevant for disease- and treatment monitoring. It enables the quantitative assessment of large amounts of data and the corresponding study of disease characteristics. The presence of hyperreflective foci (HRF) is related to disease progression in various retinal diseases. Manual identification of HRF in spectral-domain optical coherence tomography (SD-OCT) scans is error-prone and tedious. We present a fully automated machine learning approach for segmenting HRF in SD-OCT scans. Evaluation on annotated OCT images of the retina demonstrates that a residual U-Net allows to segment HRF with high accuracy. As our dataset comprised data from different retinal diseases including age-related macular degeneration, diabetic macular edema and retinal vein occlusion, the algorithm can safely be applied in all of them though different pathophysiological origins are known."}, "keywords": ["Cirrus/Spectralis OCT scanner"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A fully automated machine learning approach for segmenting hyperreflective foci in spectral-domain optical coherence tomography scans was presented in a recent study (#REFR), demonstrating high accuracy in identifying disease-related entities.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this study presented a fully automated machine learning approach for segmenting hyperreflective foci (HRF) in spectral-domain optical coherence tomography (SD-OCT) scans, achieving high accuracy on annotated OCT images of the retina from different retinal diseases including age-related macular degeneration, diabetic macular edema, and retinal vein occlusion, which makes it applicable to all of them despite different pathophysiological origins #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of the manuscript present an unsupervised approach for detecting small hyperreflective features in ultrahigh resolution OCT scans, which is different from the fully automated machine learning approach for segmenting hyperreflective foci in SD-OCT scans presented in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["Cirrus/Spectralis OCT scanner"]}]}
{"citing_id": "2304.00257v1", "cited_id": "1711.07971", "section_title": "Three-Dimensional Convolutional Neural Network (Cnn) Architecture", "citation": "Our approach is adopted from #REFR , whereby a 2D \u00d7 kernel can be inflated to a 3D \u00d7 \u00d7 kernel that spreads across frames.", "text_before_citation": ["Resnet18 was used as the backbone of our CNN model.", "It serves as our feature extractor which compresses the image representation into a vector with dimensionality of 512 as shown in Fig. 5 .", "The base architecture, which was designed for image classification was extended to a three-dimensional (3D) model by \"inflating\" the kernels #OTHEREFR ."], "text_after_citation": ["The weights from the pretrained 2D Resnet 18 model are used to initialize the kernels, whereby each of the planes in the \u00d7 \u00d7 kernel are initialized with the pre-trained \u00d7 weights, rescaled by 1/ .", "This initialization setup produces the same results as the 2D pre-trained model run on a single static frame, repeated in the time domain.", "In our setup, the first convolutional kernel in each Resnet layer is initialized with a 3 \u00d7 3 \u00d7 3 kernel.", "The rest of the kernels are simply extended to their 3D counterpart that perform the same operation, by initializing them as a 1 \u00d7 3 \u00d7 3 kernel.", "This approach does not increase the parameters significantly, thus alleviating overfitting. The CNN architecture is visualized in Fig. 2 ."], "citing_paper_content": {"title": "Radifusion: A Multi-Radiomics Deep Learning Based Breast Cancer Risk Prediction Model Using Sequential Mammographic Images With Image Attention And Bilateral Asymmetry Refinement", "abstract": "Breast cancer is a significant public health concern and early detection is critical for triaging high risk patients. Sequential screening mammograms can provide important spatiotemporal information about changes in breast tissue over time. In this study, we propose a deep learning architecture called RADIFUSION that utilizes sequential mammograms and incorporates a linear image attention mechanism, radiomic features, a new gating mechanism to combine different mammographic views, and bilateral asymmetry-based finetuning for breast cancer risk assessment. We evaluate our model on a screening dataset called Cohort of Screen-Aged Women (CSAW) dataset. Based on results obtained on the independent testing set consisting of 1,749 women, our approach achieved superior performance compared to other state-of-the-art models with area under the receiver operating characteristic curves (AUCs) of 0.905, 0.872 and 0.866 in the three respective metrics of 1-year AUC, 2-year AUC and > 2-year AUC. Our study highlights the importance of incorporating various deep learning mechanisms, such as image attention, radiomic features, gating mechanism, and bilateral asymmetry-based fine-tuning, to improve the accuracy of breast cancer risk assessment. We also demonstrate that our model's performance was enhanced by leveraging spatiotemporal information from sequential mammograms. Our findings suggest that RADIFUSION can provide clinicians with a powerful tool for breast cancer risk assessment."}, "cited_paper_content": {"title": "Non-Local Neural Networks", "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at this https URL ."}, "keywords": ["2D \u00d7 kernel"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Resnet18 was used as the backbone of our CNN model. It serves as our feature extractor which compresses the image representation into a vector with dimensionality of 512 as shown in Fig. 5. The base architecture, which was designed for image classification was extended to a three-dimensional (3D) model by \"inflating\" the kernels #REFR. Inspired by the classical non-local means method in computer vision, non-local operations were used as a generic family of building blocks for capturing long-range dependencies.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To capture long-range dependencies in sequential mammograms, we utilized non-local operations as a generic family of building blocks #REFR, which improved the accuracy of our breast cancer risk assessment model.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To capture long-range dependencies in sequential mammograms, we utilized non-local operations as a building block in our CNN model, which computes the response at a position as a weighted sum of the features at all positions #REFR.", "given_citation_intent": "method", "given_keywords": ["2D \u00d7 kernel"]}]}
{"citing_id": "2304.03424v1", "cited_id": "1908.09048", "section_title": "Introduction", "citation": "Other works such as Griffon #REFR used machine learning models to predict the minor slowdown in runtimes for a limited number of job templates.", "text_before_citation": ["In production systems, jobs are often scheduled or pipelined with strong data dependencies (jobs using other jobs' output data as inputs) #OTHEREFR .", "Stability and predictability of job runtimes are important factors that affect the fundamental design and architecture of data processing pipelines.", "Unfortunately, they are often neglected by operators due to the difficulties of assessment even though job slowdowns are inevitable #OTHEREFR .", "Even with massive amounts of telemetry data, cloud providers still default to a manual triage process due to the difficulty of capturing the compounding factors that impact job runtime and its stability, which is not scalable and error-prone.", "Although prior works #OTHEREFR have empirically characterized runtime variation, they do not propose methods to predict the variation nor the likelihood of a new run being an outlier compared to the average or median runtimes."], "text_after_citation": ["They are unable to predict significant slowdowns that appear as outliers.", "As ML models are notoriously bad at handling outliers especially with a low existence, prior time-series based approaches #OTHEREFR are not applicable.", "In this paper, we aim to address this gap for production data analytics systems by developing a novel and systematic approach for modeling, predicting, and explaining the job runtime variation, allowing for finer-grained differentiation in characteristics.", "For our study, we comprehensively examine the runtime variation for millions of production SCOPE #OTHEREFR jobs on Cosmos #OTHEREFR , an exabyte-scale analytics platform at Microsoft that supports a broad spectrum of Microsoft products #OTHEREFR .", "Our key contribution is a framework for systematically analyzing, predicting and explaining runtime variation that includes:"], "citing_paper_content": {"title": "Runtime Variation In Big Data Analytics", "abstract": "The dynamic nature of resource allocation and runtime conditions on Cloud can result in high variability in a job's runtime across multiple iterations, leading to a poor experience. Identifying the sources of such variation and being able to predict and adjust for them is crucial to cloud service providers to design reliable data processing pipelines, provision and allocate resources, adjust pricing services, meet SLOs and debug performance hazards. In this paper, we analyze the runtime variation of millions of production SCOPE jobs on Cosmos, an exabyte-scale internal analytics platform at Microsoft. We propose an innovative 2-step approach to predict job runtime distribution by characterizing typical distribution shapes combined with a classification model with an average accuracy of >96%, out-performing traditional regression models and better capturing long tails. We examine factors such as job plan characteristics and inputs, resource allocation, physical cluster heterogeneity and utilization, and scheduling policies. To the best of our knowledge, this is the first study on predicting categories of runtime distributions for enterprise analytics workloads at scale. Furthermore, we examine how our methods can be used to analyze what-if scenarios, focusing on the impact of resource allocation, scheduling, and physical cluster provisioning decisions on a job's runtime consistency and predictability. CCS Concepts: \u2022 Computer systems organization \u2192 Cloud computing; \u2022 Computing methodologies \u2192 Causal reasoning and diagnostics; \u2022 Information systems \u2192 Data analytics."}, "cited_paper_content": {"title": "Griffon: Reasoning About Job Anomalies With Unlabeled Data In Cloud-Based Platforms", "abstract": "Microsoft's internal big data analytics platform is comprised of hundreds of thousands of machines, serving over half a million jobs daily, from thousands of users. The majority of these jobs are recurring and are crucial for the company's operation. Although administrators spend significant effort tuning system performance, some jobs inevitably experience slowdowns, i.e., their execution time degrades over previous runs. Currently, the investigation of such slowdowns is a labor-intensive and error-prone process, which costs Microsoft significant human and machine resources, and negatively impacts several lines of businesses. In this work, we present Griffin, a system we built and have deployed in production last year to automatically discover the root cause of job slowdowns. Existing solutions either rely on labeled data (i.e., resolved incidents with labeled reasons for job slowdowns), which is in most cases non-existent or non-trivial to acquire, or on time-series analysis of individual metrics that do not target specific jobs holistically. In contrast, in Griffin we cast the problem to a corresponding regression one that predicts the runtime of a job, and show how the relative contributions of the features used to train our interpretable model can be exploited to rank the potential causes of job slowdowns. Evaluated over historical incidents, we show that Griffin discovers slowdown causes that are consistent with the ones validated by domain-expert engineers, in a fraction of the time required by them."}, "keywords": ["runtimes", "machine learning models"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Even with massive amounts of telemetry data, cloud providers still default to a manual triage process due to the difficulty of capturing the compounding factors that impact job runtime and its stability, which is not scalable and error-prone. Although prior works have empirically characterized runtime variation, they do not propose methods to predict the variation nor the likelihood of a new run being an outlier compared to the average or median runtimes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Prior works have empirically characterized runtime variation, but they do not propose methods to predict the variation nor the likelihood of a new run being an outlier compared to the average or median runtimes #REFR, which is addressed in the reference paper.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Prior works have empirically characterized runtime variation, but they do not propose methods to predict the variation nor the likelihood of a new run being an outlier compared to the average or median runtimes, which can be addressed by machine learning models such as Griffin proposed by Microsoft researchers #REFR.", "given_citation_intent": "background", "given_keywords": ["runtimes", "machine learning models"]}]}
{"citing_id": "2303.07985v1", "cited_id": "1908.05268", "section_title": "Conclusion", "citation": "We contrast this with the work of Kiefer & Neuen #REFR Theorem 6 .4], who showed that k-WL identifies graphs of treewidth k, though they did not control for rounds.", "text_before_citation": ["We showed that the (3k + 4)-WL identifies graphs of treewidth k in O(log n) rounds, improving upon the work of Grohe & Verbitsky #OTHEREFR , who established the analogous result for (4k + 3)-WL.", "As a corollary, we obtained that graphs of treewidth k are identified by FO + C formulas with (3k + 5)-variables and quantifier depth O(log n)."], "text_after_citation": ["Naturally, it would be of interest to close the gap between our upper bound of (3k + 4) on the Weisfeiler-Leman dimension required to achieve O(log n) rounds and the best known upper bound of k on the Weisfeiler-Leman dimension (without controlling for rounds) of graphs of treewidth k.", "One approach would be to improve the result on #OTHEREFR , to provide a tree decomposition of width \u2264 3k + 2 and height O(log n) for graphs of treewidth k.", "It would also be of interest to examine special families of graphs with bounded treewidth that possess additional structure, which Weisfeiler-Leman can exploit to achieve O(log n) rounds with only k + O(1) pebbles.", "Kiefer & Neuen #OTHEREFR Theorem 6 .1] also established a lower bound of \u2308k/2\u2309 \u2212 2 on the Weisfeiler-Leman dimension (again, without controlling for rounds) of graphs of treewidth k.", "It would be of interest to strengthen their lower bound on the Weisfeiler-Leman dimension when restricting to O(log n) rounds."], "citing_paper_content": {"title": "Logarithmic Weisfeiler-Leman And Treewidth *", "abstract": "In this paper, we show that the (3k + 4)-dimensional Weisfeiler-Leman algorithm can identify graphs of treewidth k in O(log n) rounds. This improves the result of Grohe & Verbitsky (ICALP 2006), who previously established the analogous result for (4k + 3)-dimensional Weisfeiler-Leman. In light of the equivalence between Weisfeiler-Leman and the logic FO + C (Cai, F\u00fcrer, & Immerman, Combinatorica 1992), we obtain an improvement in the descriptive complexity for graphs of treewidth k. Precisely, if G is a graph of treewidth k, then there exists a (3k + 5)-variable formula \u03d5 in FO + C with quantifier depth O(log n) that identifies G up to isomorphism."}, "cited_paper_content": {"title": "The Power Of The Weisfeiler-Leman Algorithm To Decompose Graphs", "abstract": "The Weisfeiler-Leman procedure is a widely-used approach for graph isomorphism testing that works by iteratively computing an isomorphism-invariant coloring of vertex tuples. Meanwhile, a fundamental tool in structural graph theory, which is often exploited in approaches to tackle the graph isomorphism problem, is the decomposition into 2- and 3-connected components. ::: We prove that the 2-dimensional Weisfeiler-Leman algorithm implicitly computes the decomposition of a graph into its 3-connected components. Thus, the dimension of the algorithm needed to distinguish two given graphs is at most the dimension required to distinguish the corresponding decompositions into 3-connected components (assuming it is at least 2). ::: This result implies that for k >= 2, the k-dimensional algorithm distinguishes k-separators, i.e., k-tuples of vertices that separate the graph, from other vertex k-tuples. As a byproduct, we also obtain insights about the connectivity of constituent graphs of association schemes. ::: In an application of the results, we show the new upper bound of k on the Weisfeiler-Leman dimension of graphs of treewidth at most k. Using a construction by Cai, Furer, and Immerman, we also provide a new lower bound that is asymptotically tight up to a factor of 2."}, "keywords": ["treewidth k"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We showed that the (3k + 4)-WL identifies graphs of treewidth k in O(log n) rounds, improving upon the work of Grohe & Verbitsky #REFR, who established the analogous result for (4k + 3)-WL. As a corollary, we obtained that graphs of treewidth k are identified by FO + C formulas with (3k + 5)-variables and quantifier depth O(log n). The reference paper provides a proof that the 2-dimensional Weisfeiler-Leman algorithm implicitly computes the decomposition of a graph into its 3-connected components, which implies that for k >= 2, the k-dimensional algorithm distinguishes k-separators from other vertex k-tuples.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our work improves upon the previous result of Grohe & Verbitsky #OTHEREFR by showing that the (3k + 4)-dimensional Weisfeiler-Leman algorithm can identify graphs of treewidth k in O(log n) rounds, and this result is also supported by the findings of the Weisfeiler-Leman algorithm's ability to implicitly compute the decomposition of a graph into its 3-connected components in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Weisfeiler-Leman algorithm implicitly computes the decomposition of a graph into its 3-connected components, which is useful for identifying graphs of treewidth k in O(log n) rounds as shown in this paper #REFR.", "given_citation_intent": "result", "given_keywords": ["treewidth k"]}]}
{"citing_id": "2304.03754v1", "cited_id": "1810.04805", "section_title": "Video Descriptions", "citation": "For question and answer features, we pre-trained BERT #REFR on our generated training set and extracted QA features adhering to the NExT-QA setting.", "text_before_citation": ["9 OpenAI text-davinci-003 API https://openai.com/api/ #OTHEREFR Our empirical findings indicate that adjusting the max len and top k parameters in the prompt for GPT-3 yields favorable results.", "As such, we utilize the default max len and top k settings and use prompts in the format of \"what is the intention of {Cap}? Provide {top k} answers within vide examples of few-shot prompting by randomly sampling 5 QAs from NExT-QA and transferring the question to the declared sentence.", "For the distillation experiments (Section 3.4), we distill from GPT-3 to T5-large 11 model.", "Video QA training In all of our experiments, we followed the NExT-QA #OTHEREFR video preprocessing method, where we uniformly sampled eight segments of 16 consecutive frames.", "For visual features, we used Resnet101 #OTHEREFR pre-trained on ImageNet #OTHEREFR and inflated 3D ResNeXt-101 #OTHEREFR pre-trained on Kinetics #OTHEREFR as our feature extractors."], "text_after_citation": ["To adapt an open-ended QA model for multiple-choice QA, we concatenated each candidate answer with the question and optimized the model with Hinge Loss, following the NExT-QA implementation.", "For video QA training, we employ the default NeXT-QA implementation #OTHEREFR with the exception of setting the patience in the ReduceLROnPlateau to 2 instead of 5, and the maximum number of epochs to 25 instead of 50, as we observed a faster convergence during training.", "We conduct the training on a single NVIDIA TITAN RTX GPU, and each experiment takes around 18 to 24 hours at most."], "citing_paper_content": {"title": "Language Models Are Causal Knowledge Extractors For Zero-Shot Video Question Answering", "abstract": "Causal Video Question Answering (CVidQA) queries not only association or temporal relations but also causal relations in a video. Existing question synthesis methods pretrained question generation (QG) systems on reading comprehension datasets with text descriptions as inputs. However, QG models only learn to ask association questions (e.g., \"what is someone doing...\") and result in inferior performance due to the poor transfer of association knowledge to CVidQA, which focuses on causal questions like \"why is someone doing ...\". Observing this, we proposed to exploit causal knowledge to generate question-answer pairs, and proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging causal commonsense knowledge from language models to tackle CVidQA. To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., \"score a goal\" triggers \"soccer player kicking ball\") by prompting LM with the action (soccer player kicking ball) to retrieve the intention (to score a goal). CaKE-LM significantly outperforms conventional methods by 4% to 6% of zero-shot CVidQA accuracy on NExT-QA and Causal-VidQA datasets. We also conduct comprehensive analyses and provide key findings for future research."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["pre-trained BERT"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another (e.g., \"score a goal\" triggers \"soccer player kicking ball\") by prompting LM with the action (soccer player kicking ball) to retrieve the intention (to score a goal) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To extract causal commonsense knowledge from language models, the authors proposed a novel framework, Causal Knowledge Extraction from Language Models (CaKE-LM), leveraging the method of pre-training deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, as introduced in the reference paper \"Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To extract knowledge from LMs, CaKE-LM generates causal questions containing two events with one triggering another by prompting LM with the action to retrieve the intention, leveraging pre-trained BERT model as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["pre-trained BERT"]}]}
{"citing_id": "2303.12132v1", "cited_id": "1511.06349", "section_title": "Generative Deep Neural Language Models", "citation": "Autoencoding models are trained similarly, except that the masked word can be located anywhere in the randomly sampled suite of words #REFR .", "text_before_citation": ["From the point of view of a Transformer-based LLM, generating new text is equivalent to the generation of a translation, except that there is not necessarily an embedding space representation.", "Instead, it's the continuation of an initial word sequence, where the word to be generated can be located either at the end of the original sentence or in the middle of it.", "These two cases correspond to the two main approaches to training Generative Deep Neural Language Models.", "Autoregressive models are trained by a random sampling suite of words in the source dataset (\"utterances\"), masking a word and training the model to predict the masked word accurately #OTHEREFR .", "Autoregressive models are best thought of as autocomplete -they are trained to complete sentences in a way that is representative of their training dataset."], "text_after_citation": ["Autoencoding models are best thought of as search or autocorrect suggestions -they are trained to rewrite whole sentences in a way that is representative of their training dataset.", "Just as Google search suggestions, they can suggest words at the end of a query to refine it, but they can also add one at the beginning or even rewrite a word (for instance, to correct a typo).", "While Autoencoding models can be used to generate text based on utterances, their strength is rather in applications that require understanding the utterance as a whole.", "While the autoencoding models are generally considered to be more powerful than autoregressive ones, their generative capabilities are not necessarily optimal for the size of the model, training dataset, or the computational resources spent training the model.", "After all, the training mode relevant to the generation represents only a fraction of the training time of autoregressive models."], "citing_paper_content": {"title": "Fundamentals Of Generative Large Language Models And Perspectives In Cyber-Defense", "abstract": "Generative Language Models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with AI (conversational models). Arguably the focal point of public attention has been such a refinement of the GPT3 model-the ChatGPT and its subsequent integration with auxiliary capabilities, including search as part of Microsoft Bing. Despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. However, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. This has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. This review aims to provide a brief overview of the history, state of the art, and implications of Generative Language Models in terms of their principles, abilities, limitations, and future prospects-especially in the context of cyber-defense, with a focus on the Swiss operational environment."}, "cited_paper_content": {"title": "Generating Sentences From A Continuous Space", "abstract": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling."}, "keywords": ["Autoencoding models"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Autoregressive models are best thought of as autocomplete -they are trained to complete sentences in a way that is representative of their training dataset. To explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features, an RNN-based variational autoencoder generative model was introduced and studied in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "One approach to explicitly modeling holistic properties of sentences is through an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Autoencoding models have been shown to incorporate distributed latent representations of entire sentences, allowing them to model holistic properties of sentences such as style, topic, and high-level syntactic features, and to produce diverse and well-formed sentences through simple deterministic decoding #REFR.", "given_citation_intent": "method", "given_keywords": ["Autoencoding models"]}]}
{"citing_id": "2303.04393v2", "cited_id": "2002.07953", "section_title": "C. Moving-Threshold Estimation", "citation": "Because the entropy of unknown samples is usually larger than known ones', we could set a pre-defined threshold \u03c1 and recognize those samples whose entropy is larger than \u03c1 as unknown. Following #REFR , \u03c1 is set to be ln(K)/2.", "text_before_citation": ["The key challenge of open set domain adaptation is how to separate common samples from private samples in the target domain.", "Assuming the model has more confidence (lower entropy) in shared-class samples than unknown samples, one mainstream method in OSDA is to set a confidence threshold on the entropy of the output of the classifier."], "text_after_citation": ["However, the threshold leads the model to be sensitive to hyperparameter tuning that may influence the robustness of the model in real world scenarios.", "We tackle this problem by proposing the movingthreshold estimation method.", "One important drawback of the previous methods is that they ignore the semantic knowledge within unknown samples.", "In fact, there are various kinds of unknown images in an open world that could become unknown target samples.", "As a result, the relationships between unknown classes and known classes can vary greatly."], "citing_paper_content": {"title": "Imbalanced Open Set Domain Adaptation Via Moving-Threshold Estimation And Gradual Alignment", "abstract": "Multimedia applications are often associated with cross-domain knowledge transfer, where Unsupervised Domain Adaptation (UDA) can be used to reduce the domain shifts. Open Set Domain Adaptation (OSDA) aims to transfer knowledge from a well-labeled source domain to an unlabeled target domain under the assumption that the target domain contains unknown classes. Existing OSDA methods consistently lay stress on the covariate shift, ignoring the potential label shift problem. The performance of OSDA methods degrades drastically under intradomain class imbalance and inter-domain label shift. However, little attention has been paid to this issue in the community. In this paper, the Imbalanced Open Set Domain Adaptation (IOSDA) is explored where the covariate shift, label shift and category mismatch exist simultaneously. To alleviate the negative effects raised by label shift in OSDA, we propose Open-set Movingthreshold Estimation and Gradual Alignment (OMEGA)-a novel architecture that improves existing OSDA methods on classimbalanced data. Specifically, a novel unknown-aware target clustering scheme is proposed to form tight clusters in the target domain to reduce the negative effects of label shift and intra-domain class imbalance. Furthermore, moving-threshold estimation is designed to generate specific thresholds for each target sample rather than using one for all. Extensive experiments on IOSDA, OSDA and OPDA benchmarks demonstrate that our method could significantly outperform existing state-of-the-arts. Code and data are available at https://github.com/mendicant04/OMEGA."}, "cited_paper_content": {"title": "Universal Domain Adaptation Through Self Supervision", "abstract": "Unsupervised domain adaptation methods traditionally assume that all source categories are present in the target domain. In practice, little may be known about the category overlap between the two domains. While some methods address target settings with either partial or open-set categories, they assume that the particular setting is known a priori. We propose a more universally applicable domain adaptation approach that can handle arbitrary category shift, called Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE). DANCE combines two novel ideas: First, as we cannot fully rely on source categories to learn features discriminative for the target, we propose a novel neighborhood clustering technique to learn the structure of the target domain in a self-supervised way. Second, we use entropy-based feature alignment and rejection to align target features with the source, or reject them as unknown categories based on their entropy. We show through extensive experiments that DANCE outperforms baselines across open-set, open-partial and partial domain adaptation settings."}, "keywords": ["unknown samples", "whose entropy"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One mainstream method in OSDA is to set a confidence threshold on the entropy of the output of the classifier, assuming the model has more confidence in shared-class samples than unknown samples. Recently, a more universally applicable domain adaptation approach called Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE) has been proposed to handle arbitrary category shift by combining a novel neighborhood clustering technique with entropy-based feature alignment and rejection #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of open set domain adaptation, one novel approach proposed in the literature is Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE), which uses entropy-based feature alignment and rejection to align target features with the source, or reject them as unknown categories based on their entropy. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of separating common samples from private samples in the target domain, one mainstream method in OSDA is to set a confidence threshold on the entropy of the output of the classifier, assuming the model has more confidence in shared-class samples than #REFR unknown samples, which is also proposed in the Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE) method.", "given_citation_intent": "method", "given_keywords": ["unknown samples", "whose entropy"]}]}
{"citing_id": "2303.14259v1", "cited_id": "1806.00680", "section_title": "Introduction", "citation": "Recent research has shown how to leverage modern networks to achieve high throughput and low latency with an RPC-based system #REFR .", "text_before_citation": ["Ordered key-value stores expand the set of supported applications by providing an efficient SCAN operation to retrieve the key-value pairs whose keys are within a specified range.", "For example, distributed file systems can use SCAN to map ranges of logical file offsets to the nodes storing the data #OTHEREFR .", "It is also used to query graph stores #OTHEREFR 50] and the popular Redis [14] offers sorted sets.", "We describe Honeycomb, a system that provides hardware acceleration for an in-memory ordered key-value store.", "There is a large body of research on improving the performance of ordered key-value stores, e.g., #OTHEREFR ."], "text_after_citation": ["Other research has explored using one-sided RDMA reads to bypass the server CPU for GET and SCAN operations #OTHEREFR .", "Since RDMA NICs only provide simple one-sided reads of contiguous memory regions, these systems require at least two RDMA reads per operation when supporting variable-sized keys or values, and they require client-side caching to avoid additional RDMAs when traversing the tree data structures stored by the servers.", "Honeycomb accelerates an ordered key-value store using an FPGA-based SmartNIC #OTHEREFR attached to a host CPU.", "These SmartNICs are widely deployed in data centers #OTHEREFR .", "They enable effective CPU offload by avoiding the functionality limitations of RDMA and the performance problems of SmartNICs based on general-purpose, low-power cores #OTHEREFR ."], "citing_paper_content": {"title": "Honeycomb: Ordered Key-Value Store Acceleration On An Fpga-Based Smartnic", "abstract": "In-memory ordered key-value stores are an important building block in modern distributed applications. We present Honeycomb, a hybrid software-hardware system for accelerating read-dominated workloads on ordered key-value stores that provides linearizability for all operations including scans. Honeycomb stores a B-Tree in host memory, and executes SCAN and GET on an FPGA-based SmartNIC, and PUT, UPDATE and DELETE on the CPU. This approach enables large stores and simplifies the FPGA implementation but raises the challenge of data access and synchronization across the slow PCIe bus. We describe how Honeycomb overcomes this challenge with careful data structure design, caching, request parallelism with out-of-order request execution, wait-free read operations, and batching synchronization between the CPU and the FPGA. For read-heavy YCSB workloads, Honeycomb improves the throughput of a state-of-the-art ordered key-value store by at least 1.8\u00d7. For scan-heavy workloads inspired by cloud storage, Honeycomb improves throughput by more than 2\u00d7. The cost-performance, which is more important for large-scale deployments, is improved by at least 1.5\u00d7 on these workloads. * This work was done when affiliated with Microsoft \u2020 This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible."}, "cited_paper_content": {"title": "Datacenter Rpcs Can Be General And Fast", "abstract": "It is commonly believed that datacenter networking software must sacrifice generality to attain high performance. The popularity of specialized distributed systems designed specifically for niche technologies such as RDMA, lossless networks, FPGAs, and programmable switches testifies to this belief. In this paper, we show that such specialization is not necessary. eRPC is a new general-purpose remote procedure call (RPC) library that offers performance comparable to specialized systems, while running on commodity CPUs in traditional datacenter networks based on either lossy Ethernet or lossless fabrics. eRPC performs well in three key metrics: message rate for small messages; bandwidth for large messages; and scalability to a large number of nodes and CPU cores. It handles packet loss, congestion, and background request execution. In microbenchmarks, one CPU core can handle up to 10 million small RPCs per second, or send large messages at 75 Gbps. We port a production-grade implementation of Raft state machine replication to eRPC without modifying the core Raft source code. We achieve 5.5 \u00b5s of replication latency on lossy Ethernet, which is faster than or comparable to specialized replication systems that use programmable switches, FPGAs, or RDMA."}, "keywords": ["high throughput", "RPC-based system"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "There is a large body of research on improving the performance of ordered key-value stores, e.g., Datacenter RPCs Can Be General and Fast #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of in-memory ordered key-value stores, recent research has explored hardware acceleration, such as the eRPC library, which offers performance comparable to specialized systems while running on commodity CPUs in traditional datacenter networks based on either lossy Ethernet or lossless fabrics #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve high throughput in distributed systems, RPC-based systems have been widely used #REFR.", "given_citation_intent": "background", "given_keywords": ["high throughput", "RPC-based system"]}]}
{"citing_id": "2305.02981v1", "cited_id": "1812.04948", "section_title": "Dataset Preparation", "citation": "We take the original human face FFHQ dataset #REFR and create a set of image masks based on our matting network.", "text_before_citation": ["We utilize Fast Multi-Level Foreground Estimation #OTHEREFR to extract foregrounds.", "These foregrounds are blended (1) with new backgrounds, and the resulting compositions are used as inputs for StyleMatte.", "Additional background images were obtained from the BG-20k dataset #OTHEREFR ."], "text_after_citation": ["First, we filter the dataset to contain images with only one person using an instance segmentation neural network #OTHEREFR .", "The filtration step is obligatory because some visual artifacts arise when skipping it.", "We obtain the results of StyleMatte for the remaining 90% images as additional \u03b1channels.", "We perform an additional filtration step to get Method FID FFHQ AFHQ v2 PSeg #OTHEREFR 62 rid of some masks that are visually inconsistent with the contours of the portrait.", "To do so, we extract segmentation masks s with a basic semantic segmentation network #OTHEREFR pretrained on ResNet50 #OTHEREFR and choose only those that are aligned with the matte."], "citing_paper_content": {"title": "Adversarially-Guided Portrait Matting", "abstract": "We present a method for generating alpha mattes using a limited data source. We pretrain a novel transformerbased model (StyleMatte) on portrait datasets. We utilize this model to provide image-mask pairs for the StyleGAN3based network (StyleMatteGAN). This network is trained unsupervisedly and generates previously unseen imagemask training pairs that are fed back to StyleMatte. We demonstrate that the performance of the matte pulling network improves during this cycle and obtains top results on the used datasets. Furthermore, StyleMatte-GAN provides high-resolution, privacy-preserving portraits with alpha mattes, making it suitable for various image composition tasks. Our code is available at https://github.com/chroneus/stylematte."}, "cited_paper_content": {"title": "A Style-Based Generator Architecture For Generative Adversarial Networks", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces."}, "keywords": ["image masks", "original human face"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We utilize Fast Multi-Level Foreground Estimation #OTHEREFR to extract foregrounds. These foregrounds are blended (1) with new backgrounds, and the resulting compositions are used as inputs for StyleMatte. Additional background images were obtained from the BG-20k dataset #OTHEREFR. The proposed generator architecture for generative adversarial networks in A Style-Based Generator Architecture For Generative Adversarial Networks improves the state-of-the-art in terms of traditional distribution quality metrics and leads to better disentanglement of the latent factors of variation. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide image-mask pairs for the StyleGAN3-based network, we utilized a novel transformer-based model pre-trained on portrait datasets, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize foregrounds extracted by Fast Multi-Level Foreground Estimation #OTHEREFR, which are blended with new backgrounds to create compositions that serve as inputs for StyleMatte #REFR, a method that generates high-quality alpha masks for privacy-preserving portraits of original human faces.", "given_citation_intent": "method", "given_keywords": ["image masks", "original human face"]}]}
{"citing_id": "2304.01950v1", "cited_id": "1703.05175", "section_title": "Because Of The Challenges Of Limitation In Network Bandwidth", "citation": "Inspired by prototypical networks #REFR , which adopts a single prototype to represent each class by calculating the mean of the class's embedding space.", "text_before_citation": ["FedPer #OTHEREFR proposes a strategy of adding a personalized layer to the base layer and suggests updating only the base layer during the federated training process.", "Afterwards, clients can update their personalized layer based on their own local data.", "Additionally, #OTHEREFR explores a benchmark for non-IID settings, they divide non-IID settings into five cases, such as label distribution skew, feature distribution skew, quantity skew, etc.", "Further, as #OTHEREFR mentioned, some existing studies #OTHEREFR - #OTHEREFR , #OTHEREFR cover only one non-IID case, which do not give sufficient evaluations to this challenge.", "Therefore, to avoid the influence of biased global models and to evaluate non-IID cases as comprehensively as possible, we focus on personalized FL by optimizing the local objective of each local client under the label and feature distribution skewness."], "text_after_citation": ["This prototype can serve as an important information carrier to boost the performance of various learning domains, and has been successfully applied in meta-learning #OTHEREFR , multi-task learning #OTHEREFR , and transfer learning #OTHEREFR .", "There have been some existing works #OTHEREFR , #OTHEREFR - #OTHEREFR introducing the concept of prototypes into FL.", "FedProto #OTHEREFR proposes to reduce communication overhead by only exchanging prototypes between clients and the server, instead of exchanging gradients or model parameters.", "FedPCL #OTHEREFR proposes to use multiple pre-trained models to extract the features separately, and then they use a projection network to fuse these extracted features in a personalized way while keeping the shared representation compact for efficient communication.", "These works adopt a single prototype to represent each class and argue that directly averaging the representations from heterogeneous data across clients can effectively capture the embedding representations of each class."], "citing_paper_content": {"title": "Mp-Fedcl: Multi-Prototype Federated Contrastive Learning For Edge Intelligence", "abstract": "Federated learning-assisted edge intelligence enables privacy protection in modern intelligent services. However, not Independent and Identically Distributed (non-IID) distribution among edge clients can impair the local model performance. The existing single prototype-based strategy represents a sample by using the mean of the feature space. However, feature spaces are usually not clustered, and a single prototype may not represent a sample well. Motivated by this, this paper proposes a multiprototype federated contrastive learning approach (MP-FedCL) which demonstrates the effectiveness of using a multi-prototype strategy over a single-prototype under non-IID settings, including both label and feature skewness. Specifically, a multi-prototype computation strategy based on k-means is first proposed to capture different embedding representations for each class space, using multiple prototypes (k centroids) to represent a class in the embedding space. In each global round, the computed multiple prototypes and their respective model parameters are sent to the edge server for aggregation into a global prototype pool, which is then sent back to all clients to guide their local training. Finally, local training for each client minimizes their own supervised learning tasks and learns from shared prototypes in the global prototype pool through supervised contrastive learning, which encourages them to learn knowledge related to their own class from others and reduces the absorption of unrelated knowledge in each global iteration. Experimental results on MNIST, Digit-5, Office-10, and DomainNet show that our method outperforms multiple baselines, with an average test accuracy improvement of about 4.6% and 10.4% under feature and label non-IID distributions, respectively."}, "cited_paper_content": {"title": "Prototypical Networks For Few-Shot Learning", "abstract": "A recent approach to few-shot classification called matching networks has demonstrated the benefits of coupling metric learning with a training procedure that mimics test. This approach relies on a complicated fine-tuning procedure and an attention scheme that forms a distribution over all points in the support set, scaling poorly with its size. We propose a more streamlined approach, prototypical networks, that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points. Our method is competitive with state-of-the-art one-shot classification approaches while being much simpler and more scalable with the size of the support set. We empirically demonstrate the performance of our approach on the Omniglot and mini-ImageNet datasets. We further demonstrate that a similar idea can be used for zero-shot learning, where each class is described by a set of attributes, and achieve state-of-the-art results on the Caltech UCSD bird dataset."}, "keywords": ["single prototype", "prototypical networks"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Motivated by the limitations of the existing single-prototype-based strategy in non-IID settings, this paper proposes a multi-prototype federated contrastive learning approach (MP-FedCL) that uses multiple prototypes to represent a class in the embedding space, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of using a single prototype to represent a sample in non-IID settings, the authors propose a more streamlined approach called prototypical networks, which learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class, rather than individual points (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of the single prototype-based strategy, which may not represent a sample well, prototypical networks #REFR are proposed as a more streamlined approach that learns a metric space in which few-shot classification can be performed by computing Euclidean distances to prototype representations of each class.", "given_citation_intent": "background", "given_keywords": ["single prototype", "prototypical networks"]}]}
{"citing_id": "2304.09026v1", "cited_id": "1910.04032", "section_title": "Fog Data Processing", "citation": "Personally identifiable data can be anonymized using on-device resources before it is further processed by a third party #REFR .", "text_before_citation": ["Fog computing is an abstraction around the combined compute and storage resources at the edge, in the core network, and in the cloud #OTHEREFR .", "Fog computing is a promising paradigm for emerging domains such as the IoT: By extending cloud resources close to the edge of the network, where data is generated, fog applications can benefit from low-latency, high-bandwidth, privacy-preserving data processing infrastructure #OTHEREFR .", "For example, IoT sensor data can be processed by compute resources at a local radio gateway and sent to actuators at the edge without incurring a significant network delay #OTHEREFR .", "Similarly, data from multiple sensors can be aggregated and filtered at the edge in order to limit the network strain of sending all data to the cloud."], "text_after_citation": ["The downside of geo-distributed, heterogeneous fog infrastructure is the complexity of its management for applications, i.e., building approaches for distributing data, deploying services, and managing fault-tolerance #OTHEREFR .", "Researchers have proposed abstractions in the form of fog data processing platforms that manage this complexity for applications.", "For example, NebulaStream #OTHEREFR is an end-to-end IoT data management system.", "A key novelty of NebulaStream is its ability to autonomously deploy different data processing approaches such as stream and complex event processing in a geo-distributed environment.", "SoFA #OTHEREFR uses Apache Spark to combine all available fog resources for stream processing operator deployment."], "citing_paper_content": {"title": "Towards A Benchmark For Fog Data Processing", "abstract": "Fog data processing systems provide key abstractions to manage data and event processing in the geo-distributed and heterogeneous fog environment. The lack of standardized benchmarks for such systems, however, hinders their development and deployment, as different approaches cannot be compared quantitatively. Existing cloud data benchmarks are inadequate for fog computing, as their focus on workload specification ignores the tight integration of application and infrastructure inherent in fog computing. In this paper, we outline an approach to a fog-native data processing benchmark that combines workload specifications with infrastructure specifications. This holistic approach allows researchers and engineers to quantify how a software approach performs for a given workload on given infrastructure. Further, by basing our benchmark in a realistic IoT sensor network scenario, we can combine paradigms such as low-latency event processing, machine learning inference, and offline data analytics, and analyze the performance impact of their interplay in a fog data processing system."}, "cited_paper_content": {"title": "Fog Computing As Privacy Enabler", "abstract": "Despite broad discussions on privacy challenges arising from fog computing, the authors argue that privacy and security requirements might actually drive the adoption of fog computing. They present four patterns of fog computing fostering data privacy and the security of business secrets. Their practical application is illuminated on the basis of three case studies."}, "keywords": ["identifiable data"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Fog computing can foster data privacy and security of business secrets, which are crucial requirements for the emerging IoT domain #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors present four patterns of fog computing that foster data privacy and security of business secrets, which could be useful for fog-native data processing benchmarking #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors argue that fog computing can foster data privacy and security of business secrets, which can drive the adoption of fog computing in the IoT, where identifiable data needs to be processed close to the edge of the network #REFR.", "given_citation_intent": "method", "given_keywords": ["identifiable data"]}]}
{"citing_id": "2303.13371v1", "cited_id": "1909.05506", "section_title": "C. Properties Of Rcr And Rar", "citation": "Cross-Modal Adaptive Message Passing (CAMP) #REFR explores a region-word affinity matrix via inner product and transfers cross-modality contents to improve the region and word representations, which are then aggregated as the holistic image and text features to compute the final similarity.", "text_before_citation": ["To demonstrate their great applicability, we apply these two regulators to many existing methods based on cross-modal interaction:", "Stacked Cross Attention (SCAN) #OTHEREFR first computes all region-word similarities and aligns each region/word with its corresponding words/regions.", "The final similarity is obtained by averaging all region/word-based cosine distances.", "Bidirectional Focal Attention (BFAN) #OTHEREFR extends the generic attention by reassigning more fine-grained attention weight for each region-word pair and calculates the matching result by summing up region-based and word-based scores.", "Position Focused Attention (PFAN) #OTHEREFR enhances region features by introducing extra position information to promote region-word correspondences and integrates all region/wordattended cosine similarities as the prediction."], "text_after_citation": ["Similarity Graph Reasoning and Attention Filtration (SGRAF) #OTHEREFR adopts cosine similarities multiplied with a fixed temperature as region-word attention weights, followed by the complex graph and attention modules to map hierarchical similarity features into a matching score. Fig.", "4 illustrates how we plug the RCR or RAR into the above matching approaches.", "Specifically, cross-modal attention utilizes the cosine metric or inner product as region-word affinity weights, and outputs each region/word along with its related words/regions.", "With these paired features, the RCR first constructs the alignment vectors and then learns the corresponding weight vectors and temperature factors via Eq.", "(7)- #OTHEREFR , which in turn refine the region-word feature distances and optimize the cross-modal interaction via Eq. (9)- #OTHEREFR ."], "citing_paper_content": {"title": "Plug-And-Play Regulators For Image-Text Matching", "abstract": "Exploiting fine-grained correspondence and visualsemantic alignments has shown great potential in image-text matching. Generally, recent approaches first employ a crossmodal attention unit to capture latent region-word interactions, and then integrate all the alignments to obtain the final similarity. However, most of them adopt one-time forward association or aggregation strategies with complex architectures or additional information, while ignoring the regulation ability of network feedback. In this paper, we develop two simple but quite effective regulators which efficiently encode the message output to automatically contextualize and aggregate cross-modal representations. Specifically, we propose (i) a Recurrent Correspondence Regulator (RCR) which facilitates the cross-modal attention unit progressively with adaptive attention factors to capture more flexible correspondence, and (ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation weights repeatedly to increasingly emphasize important alignments and dilute unimportant ones. Besides, it is interesting that RCR and RAR are \"plug-and-play\": both of them can be incorporated into many frameworks based on cross-modal interaction to obtain significant benefits, and their cooperation achieves further improvements. Extensive experiments on MSCOCO and Flickr30K datasets validate that they can bring an impressive and consistent R@1 gain on multiple models, confirming the general effectiveness and generalization ability of the proposed methods."}, "cited_paper_content": {"title": "Camp: Cross-Modal Adaptive Message Passing For Text-Image Retrieval", "abstract": "Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach."}, "keywords": ["cross-modality contents", "Cross-Modal Adaptive Message"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To demonstrate their great applicability, we apply these two regulators to many existing methods based on cross-modal interaction: Stacked Cross Attention (SCAN) #REFR, Bidirectional Focal Attention (BFAN) #OTHEREFR, and Position Focused Attention (PFAN) #OTHEREFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To further improve the performance of cross-modal interaction methods, the authors of this manuscript incorporate the Cross-modal Adaptive Message Passing (CAMP) method proposed in #REFR, which adaptively controls the information flow for message passing across modalities and takes comprehensive and fine-grained cross-modal interactions into account.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To capture fine-grained cross-modality contents, the authors incorporate Cross-Modal Adaptive Message Passing (CAMP) from #REFR into their proposed Recurrent Correspondence Regulator (RCR) and Recurrent Aggregation Regulator (RAR) for image-text matching.", "given_citation_intent": "method", "given_keywords": ["cross-modality contents", "Cross-Modal Adaptive Message"]}]}
{"citing_id": "2303.00923v1", "cited_id": "1602.01585", "section_title": "B Baseline Systems", "citation": "They design a regression model using BERT-based features extracted from review texts, star rating, and product type information from Amazon product review dataset #REFR .", "text_before_citation": ["These features are fed into conventional classifiers such as SVM, Random Forest, and gradient boosting to identify helpful reviews.", "\u2022 TextCNN #OTHEREFR employs a textbased CNN model #OTHEREFR to automatically capture the character-level, word-level, and topic-level features for helpfulness prediction.", "\u2022 MTNL (Fan et al., 2018) utilizes end-to-end multi-task neural learning (MTNL) architecture for classifying helpful reviews.", "They take the help of an auxiliary task, such as rating regression, to boost the performance of the original task, which is review helpfulness identification.", "\u2022 BERTHelp #OTHEREFR develop their helpfulness prediction model using pre-trained BERT #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "On The Role Of Reviewer Expertise In Temporal Review Helpfulness Prediction", "abstract": "Helpful reviews have been essential for the success of e-commerce services, as they help customers make quick purchase decisions and benefit the merchants in their sales. While many reviews are informative, others provide little value and may contain spam, excessive appraisal, or unexpected biases. With the large volume of reviews and their uneven quality, the problem of detecting helpful reviews has drawn much attention lately. Existing methods for identifying helpful reviews primarily focus on review text and ignore the two key factors of (1) who post the reviews and (2) when the reviews are posted. Moreover, the helpfulness votes suffer from scarcity for less popular products and recently submitted (a.k.a., coldstart) reviews. To address these challenges, we introduce a dataset and develop a model that integrates the reviewer's expertise, derived from the past review history of the reviewers, and the temporal dynamics of the reviews to automatically assess review helpfulness. We conduct experiments on our dataset to demonstrate the effectiveness of incorporating these factors and report improved results compared to several well-established baselines."}, "cited_paper_content": {"title": "Ups And Downs: Modeling The Visual Evolution Of Fashion Trends With One-Class Collaborative Filtering", "abstract": "Building a successful recommender system depends on understanding both the dimensions of people's preferences as well as their dynamics. In certain domains, such as fashion, modeling such preferences can be incredibly difficult, due to the need to simultaneously model the visual appearance of products as well as their evolution over time. The subtle semantics and non-linear dynamics of fashion evolution raise unique challenges especially considering the sparsity and large scale of the underlying datasets. In this paper we build novel models for the One-Class Collaborative Filtering setting, where our goal is to estimate users' fashion-aware personalized ranking functions based on their past feedback. To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community. Experimentally we evaluate our method on two large real-world datasets from Amazon.com, where we show it to outperform state-of-the-art personalized ranking measures, and also use it to visualize the high-level fashion trends across the 11-year span of our dataset."}, "keywords": ["review texts", "dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community (Ups And Downs: Modeling The Visual Evolution Of Fashion Trends With One-Class Collaborative Filtering).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To uncover the complex and evolving visual factors that people consider when evaluating products, our method combines high-level visual features extracted from a deep convolutional neural network, users' past feedback, as well as evolving trends within the community (Ups And Downs: Modeling The Visual Evolution Of Fashion Trends With One-Class Collaborative Filtering). #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of detecting helpful reviews, the authors of this manuscript incorporate the reviewer's expertise and the temporal dynamics of the reviews, and develop a model that assesses review helpfulness using a dataset, as proposed in the reference paper \"Ups And Downs: Modeling The Visual Evolution Of Fashion Trends With One-Class Collaborative Filtering\" (#REFR).", "given_citation_intent": "method", "given_keywords": ["review texts", "dataset"]}]}
{"citing_id": "2303.13332v1", "cited_id": "1811.02840", "section_title": "Introduction", "citation": "Tellez et al., #REFR showed in a benchmark study that VAE compression of medical tissue images to a latent space of 128 (>5000:1 compression ratio) retained the most details of the original whole slide image compared to 4 other encoders.", "text_before_citation": ["Compression and scaling has also been found to adversely effect tissue segmentation up to ratios of 50:1 #OTHEREFR .", "In contrast to discrete cosine transformation models, neural networks have been proven to retain high efficiency and fidelity in the lossy compression of image data #OTHEREFR .", "While neural networks seek to store image data in latent space representations, not every network does this at equivalent efficiency or accuracy #OTHEREFR .", "For better visualization, early stopping is not used for these experiments.", "coders (VAEs) retain higher image quality and lower noise ratios at extreme compression ratios #OTHEREFR ."], "text_after_citation": ["In the current study, we develop a VAE to compress and index images in latent space for fast complex search of whole slide H&E cancer images."], "citing_paper_content": {"title": "Clinically Relevant Latent Space Embedding Of Cancer Histopathology Slides Through Variational Autoencoder Based Image Compression", "abstract": "In this paper, we introduce a Variational Autoencoder (VAE) based training approach that can compress and decompress cancer pathology slides at a compression ratio of 1:512, which is better than the previously reported state of the art (SOTA) in the literature, while still maintaining accuracy in clinical validation tasks. The compression approach was tested on more common computer vision datasets such as CIFAR10, and we explore which image characteristics enable this compression ratio on cancer imaging data but not generic images. We generate and visualize embeddings from the compressed latent space and demonstrate how they are useful for clinical interpretation of data, and how in the future such latent embeddings can be used to accelerate search of clinical imaging data."}, "cited_paper_content": {"title": "Neural Image Compression For Gigapixel Histopathology Image Analysis", "abstract": "We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets. We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts."}, "keywords": ["medical tissue images"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In contrast to discrete cosine transformation models, the Neural Image Compression (NIC) method proposed by the authors in #REFR retains high-level information while suppressing pixel-level noise, making it an efficient and accurate approach for lossy compression of gigapixel histopathology images.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In their study, the authors of #REFR demonstrated that their Neural Image Compression (NIC) method successfully integrates both global and local visual information, which could potentially address the issue of tissue segmentation loss caused by compression and scaling.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Neural Image Compression (NIC) method in #REFR successfully integrates global and local visual information for medical tissue images, avoiding the need for fine-grained manual annotations and achieving high efficiency and fidelity in lossy compression.", "given_citation_intent": "result", "given_keywords": ["medical tissue images"]}]}
{"citing_id": "2303.03770v3", "cited_id": "1602.07868", "section_title": "Experimental Setup", "citation": "For experiments on PACS and VisDA-C, we also apply WeightNorm #REFR on the classifier.", "text_before_citation": ["Following #OTHEREFR , we use a subset of it that contains 126 classes from 4 domains (Real, Sketch, Clipart, Painting) and we refer to it as DomainNet-126.", "We evaluate 7 domain shifts built from the 4 domains and we report the top-1 accuracy under each domain shift as well as their average (Avg.). Implementation details.", "We use standard classification architectures comprising a feature extractor followed by a Single-Source UDA Method SF P \u2192 A P \u2192 C P \u2192 S A \u2192 P A \u2192 C A \u2192 S Avg.", "NEL #OTHEREFR Table) .", "Following SHOT #OTHEREFR , we add an extra 256-dimensional fully-connected+BatchNorm bottleneck after the encoder output."], "text_after_citation": ["For source training, we initialise the ResNet backbone with ImageNet-1K #OTHEREFR pre-trained weights available in the Pytorch model zoo.", "We train the source model with the standard cross-entropy loss and with label-smoothing like in #OTHEREFR .", "For the adaptation phase, the target model is initialised with the source model's parameters.", "For more details, the code that is available at https://github.com/MattiaLitrico/Guiding-Pseudo-labels-with-Uncertainty-Estimation-for-Sourcefree-Unsupervised-Domain-Adaptation."], "citing_paper_content": {"title": "Guiding Pseudo-Labels With Uncertainty Estimation For Source-Free Unsupervised Domain Adaptation", "abstract": "Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate the Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples sharing the same class, even in presence of some noise in the pseudo-labels. Our method outperforms previous methods on three major benchmarks by a large margin. We set the new SF-UDA state-of-the-art on VisDA-C and DomainNet with a performance gain of +1.8% on both benchmarks and on PACS with +12.3% in the single-source setting and +6.6% in multi-target adaptation. Additional analyses demonstrate that the proposed approach is robust to the noise, which results in significantly more accurate pseudo-labels compared to state-of-the-art approaches."}, "cited_paper_content": {"title": "Weight Normalization: A Simple Reparameterization To Accelerate Training Of Deep Neural Networks", "abstract": "We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning."}, "keywords": ["classifier", "WeightNorm"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Following the Single-Source UDA Method SF P \u2192 A P \u2192 C P \u2192 S A \u2192 P A \u2192 C A \u2192 S Avg. NEL #OTHEREFR Table), we add an extra 256-dimensional fully-connected+BatchNorm bottleneck after the encoder output, inspired by the weight normalization technique proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Following SHOT #OTHEREFR, an extra 256-dimensional fully-connected+BatchNorm bottleneck was added after the encoder output, and the weights were normalized using the weight normalization method proposed in the #REFR paper.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We use standard classification architectures comprising a feature extractor followed by a Single-Source UDA Method SF P \u2192 A P \u2192 C P \u2192 S A \u2192 P A \u2192 C A \u2192 S Avg. NEL #OTHEREFR Table) and an extra 256-dimensional fully-connected+BatchNorm bottleneck after the encoder output, inspired by WeightNorm #REFR.", "given_citation_intent": "method", "given_keywords": ["classifier", "WeightNorm"]}]}
{"citing_id": "2303.09660v1", "cited_id": "1810.03292", "section_title": "Literature Review", "citation": "Gradientbased methods may generate the same results using an untrained model as those from a trained model, indicating that the method demonstrates only general model characteristics #REFR .", "text_before_citation": ["However, as inferred from the algorithm strategy, the computation of integrated gradients requires repeated calculation over multiple iterations. To reduce computational cost, #OTHEREFR", "(2021) proposed a special class of CNNs to compute the result of integrated gradients with only one forward-backward pass.", "As a result, the saliency map itself can be used as an effective tool; for example, it can be integrated into regular training as priors #OTHEREFR .", "Each of the discussed methods has its own strengths and weaknesses.", "For example, the perturbation-based method may underestimate the importance of features because other features already saturate the output #OTHEREFR ."], "text_after_citation": ["Although CAM methods can generate a saliency map by passing through fewer non-linear layers and potentially suffer from fewer issues, they may predict inaccurate object locations due to the coarse resolution of the saliency map.", "In this paper, we explore the characteristics of different \"feature attribution-based\" model explanation approaches and use two GeoAI-ready natural feature datasets in an image classification task to compare model-learned features with human-understandable features.", "In addition to interpreting a model's reasoning process, we compare the strengths and weaknesses of these popular approaches and discuss ways to improve them within the growing field of explainable GeoAI."], "citing_paper_content": {"title": "Explainable Geoai: Can Saliency Maps Help Interpret Artificial Intelligence'S Learning Process? An Empirical Study On Natural Feature Detection", "abstract": "Improving the interpretability of geospatial artificial intelligence (GeoAI) models has become critically important to open the \"black box\" of complex AI models, such as deep learning. This paper compares popular saliency map generation techniques and their strengths and weaknesses in interpreting GeoAI and deep learning models' reasoning behaviors, particularly when applied to geospatial analysis and image processing tasks. We surveyed two broad classes of model explanation methods: perturbation-based and gradient-based methods. The former identifies important image areas, which help machines make predictions by modifying a localized area of the input image. The latter evaluates the contribution of every single pixel of the input image to the model's prediction results through gradient backpropagation. In this study, three algorithms-the occlusion method, the integrated gradients method, and the class activation map method-are examined for a natural feature detection task using deep learning. The algorithms' strengths and weaknesses are discussed, and the consistency between model-learned and human-understandable concepts for object recognition is also compared. The experiments used two GeoAIready datasets to demonstrate the generalizability of the research findings."}, "cited_paper_content": {"title": "Sanity Checks For Saliency Maps", "abstract": "Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings."}, "keywords": ["Gradientbased methods", "trained model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To reduce computational cost, #OTHEREFR (2021) proposed a special class of CNNs to compute the result of integrated gradients with only one forward-backward pass. As a result, the saliency map itself can be used as an effective tool; for example, it can be integrated into regular training as priors #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The computation of saliency maps can be integrated into regular training as priors, which has been proposed by #OTHEREFR (2021) and can be an effective tool to reduce computational cost.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Gradient-based methods, such as the integrated gradients method, evaluate the contribution of every single pixel of the input image to the model's prediction results, and have been extensively used in saliency map generation techniques #REFR.", "given_citation_intent": "method", "given_keywords": ["Gradientbased methods", "trained model"]}]}
{"citing_id": "2304.11118v1", "cited_id": "1711.05101", "section_title": "Experiments", "citation": "We use AdamW optimizer #REFR with a learning rate of 1e \u2212 4, batch size of 256, without weight decay.", "text_before_citation": ["During training, we use \u03bb vlb = 1.0, and define t to vary between [1, T ], where T = 1000 corresponds to a pure Gaussian distribution.", "At inference, we start from We report the results after retraining AvatarPoser, and report the same results as in #OTHEREFR for methods with a star (*).", "pure Gaussian noise, and we use DDIM sampling [47] with 50 steps.", "We set the variance \u03a3 \u03b8 of the reverse noise to zero.", "This configuration turns the model into a deterministic mapping from Gaussian noise to motions, allowing it to do much fewer denoising steps without degrading the quality of synthesized motions."], "text_after_citation": ["Our model has 22M parameters and is trained for 1.5 days on four NVIDIA Quadro RTX 8000. More implementation details are in the Supplementary Material (Sec. A.1).", "Our approach has no limitations concerning the length of the generated sequences.", "We can synthesize motions of arbitrary length by applying BoDiffusion in an autoregressive manner using a sliding window over the input data.", "We refer the reader to the Supplementary Material for more explanation of our inference-time protocol (see Sec. A.2)."], "citing_paper_content": {"title": "Bodiffusion: Diffusing Sparse Observations For Full-Body Human Motion Synthesis", "abstract": "Mixed reality applications require tracking the user's fullbody motion to enable an immersive experience. However, typical head-mounted devices can only track head and hand movements, leading to a limited reconstruction of full-body motion due to variability in lower body configurations. We propose BoDiffusion-a generative diffusion model for motion synthesis to tackle this under-constrained reconstruction problem. We present a time and space conditioning scheme that allows BoDiffusion to leverage sparse tracking inputs while generating smooth and realistic full-body motion sequences. To the best of our knowledge, this is the first approach that uses the reverse diffusion process to model full-body tracking as a conditional sequence generation task. We conduct experiments on the large-scale motion-capture dataset AMASS and show that our approach outperforms the state-of-the-art approaches by a significant margin in terms of full-body motion realism and joint reconstruction error."}, "cited_paper_content": {"title": "Fixing Weight Decay Regularization In Adam", "abstract": "We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW"}, "keywords": ["weight decay", "AdamW optimizer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We also use a normalized variant of weight decay to reduce the dependence on longer optimization runs, as proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the under-constrained reconstruction problem, BoDiffusion leverages sparse tracking inputs and generates smooth and realistic full-body motion sequences using a generative diffusion model, as proposed in the reference paper \"Fixing Weight Decay Regularization In Adam\" (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the weight decay limitation in the Adam optimizer, the authors proposed a simple modification that decouples weight decay and optimization steps, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["weight decay", "AdamW optimizer"]}]}
{"citing_id": "2303.16521v1", "cited_id": "1702.08720", "section_title": "Datasets And Metrics", "citation": "However, entropy maximization occasionally also reaches the state with all points in the same cluster (this is consistent with previous literature, e.g., #REFR ).", "text_before_citation": ["Our method significantly outperforms others on all datasets and metrics.", "Observe that the unregularized model exhibits total collapse in all experiments, placing all points in the same cluster and consequently achieving a cluster performance no better than random guessing.", "In many of our experiments, the performance of SS is not much better.", "By making a slight change to the author's original method, we could actually significantly improve its results (see appendix), but it was still unreliable and less accurate than our method.", "The other two existing partition support methods do a reasonable job of avoiding partition collapse."], "text_after_citation": ["The Sinkhorn-Knopp method is more reliable, but by far the most uniform cluster sizes are produced by our method.", "Note that we do not employ our assignment algorithm at inference time, instead we just assign each point to the cluster with the nearest centroid.", "This shows that our cluster centroids are well-distributed around the data manifold, each capturing a sizeable subset of the data even when the explicit support is removed.", "Together, these figures show that (a) some form of partition support is necessary to learn anything meaningful, (b) our method of combination assignment is better at avoiding partition Table 1 : Effect, on cluster size and clustering performance, of our method compared to two existing methods of preventing partition collapse.", "\"CA\" refers to our method of combination assignment, \"SK\" refers to Sinkhorn-Knopp regularization, as proposed by #OTHEREFR and #OTHEREFR , \"Ent\" refers to entropy maximization, as used by #OTHEREFR , \"SS\" is the sum of squares minimization proposed in #OTHEREFR and others, and \"No-Reg\" is the model without any partition support component."], "citing_paper_content": {"title": "Hard Regularization To Prevent Deep Clustering Collapse Without Data Augmentation", "abstract": "Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, it consistently avoids collapse more robustly than other methods and leads to more accurate clustering. We also conduct further experiments and analyses justifying our choice to regularize the hard cluster assignments."}, "cited_paper_content": {"title": "Learning Discrete Representations Via Information Maximizing Self Augmented Training", "abstract": "Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invari-ance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning."}, "keywords": ["cluster", "entropy maximization"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our method significantly outperforms others on all datasets and metrics. Observe that the unregularized model exhibits total collapse in all experiments, placing all points in the same cluster and consequently achieving a cluster performance no better than random guessing. In many of our experiments, the performance of SS is not much better. By making a slight change to the author's original method, we could actually significantly improve its results (see appendix), but it was still unreliable and less accurate than our method. The other two existing partition support methods do a reasonable job of avoiding partition collapse. We propose a method called Information Maximizing Self-Augmented Training (IMSAT) that can be used to learn discrete representations of data without collapsing. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our method significantly outperforms others on all datasets and metrics, achieving state-of-the-art results for both clustering and unsupervised hash learning #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in our study achieves state-of-the-art results for both clustering and unsupervised hash learning, by using data augmentation to impose invariance on discrete representations and maximizing the information-theoretic dependency between data and their predicted discrete representations, as described in #REFR.", "given_citation_intent": "result", "given_keywords": ["cluster", "entropy maximization"]}]}
{"citing_id": "2303.06545v1", "cited_id": "1409.0473", "section_title": "Positive Moment Estimation", "citation": "We use an attentionbased Recurrent Neural Network #REFR as the language generator \u03a6(\u2022), which utilizes these features to reconstruct nouns and verbs of one query.", "text_before_citation": ["Researchers #OTHEREFR have observed that neural networks will fit correct labels (informative labels) much faster than wrong labels (uninformative labels).", "Thus, from a perspective of algorithm optimization, networks will predict unobserved positive moments more readily than negative ones when the number of positives is constrained #OTHEREFR .", "Semantic reconstruction To better uncover potential positive moments, we build a lightweight semanticreconstruction model that reconstructs semantic information of queries from proposal features.", "Based on a single positive label, a sequential video embedding within its interval is first extracted from the video embedding V.", "Then, a random weighted averaging operation is applied to augment N s fixed-dimensional features {f v i } Ns i=1 ."], "text_after_citation": ["In the optimization phase, a standard captioning loss is used to maximize the normalized log-likelihood of the correct words,", "EQUATION", "where x l denotes a noun/verb of one query and T s is total time step of the recurrent model.", "Intuitively, positive moments can better reconstruct the semantic information of a query.", "In the estimation phase, we leverage this recurrent model to generate words for both multi-scale proposals and the labeled moment, and then compare semantic similarities between them."], "citing_paper_content": {"title": "Towards Diverse Temporal Grounding Under Single Positive Labels", "abstract": "Temporal grounding aims to retrieve moments of the described event within an untrimmed video by a language query. Typically, existing methods assume annotations are precise and unique, yet one query may describe multiple moments in many cases. Hence, simply taking it as a one-vs-one mapping task and striving to match single-label annotations will inevitably introduce false negatives during optimization. In this study, we reformulate this task as a one-vs-many optimization problem under the condition of single positive labels. The unlabeled moments are considered unobserved rather than negative, and we explore mining potential positive moments to assist in multiple moment retrieval. In this setting, we propose a novel Diverse Temporal Grounding framework, termed DTG-SPL, which mainly consists of a positive moment estimation (PME) module and a diverse moment regression (DMR) module. PME leverages semantic reconstruction information and an expected positive regularization to uncover potential positive moments in an online fashion. Under the supervision of these pseudo positives, DMR is able to localize diverse moments in parallel that meet different users. The entire framework allows for end-toend optimization as well as fast inference. Extensive experiments on Charades-STA and ActivityNet Captions show that our method achieves superior performance in terms of both single-label and multi-label metrics."}, "cited_paper_content": {"title": "Neural Machine Translation By Jointly Learning To Align And Translate", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."}, "keywords": ["attentionbased Recurrent Neural"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To better uncover potential positive moments, researchers have proposed a lightweight semantic reconstruction model that reconstructs semantic information of queries from proposal features #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To uncover potential positive moments, the authors propose a positive moment estimation (PME) module that leverages semantic reconstruction information and an expected positive regularization, which is inspired by the soft-search mechanism in the neural machine translation model proposed in the paper titled \"Neural Machine Translation By Jointly Learning To Align And Translate\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the bottleneck of fixed-length vectors in the basic encoder-decoder architecture, an attention-based Recurrent Neural Network model was proposed in the reference paper #REFR, which allows the model to automatically search for relevant parts of a source sentence to predict a target word, achieving a comparable translation performance to the existing state-of-the-art phrase-based system.", "given_citation_intent": "method", "given_keywords": ["attentionbased Recurrent Neural"]}]}
{"citing_id": "2303.12086v1", "cited_id": "2002.11450", "section_title": "Settings", "citation": "We have implemented the LL simulator for IEEE 802.11p and LTE C-V2X in Python in our previous work where more simulator details can be found in #REFR .", "text_before_citation": ["For the link simulation, it's necessary to build the entire transmission and receiving operations."], "text_after_citation": ["The simulation pipeline is the same as the Section.III.B in #OTHEREFR . In Fig. 3 , the simulation processing procedure is shown. Here we give short introductions about each processing step.", "1) Control channel processing: In this step, a new SCI message is created which includes the MCS value, resource indication value, group destination identity, etc.", "The created SCI is a binary message which is encoded by using a convolutional encoder followed by rate matching, interleaving, and a 16-bit Cyclic Redundancy Check (CRC) attached to the encoded message.", "Once we have the binary codes, the next step is to process PSCCH scrambling.", "There are 240 PSCCHgenerated symbols and are cyclically shifted with a random value chosen from the set [0, 3, 6, 9] to reduce the effect of interference."], "citing_paper_content": {"title": "Effect Of Variable Physical Numerologies On Link-Level Performance Of 5G Nr V2X", "abstract": "With technology and societal development, the 5th generation wireless communication (5G) contributes significantly to different societies like industries or academies. Vehicle-to-Everything (V2X) communication technology has been one of the leading services for 5G which has been applied in vehicles. It's used to exchange their status information with other traffic and traffic participants to increase traffic safety and efficiency. Cellular-V2X (C-V2X) is one of the emerging technologies to enable V2X communications. The first Long-Term Evolution (LTE) based C-V2X was released on the 3rd Generation Partnership Project (3GPP) standard. 3GPP is working towards the development of New Radio (NR) systems that it's called 5G NR V2X. One single numerology in LTE cannot satisfy most performance requirements because of the variety of deployment options and scenarios. For this reason, in order to meet the diverse requirements, the 5G NR Physical Layer (PHY) is designed to provide a highly flexible framework. Scalable Orthogonal Frequency-Division Multiplexing (OFDM) numerologies make flexibility possible. The term numerology refers to the PHY waveform parametrization and allows different Subcarrier Spacings (SCSs), symbols, and slot duration. This paper implements the Link-Level (LL) simulations of LTE C-V2X communication and 5G NR V2X communication where simulation results are used to compare similarities and differences between LTE and 5G NR. We detect the effect of variable PHY Numerologies of 5G NR on the LL performance of V2X. The simulation results show that the performance of 5G NR improved by using variable numerologies."}, "cited_paper_content": {"title": "Link Level Performance Comparison Of C-V2X And Its-G5 For Vehicular Channel Models", "abstract": "V2X communications plays a significant role in increasing traffic safety and efficiency by enabling vehicles to exchange their status information with other vehicles and traffic entities in their proximity. In this regard, two technologies emerged as the main contenders for enabling V2X communications which have stringent requirements in terms of latency and reliability due to their apparent safety criticality. The first one is the DSRC standard (referred to as ITS-G5 in Europe) that is well researched since 20 years and has attained enough technical maturity for current deployment. The second one is the relatively new CV2X standard that is nevertheless, based on the 3GPP standard family that have successful deployments in almost every corner of the globe. In this work, we compare the link level performance of the PHY protocols for both the technologies for different vehicular fading channel models. To this end, we construct and simulate the PHY pipelines and show the performance results by means of BLER} versus SNR graphs. Our investigations show that CV2X performs better than ITS-G5 for almost all the considered channel models due to better channel coding and estimation schemes."}, "keywords": ["LTE C-V2X"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the link simulation, it's necessary to build the entire transmission and receiving operations. The authors have compared the link level performance of the PHY protocols for both ITS-G5 and C-V2X for different vehicular fading channel models, and their investigations show that CV2X performs better than ITS-G5 for almost all the considered channel models due to better channel coding and estimation schemes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To build the entire transmission and receiving operations for the link simulation, the authors cited the reference paper on the link level performance comparison of C-V2X and ITS-G5 for vehicular channel models #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To build the entire transmission and receiving operations for the link simulation of LTE C-V2X communication, the authors cited the performance comparison of C-V2X and ITS-G5 for vehicular channel models in their previous work #REFR.", "given_citation_intent": "method", "given_keywords": ["LTE C-V2X"]}]}
{"citing_id": "2303.08714v1", "cited_id": "1505.04597", "section_title": "Hf-Guided Ca", "citation": "In the original U-net architecture, the encoder features are directly concatenated with the features obtained by the decoder #REFR .", "text_before_citation": [], "text_after_citation": ["This fusion facilitates the network to integrate the higher and lower-layer features effectively but lacks the ability to extract high-frequency features.", "To tackle this issue, we introduce a High-Frequency feature guided Cross-Attention mechanism (HF-guided CA) to recover fine-grained high-frequency details.", "The flow of the HF-guided CA is illustrated in Fig.5 .", "We utilize the pre-trained CNN prediction by extracting the\u0124 i ,V i , andD i coefficients at the i-th level of the DWT.", "By adding these extracted coefficients with a linear projection, we obtain the feature map Q with aggregated highfrequency information:"], "citing_paper_content": {"title": "Resdiff: Combining Cnn And Diffusion Model For Image Super-Resolution", "abstract": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is wasteful, given that a simple Convolutional Neural Network (CNN) can recover the main low-frequency content. Therefore, we present ResDiff, a novel Diffusion Probabilistic Model based on Residual structure for Single Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN, which restores primary low-frequency components, and a DPM, which predicts the residual between the ground-truth image and the CNNpredicted image. In contrast to the common diffusion-based methods that directly use LR images to guide the noise towards HR space, ResDiff utilizes the CNN's initial prediction to direct the noise towards the residual space between HR space and CNN-predicted space, which not only accelerates the generation process but also acquires superior sample quality. Additionally, a frequency-domain-based loss function for CNN is introduced to facilitate its restoration, and a frequency-domain guided diffusion is designed for DPM on behalf of predicting high-frequency details. The extensive experiments on multiple benchmark datasets demonstrate that ResDiff outperforms previous diffusionbased methods in terms of shorter model convergence time, superior generation quality, and more diverse samples."}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["encoder features", "original U-net architecture"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors utilized a contracting and symmetric expanding path network architecture for image segmentation, which was trained end-to-end from very few images and outperformed the prior best method. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To predict high-frequency details, ResDiff utilizes a frequency-domain guided diffusion method, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To capture context and enable precise localization, ResDiff utilizes a novel Diffusion Probabilistic Model based on Residual structure for Single Image Super-Resolution (SISR) that combines a Convolutional Neural Network (CNN) to restore primary low-frequency components and a symmetric expanding path, similar to the original U-Net architecture, to predict the residual between the ground-truth image and the CNN-predicted image #REFR.", "given_citation_intent": "method", "given_keywords": ["encoder features", "original U-net architecture"]}]}
{"citing_id": "2304.02886v1", "cited_id": "1904.03323", "section_title": "C. Comparison With Other Works", "citation": "It is difficult to compare the results, since these works do not use the same evaluation dataset and English works can benefit from specialized models such as ClinicalBERT #REFR .", "text_before_citation": ["The Table IV shows the model with the highest F 1 -score of this paper with the results of previous work on ICD-10 code association."], "text_after_citation": ["For French baseline, we implemented and trained the model proposed in #OTHEREFR on ICD-10-HNFC dataset. The result is shown in parallel with our proposal.", "Our model clearly outperforms the classification method used in #OTHEREFR .", "On the same validation dataset, with class reduction (1564 labels) the F 1 -score goes from 0.35 obtained with the model proposed in #OTHEREFR to 0.55 with our proposal, i.e. an improvement of 57%.", "With the raw codes (6161 labels), the F 1 -score goes from 0.27 to 0.45, i.e. an improvement of 66.6%.", "The difference in scores with the results of PLM-ICD can be explained by the use of a context specific (medical) Transformers which has a vocabulary more adapted to the content of the documents."], "citing_paper_content": {"title": "Automatic Icd-10 Code Association: A Challenging Task On French Clinical Texts", "abstract": "Automatically associating ICD codes with electronic health data is a well-known NLP task in medical research. NLP has evolved significantly in recent years with the emergence of pre-trained language models based on Transformers architecture, mainly in the English language. This paper adapts these models to automatically associate the ICD codes. Several neural network architectures have been experimented with to address the challenges of dealing with a large set of both input tokens and labels to be guessed. In this paper, we propose a model that combines the latest advances in NLP and multi-label classification for ICD-10 code association. Fair experiments on a Clinical dataset in the French language show that our approach increases the F1-score metric by more than 55% compared to state-of-the-art results."}, "cited_paper_content": {"title": "Publicly Available Clinical Bert Embeddings", "abstract": "Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text."}, "keywords": ["ClinicalBERT"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The Table IV shows the model with the highest F 1 -score of this paper with the results of previous work on ICD-10 code association, using domain-specific BERT models for clinical text #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The model proposed in this paper outperforms the state-of-the-art results on a clinical dataset in the French language by more than 55% in terms of F1-score #REFR, demonstrating the effectiveness of using domain-specific models for clinical text.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Table IV shows the model with the highest F 1 -score of this paper with the results of previous work on ICD-10 code association, and using ClinicalBERT embeddings for clinical text classification has been shown to improve performance on common clinical NLP tasks compared to nonspecific embeddings (#REFR).", "given_citation_intent": "result", "given_keywords": ["ClinicalBERT"]}]}
{"citing_id": "2303.01991v1", "cited_id": "1801.00868", "section_title": "A. Panoptic Segmentation", "citation": "Panoptic segmentation #REFR is a task that aims to accurately segment both object instances and stuff regions in an image, providing a comprehensive understanding of the scene.", "text_before_citation": [], "text_after_citation": ["General approaches to this task include the use of a two-stage pipeline #OTHEREFR - #OTHEREFR , where instance segmentation and semantic segmentation are performed independently and then fused together.", "An alternative approach is the use of a single network #OTHEREFR - #OTHEREFR that is able to predict both instancelevel and semantic-level segmentation maps simultaneously.", "The Panoptic FCN methodology #OTHEREFR is a single-stage approach that has been shown to achieve state-of-the-art performance using an elegant dynamic-convolutions based approach, which has the property that each detected object has a learned compressed representation, i.e. kernel, that could be used for tracking."], "citing_paper_content": {"title": "Unified Perception: Efficient Depth-Aware Video Panoptic Segmentation With Minimal Annotation Costs", "abstract": "Depth-aware video panoptic segmentation is a promising approach to camera based scene understanding. However, the current state-of-the-art methods require costly video annotations and use a complex training pipeline compared to their image-based equivalents. In this paper, we present a new approach titled Unified Perception that achieves state-of-theart performance without requiring video-based training. Our method employs a simple two-stage cascaded tracking algorithm that (re)uses object embeddings computed in an imagebased network. Experimental results on the Cityscapes-DVPS dataset demonstrate that our method achieves an overall DVPQ of 57.1, surpassing state-of-the-art methods. Furthermore, we show that our tracking strategies are effective for long-term object association on KITTI-STEP, achieving an STQ of 59.1 which exceeded the performance of state-of-the-art methods that employ the same backbone network."}, "cited_paper_content": {"title": "Panoptic Segmentation", "abstract": "We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper: {\\small\\url{https://arxiv.org/abs/1801.00868}}."}, "keywords": ["Panoptic segmentation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this paper, we present a new approach titled Unified Perception that achieves state-of-the-art performance without requiring video-based training. Our method employs a simple two-stage cascaded tracking algorithm that (re)uses object embeddings computed in an image-based network. Experimental results on the Cityscapes-DVPS dataset demonstrate that our method achieves an overall DVPQ of 57.1, surpassing state-of-the-art methods. Furthermore, we show that our tracking strategies are effective for long-term object association on KITTI-STEP, achieving an STQ of 59.1 which exceeded the performance of state-of-the-art methods that employ the same backbone network. To achieve this, we propose a novel approach that unifies semantic and instance segmentation, which is discussed in detail in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide additional context about the proposed task of panoptic segmentation, the authors cite the reference paper on panoptic segmentation [#REFR].", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Panoptic segmentation, which unifies the tasks of semantic segmentation and instance segmentation, has been proposed as a coherent scene segmentation that is rich and complete, and the authors of this manuscript cite the proposed task as an important step toward real-world vision systems. #REFR", "given_citation_intent": "background", "given_keywords": ["Panoptic segmentation"]}]}
{"citing_id": "2304.05635v1", "cited_id": "1902.09843", "section_title": "Methodology", "citation": "Inspired by #REFR , we clip and make w \u2208 [0, 1], \u2200w \u2208 W i for regularization via \u03c3(w) = max(0, min(1, w)).", "text_before_citation": ["Adaptive Head Aggregation.", "Following existing pFL paradigms, we globally share the representation part of the segmentation network (encoder F e and the SCR module) and personalize the task head (decoder F d ).", "To element-wisely aggregate two models without introducing multiple aggregation weight matrices, we adopt an adaptive learning based approach similar to residual learning for updating the weight matrices. The adaptive head aggregation is formulated a\u015d", "EQUATION", "where W i is a learnable weight matrix."], "text_after_citation": ["At the beginning, every element in W i is initialized to be 1, and then iteratively gets updated via", "EQUATION", "In the update process of W i , Eqs.", "3and 4are respectively utilized to alternatively update\u03b8 t i and W i .", "Upon convergence, the initialization parameter\u03b8 t i for F d in each federation round is obtained, after which local models are trained using Eq. (1)."], "citing_paper_content": {"title": "Unifying And Personalizing Weakly-Supervised Federated Medical Image Segmentation Via Adaptive Representation And Aggregation", "abstract": "Federated learning (FL) enables multiple sites to collaboratively train powerful deep models without compromising data privacy and security. The statistical heterogeneity (e.g., non-IID data and domain shifts) is a primary obstacle in FL, impairing the generalization performance of the global model. Weakly supervised segmentation, which uses sparsely-grained (i.e., point-, bounding box-, scribble-, block-wise) supervision, is increasingly being paid attention to due to its great potential of reducing annotation costs. However, there may exist label heterogeneity, i.e., different annotation forms across sites. In this paper, we propose a novel personalized FL framework for medical image segmentation, named FedICRA, which uniformly leverages heterogeneous weak supervision via adaptIve Contrastive Representation and Aggregation. Concretely, to facilitate personalized modeling and to avoid confusion, a channel selection based site contrastive representation module is employed to adaptively cluster intra-site embeddings and separate inter-site ones. To effectively integrate the common knowledge from the global model with the unique knowledge from each local model, an adaptive aggregation module is applied for updating and initializing local models at the element level. Additionally, a weakly supervised objective function that leverages a multiscale tree energy loss and a gated CRF loss is employed to generate more precise pseudo-labels and further boost the segmentation performance. Through extensive experiments on two distinct medical image segmentation tasks of different modalities, the proposed FedICRA demonstrates overwhelming performance over other state-ofthe-art personalized FL methods. Its performance even approaches that of fully supervised training on centralized data. Our code and data are available at https://github.com/llmir/FedICRA."}, "cited_paper_content": {"title": "Adaptive Gradient Methods With Dynamic Bound Of Learning Rate", "abstract": "Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at this https URL ."}, "keywords": ["regularization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve a gradual and smooth transition from adaptive methods to SGD, the authors of FedICRA proposed a novel personalized FL framework for medical image segmentation, which employs an adaptive learning-based approach for updating the weight matrices, similar to residual learning, as demonstrated in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To element-wisely aggregate two models without introducing multiple aggregation weight matrices, the authors adopted an adaptive learning based approach similar to residual learning for updating the weight matrices, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To avoid unstable and extreme learning rates, an adaptive method with dynamic bounds on learning rates called AdaBound, proposed in #REFR, can be employed to achieve a gradual and smooth transition from adaptive methods to SGD, which can improve the generalization gap between adaptive methods and SGD, especially on complex deep networks.", "given_citation_intent": "method", "given_keywords": ["regularization"]}]}
{"citing_id": "2303.08599v1", "cited_id": "2002.10118", "section_title": "Results", "citation": "In addition, according to a recent theorem #REFR that capturing uncertainty information and correcting overconfidence can be achieved by making only the last layer of a model in binary classification, we can assume that adding a GP layer is Bayesian enough so that GPF-BERT can achieve better calibration.", "text_before_citation": ["\"\u2191\" represents higher is better and \"\u2193\" means lower is better.", "All the models are trained in one dataset and test in the other dataset.", "ble, GPF-BERT has a significant decrease (at least 8 times) in inference time.", "While not completely free, GPF-BERT only adds negligible computational cost, but it greatly improves the calibration, which facilitates adaptation to other models.", "We believe that GP maintains a distribution over functions rather than model parameters, which enables GPF-BERT to improve uncertainty calibration for dialog response retrieval models."], "text_after_citation": [], "citing_paper_content": {"title": "Efficient Uncertainty Estimation With Gaussian Process For Reliable Dialog Response Retrieval", "abstract": "Deep neural networks have achieved remarkable performance in retrieval-based dialogue systems, but they are shown to be ill calibrated. Though basic calibration methods like Monte Carlo Dropout and Ensemble can calibrate well, these methods are time-consuming in the training or inference stages. To tackle these challenges, we propose an efficient uncertainty calibration framework GPF-BERT for BERT-based conversational search, which employs a Gaussian Process layer and the focal loss on top of the BERT architecture to achieve a highquality neural ranker. Extensive experiments are conducted to verify the effectiveness of our method. In comparison with basic calibration methods, GPF-BERT achieves the lowest empirical calibration error (ECE) in three in-domain datasets and the distributional shift tasks, while yielding the highest R 10 @1 and MAP performance on most cases. In terms of time consumption, our GPF-BERT has an 8\u00d7 speedup."}, "cited_paper_content": {"title": "Being Bayesian, Even Just A Bit, Fixes Overconfidence In Relu Networks", "abstract": "The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian posterior distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is ``to be a bit Bayesian''. These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations."}, "keywords": ["GPF-BERT", "uncertainty information"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We believe that GP maintains a distribution over functions rather than model parameters, which enables GPF-BERT to improve uncertainty calibration for dialog response retrieval models. As shown in a recent study by [#REFR], even a simplistic Bayesian approximation can fix the overconfidence problem in ReLU networks, indicating that a calibrated uncertainty on a ReLU network only requires \"to be a bit Bayesian\".", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the calibration of neural rankers in conversational search, GPF-BERT employs a Gaussian Process layer and focal loss, which has been shown to achieve the lowest empirical calibration error (ECE) and highest R 10 @1 and MAP performance on most cases, while adding negligible computational cost and greatly improving calibration. This approach is motivated by the theoretical analysis of approximate Gaussian posterior distributions on the weights of ReLU networks, which has been shown to fix the overconfidence problem and validate the usage of last-layer Bayesian approximation. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the calibration of uncertainty information in dialog response retrieval models, GPF-BERT employs a Gaussian Process layer and focal loss, achieving the lowest empirical calibration error and highest R 10 @1 and MAP performance on most cases, while maintaining a negligible computational cost and a significant decrease in inference time compared to basic calibration methods. This approach is motivated by the theoretical analysis of approximate Gaussian posterior distributions on the weights of ReLU networks, which has been shown to fix the overconfidence problem and validate the usage of last-layer Bayesian approximation, even with a simplistic, thus cheap, Bayesian approximation, indicating that a sufficient condition for a calibrated uncertainty on a ReLU network is \"to be a bit Bayesian\" (#REFR).", "given_citation_intent": "background", "given_keywords": ["GPF-BERT", "uncertainty information"]}]}
{"citing_id": "2303.08403v1", "cited_id": "2002.05709", "section_title": "Contrastive Self-Supervised Learning", "citation": "Sim-CLR #REFR utilizes augmented images as positives while the other images in the same batch as negatives.", "text_before_citation": ["The fundamental idea of contrastive learning is to minimize the distance between similar (i.e., positive) instances while maximizing the distance among dissimilar (i.e., negative) instances #OTHEREFR ."], "text_after_citation": ["Maintaining a similar contrastive concept, MoCo #OTHEREFR exploits a momentum encoder and proposes a dynamic dictionary with a queue to handle negative samples efficiently in both performance and memory perspectives. InfoNCE loss #OTHEREFR is often used in contrastive learning.", "Minimizing this loss increases mutual information between positive pairs so that the model can extract the consistent features between the original and augmented samples."], "citing_paper_content": {"title": "Dualfair: Fair Representation Learning At Both Group And Individual Levels Via Contrastive Self-Supervision", "abstract": "Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria-group fairness and counterfactual fairness-and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair intelligent Web applications. CCS CONCEPTS \u2022 Computing methodologies \u2192 Philosophical/theoretical foundations of artificial intelligence; Machine learning."}, "cited_paper_content": {"title": "A Simple Framework For Contrastive Learning Of Visual Representations", "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."}, "keywords": ["negatives", "augmented images"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The fundamental idea of contrastive learning is to minimize the distance between similar (i.e., positive) instances while maximizing the distance among dissimilar (i.e., negative) instances (OTHEREFR). SimCLR, a simple framework for contrastive learning of visual representations, has shown that introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, which could be relevant for our self-supervised model (REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The SimCLR framework proposed in #REFR simplifies contrastive self-supervised learning algorithms and achieves state-of-the-art performance on ImageNet by combining effective predictive tasks, a learnable nonlinear transformation, and larger batch sizes and more training steps compared to supervised learning.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate effective predictive tasks in contrastive learning, the composition of data augmentations plays a critical role, as shown in the SimCLR framework #REFR, which simplifies the contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank and benefits from larger batch sizes and more training steps compared to supervised learning, according to the provided local context. Additionally, maximizing the distance among dissimilar (i.e., negative) instances is a fundamental idea of contrastive learning, which is relevant to the specified keywords \"negatives\" and \"augmented images\".", "given_citation_intent": "background", "given_keywords": ["negatives", "augmented images"]}]}
{"citing_id": "2304.12849v1", "cited_id": "1804.02771", "section_title": "D.2 Sparse Depth Label", "citation": "Our sparsification strategy is also distinct from previous works that uniformly remove pixels through the entire depth range #REFR since we sparsify the label by restricting the observable depth range during the training.", "text_before_citation": ["In many depth estimation datasets, ground truth (GT) depth labels are sparsely annotated due to the hardware limitation of depth sensors, such as LiDAR, Radar, Structured-Light, and Time-of-Flight #OTHEREFR You et al., 2019b] .", "To better utilize this sparse information, approaches to incorporate sparse GT as additional input have been proposed #OTHEREFR .", "This task, also known as depth completion, differs from MDE because an additional sparse depth map is used as an input to supplement RGB information.", "In our proposed MDE setup, we also sparsify the depth map but it is only used as a label (not input) in the training process."], "text_after_citation": ["When compared to previously studied sparse setups, the proposed setup is far more challenging because the distribution of GT labels significantly differs for training and test phases."], "citing_paper_content": {"title": "Depth-Relative Self Attention For Monocular Depth Estimation", "abstract": "Monocular depth estimation is very challenging because clues to the exact depth are incomplete in a single RGB image. To overcome the limitation, deep neural networks rely on various visual hints such as size, shade, and texture extracted from RGB information. However, we observe that if such hints are overly exploited, the network can be biased on RGB information without considering the comprehensive view. We propose a novel depth estimation model named RElative Depth Transformer (RED-T) that uses relative depth as guidance in selfattention. Specifically, the model assigns high attention weights to pixels of close depth and low attention weights to pixels of distant depth. As a result, the features of similar depth can become more likely to each other and thus less prone to misused visual hints. We show that the proposed model achieves competitive results in monocular depth estimation benchmarks and is less biased to RGB information. In addition, we propose a novel monocular depth estimation benchmark that limits the observable depth range during training in order to evaluate the robustness of the model for unseen depths."}, "cited_paper_content": {"title": "Estimating Depth From Rgb And Sparse Sensing", "abstract": "We present a deep model that can accurately produce dense depth maps given an RGB image with known depth at a very sparse set of pixels. The model works simultaneously for both indoor/outdoor scenes and produces state-of-the-art dense depth maps at nearly real-time speeds on both the NYUv2 and KITTI datasets. We surpass the state-of-the-art for monocular depth estimation even with depth values for only 1 out of every \\({\\sim }10000\\) image pixels, and we outperform other sparse-to-dense depth methods at all sparsity levels. With depth values for \\(1{\\slash }256\\) of the image pixels, we achieve a mean error of less than \\(1\\%\\) of actual depth on indoor scenes, comparable to the performance of consumer-grade depth sensor hardware. Our experiments demonstrate that it would indeed be possible to efficiently transform sparse depth measurements obtained using e.g. lower-power depth sensors or SLAM systems into high-quality dense depth maps."}, "keywords": ["observable depth range"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To better utilize sparse ground truth depth labels in depth estimation datasets, approaches to incorporate them as additional input have been proposed #OTHEREFR. This task, also known as depth completion, differs from monocular depth estimation because it uses an additional sparse depth map as input.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitation of sparse depth labels in depth estimation datasets, approaches to incorporate sparse ground truth as additional input have been proposed, but in our proposed monocular depth estimation setup, we sparsify the depth map and use it only as a label in the training process #OTHEREFR (You et al., 2019b). Additionally, the authors of #REFR present a deep model that accurately produces dense depth maps given an RGB image with known depth at a very sparse set of pixels, which could be a useful method for future research in this area.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the robustness of our proposed model for unseen depths, we limit the observable depth range during training, which is a common practice in depth completion tasks #OTHEREFR [You et al., 2019b].", "given_citation_intent": "method", "given_keywords": ["observable depth range"]}]}
{"citing_id": "2304.07060v1", "cited_id": "1812.04948", "section_title": "C. Analysis", "citation": "Our DDPM of choice is trained on FFHQ #REFR dataset which contains 70, 000 unlabeled high-quality images.", "text_before_citation": ["C.1 Unique Subject Counts. In Fig.", "3 , we plot the number of unique subjects that can be sampled as we increase the sample size.", "The blue curve shows that the number of unique samples that can be generated by a DDPM of our choice does not saturate when we sample 200, 000 samples.", "At 200, 000 samples, the unique subjects are about 60, 000.", "And by extrapolating the curve, we estimate the number might reach 80, 000 with more samples."], "text_after_citation": ["The orange line shows the number of unique samples that are sufficiently different from the subjects in the CASIA-WebFace dataset.", "The green line shows the number of unique samples left after filtering images that contain sunglasses.", "The flat region is due to the filtering stage reducing the total candidates.", "The plot shows that DDPM trained on FFHQ dataset can sufficiently generate a large number of unique and new samples that are different from CASIA-WebFace dataset.", "However, with more samples, eventually there is a limit to the number of unique samples that can be generated."], "citing_paper_content": {"title": "Dcface: Synthetic Face Generation With Dual Condition Diffusion Model", "abstract": "Generating synthetic datasets for training face recognition models is challenging because dataset generation entails more than creating high fidelity images. It involves generating multiple images of same subjects under different factors (e.g., variations in pose, illumination, expression, aging and occlusion) which follows the real image conditional distribution. Previous works have studied the generation of synthetic datasets using GAN or 3D models. In this work, we approach the problem from the aspect of combining subject appearance (ID) and external factor (style) conditions. These two conditions provide a direct way to control the inter-class and intra-class variations. To this end, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our novel Patch-wise style extractor and Time-step dependent ID loss enables DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition models trained on synthetic images from the proposed DCFace provide higher verification accuracies compared to previous works by 6.11% on average in 4 out of 5 test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Code Link"}, "cited_paper_content": {"title": "A Style-Based Generator Architecture For Generative Adversarial Networks", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces."}, "keywords": ["high-quality images"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Based on the proposed alternative generator architecture for generative adversarial networks in #REFR, it is possible to achieve an unsupervised separation of high-level attributes and stochastic variation in generated images, which could be useful for generating multiple images of the same subject under different styles, as discussed in this manuscript.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate synthetic datasets for training face recognition models, the authors propose a Dual Condition Face Generator (DCFace) based on a diffusion model, which enables precise control over inter-class and intra-class variations by combining subject appearance and external factor conditions, as demonstrated in their experiments on multiple test datasets with higher verification accuracies than previous works by 6.11% on average, according to their results. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate high-quality images of human faces with precise control over inter-class and intra-class variations, the authors propose a Dual Condition Face Generator (DCFace) based on a diffusion model, which is inspired by the style transfer literature #REFR.", "given_citation_intent": "method", "given_keywords": ["high-quality images"]}]}
{"citing_id": "2304.11062v1", "cited_id": "1706.03762", "section_title": "Introduction", "citation": "The Transformer model #REFR has been widely adopted and used in various research areas and industrial applications.", "text_before_citation": [], "text_after_citation": ["The most important issue of the model is quadratic complexity of attention operation, that makes large models increasingly difficult to apply to longer inputs. arXiv:2304.11062v1 [cs.CL]"], "citing_paper_content": {"title": "Scaling Transformer To 1M Tokens And Beyond With Rmt", "abstract": "Figure 1: Recurrent Memory Transformer retains information across up to 2\u00d710 6 tokens. By augmenting a pre-trained BERT model with recurrent memory (Bulatov et al., 2022), we enabled it to store task-specific information across 7 segments of 512 tokens each. During inference, the model effectively utilized memory for up to 4,096 segments with a total length of 2,048,000 tokens-significantly exceeding the largest input size reported for transformer models (64K tokens for CoLT5 (Ainslie et al., 2023), and 32K tokens for GPT-4 (OpenAI, 2023)). This augmentation maintains the base model's memory size at 3.6 GB in our experiments."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["Transformer model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To enable the Transformer model to store task-specific information across multiple segments of tokens, the authors augmented a pre-trained BERT model with recurrent memory, as proposed in Bulatov et al. (2022) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable the storage of task-specific information across multiple segments, the authors augmented a pre-trained BERT model with recurrent memory, as proposed in Bulatov et al. (2022) and effectively utilized memory for up to 4,096 segments with a total length of 2,048,000 tokens, significantly exceeding the largest input size reported for transformer models (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable the storage of task-specific information across multiple segments of a Transformer model, the authors augmented a pre-trained BERT model with recurrent memory, as proposed in (Vaswani et al., 2017), resulting in a Recurrent Memory Transformer that can effectively utilize memory for up to 4,096 segments with a total length of 2,048,000 tokens, significantly exceeding the largest input size reported for Transformer models, including the Transformer model proposed in (Vaswani et al., 2017). #REFR", "given_citation_intent": "method", "given_keywords": ["Transformer model"]}]}
{"citing_id": "2303.02162v1", "cited_id": "1610.02415", "section_title": "A.4.1 Discussion On Comparison Among Generation-Based Methods", "citation": "However, there is a phenomenon that is observed in other VAE-based generative approaches #REFR : while the latent vector is searched under a guidance to maximize desired properties, its decoded instance does not always have the properties or are not even valid.", "text_before_citation": ["Both MCTS and BP-VAE hardly generate any qualified TCRs (very low q%: 0.01\u00b10.03% for MCTS and 0.04\u00b10.09% for BP-VAEin P McPAS ).", "MCTS explores potentially the entire sequence space, including both valid TCRs and non-TCRs.", "Although it uses R to guide the search, with substantially more calls to calculate R than other methods, due to the fact that valid TCRs may only occupy an extreme small portion of the entire sequence space, it is extremely challenging for MCTS to find qualified TCRs.", "In P VDJDB , MCTS even cannot find any qualified TCR for all the peptides within 1,000 rollouts, and thus has zero values at s v (C q ) and s r (C q ).", "An estimation using TCRdb over all possible length-15 sequence space results at most This vector could correspond to a binding TCR and then is decoded to a TCR sequence."], "text_after_citation": ["This phenomenon also appears in BP-VAE: the decoded TCRs are not qualified most of the times.", "This might be due to the propagation or magnification of the errors from the predictor in the latent space, or the exploration in the latent space ends at a region far away from that of valid TCRs.", "However, theoretical justification behind VAE-based generative approaches is out of the scope of this paper."], "citing_paper_content": {"title": "T-Cell Receptor Optimization With Reinforcement Learning And Mutation Polices For Precision Immunotherapy", "abstract": "T cells monitor the health status of cells by identifying foreign peptides displayed on their surface. T-cell receptors (TCRs), which are protein complexes found on the surface of T cells, are able to bind to these peptides. This process is known as TCR recognition and constitutes a key step for immune response. Optimizing TCR sequences for TCR recognition represents a fundamental step towards the development of personalized treatments to trigger immune responses killing cancerous or virus-infected cells. In this paper, we formulated the search for these optimized TCRs as a reinforcement learning (RL) problem, and presented a framework TCRPPO with a mutation policy using proximal policy optimization. TCRPPO mutates TCRs into effective ones that can recognize given peptides. TCRPPO leverages a reward function that combines the likelihoods of mutated sequences being valid TCRs measured by a new scoring function based on deep autoencoders, with the probabilities of mutated sequences recognizing peptides from a peptide-TCR interaction predictor. We compared TCRPPO with multiple baseline methods and demonstrated that TCRPPO significantly outperforms all the baseline methods to generate positive binding and valid TCRs. These results demonstrate the potential of TCRPPO for both precision immunotherapy and peptide-recognizing TCR motif discovery."}, "cited_paper_content": {"title": "Automatic Chemical Design Using A Data-Driven Continuous Representation Of Molecules", "abstract": "We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This generative model allows efficient search and optimization through open-ended spaces of chemical compounds. We train deep neural networks on hundreds of thousands of existing chemical structures to construct two coupled functions: an encoder and a decoder. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to the discrete representation from this latent space. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the design of drug-like molecules as well as organic light-emitting diodes."}, "keywords": ["VAE-based generative approaches"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Both MCTS and BP-VAE hardly generate any qualified TCRs (very low q%: 0.01\u00b10.03% for MCTS and 0.04\u00b10.09% for BP-VAEin P McPAS ). MCTS explores potentially the entire sequence space, including both valid TCRs and non-TCRs. Although it uses R to guide the search, with substantially more calls to calculate R than other methods, due to the fact that valid TCRs may only occupy an extreme small portion of the entire sequence space, it is extremely challenging for MCTS to find qualified TCRs. In P VDJDB , MCTS even cannot find any qualified TCR for all the peptides within 1,000 rollouts, and thus has zero values at s v (C q ) and s r (C q ). An estimation using TCRdb over all possible length-15 sequence space results at most This vector could correspond to a binding TCR and then is decoded to a TCR sequence. #REFR's method of converting discrete representations of molecules to and from a multidimensional continuous representation allows efficient search and optimization through open-ended spaces of chemical compounds, which could potentially be applied to the search for optimized TCRs.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Continuous representations of molecules allow for the automatic generation of novel chemical structures, as well as the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "VAE-based generative approaches have been shown to efficiently search and optimize through open-ended spaces of chemical compounds, allowing the automatic generation of novel chemical structures by performing simple operations in the latent space, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["VAE-based generative approaches"]}]}
{"citing_id": "2303.15414v1", "cited_id": "1703.07402", "section_title": "Inference Details", "citation": "After matching, like DeepSORT #REFR , we need to handle the born and death of tracklets.", "text_before_citation": ["Due to the continuous relaxation, the output of the QP layer may not be binary.", "To get a valid assignment, we use the greedy rounding strategy to generate the final permutation matrix from the predicted matching score map, i.e., we match the detection with the tracklet with the maximum score."], "text_after_citation": ["We keep the matching between detection and tracklet only if it satisfies all the following constraints: 1) The appearance similarity between detection and tracklet is above the threshold \u03c3.", "2) The detection is not far away from the tracklet.", "We set a threshold \u03ba as the Mahalanobis distance between the predicted distribution of the tracklet bounding box by the motion model and the detection bounding box in pixel coordinates, called the motion gate.", "3) The detection bounding box overlaps with the position of tracklet predicted by the motion model. The constraints above can be written as", "EQUATION"], "citing_paper_content": {"title": "Learnable Graph Matching: A Practical Paradigm For Data Association", "abstract": "Data association is at the core of many computer vision tasks, e.g., multiple object tracking, image matching, and point cloud registration. Existing methods usually solve the data association problem by network flow optimization, bipartite matching, or end-to-end learning directly. Despite their popularity, we find some defects of the current solutions: they mostly ignore the intra-view context information; besides, they either train deep association models in an end-to-end way and hardly utilize the advantage of optimization-based assignment methods, or only use an off-the-shelf neural network to extract features. In this paper, we propose a general learnable graph matching method to address these issues. Especially, we model the intra-view relationships as an undirected graph. Then data association turns into a general graph matching problem between graphs. Furthermore, to make optimization end-to-end differentiable, we relax the original graph matching problem into continuous quadratic programming and then incorporate training into a deep graph neural network with KKT conditions and implicit function theorem. In MOT task, our method achieves state-of-the-art performance on several MOT datasets. For image matching, our method outperforms state-of-the-art methods with half training data and iterations on a popular indoor dataset, ScanNet. Code will be available at https://github.com/jiaweihe1996/GMTracker."}, "cited_paper_content": {"title": "Simple Online And Realtime Tracking With A Deep Association Metric", "abstract": "Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates."}, "keywords": ["tracklets"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To get a valid assignment, we use the greedy rounding strategy to generate the final permutation matrix from the predicted matching score map, i.e., we match the detection with the tracklet with the maximum score, as proposed in the Simple Online And Realtime Tracking With A Deep Association Metric paper by #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To incorporate appearance information and improve the performance of SORT, the authors of this manuscript propose a deep association metric learned on a large-scale person re-identification dataset, as described in the reference paper \"Simple Online And Realtime Tracking With A Deep Association Metric\" (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To match the detection with the tracklet with the maximum score, the authors use the greedy rounding strategy, as described in #REFR, which improves the performance of the data association problem.", "given_citation_intent": "method", "given_keywords": ["tracklets"]}]}
{"citing_id": "2303.18187v1", "cited_id": "1908.08655", "section_title": "Benchmark Classification Results", "citation": "We remark that our comparison does not include the spiking predictive coding model reported in #REFR , even though a slightly better generalization error was reported on MNIST.", "text_before_citation": ["In Table 1 , we present our simulation results for our recurrent spiking model trained with event-driven forwardforward learning.", "Notice that, like many bio-physical spiking networks, even though we do not quite match the performance of backprop-based feedforward networks (essentially a purely rate-coded system), our generalization error comes surprisingly close.", "This is particularly promising given that the predicted context layer is a layer of spiking neurons itself.", "Furthermore, our ED-FF model comes the closest to matching the BP-FNN rate-coded baseline compared to the other SNN credit assignment algorithms.", "The BFA SNN comes in a close second place, illustrating that feedback synapses are still quite a powerful mechanism even though our goal was to demonstrate effective learning without feedback."], "text_after_citation": ["This model was not included given that is unsupervised and required post-fitting a log-linear classifier to rate code approximations of its top-most spike-train representations (and our focus in this study was on spiking models that jointly learned a spiking classifier with the internal representations of the sensory input).", "In Figure 2 , we examine the clusters that emerge within the latent space induced by a recurrent spiking network trained with ED-FF.", "To compute the latent vectors/codes, we form an approximate rate code from each data point's resultant top-layer (layer s 3 t ) spike trains as follows:", "EQUATION", "with \u03b3 c = 1."], "citing_paper_content": {"title": "Learning Spiking Neural Systems With The Event-Driven Forward-Forward Process", "abstract": "We develop a novel credit assignment algorithm for information processing with spiking neurons without requiring feedback synapses. Specifically, we propose an event-driven generalization of the forward-forward and the predictive forward-forward learning processes for a spiking neural system that iteratively processes sensory input over a stimulus window. As a result, the recurrent circuit computes the membrane potential of each neuron in each layer as a function of local bottom-up, top-down, and lateral signals, facilitating a dynamic, layer-wise parallel form of neural computation. Unlike spiking neural coding, which relies on feedback synapses to adjust neural electrical activity, our model operates purely online and forward in time, offering a promising way to learn distributed representations of sensory data patterns with temporal spike signals. Notably, our experimental results on several pattern datasets demonstrate that the even-driven forward-forward (ED-FF) framework works well for training a dynamic recurrent spiking system capable of both classification and reconstruction."}, "cited_paper_content": {"title": "Spiking Neural Predictive Coding For Continual Learning From Data Streams", "abstract": "For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of \"guess-and-check\", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity rules. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, capable of conducting online semi-supervised learning, and more computationally economical and biologically-plausible than popular artificial neural networks."}, "keywords": ["spiking predictive coding"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In Table 1, we present our simulation results for our recurrent spiking model trained with event-driven forward-forward learning, which is a promising way to learn distributed representations of sensory data patterns with temporal spike signals. Notably, our ED-FF model comes the closest to matching the BP-FNN rate-coded baseline compared to the other SNN credit assignment algorithms, illustrating the potential of the proposed model. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our experimental results demonstrate that the event-driven forward-forward (ED-FF) framework works well for training a dynamic recurrent spiking system capable of both classification and reconstruction, and our model is more computationally economical and biologically-plausible than popular artificial neural networks #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our experimental results demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication, making it a promising alternative to backprop-based feedforward networks for learning distributed representations of sensory data patterns with temporal spike signals, as shown in #REFR. The reference paper's spiking neural predictive coding approach aligns well with our model's interactive, iterative nature, which operates in a never-ending process of \"guess-and-check\", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions.", "given_citation_intent": "result", "given_keywords": ["spiking predictive coding"]}]}
{"citing_id": "2303.17966v1", "cited_id": "1609.02907", "section_title": "Introduction", "citation": "GCNs obtain convolution-like operations by aggregating information about the neighbors of a node using methods from spectral graph theory and using nonlinear activation functions to obtain low-dimensional features of graph nodes #REFR .", "text_before_citation": ["Data in these domains are different from traditional Euclidean data in that it has an asymmetric and irregular structure.", "And describing these data by graphs can well express the structural characteristics of these data and show powerful representation.", "Meanwhile, graph analysis methods in machine learning can make good use of such non-Euclidean structured data and be used for tasks such as node classification, link prediction, and clustering.", "Graph neural networks (GNNs), a widely used graph analysis method, process graph-structured data by performing deep learning operations on the graph domain and have made remarkable progress due to their excellent performance.", "While in Graph Neural Networks (GNNs), Convolutional Neural Networks (CNNs) are extended to graph-structured data by introducing Graph Convolutional Networks (GCNs)."], "text_after_citation": ["Then, GCNs requires the entire graph to be loaded into memory for convolution during training, which is inefficient for large graph transformation work patterns.", "GraphSAGE #OTHEREFR is an inductive graph convolution method that uses sampling of some of the nodes for learning and aggregation functions to learn feature information aggregated from the neighborhoods of the nodes.", "It is GraphSAGE learning by sampling that makes the convolution operation does not need to load the complete graph, which is a significant improvement over GCNs in the processing of large graphs.", "Many different improved versions have been proposed by researchers since then #OTHEREFR , and all have achieved promising results.", "GCNs are mainly built on a semi-supervised paradigm #OTHEREFR , where the Laplacian matrix of the graph is utilized for convolutional operations, and message aggregation is constrained by the adjacency matrix of the graph."], "citing_paper_content": {"title": "Hd-Gcn:A Hybrid Diffusion Graph Convolutional Network", "abstract": "In tasks involving graph-structured data, Graph Convolutional Networks (GCNs) have brought promising results by introducing convolution into Graph Neural Networks (GNNs). However, the information diffusion performance of GCNs and its variant models is limited by the adjacency matrix, which can lower their performance. Therefore, we introduce a new framework for graph convolutional networks called the Hybrid Diffusion Graph Convolutional Network (HD-GCN) to address the limitations of information diffusion caused by the adjacency matrix. In the HD-GCN framework, we initially utilize diffusion maps to facilitate the diffusion of information among nodes that are adjacent to each other in the feature space. This allows for the diffusion of information between similar points that may not have an adjacent relationship. Next, we utilize graph convolution to further propagate information among adjacent nodes after the diffusion maps, thereby enabling the spread of information among similar nodes that are adjacent in the graph. Finally, we employ the diffusion distances obtained through the use of diffusion maps to regularize and constrain the predicted labels of training nodes. This regularization method is then applied to the HD-GCN training, resulting in a smoother classification surface. The model proposed in this paper effectively overcomes the limitations of information diffusion imposed only by the adjacency matrix. HD-GCN utilizes hybrid diffusion by combining information diffusion between neighborhood nodes in the feature space and adjacent nodes in the adjacency matrix. This method allows for more comprehensive information propagation among nodes, resulting in improved model performance. We evaluated the performance of HD-GCN on three well-known citation network datasets and the results showed that the proposed framework is more effective than several graph-based semi-supervised learning methods."}, "cited_paper_content": {"title": "Semi-Supervised Classification With Graph Convolutional Networks", "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."}, "keywords": ["graph nodes"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Graph Convolutional Networks (GCNs) have brought promising results by introducing convolution into Graph Neural Networks (GNNs) for tasks involving graph-structured data #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Graph Convolutional Networks (GCNs) have shown promising results in tasks involving graph-structured data by introducing convolution into Graph Neural Networks (GNNs) #REFR, which operate directly on graphs and encode both local graph structure and features of nodes.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Graph Convolutional Networks (GCNs) have been widely used in graph analysis methods to process graph-structured data by performing deep learning operations on the graph domain, and have made remarkable progress due to their excellent performance in tasks such as node classification, link prediction, and clustering. The authors of this manuscript propose a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs, and demonstrate that their approach outperforms related methods by a significant margin on citation networks and on a knowledge graph dataset. Therefore, the proposed method in #REFR can be a promising solution for improving the performance of information propagation among graph nodes.", "given_citation_intent": "method", "given_keywords": ["graph nodes"]}]}
{"citing_id": "2304.01919v1", "cited_id": "1907.13568", "section_title": "Introduction", "citation": "Most visualization software aims for the accurate reproduction of data using simple, abstract 2-dimensional graphical marks: the rectangles (\u25fb) of the bar chart; the arcs (\u2aa6) of the pie; and the mix of circles (\u25ef), rectangles (\u25fb), and occasional stars ( ) in scatter plots #REFR .", "text_before_citation": [], "text_after_citation": ["In many situations, these representations are insufficient for a creative design vision. Graphical 'embellishments' require additional styling and design work.", "A designer might choose to stylize a visualization by replacing simple abstract marks with alternative representations: candles instead of bars, slices of fruit instead of arcs in a pie chart, and flowers instead of circles in a scatter plot.", "Beyond the simple replacement of marks, visualization styling can use alternative artistic forms and media (a sketch, a painting, a photograph, clay, etc.).", "Our particular focus is on stylized forms that represent a significant stylistic change to how the mark is represented (e.g., stacked coffee cups), but are still recognizable in their original idiomatic forms (e.g., a bar chart) #OTHEREFR .", "Due to the uniqueness of many of these representations, for a designer to achieve their vision, they need additional tools, skills, and procedures."], "citing_paper_content": {"title": "Viz2Viz: Prompt-Driven Stylized Visualization Generation Using A Diffusion Model", "abstract": "Fig. 1: viz2viz generated samples (original data inset). From left to right: bird eye view of green forests with many trees, blue ocean with ships, grey city with many buildings, orange deserts; realistic stacks of red coca-cola coke cans, brown tea cups, glass wine bottles, starbucks paper coffee cups; realistic pink tulips; Ukiyo-e style side view of red wooden roller coaster, Ukiyo-e style blue sea waves and surfers. Note that in some examples in this paper, we select prompts to demonstrate the capabilities of viz2viz rather than for their aesthetic or design qualities."}, "cited_paper_content": {"title": "Critical Reflections On Visualization Authoring Systems", "abstract": "An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed \u2014Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems."}, "keywords": ["visualization software"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors present critical reflections on three visualization authoring systems they developed, including Lyra, Data Illustrator, and Charticulator, to compare and contrast their limitations and trade-offs between expressivity and learnability, which provides valuable insights for future research in visualization authoring systems #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors developed three visualization authoring systems, Lyra, Data Illustrator, and Charticulator, and presented critical reflections on their limitations and trade-offs between expressivity and learnability, which could inform future research directions in visualization authoring systems. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors developed Viz2Viz, a prompt-driven stylized visualization generation system, which supports expressive information visualization without textual programming, similar to the emerging generation of #KEYWORD1. To contextualize their decisions in the broader design space, they presented critical reflections on three systems they developed, including Lyra, Data Illustrator, and Charticulator, and compared their limitations and trade-offs between expressivity and learnability, providing valuable insights for future research in #KEYWORD1 #REFR.", "given_citation_intent": "method", "given_keywords": ["visualization software"]}]}
{"citing_id": "2303.05800v1", "cited_id": "1409.1556", "section_title": "Introduction:", "citation": "One way to circumvent these difficulties is to embed pooling layers along the CLs #REFR .", "text_before_citation": ["In addition, given a deep architecture and the ratios between the depths of consecutive CLs, SRs increase as a function of the first CL depth #OTHEREFR .", "The deep learning strategy resulted in several practical difficulties, including the following.", "First, although the depth increases along the deep architecture, the input size of the layers remains fixed.", "The second difficulty is that the last CL output size, depth \u00d7 layer input size, becomes very large, serving as the first FC layer input, which consists of a large number of tunable parameters.", "These computational complexities overload even powerful GPUs, limited by the accelerated utilization of a large number of filters and sizes of the FC layers."], "text_after_citation": ["Each pooling reduces the output dimension of a CL by combining a cluster of outputs, e.g., 2 \u00d7 2, at one, and such operations along the deep architecture reduce the CL dimension by a factor 4 .", "The most popular pooling operators are max-pooling (MP) #OTHEREFR , which implements the maximal value of each cluster, and average pooling (AP) #OTHEREFR , which implements the average value of each cluster; however, more types of pooling operators exist #OTHEREFR .", "The core question in this work is whether SRs can be enhanced depending on the location of the pooling operators along the CLs of a given deep architecture.", "For instance, VGG16 consists of 13 CLs, three FC layers, and five (2 \u00d7 2) MP operators located along the CLs 2 (Fig. 1A) .", "The results indicate that SRs can be significantly increased by a smaller number of pooling operators adjacent to the last CL with optionally larger pooling sizes, for example, the advanced VGG16 (Fig. 1B) ."], "citing_paper_content": {"title": "Enhancing The Success Rates By Performing Pooling Decisions Adjacent To The Output Layer", "abstract": "Learning classification tasks of (2 \u00d7 2) inputs typically consist of \u2264 (2 \u00d7 2) max-pooling (MP) operators along the entire feedforward deep architecture. Here we show, using the CIFAR-10 database, that pooling decisions adjacent to the last convolutional layer significantly enhance accuracy success rates (SRs). In particular, average SRs of the advanced-VGG with layers (A-VGGm) architectures are 0.936, 0.940, 0.954, 0.955, and 0.955 for m=6, 8, 14, 13, and 16, respectively. The results indicate A-VGG8's SR is superior to VGG16's, and that the SRs of A-VGG13 and A-VGG16 are equal, and comparable to that of Wide-ResNet16. In addition, replacing the three fully connected (FC) layers with one FC layer, A-VGG6 and A-VGG14, or with several linear activation FC layers, yielded similar SRs. These significantly enhanced SRs stem from training the most influential input-output routes, in comparison to the inferior routes selected following multiple MP decisions along the deep architecture. In addition, SRs are sensitive to the order of the non-commutative MP and average pooling operators adjacent to the output layer, varying the number and location of training routes. The results call for the reexamination of previously proposed deep architectures and their SRs by utilizing the proposed pooling strategy adjacent to the output layer."}, "cited_paper_content": {"title": "Very Deep Convolutional Networks For Large-Scale Image Recognition", "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."}, "keywords": ["layers"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In addition, given a deep architecture and the ratios between the depths of consecutive CLs, SRs increase as a function of the first CL depth #OTHEREFR. The deep learning strategy resulted in several practical difficulties, including the following. First, although the depth increases along the deep architecture, the input size of the layers remains fixed. The second difficulty is that the last CL output size, depth \u00d7 layer input size, becomes very large, serving as the first FC layer input, which consists of a large number of tunable parameters. These computational complexities overload even powerful GPUs, limited by the accelerated utilization of a large number of filters and sizes of the FC layers.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The depth of convolutional networks has a significant impact on their accuracy in large-scale image recognition, as demonstrated by the findings in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The computational complexities of deep architectures, such as the fixed input size of layers and the large number of tunable parameters in the last convolutional layer, have been addressed by recent research on the effect of convolutional network depth on accuracy, which found that pushing the depth to 16-19 weight layers can significantly improve prior-art configurations #REFR.", "given_citation_intent": "background", "given_keywords": ["layers"]}]}
{"citing_id": "2303.13371v1", "cited_id": "1803.08024", "section_title": "C. Ablation Studies", "citation": "The Baseline employs the T2I attention from SCAN #REFR and averages all the cosine similarities as the final score. 1) Correspondence regulator. Eq.", "text_before_citation": ["In this section, we first report the configurations of our proposed regulators, as well as the initialization and optimization of the attention factors.", "Then, we delve into the RAR and RCR to display how the aggregation weights and cross-attention distributions are progressively refined. Finally, we also explore alternative strategies and architectures.", "All comparisons are implemented based on SCAN #OTHEREFR unless otherwise noted.", "Residual mechanism of the regulators.", "In TABLE III, we carry out critical analyses of the influence of residual architectures."], "text_after_citation": ["#OTHEREFR indicates that the current adaptive weight vector e at the last step.", "Here, we remove these two variables to construct a no-residual version of the RCR.", "Compared with the residual structure, the RCR without residual design results in an obvious R@1 drop in TABLE III, indicating that RCR is inclined to predict offsets against the current state to adjust previous regulation dynamically.", "To be specific, 1-step RCR without residual fashion produces better results than Baseline.", "This is because in the beginning, each word shares the same initialization of a weight vector e (0) =1 d and temperature \u03bb (0) =10, and the RCR barely infers the absolute value of these attention factors in the next step."], "citing_paper_content": {"title": "Plug-And-Play Regulators For Image-Text Matching", "abstract": "Exploiting fine-grained correspondence and visualsemantic alignments has shown great potential in image-text matching. Generally, recent approaches first employ a crossmodal attention unit to capture latent region-word interactions, and then integrate all the alignments to obtain the final similarity. However, most of them adopt one-time forward association or aggregation strategies with complex architectures or additional information, while ignoring the regulation ability of network feedback. In this paper, we develop two simple but quite effective regulators which efficiently encode the message output to automatically contextualize and aggregate cross-modal representations. Specifically, we propose (i) a Recurrent Correspondence Regulator (RCR) which facilitates the cross-modal attention unit progressively with adaptive attention factors to capture more flexible correspondence, and (ii) a Recurrent Aggregation Regulator (RAR) which adjusts the aggregation weights repeatedly to increasingly emphasize important alignments and dilute unimportant ones. Besides, it is interesting that RCR and RAR are \"plug-and-play\": both of them can be incorporated into many frameworks based on cross-modal interaction to obtain significant benefits, and their cooperation achieves further improvements. Extensive experiments on MSCOCO and Flickr30K datasets validate that they can bring an impressive and consistent R@1 gain on multiple models, confirming the general effectiveness and generalization ability of the proposed methods."}, "cited_paper_content": {"title": "Stacked Cross Attention For Image-Text Matching", "abstract": "In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: https://github.com/kuanghuei/SCAN."}, "keywords": ["T2I attention"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section, we first report the configurations of our proposed regulators, as well as the initialization and optimization of the attention factors. Then, we delve into the RAR and RCR to display how the aggregation weights and cross-attention distributions are progressively refined. Finally, we also explore alternative strategies and architectures. All comparisons are implemented based on SCAN #REFR unless otherwise noted. Residual mechanism of the regulators. In TABLE III, we carry out critical analyses of the influence of residual architectures.\n\nThe proposed regulators in this manuscript are compared with the state-of-the-art methods on the MS-COCO and Flickr30K datasets, and the results show that the proposed approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1) on Flickr30K, and improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set) on MS-COCO, demonstrating the effectiveness and generalization ability of the proposed methods.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity, the authors proposed Stacked Cross Attention in #REFR, which achieved state-of-the-art results on the MS-COCO and Flickr30K datasets.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity, the authors proposed Stacked Cross Attention in their previous work #REFR, which could be a potential solution for the T2I attention problem discussed in this paper.", "given_citation_intent": "method", "given_keywords": ["T2I attention"]}]}
{"citing_id": "2303.04906v1", "cited_id": "1912.04977", "section_title": "Introduction", "citation": "Federated Learning (FL) is a Machine Learning (ML) technique that has gained tremendous popularity in the last years #REFR .", "text_before_citation": [], "text_after_citation": ["Its core idea is to orchestrate the training of a global ML model without ever exchanging the data owned by each party or requiring it to be gathered in one common computational infrastructure.", "The popularity of FL caused the development of a plethora of FL frameworks, e.g., Intel \u00ae OpenFL #OTHEREFR , Flower #OTHEREFR , TensorFlow Federated [1] , and HPE Swarm Learning #OTHEREFR to cite a few.", "This software only supports one ML model type: Deep Neural Networks (DNNs).", "While DNNs have shown unprecedented results across a wide range of applications, from image recognition #OTHEREFR to natural language processing #OTHEREFR , from drug discovery #OTHEREFR to fraud detection #OTHEREFR , they are not the best ML model for every use case.", "First, DNNs require massive amounts of data, and collecting and labelling enough high-quality samples is often prohibitive."], "citing_paper_content": {"title": "Model-Agnostic Federated Learning", "abstract": "Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs). On the one hand, this allowed its development and widespread use as DNNs proliferated. On the other hand, it neglected all those scenarios in which using DNNs is not possible or advantageous. The fact that most current FL frameworks only allow training DNNs reinforces this problem. To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a modelagnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel \u00ae OpenFL. MAFL is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees. We test MAFL from multiple points of view, assessing its correctness, flexibility and scaling properties up to 64 nodes. We optimised the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is compatible with x86-64, ARM-v8, Power and RISC-V."}, "cited_paper_content": {"title": "Advances And Open Problems In Federated Learning", "abstract": "Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges."}, "keywords": ["Federated Learning"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning), which marries a model-agnostic FL algorithm with an open industry-grade FL framework, and is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the lack of FL solutions for non-DNN-based use cases, the authors propose MAFL (Model-Agnostic Federated Learning), which marries a model-agnostic FL algorithm with an open industry-grade FL framework, as discussed in the reference paper \"Advances And Open Problems In Federated Learning\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the lack of Federated Learning solutions for non-DNN-based use cases, the authors propose MAFL, a model-agnostic Federated Learning algorithm that marries AdaBoost.F with an open industry-grade FL framework, as discussed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["Federated Learning"]}]}
{"citing_id": "2304.12180v1", "cited_id": "1703.03864", "section_title": "L( + )", "citation": "This estimator can be massively parallelized (we denote the number of such iid estimators by N ) #REFR ; however, it is not online and might incur large latency between gradient updates when T is large.", "text_before_citation": ["With the score function gradient estimator trick #OTHEREFR , an unbiased estimator of (2) is given by 1 \u03c3 2 L([\u03b8 + ] \u00d7T ) .", "This estimator is zeroth-order since it only requires the loss function evaluation but not an explicit computation of its gradient, which allows for its effective use in cases when the gradients are noninformative (due to chaos) or not directly computable (black-box/discontinuous loss).", "To reduce the variance of this estimator, antithetic sampling is used and we call this finite-difference-style estimator FullES:", "EQUATION", "Here the term Full highlights that this estimator can only produce a gradient estimate by averaging the antithetic particles' losses (see Equation (1)) after a full unroll of T steps."], "text_after_citation": ["\u03b8 Worker 1 1 N N \u2211 i=1 g i \u23df \u2192 s \u03c4 N +1 \u2192 \u2191 \u03b8 \u2191 \u2026 \u2192 s \u03c4 N +W \u2191 \u2191 L \u03c4 N +1 L \u03c4 N +W s \u03c4 N g N g 1 Horizon length T Truncation window length W +\u03f5 \u03b8 +\u03f5 Worker N Online Evolution Strategies Protocol (a) PESWorker \u03f5 1 \u03f5 2 \u03f5 3 Accumulated noise \u03be \u03f5 1 GPES Worker K=2W \u03f5 1 \u03f5 2 \u03f5 1 \u03f5 1 Reuse \u03f5 1 \u03f5 1 Sample", "= \u03be = (b)", "Figure 2: (a) Illustration of step-unlocked Online ES workers working independently at different truncation windows.", "Here a central server sends \u03b8 (whose gradient to be estimated) to each worker and receives the estimates over partial unrolls from each.", "The averaged gradient is then used in a first-order optimization algorithm (like Adam)."], "citing_paper_content": {"title": "Noise-Reuse In Online Evolution Strategies", "abstract": "Online evolution strategies have become an attractive alternative to automatic differentiation (AD) due to their ability to handle chaotic and black-box loss functions, while also allowing more frequent gradient updates than vanilla Evolution Strategies (ES). In this work, we propose a general class of unbiased online evolution strategies. We analytically and empirically characterize the variance of this class of gradient estimators and identify the one with the least variance, which we term Noise-Reuse Evolution Strategies (NRES). Experimentally, we show that NRES results in faster convergence than existing AD and ES methods in terms of wall-clock speed and total number of unroll steps across a variety of applications, including learning dynamical systems, meta-training learned optimizers, and reinforcement learning."}, "cited_paper_content": {"title": "Evolution Strategies As A Scalable Alternative To Reinforcement Learning", "abstract": "We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation."}, "keywords": ["gradient updates"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors explore the use of Evolution Strategies (ES) as an alternative to popular MDP-based RL techniques and show that ES is a viable solution strategy that scales extremely well with the number of CPUs available #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of Evolution Strategies (ES) as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients has been explored in previous work #REFR, highlighting several advantages of ES as a black box optimization technique, including its invariance to action frequency and delayed rewards, tolerance of extremely long horizons, and not needing temporal discounting or value function approximation.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Evolution Strategies (ES) have been shown to be a viable solution strategy that scales extremely well with the number of CPUs available, allowing for more frequent #REFR gradient updates and faster convergence than existing methods in certain applications.", "given_citation_intent": "background", "given_keywords": ["gradient updates"]}]}
{"citing_id": "2303.02762v1", "cited_id": "1910.00350", "section_title": "C. Tool-Chain Workflow", "citation": "HAL #REFR was used to perform fundamental operations on the netlist such as graph traversal and storing and retrieving data on design elements.", "text_before_citation": ["An RTL description is written for all the identified modules.", "They are instantiated in the top-level module to form a complete netlist.", "The obtained high-level RTL is then verified using JasperGold for equivalence checking with the source HDL to confirm functional validity.", "However, to ensure equivalence in sequential circuits, we reset all flip-flops in the source HDL that do not have an initialization.", "The techniques described in this paper were implemented using Python."], "text_after_citation": ["All the benchmark designs were first synthesized using Artix 7-series and Zync 7series FPGAs using Xilinx Vivado.", "We set the flatten hierarchy option to full, while we kept the other default synthesis and optimization settings.", "We delete all the original net IDs that may indicate the signal name in the original model and generate unique random IDs for all nets. We use these netlists for our reverse engineering experiments. Yosys #OTHEREFR was used for the QBF SAT problem.", "Permutation-independent Boolean matching was performed using testnpn #OTHEREFR command in abc #OTHEREFR .", "All experiments were done on an AMD Ryzen 7 processor with 16GB RAM."], "citing_paper_content": {"title": "Reverse Engineering Word-Level Models From Look-Up Table Netlists", "abstract": "Reverse engineering of FPGA designs from bitstreams to RTL models aids in understanding the high level functionality of the design and for validating and reconstructing legacy designs. Fast carry-chains are commonly used in synthesis of operators in FPGA designs. We propose a method to detect word-level structures by analyzing these carry-chains in LUT (Look-Up Table) level netlists. We also present methods to adapt existing techniques to identify combinational operations and sequential modules in ASIC netlists to LUT netlists. All developed and adapted techniques are consolidated into an integrated tool-chain to aid in reverse engineering of word-level designs from LUT-level netlists. When evaluated on a set of real-world designs, the tool-chain infers 34% to 100% of the elements in the netlist to be part of a known word-level operation or a known sequential module."}, "cited_paper_content": {"title": "Highway To Hal: Open-Sourcing The First Extendable Gate-Level Netlist Reverse Engineering Framework", "abstract": "Since hardware oftentimes serves as the root of trust in our modern interconnected world, malicious hardware manipulations constitute a ubiquitous threat in the context of the Internet of Things (IoT). Hardware reverse engineering is a prevalent technique to detect such manipulations. Over the last years, an active research community has significantly advanced the field of hardware reverse engineering. Notably, many open research questions regarding the extraction of functionally correct netlists from Field Programmable Gate Arrays (FPGAs) or Application Specific Integrated Circuits (ASICs) have been tackled. In order to facilitate further analysis of recovered netlists, a software framework is required, serving as the foundation for specialized algorithms. Currently, no such framework is publicly available. Therefore, we provide the first open-source gate-library agnostic framework for gate-level netlist analysis. In this positional paper, we demonstrate the workflow of our modular framework HAL on the basis of two case studies and provide profound insights on its technical foundations."}, "keywords": ["netlist"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To facilitate further analysis of recovered netlists, a software framework is required, serving as the foundation for specialized algorithms. Currently, no such framework is publicly available. Therefore, the authors of this manuscript proposed an integrated tool-chain to aid in reverse engineering of word-level designs from LUT-level netlists, which was implemented using Python #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect word-level structures in LUT-level netlists, the authors propose a method that analyzes carry-chains, which is adapted from techniques used to identify combinational operations and sequential modules in ASIC netlists. The method is consolidated into an integrated tool-chain that aids in reverse engineering of word-level designs from LUT-level netlists, and was implemented using Python (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To facilitate the analysis of recovered netlists, an open-source gate-library agnostic framework for gate-level netlist analysis was proposed in the reference paper #REFR, which could be used as a method to identify combinational operations and sequential modules in ASIC netlists to LUT netlists, as described in this manuscript.", "given_citation_intent": "method", "given_keywords": ["netlist"]}]}
{"citing_id": "2303.14337v1", "cited_id": "1911.04118", "section_title": "Question-Driven Claim Extraction With Validation", "citation": "To this end, we employ an answer sentence selection model #REFR that validates each of the extracted contexts (from Section 4.3.1) separately against the strategic question.", "text_before_citation": ["Following #OTHEREFR , we adopt a Question Answering (QA) formulation to identify claims relevant to a given strategic question.", "Specifically, we design a QA pipeline, utilizing a transformer-based RoBERTa-large encoder model #OTHEREFR variant 6 that has been trained on SQuAD 2.0 #OTHEREFR and Natural Questions #OTHEREFR .", "The pipeline takes as input the news corpus split into snippets along with the strategic question, and outputs short answer extractions to these questions.", "The identified short answers are then expanded, by including the 3-sentence window around it to provide additional context.", "However, there is still a risk of false positives #OTHEREFR being identified as candidate answers with high confidence, thus necessitating the validation #OTHEREFR of extracted answers."], "text_after_citation": ["We concatenate the question and extracted context as input to a binary classification model 7 , with an underlying RoBERTa-large backbone, that is trained on Natural Questions #OTHEREFR and WikiQA #OTHEREFR .", "The output of the model is a validation score, between 0 (incorrect answer selection) and 1 (correct answer selection), used to select the top-5 relevant contexts for summarization. Fig.", "3: Figure showing an example for how multimodal information (in the form of images) supports and provides additional context to the claims presented in SmartBook.", "In this example, the presence of anti-aircraft weapons (as seen in the image) in Ukraine provides background for the discussion in NATO on whether to impose a no-fly zone.", "Situation reports often rely on a variety of sources, including text, images, videos, and audio recordings, to provide a holistic view of events."], "citing_paper_content": {"title": "Smartbook: Ai-Assisted Situation Report Generation", "abstract": "Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a time-sensitive comprehensive understanding of the situation to allow for appropriate decision-making and effective action response. Automated generation of situation reports can significantly reduce the time, effort, and cost for domain experts when preparing their official human-curated reports. However, AI research toward this goal has been very limited, and no successful trials have yet been conducted to automate such report generation. Pre-existing natural language processing methods, large language model based text generation, and information retrieval techniques are insufficient to identify, locate, and summarize important information, and lack detailed, structured, and strategic awareness. We propose SmartBook, a novel task formulation targeting situation report generation, which consumes large volumes of news data to produce a structured situation report with multiple hypotheses (claims) summarized and grounded with rich links to factual evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically generating intelligence analysis reports to assist expert analysts. The machine-generated reports are structured in the form of timelines, with each timeline organized by major events (or chapters), corresponding strategic questions (or sections) and their grounded summaries (or section content). Our proposed framework automatically detects realtime event-related strategic questions, which are more directed than manually-crafted analyst questions, which tend to be too complex, hard"}, "cited_paper_content": {"title": "Tanda: Transfer And Adapt Pre-Trained Transformer Models For Answer Sentence Selection", "abstract": "We propose TANDA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset. We then perform a second fine-tuning step to adapt the transferred model to the target domain. We demonstrate the benefits of our approach for answer sentence selection, which is a well-known inference task in Question Answering. We built a large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. Our approach establishes the state of the art on two well-known benchmarks, WikiQA and TREC-QA, achieving MAP scores of 92% and 94.3%, respectively, which largely outperform the previous highest scores of 83.4% and 87.5%, obtained in very recent work. We empirically show that TANDA generates more stable and robust models reducing the effort required for selecting optimal hyper-parameters. Additionally, we show that the transfer step of TANDA makes the adaptation step more robust to noise. This enables a more effective use of noisy datasets for fine-tuning. Finally, we also confirm the positive impact of TANDA in an industrial setting, using domain specific datasets subject to different types of noise."}, "keywords": ["answer sentence selection"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We utilize a transformer-based RoBERTa-large encoder model variant 6 that has been fine-tuned on Natural Questions and SQuAD 2.0 datasets (TANDA) #REFR to identify claims relevant to a given strategic question in a Question Answering (QA) formulation.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize a two-step fine-tuning technique proposed in TANDA #REFR to adapt a pre-trained Transformer model to our target domain for answer sentence selection.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To fine-tune pre-trained Transformer models for natural language tasks, TANDA was proposed, which includes a transfer step and an adaptation step, and was shown to achieve state-of-the-art results on answer sentence selection benchmarks #REFR.", "given_citation_intent": "method", "given_keywords": ["answer sentence selection"]}]}
{"citing_id": "2303.14470v1", "cited_id": "2003.03488", "section_title": "B.1. Implementation Details", "citation": "We follow the two-step scheme (as detailed in \u00a7 4) and the training settings in #REFR .", "text_before_citation": ["Implementation of ImageNet training."], "text_after_citation": ["Specifically, for each step, the model is trained for 640k training iterations with batch size 512.", "We adopt the Adam optimizer #OTHEREFR and set the initial learning rate to 10 \u22123 .", "Weight decay rates in the first and second steps are 10 \u22125 and 0, respectively.", "For experiments on ImageNet, models are trained with 8 V100 GPUs.", "We follow the training settings and data augmentation strategies in #OTHEREFR ."], "citing_paper_content": {"title": "Compacting Binary Neural Networks By Sparse Kernel Selection", "abstract": "Binary Neural Network (BNN) represents convolution weights with 1-bit values, which enhances the efficiency of storage and computation. This paper is motivated by a previously revealed phenomenon that the binary kernels in successful BNNs are nearly power-law distributed: their values are mostly clustered into a small number of codewords. This phenomenon encourages us to compact typical BNNs and obtain further close performance through learning nonrepetitive kernels within a binary kernel subspace. Specifically, we regard the binarization process as kernel grouping in terms of a binary codebook, and our task lies in learning to select a smaller subset of codewords from the full codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the codeword selection process, and develop the Permutation Straight-Through Estimator (PSTE) that is able to not only optimize the selection process end-toend but also maintain the non-repetitive occupancy of selected codewords. Experiments verify that our method reduces both the model size and bit-wise computational costs, and achieves accuracy improvements compared with stateof-the-art BNNs under comparable budgets."}, "cited_paper_content": {"title": "Reactnet: Towards Precise Binary Neural Network With Generalized Activation Functions", "abstract": "In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We first construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-off between accuracy and efficiency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions, to enable explicit learning of the distribution reshape and shift at near-zero extra cost. Lastly, we adopt a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network. We show that after incorporating all these ideas, the proposed ReActNet outperforms all the state-of-the-arts by a large margin. Specifically, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0% and 3.6% respectively for the top-1 accuracy and also reduces the gap to its real-valued counterpart to within 3.0% top-1 accuracy on ImageNet dataset."}, "keywords": ["training settings"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To improve the accuracy of binary networks without additional computational cost, ReActNet proposes to generalize the traditional Sign and PReLU functions to enable explicit learning of the distribution reshape and shift, and adopts a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of binary networks, ReActNet proposes to generalize the traditional Sign and PReLU functions to enable explicit learning of the distribution reshape and shift, which is shown to be effective in the ImageNet dataset training implementation. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement the proposed ReActNet method in the training settings, the authors cited the paper on \"Reactnet: Towards Precise Binary Neural Network With Generalized Activation Functions\" for enhancing the performance of binary networks by learning the distribution reshape and shift of activation functions at near-zero extra cost. #REFR", "given_citation_intent": "method", "given_keywords": ["training settings"]}]}
{"citing_id": "2303.00411v2", "cited_id": "1312.5185", "section_title": "Introduction", "citation": "In Section 7 we include a setting for abstract wave equations, which was considered in #REFR only for the splitting scheme.", "text_before_citation": ["For other schemes the contractivity of R usually follows by a functional calculus argument (see Proposition 2.4 below).", "In the above, one usually takes Y to be a suitable intermediate space between X and D(A).", "In the special and important case that Y = D(A) one can take \u03b1 = #OTHEREFR 2 for all of the aforementioned schemes.", "More general convergence rates can be found in Table 1 and Maxwell equations are included in the main text (see Subsections 3.3, 6.4, and 6.5).", "Our results improve several results from the literature to more general schemes and general rates \u03b1."], "text_after_citation": ["We prove similar higher order convergence rates for more general schemes, and in particular recover #OTHEREFR as a special case.", "To make the above results applicable to implementable numerical schemes for SPDEs, one would additionally need a space discretisation.", "Since the main novelty of our work lies in the treatment of temporal discretisations, we will only consider the latter.", "A detailed understanding of the global Lipschitz setting is a quintessential step towards the treatment of locally Lipschitz nonlinearities, which occur more frequently in practice.", "Our result should be seen as a first step, and we plan to continue our work on uniform strong errors in a locally Lipschitz setting in the near future."], "citing_paper_content": {"title": "Pathwise Uniform Convergence Of Time Discretisation Schemes For Spdes", "abstract": "In this paper we prove convergence rates for time discretisation schemes for semilinear stochastic evolution equations with additive or multiplicative Gaussian noise, where the leading operator A is the generator of a strongly continuous semigroup S on a Hilbert space X, and the focus is on non-parabolic problems. The main results are optimal bounds for the uniform strong error E \u221e k := E sup j\u2208{0,...,N k } U (t j) \u2212 U j p 1/p , where p \u2208 [2, \u221e), U is the mild solution, U j is obtained from a time discretisation scheme, k is the step size, and N k = T /k. The usual schemes such as splitting/exponential Euler, implicit Euler, and Crank-Nicolson, etc. are included as special cases. Under conditions on the nonlinearity and the noise we show \u2022 E \u221e k k log(T /k) (linear equation, additive noise, general S); \u2022 E \u221e k \u221a k log(T /k) (nonlinear equation, multiplicative noise, contractive S); \u2022 E \u221e k k log(T /k) (nonlinear wave equation, multiplicative noise). The logarithmic factor can be removed if the splitting scheme is used with a (quasi)-contractive S. The obtained bounds coincide with the optimal bounds for SDEs. Most of the existing literature is concerned with bounds for the simpler pointwise strong error E k := sup j\u2208{0,...,N k } E U (t j) \u2212 U j p 1/p. Applications to Maxwell equations, Schr\u00f6dinger equations, and wave equations are included. For these equations our results improve and reprove several existing results with a unified method."}, "cited_paper_content": {"title": "An Exponential Integrator Scheme For Time Discretization Of Nonlinear Stochastic Wave Equation", "abstract": "This work is devoted to convergence analysis of an exponential integrator scheme for semi-discretization in time of nonlinear stochastic wave equation. A unified framework is first set forth, which covers important cases of additive and multiplicative noises. Within this framework, the proposed scheme is shown to converge uniformly in the strong $L^p$-sense with precise convergence rates given. The abstract results are then applied to several concrete examples. Further, weak convergence rates of the scheme are examined for the case of additive noise. To analyze the weak error for the nonlinear case, techniques based on the Malliavin calculus were usually exploited in the literature. Under certain appropriate assumptions on the nonlinearity, this paper provides a weak error analysis, which does not rely on the Malliavin calculus. The rates of weak convergence can, as expected, be improved in comparison with the strong rates. Both strong and weak convergence results obtained here show that the proposed method achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations. Numerical results are finally reported to confirm our theoretical findings."}, "keywords": ["abstract wave equations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For other schemes the contractivity of R usually follows by a functional calculus argument (see Proposition 2.4 below). In the above, one usually takes Y to be a suitable intermediate space between X and D(A). In the special and important case that Y = D(A) one can take \u03b1 = #REFR 2 for all of the aforementioned schemes. More general convergence rates can be found in Table 1 and Maxwell equations are included in the main text (see Subsections 3.3, 6.4, and 6.5). Our results improve several results from the literature to more general schemes and general rates \u03b1.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed scheme in #REFR achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations, as shown in the strong and weak convergence results obtained in the paper.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed scheme for semi-discretization in time of nonlinear stochastic #KEYWORD1, which achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations, is shown to converge uniformly in the strong $L^p$-sense with precise convergence rates given in #REFR.", "given_citation_intent": "background", "given_keywords": ["abstract wave equations"]}]}
{"citing_id": "2304.02599v1", "cited_id": "1911.02212", "section_title": "Lower Bound For Inverse Trace Estimation", "citation": "Note that this result is very similar to that of #REFR , except that we work with the inverse trace rather than the minimum eigenvalue.", "text_before_citation": [", w n , the matrix W has the Wishart(d \u2212 n) distribution.", "Proposition 22 ([BHSW20, Lemma 3.5]).", "For any matrices Y 1 \u2208 R n\u00d7n , Y 2 \u2208 R (d\u2212n)\u00d7n , and any symmetric matrix W \u2208 R (d\u2212n)\u00d7(d\u2212n) , it holds that", "\u03bb min Y 1 Y 1 Y 1 Y 2 Y 2 Y 1 Y 2 Y 2 + W \u2264 \u03bb min ( W ) .", "We are now ready to prove Theorem 18."], "text_after_citation": ["Proof.", "[Proof of Theorem 18] Let \u03b4 > 0 be chosen later.", "We first argue that tr must not be too large.", "Applying Proposition 24, we conclude that there is a universal constant C > 0 such that tr(W \u22121 ) \u2264 C d 2 with probability at least 1/2. Hence,", "P tr \u2264 CC d 2 \u2265 P tr(W \u22121 ) \u2264 C d 2 and tr \u2264 C tr(W \u22121 ) \u2265 P{tr(W \u22121 ) \u2264 C d 2 } \u2212 P{ tr > C tr(W \u22121 )} \u2265 1 2 \u2212 \u03b4 ."], "citing_paper_content": {"title": "Query Lower Bounds For Log-Concave Sampling", "abstract": "Log-concave sampling has witnessed remarkable algorithmic advances in recent years, but the corresponding problem of proving lower bounds for this task has remained elusive, with lower bounds previously known only in dimension one. In this work, we establish the following query lower bounds: (1) sampling from strongly log-concave and log-smooth distributions in dimension d \u2265 2 requires \u2126(log \u03ba) queries, which is sharp in any constant dimension, and (2) sampling from Gaussians in dimension d (hence also from general log-concave and log-smooth distributions in dimension d) requires \u2126(min(\u221a \u03ba log d, d)) queries, which is nearly sharp for the class of Gaussians. Here \u03ba denotes the condition number of the target distribution. Our proofs rely upon (1) a multiscale construction inspired by work on the Kakeya conjecture in harmonic analysis, and (2) a novel reduction that demonstrates that block Krylov algorithms are optimal for this problem, as well as connections to lower bound techniques based on Wishart matrices developed in the matrix-vector query literature."}, "cited_paper_content": {"title": "The Gradient Complexity Of Linear Regression", "abstract": "We investigate the computational complexity of several basic linear algebra primitives, including largest eigenvector computation and linear regression, in the computational model that allows access to the data via a matrix-vector product oracle. We show that for polynomial accuracy, $\\Theta(d)$ calls to the oracle are necessary and sufficient even for a randomized algorithm. ::: Our lower bound is based on a reduction to estimating the least eigenvalue of a random Wishart matrix. This simple distribution enables a concise proof, leveraging a few key properties of the random Wishart ensemble."}, "keywords": ["minimum eigenvalue"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our proofs rely upon a novel reduction that demonstrates that block Krylov algorithms are optimal for this problem, as well as connections to lower bound techniques based on Wishart matrices developed in the matrix-vector query literature #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors prove the optimality of block Krylov algorithms for log-concave sampling, which is connected to lower bound techniques based on Wishart matrices developed in the matrix-vector query literature #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors prove a lower bound for estimating the minimum eigenvalue of a random Wishart matrix, which is based on a reduction to the computational complexity of linear regression, and show that $\\Theta(d)$ calls to the matrix-vector product oracle are necessary and sufficient for polynomial accuracy even for a randomized algorithm. #REFR", "given_citation_intent": "result", "given_keywords": ["minimum eigenvalue"]}]}
{"citing_id": "2303.16574v1", "cited_id": "1901.05555", "section_title": "Comparisons With Others", "citation": "The Traj++ EWTA+reweighting #REFR performs best on the average ADE/FDE, but its performance gains on tailed samples are relatively little.", "text_before_citation": ["For long-tailed classification methods #OTHEREFR , we construct a classification head after the encoder of Traj++ EWTA to use it to classify the trajectories according to the discretization of Kalman filter errors, same as Makansi et al.", "#OTHEREFR , and the classification loss is trained along with the prediction loss.", "Table 1 summarizes our experimental results on ETH-UCY using a best-of-20 evaluation #OTHEREFR .", "We can see that our method stably outperforms all comparing methods on all the top 1% \u2212 5% long-tail samples.", "Specifically, our framework outperforms the second best method: Traj++ EWTA+contrastive #OTHEREFR by 9.5% on ADE and 8.5% on FDE on the top 1% hardest samples, and maintains the average ADE and FDE nearly stable."], "text_after_citation": ["The Traj++ EWTA+resampling #OTHEREFR gets more gains on the most tailed samples, but its average ADE/FDE become much worse.", "Unlike simply doing resampling or loss reweighting, hypernetwork can decouple head samples and tail samples in the parameter space of decoder, therefore achieves better performances.", "Quantitative comparisons on Traj++ EWTA on Nuscenes.", "Comparison results with the previous best long-tail prediction method #OTHEREFR on Nuscenes are in Table 2.", "We find out that the resampling operation in the original Traj++ EWTA does not work well with FEND, probably because of causing overfit on hypernetwork."], "citing_paper_content": {"title": "Fend: A Future Enhanced Distribution-Aware Contrastive Learning Framework For Long-Tail Trajectory Prediction", "abstract": "Predicting the future trajectories of the traffic agents is a gordian technique in autonomous driving. However, trajectory prediction suffers from data imbalance in the prevalent datasets, and the tailed data is often more complicated and safety-critical. In this paper, we focus on dealing with the long-tail phenomenon in trajectory prediction. Previous methods dealing with long-tail data did not take into account the variety of motion patterns in the tailed data. In this paper, we put forward a future enhanced contrastive learning framework to recognize tail trajectory patterns and form a feature space with separate pattern clusters.Furthermore, a distribution aware hyper predictor is brought up to better utilize the shaped feature space. Our method is a model-agnostic framework and can be plugged into many well-known baselines. Experimental results show that our framework outperforms the state-of-the-art longtail prediction method on tailed samples by 9.5% on ADE and 8.5% on FDE, while maintaining or slightly improving the averaged performance. Our method also surpasses many long-tail techniques on trajectory prediction task."}, "cited_paper_content": {"title": "Class-Balanced Loss Based On Effective Number Of Samples", "abstract": "With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula $(1-\\beta^{n})/(1-\\beta)$, where $n$ is the number of samples and $\\beta \\in [0,1)$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets."}, "keywords": ["tailed samples"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For long-tailed classification methods, we construct a classification head after the encoder of Traj++ EWTA to use it to classify the trajectories according to the discretization of Kalman filter errors, same as Makansi et al. ( #REFR ), and the classification loss is trained along with the prediction loss.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of long-tailed data distribution, the authors of this paper proposed a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point, and introduced a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, yielding a class-balanced loss #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Long-tailed data distribution is a common problem in many real-world datasets, and the proposed class-balanced loss based on the effective number of samples in #REFR can be used to re-balance the loss and achieve significant performance gains on tailed samples.", "given_citation_intent": "background", "given_keywords": ["tailed samples"]}]}
{"citing_id": "2303.15244v1", "cited_id": "1206.5538", "section_title": "Introduction", "citation": "Therefore, recent methods rely on the so-called manifold hypothesis #REFR , stating that even complex and high-dimensional datasets are contained in a low-dimensional manifold.", "text_before_citation": ["Manifold learning.", "The treatment of high-dimensional data is often computationally costly and numerically unstable.", "Therefore, in many applications, it is important to find a low-dimensional representation of high-dimensional datasets.", "Classical methods, like the principal component analysis (PCA) #OTHEREFR , assume that the data is contained in a low-dimensional subspace.", "However, for complex datasets this assumption appears to be too restrictive, particularly when working with image datasets."], "text_after_citation": ["Based on this hypothesis, in recent years, many successful approaches have been based on generative models, able to represent high dimensional data in R n by a generator D : R d \u2192 R n with d n: these include generative adversarial networks (GANs) #OTHEREFR , variational autoencoders (VAEs) #OTHEREFR , injective flows #OTHEREFR and score-based diffusion models #OTHEREFR .", "For a survey on older approaches to manifold learning, the reader is referred to #OTHEREFR and to the references therein.", "Learning manifolds with multiple charts.", "Under the assumption that D is injective, the set of generated points {D(z) : z \u2208 R d } forms a manifold that approximates the training set.", "However, this requires that the data manifold admits a global parameterization."], "citing_paper_content": {"title": "Manifold Learning By Mixture Models Of Vaes For Inverse Problems", "abstract": "Representing a manifold of very high-dimensional data with generative models has been shown to be computationally efficient in practice. However, this requires that the data manifold admits a global parameterization. In order to represent manifolds of arbitrary topology, we propose to learn a mixture model of variational autoencoders. Here, every encoder-decoder pair represents one chart of a manifold. We propose a loss function for maximum likelihood estimation of the model weights and choose an architecture that provides us the analytical expression of the charts and of their inverses. Once the manifold is learned, we use it for solving inverse problems by minimizing a data fidelity term restricted to the learned manifold. To solve the arising minimization problem we propose a Riemannian gradient descent algorithm on the learned manifold. We demonstrate the performance of our method for low-dimensional toy examples as well as for deblurring and electrical impedance tomography on certain image manifolds."}, "cited_paper_content": {"title": "Representation Learning: A Review And New Perspectives", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."}, "keywords": ["low-dimensional manifold", "high-dimensional datasets"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Manifold learning. The treatment of high-dimensional data is often computationally costly and numerically unstable. Therefore, in many applications, it is important to find a low-dimensional representation of high-dimensional datasets. Classical methods, like the principal component analysis (PCA) #OTHEREFR, assume that the data is contained in a low-dimensional subspace. However, for complex datasets this assumption appears to be too restrictive, particularly when working with image datasets. Representation learning has been shown to be a promising alternative, as it can entangle and reveal the different explanatory factors of variation behind the data. In fact, recent work in the area of unsupervised feature learning and deep learning, as reviewed in the paper by Bengio et al. (#REFR), has shown significant advances in this field.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To represent manifolds of arbitrary topology, the authors propose to learn a mixture model of variational autoencoders, where every encoder-decoder pair represents one chart of a manifold, as reviewed in the paper \"Representation Learning: A Review And New Perspectives\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To represent a low-dimensional manifold of high-dimensional datasets, the authors propose to learn a mixture model of variational autoencoders, where every encoder-decoder pair represents one chart of a manifold #REFR.", "given_citation_intent": "method", "given_keywords": ["low-dimensional manifold", "high-dimensional datasets"]}]}
{"citing_id": "2303.08514v1", "cited_id": "1804.01962", "section_title": "Novel Neural Networks-Based Approaches", "citation": "This paper combines traditional Gabor waveletbased iris coding with a DNN driven by post-mortem iris data for feature extraction, reducing the recognition error rate by one-third compared to the baseline method #REFR .", "text_before_citation": ["The two respective feature maps are combined into a single feature map reflecting the differences between domains, which is then fed into a subsequent convolutional layer to extract high-level semantic information further.", "The EER values of this research method on Q-FIRE and CASIA are 0.15% and 0.31%, respectively.", "The iris recognition technique proposed by #OTHEREFR uses a local circular Gabor filter for initial feature extraction before input to the CNN to retain all directional information.", "This design solves the problem of traditional Gabor wavelet transform insensitivity to circular orientation versus the difficulty of neural networks extracting directional features on the circular structure of the iris.", "#OTHEREFR proposes an iris recognition method explicitly designed for post-mortem samples, thus enabling the application of iris biometrics in forensic science."], "text_after_citation": ["In addition to filters, attention mechanisms and feature histogram methods are also applied to optimize the feature extraction process of IR.", "#OTHEREFR proposes a spatial attention feature fusion module to fuse features at different levels.", "Spatial attention #OTHEREFR can encode the different positions of each importance in the feature map.", "The fact that iris features are local features suggests that iris features have other spatial significance in different local regions; therefore, spatial attention feature fusion is well suited for iris recognition.", "The spatial attention feature fusion module in the dual spatial attention network proposed in this study can learn the weights of each location and effectively fuse features at different levels."], "citing_paper_content": {"title": "Deep Learning For Iris Recognition: A Review", "abstract": "Iris recognition is a secure biometric technology known for its stability and privacy. With no two irises being identical and little change throughout a person's lifetime, iris recognition is considered more reliable and less susceptible to external factors than other biometric recognition methods. Unlike traditional machine learning-based iris recognition methods, deep learning technology does not rely on feature engineering and boasts excellent performance. This paper collects 120 relevant papers to summarize the development of iris recognition based on deep learning. We first introduce the background of iris recognition and the motivation and contribution of this survey. Then, we present the common datasets widely used in iris recognition. After that, we summarize the key tasks involved in the process of iris recognition based on deep learning technology, including identification, segmentation, presentation attack detection, and localization. Finally, we discuss the challenges and potential development of iris recognition. This review provides a comprehensive sight of the research of iris recognition based on deep learning."}, "cited_paper_content": {"title": "Iris Recognition After Death", "abstract": "This paper presents a comprehensive study of post-mortem human iris recognition carried out for 1200 near-infrared and 1787 visible-light samples collected from 37 deceased individuals kept in mortuary conditions. We used four independent iris recognition methods (three commercial and one academic) to analyze genuine and impostor comparison scores and check the dynamics of iris quality decay over a period of up to 814 h after death. This study shows that post-mortem iris recognition may be close-to-perfect approximately 5\u20137 h after death and occasionally is still viable even 21 days after death. These conclusions contradict the statements present in the past literature that the iris is unusable as a biometrics shortly after death, and show that the dynamics of post-mortem changes to the iris that are important for biometric identification are more moderate than previously hypothesized. This paper contains a thorough medical commentary that helps to understand which post-mortem metamorphoses of the eye may impact the performance of automatic iris recognition. An important finding is that false-match probability is higher when live iris images are compared with post-mortem samples than when only live samples are used in comparisons. This paper conforms to reproducible research and the database used in this study is made publicly available to facilitate research on post-mortem iris recognition. To the best of our knowledge, this paper offers the most comprehensive evaluation of post-mortem iris recognition and the largest database of post-mortem iris images."}, "keywords": ["post-mortem iris data"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The iris recognition method proposed by #REFR is explicitly designed for post-mortem samples, thus enabling the application of iris biometrics in forensic science.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The iris recognition method proposed by #REFR is explicitly designed for post-mortem samples, enabling the application of iris biometrics in forensic science, and uses a local circular Gabor filter for initial feature extraction before input to the CNN to retain all directional information.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The iris recognition method proposed by #REFR is explicitly designed for post-mortem iris data, enabling the application of iris biometrics in forensic science.", "given_citation_intent": "method", "given_keywords": ["post-mortem iris data"]}]}
{"citing_id": "2304.12479v1", "cited_id": "0706.3639", "section_title": "Defining Agi: A General Perspective", "citation": "For instance, an AGI agent shall be capable of understanding, learning, and carrying out any intellectual work that a human person is capable of #REFR .", "text_before_citation": ["AGI usually refers to machine intelligence that possesses human-like cognitive abilities."], "text_after_citation": ["In contrast, to narrow/limited AI, which is created to excel in specific tasks or domains, AGI systems mimic humans' general-purpose problem-solving abilities #OTHEREFR .", "AGI differs from limited AI in several significant ways, demonstrating the general objective of reaching humanlike intelligence in AI systems.", "The ability of AGI systems to function autonomously, making judgments and conducting actions without the need for ongoing human supervision, is one of these features.", "Thanks to this degree of autonomy, AGI may work well in complicated, dynamic situations, enabling it to adjust to unforeseen conditions #OTHEREFR .", "AGI also has the capacity for general-purpose learning, allowing it to reason and learn across various areas."], "citing_paper_content": {"title": "Artificial General Intelligence (Agi) For Education", "abstract": "Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting educational goals, designing pedagogy and curriculum, and performing assessments. We also provide rich discussions over various ethical issues in education faced by AGI and how AGI will affect human educators. The development of AGI necessitates interdisciplinary collaborations between educators and AI engineers to advance research and application efforts."}, "cited_paper_content": {"title": "A Collection Of Definitions Of Intelligence", "abstract": "This chapter is a survey of a large number of informal definitions of \u201cintelligence\u201d that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70 odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is."}, "keywords": ["AGI agent", "intellectual work"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "According to (A Collection Of Definitions Of Intelligence), AGI usually refers to machine intelligence that possesses human-like cognitive abilities. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "AGI usually refers to machine intelligence that possesses human-like cognitive abilities, and as described in the reference paper \"#REFR\", there are numerous informal definitions of intelligence that have been collected over the years.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the authors, intelligence can be defined in various ways, and compiling a complete list is impossible as many definitions are buried deep inside articles and books. #REFR provides a large and well-referenced collection of informal definitions of intelligence, which can be useful for developing an AGI agent capable of performing intellectual work.", "given_citation_intent": "background", "given_keywords": ["AGI agent", "intellectual work"]}]}
{"citing_id": "2304.10848v1", "cited_id": "1812.11061", "section_title": "A.2 Pseudo-Linear Time In The Regime With Positive Drift", "citation": "We note that there is a non-vanishing gap between our upper and lower bound in the theorem above when = \u03a9 #REFR .", "text_before_citation": ["Recall that this estimate was conditional on a fixed value of 0 .", "Since 0 follows a binomial distribution with parameters and #OTHEREFR 2 , we have 0 \u2265 2 \u2212 3/4 with probability 1\u2212 (1), and in this case,", "[ | 0 \u2265 2 \u2212 3/4 ] \u2265 (ln( 2 \u2212 3/4 \u22121) \u2212ln( ) \u22121) = (1\u2212 (1)) ln( ),", "where the last estimate exploits our assumption = ( ). Just from the contribution of this case, we", "obtain [ ] \u2265 (1 \u2212 (1)) [ | 0 \u2265 2 \u2212 3/4 ] = (1 \u2212 (1)) ln( ). \u25a1"], "text_after_citation": ["The reason, most likely, is the argument used in the lower bound proof that the Metropolis algorithm cannot be faster than randomized local search, which ignores any negative effect of accepting inferior solutions.", "For our purposes, the theorem above is sufficient, since for all but very large values of (where the gap is negligible) the runtime of the Metropolis algorithm is dominated by the second part of the optimization process starting from a solution with ( ) = .", "The reason why we could not prove a tighter bound for all values of is that the existing multiplicative drift theorems for lower bounds, e.g., Theorem 2.2 in #OTHEREFR or Theorem 3.7 in #OTHEREFR , either are not applicable to our process or necessarily lead to a constant-factor gap to the upper bound obtained from multiplicative drift.", "Applying the variable drift theorem from #OTHEREFR to the process = min{ | \u2264 } appears to be a promising way to overcome these difficulties, but since we do not need such a precise bound, we do not follow this route any further."], "citing_paper_content": {"title": "How Well Does The Metropolis Algorithm Cope With Local Optima? *", "abstract": "The Metropolis algorithm (MA) is a classic stochastic local search heuristic. It avoids getting stuck in local optima by occasionally accepting inferior solutions. To better and in a rigorous manner understand this ability, we conduct a mathematical runtime analysis of the MA on the CLIFF benchmark. Apart from one local optimum, cliff functions are monotonically increasing towards the global optimum. Consequently, to optimize a cliff function, the MA only once needs to accept an inferior solution. Despite seemingly being an ideal benchmark for the MA to profit from its main working principle, our mathematical runtime analysis shows that this hope does not come true. Even with the optimal temperature (the only parameter of the MA), the MA optimizes most cliff functions less efficiently than simple elitist evolutionary algorithms (EAs), which can only leave the local optimum by generating a superior solution possibly far away. This result suggests that our understanding of why the MA is often very successful in practice is not yet complete. Our work also suggests to equip the MA with global mutation operators, an idea supported by our preliminary experiments. CCS CONCEPTS \u2022 Theory of computation \u2192 Theory of randomized search heuristics."}, "cited_paper_content": {"title": "A Tight Runtime Analysis For The (\u039c + \u039b) Ea", "abstract": "Despite significant progress in the theory of evolutionary algorithms, the theoretical understanding of true population-based evolutionary algorithms remains challenging and only few rigorous results exist. Already for the most basic problem, the determination of the asymptotic runtime of the (\u03bc + \u03bb) evolutionary algorithm on the simple OneMax benchmark function, only the special cases \u03bc = 1 and \u03bb = 1 have been solved. In this work, we analyze this long-standing problem and show the asymptotically tight result that the runtime T, the number of iterations until the optimum is found, satisfies [EQUATION] where log+ x := max{1, log x} for all x > 0."}, "keywords": ["theorem", "non-vanishing gap"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To better understand the asymptotic runtime of population-based evolutionary algorithms, the authors of this manuscript cite the work of [INSERT CITATION HERE] which provides a rigorous analysis of the (\u03bc + \u03bb) evolutionary algorithm on the OneMax benchmark function. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the theoretical understanding of true population-based evolutionary algorithms, the authors cite the reference paper on the tight runtime analysis for the (\u03bc + \u03bb) EA. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To the best of our knowledge, the asymptotic runtime of the (\u03bc + \u03bb) evolutionary algorithm on the OneMax benchmark function has only been solved for the special cases \u03bc = 1 and \u03bb = 1, as shown in #REFR's theorem on the tight runtime analysis for the (\u03bc + \u03bb) EA.", "given_citation_intent": "background", "given_keywords": ["theorem", "non-vanishing gap"]}]}
{"citing_id": "2303.12800v1", "cited_id": "1905.13430", "section_title": "Introduction", "citation": "In addition, most existing approaches are not applicable when the IoT devices are behind a NAT (network address translation) enabled router, as many of the features get altered in the NAT process #REFR .", "text_before_citation": ["This can help organizations effectively manage the security issues associated with IoT devices and determine whether the behavior/activity of the connected IoT devices is normal.", "Existing approaches & their limitations: Prior research suggested ways of identifying IoT devices by analyzing the network communication (Meidan et al., 2017a,b; #OTHEREFR .", "Since the methods proposed are based on machine learning, feature engineering (i.e., feature extraction, selection, and tuning) is required.", "This necessitates manual input from subject matter experts, which is both expensive and prone to errors.", "Existing approaches are timeconsuming, requiring multiple sessions to identify known and unknown (also referred to as unauthorized in this paper) IoT devices, and tend to have complex architecture, since they use multistage models."], "text_after_citation": ["Proposed solution: Our approach mitigates these limitations.", "Just a single session is needed to identify known and unknown IoT devices; in addition, it is free from the burden of feature engineering and the errors that are accompanied with feature engineering, and it has a simple architecture.", "The proposed approach also applies to IoT devices behind a NAT-enabled router, as the NAT process does not alter the payload of the communication.", "The proposed approach enables us to identify known and unknown IoT devices in the network in various scenarios.", "Organizations' use of DHCP and the ease in which MAC addresses can be spoofed has made it difficult to identify IoT devices using traditional approaches #OTHEREFR ."], "citing_paper_content": {"title": "Iot Device Identification Based On Network Communication Analysis Using Deep Learning", "abstract": "Attack vectors for adversaries have increased in organizations because of the growing use of less secure IoT devices. The risk of attacks on an organization's network has also increased due to the bring your own device (BYOD) policy which permits employees to bring IoT devices onto the premises and attach them to the organization's network. To tackle this threat and protect their networks, organizations generally implement security policies in which only white-listed IoT devices are allowed on the organization's network. To monitor compliance with such policies, it has become essential to distinguish IoT devices permitted within an organization's network from non-whitelisted (unknown) IoT devices. In this research, deep learning is applied to network communication for the automated identification of IoT devices permitted on the network. In contrast to existing methods, the proposed approach does not require complex feature engineering of the network communication, because the 'communication behavior' of IoT devices is represented as small images which are generated from the device's network communication payload. The proposed approach is applicable for any IoT device, regardless of the protocol used for communication. As our approach relies on the network communication payload, it is also applicable for the IoT devices behind a network address translation (NAT) enabled router. In this study, we trained various classifiers on a publicly accessible dataset to identify IoT devices in different scenarios, including the identification of known and unknown IoT devices, achieving over 99% overall average detection accuracy."}, "cited_paper_content": {"title": "Privacy-Preserving Detection Of Iot Devices Connected Behind A Nat In A Smart Home Setup", "abstract": "Today, telecommunication service providers (telcos) are exposed to cyber-attacks executed by compromised IoT devices connected to their customers' networks. Such attacks might have severe effects not only on the target of attacks but also on the telcos themselves. To mitigate those risks we propose a machine learning based method that can detect devices of specific vulnerable IoT models connected behind a domestic NAT, thereby identifying home networks that pose a risk to the telco's infrastructure and availability of services. As part of the effort to preserve the domestic customers' privacy, our method relies on NetFlow data solely, refraining from inspecting the payload. To promote future research in this domain we share our novel dataset, collected in our lab from numerous and various commercial IoT devices."}, "keywords": ["IoT devices"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Prior research suggested ways of identifying IoT devices by analyzing the network communication (Meidan et al., 2017a,b; #REFR). Since the methods proposed are based on machine learning, feature engineering (i.e., feature extraction, selection, and tuning) is required.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Prior research suggested ways of identifying IoT devices by analyzing the network communication (Meidan et al., 2017a,b; #REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Prior research suggested ways of identifying IoT devices by analyzing the network communication (Meidan et al., 2017a,b; #REFR).", "given_citation_intent": "background", "given_keywords": ["IoT devices"]}]}
{"citing_id": "2304.08870v1", "cited_id": "1505.04597", "section_title": "Related Works", "citation": "Typically, a UNet #REFR is used to learn to produce the denoising signal. Conditioning e.g.", "text_before_citation": ["Diffusion Model.", "Recently diffusion model #OTHEREFR have shown superior image quality and text-guided capability.", "In training, the diffusion model gradually adds noise to the image until it becomes random noise; this process is known as forward diffusion.", "The diffused random noise act as latent variables and is denoised progressively to generate an image in image sampling; this progress is known as reverse diffusion."], "text_after_citation": ["text can be applied using classifier #OTHEREFR or classified-free approaches #OTHEREFR .", "Most methods #OTHEREFR use the latter to find the direction between the conditional and unconditional in the latent space, which to be applied in sampling time to guide the model towards the conditioning direction.", "However, denoising every image pixel can be computationally expensive; therefore, LDM #OTHEREFR proposed a two-stage process.", "It first trained a variational autoencoder (VAE) #OTHEREFR to encode the image into smaller dimensional latent variables, and the diffusion model learned to produce the VAE latent variables.", "Diffusion models could provide image editing by performing text-guided diffusion on regions defined by segmentation mask #OTHEREFR .Dreambooth #OTHEREFR shows that they could encode a person's face into a text token and use a diffusion model to generate the person in a different scene."], "citing_paper_content": {"title": "Upgpt: Universal Diffusion Model For Person Image Generation, Editing And Pose Transfer", "abstract": "Existing person image generative models can do either image generation or pose transfer but not both. We propose a unified diffusion model, UPGPT to provide a universal solution to perform all the person image tasks-generative, pose transfer, and editing. With fine-grained multimodality and disentanglement capabilities, our approach offers finegrained control over the generation and the editing process of images using a combination of pose, text, and image, all without needing a semantic segmentation mask which can be challenging to obtain or edit. We also pioneer the parameterized body SMPL model in pose-guided person image generation to demonstrate new capability-simultaneous pose and camera view interpolation while maintaining a person's appearance. Results on the benchmark Deep-Fashion dataset show that UPGPT is the new state-of-theart while simultaneously pioneering new capabilities of edit and pose transfer in human image generation."}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["Conditioning", "denoising signal"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, U-Net has shown superior performance in segmentation tasks with very few annotated images #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve state-of-the-art results in pose-guided person image generation, the authors utilized the U-Net architecture proposed in \"U-Net: Convolutional Networks For Biomedical Image Segmentation\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To condition the diffusion model for person image generation, the authors utilized the U-Net architecture #REFR for precise localization and efficient use of annotated samples in training, while also leveraging its denoising signal capabilities.", "given_citation_intent": "method", "given_keywords": ["Conditioning", "denoising signal"]}]}
{"citing_id": "2303.02334v1", "cited_id": "1308.2140", "section_title": "Related Work", "citation": "Network science has introduced various types of centrality measures to determine the relative importance of nodes in a network under respective circumstances #REFR .", "text_before_citation": ["Some results on the MPC of nonlinear systems using model reduction have been reported in the literature. For example, Wiese et al. #OTHEREFR presented an MPC method for gas turbines.", "They specifically developed a lower-order internal model from a physics-based higher-order model using rigorous timescale separation arguments that can be extended to various gas turbine systems.", "Zhang and Liu #OTHEREFR considered the problem of the economic model predictive control of wastewater treatment plants based on model reduction.", "Their reduction is based on a technique called reduced-order trajectory segment linearization.", "The authors showed via numerical simulations that, while the proposed methods lead to improved computational efficiency, they do not involve reduced control performance."], "text_after_citation": ["Given this context, the interplay between control and centrality has been actively investigated. Liu et al.", "#OTHEREFR introduced the concept of control centrality to quantify the ability of a single node to control the entire network.", "Inspired by the relationship between control centrality and the hierarchical structure in networks, the authors designed efficient attack strategies against the controllability of malicious networks. Fitch et al.", "#OTHEREFR showed that the tracking performance of any leader set within a multiagent system can be quantified by a novel centrality measure called joint centrality.", "For both single and multiple leaders, the authors have analytically proven the effectiveness of the centrality measure for leader selection."], "citing_paper_content": {"title": "Reduced-Order Model Predictive Control Of A Fish Schooling Model", "abstract": "We study the problem of model predictive control (MPC) for the fish schooling model proposed by Gautrais et al. (Annales Zoologici Fennici, 2008). The high nonlinearity of the model attributed to its attraction/alignment/repulsion law suggests the need to use MPC for controlling the fish schooling's motion. However, for large schools, the hybrid nature of the law can make it numerically demanding to perform finite-horizon optimizations in MPC. Therefore, this paper proposes reducing the fish schooling model for numerically efficient MPC; the reduction is based on using the weighted average of the directions of individual fish in the school. We analytically show how using the normalized eigenvector centrality of the alignment-interaction network can yield a better reduction by comparing reduction errors. We confirm this finding on the weight and numerical efficiency of the MPC with the reduced-order model by numerical simulations. The proposed reduction allows us to control a school with up to 500 individuals. Further, we confirm that reduction with the normalized eigenvector centrality allows us to improve the control accuracy by factor of five when compared to that using constant weights."}, "cited_paper_content": {"title": "Axioms For Centrality", "abstract": "Given a social network, which of its nodes are more central? This question has been asked many times in sociology, psychology and computer science, and a whole plethora of centrality measures (a.k.a. centrality indices, or rankings) were proposed to account for the importance of the nodes of a network. In this paper, we try to provide a mathematically sound survey of the most important classic centrality measures known from the literature and propose an axiomatic approach to establish whether they are actually doing what they have been designed for. Our axioms suggest some simple, basic properties that a centrality measure should exhibit. Surprisingly, only a new simple measure based on distances, harmonic centrality, turns out to satisfy all axioms; essentially, harmonic centrality is a correction to Bavelas's classic closeness centrality designed to take unreachable nodes into account in a natural way. As a sanity check, we examine in turn each measure under the lens of information retrieval, leveraging state-of-the-art knowledge in the discipline to measure the effectiveness of the various indices in locating web pages that are relevant to a query. While there are some examples of this comparisons in the literature, here for the first time we take into consideration centrality measures based on distances, such as closeness, in an information-retrieval setting. The results match closely the data we gathered using our axiomatic approach. Our results suggest that centrality measures based on distances, which have been neglected in information retrieval in favour of spectral centrality measures in the last years, are actually of very high quality; moreover, harmonic centrality pops up as an excellent general-purpose centrality index for arbitrary directed graphs."}, "keywords": ["Network science", "centrality measures"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the reduction of the fish schooling model, the authors used the normalized eigenvector centrality of the alignment-interaction network to yield a better reduction, as shown analytically and confirmed by numerical simulations (Axioms For Centrality). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Centrality measures based on distances, such as closeness, have been neglected in information retrieval in favour of spectral centrality measures in the last years, but the results of #REFR suggest that they are actually of very high quality.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Network science has proposed a plethora of centrality measures to account for the importance of the nodes of a network, and a mathematically sound survey of the most important classic centrality measures known from the literature has been provided in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["Network science", "centrality measures"]}]}
{"citing_id": "2303.15684v1", "cited_id": "2001.06426", "section_title": "Vi. Implications And Challenges", "citation": "Complementing the work of Nayebi #REFR who reported that 87.8% of survey respondents agreed that images provide additional information compared to text, we empirically show how images are used and how essential they are to understand a question.", "text_before_citation": ["Results from RQ1 reveal six types of image contents (i.e., User interface, Source code, Error code, Diagram, Results, and Configuration) and four types of image sources (i.e., Desktop Application, Web Browser, Mobile Application, and Usercreated).", "Furthermore, we observe that images are essential and provide complementary information compared to their associated text.", "As shown in Table 2 , we find that 68% of images are essential for the questions.", "For example, in this question #OTHEREFR , the developer faced a problem when using Bootstrap input groups and applying the negative margin.", "To seek answers, the developer posted an image showing the unexpected user interface where the bottom highlight of the input was hidden to support the problem description."], "text_after_citation": ["For developers, our results provide concrete recommendations on the types of images that are commonly shared in specific situations.", "Stack Overflow in general intends to be against the usage of images as raised in this post #OTHEREFR , hence our study would be used to complement the evidence-based guidelines.", "For example, developers are commonly required to take a screenshot of their desktop (47%) to present a user interface (58%).", "Furthermore, users tend to provide images when asking about undesired outputs in their user interface.", "Meanwhile, in the aforementioned developer post, the developer expressed one of the major concerns of \"links to images fail\"."], "citing_paper_content": {"title": "Understanding The Role Of Images On Stack Overflow", "abstract": "Images are increasingly being shared by software developers in diverse channels including question-and-answer forums like Stack Overflow. Although prior work has pointed out that these images are meaningful and provide complementary information compared to their associated text, how images are used to support questions is empirically unknown. To address this knowledge gap, in this paper we specifically conduct an empirical study to investigate (I) the characteristics of images, (II) the extent to which images are used in different question types, and (III) the role of images on receiving answers. Our results first show that user interface is the most common image content and undesired output is the most frequent purpose for sharing images. Moreover, these images essentially facilitate the understanding of 68% of sampled questions. Second, we find that discrepancy questions are more relatively frequent compared to those without images, but there are no significant differences observed in description length in all types of questions. Third, the quantitative results statistically validate that questions with images are more likely to receive accepted answers, but do not speed up the time to receive answers. Our work demonstrates the crucial role that images play by approaching the topic from a new angle and lays the foundation for future opportunities to use images to assist in tasks like generating questions and identifying question-relatedness."}, "cited_paper_content": {"title": "Eye Of The Mind: Image Processing For Social Coding", "abstract": "Developers are increasingly sharing images in social coding environments alongside the growth in visual interactions within social networks. The analysis of the ratio between the textual and visual content of Mozilla's change requests and in Q/As of StackOverflow programming revealed a steady increase in sharing images over the past five years. Developers' shared images are meaningful and are providing complementary information compared to their associated text. Often, the shared images are essential in understanding the change requests, questions, or the responses submitted. Relying on these observations, we delve into the potential of automatic completion of textual software artifacts with visual content."}, "keywords": ["images"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As shown in Table 2, we find that 68% of images are essential for the questions, which is consistent with the observation in #REFR that developers' shared images are meaningful and provide complementary information compared to their associated text.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the reference paper #REFR, developers are increasingly sharing images in social coding environments, and these images are meaningful and provide complementary information compared to their associated text.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the reference paper #REFR, images shared by developers are meaningful and provide complementary information compared to their associated text, which is consistent with our finding that 68% of sampled questions require images to facilitate understanding.", "given_citation_intent": "background", "given_keywords": ["images"]}]}
{"citing_id": "2305.00869v1", "cited_id": "1809.01812", "section_title": "Representation Learning For Spatialmultiomniglot", "citation": "In line with the finding of #REFR , increasing the number of K not only helps MDRE to reach the ground truth MI, but also the quality of representations improves from 86.7% to 100% test classification accuracy.", "text_before_citation": ["Figure 7b illustrates that MDRE's encoder learns representations that achieve \u223c100% Omniglot character classification for both d = n 2 = 4, 9.", "On the other hand, the performances of the single ratio estimator and TRE (using the same exact dimension-wise mixing to construct auxiliary distributions) both degrade as the complexity of the task increases, with TRE only reaching up to 91% and 85% for d = 4 and d = 9, respectively.", "All models were trained with the same encoder architecture to ensure fair comparison.", "We further studied the effect of changing K in the d = 4 setup.", "For K = 1, we aggregate all the dimensionwise mixed samples into 1 class, whereas for K = 3, we separate them into their respective classes (corresponding to the number of dimensions mixed). We illustrate this effect in Figure 7c ."], "text_after_citation": [], "citing_paper_content": {"title": "Estimating The Density Ratio Between Distributions With High Discrepancy Using Multinomial Logistic Regression", "abstract": "Functions of the ratio of the densities p/q are widely used in machine learning to quantify the discrepancy between the two distributions p and q. For high-dimensional distributions, binary classification-based density ratio estimators have shown great promise. However, when densities are well separated, estimating the density ratio with a binary classifier is challenging. In this work, we show that the state-of-the-art density ratio estimators perform poorly on well separated cases and demonstrate that this is due to distribution shifts between training and evaluation time. We present an alternative method that leverages multi-class classification for density ratio estimation and does not suffer from distribution shift issues. The method uses a set of auxiliary densities {m k } K k=1 and trains a multi-class logistic regression to classify the samples from p, q and {m k } K k=1 into K + 2 classes. We show that if these auxiliary densities are constructed such that they overlap with p and q, then a multi-class logistic regression allows for estimating log p/q on the domain of any of the K + 2 distributions and resolves the distribution shift problems of the current state-ofthe-art methods. We compare our method to state-of-the-art density ratio estimators on both synthetic and real datasets and demonstrate its superior performance on the tasks of density ratio estimation, mutual information estimation, and representation learning. Code: https://www.blackswhan.com/mdre/"}, "cited_paper_content": {"title": "Noise Contrastive Estimation And Negative Sampling For Conditional Models: Consistency And Statistical Efficiency", "abstract": "Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and trade-offs of both methods."}, "keywords": ["100% test classification"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The performances of the single ratio estimator and TRE (using the same exact dimension-wise mixing to construct auxiliary distributions) both degrade as the complexity of the task increases, with TRE only reaching up to 91% and 85% for d = 4 and d = 9, respectively. All models were trained with the same encoder architecture to ensure fair comparison. We further studied the effect of changing K in the d = 4 setup. For K = 1, we aggregate all the dimensionwise mixed samples into 1 class, whereas for K = 3, we separate them into their respective classes (corresponding to the number of dimensions mixed). We illustrate this effect in Figure 7c. The authors need to cite the reference paper for their work on negative sampling methods and NCE-based estimation of conditional models, which provides a rigorous theoretical analysis of NCE in this setting and analyzes the statistical efficiency of the ranking-based and classification-based variants of NCE. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' multi-class logistic regression method for density ratio estimation, which does not suffer from distribution shift issues, outperforms state-of-the-art density ratio estimators on various tasks, as demonstrated in their experiments on synthetic and real datasets #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The multi-class logistic regression method proposed in this manuscript achieves superior performance on the task of 100% Omniglot character classification, compared to the single ratio estimator and TRE, as demonstrated in Figure 7b, which is consistent with the findings in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["100% test classification"]}]}
{"citing_id": "2303.14727v1", "cited_id": "1711.10275", "section_title": "Setting", "citation": "Through these results, we demonstrate that our approach can produce segmentation results that are comparable to the fully supervised baseline #REFR with only 0.02% annotation.", "text_before_citation": ["\u2020 means disabling graph propagation and relation network during inference, but note that they are still used in training. mIoU as shown in Table 4 .", "This experiment also demonstrates that our method can still achieve decent performance even though the annotator ignores several objects by mistake in \"One Thing One Click\" scheme.", "We further investigate the performance drop with a more challenging \"Four Things One Click\" scheme.", "However, the model cannot converge well in the very first iteration due to the insufficient label and the self-training fails in this case.", "Qualitative Results on ScanNet-v2 Then, we show prediction results on ScanNet-v2 in Figures 8."], "text_after_citation": ["See the error maps shown in (d) and (f) for better visualizations.", "Ablation Studies To further study the effectiveness of selftraining, graph propagation and relation network, we conduct ablation studies on these three modules on ScanNet-v2 validation set as shown in Table 5 with single view evaluation.", "\"3D U-Net\" indicates that the labels are propagated only based on the confidence score of the 3D U-Net itself, i.e., the unary term in Equation 2.", "This ablation is designed to manifest the effectiveness of self-training.", "The \"3D U-Net\" column in Table 5 manifests that the performance is consistently improved with self-training strategy even without pairwise energy term in Equation 2 and super-voxel partition."], "citing_paper_content": {"title": "One Thing One Click++: Self-Training For Weakly Supervised 3D Scene Understanding", "abstract": "3D scene understanding, e.g., point cloud semantic and instance segmentation, often requires large-scale annotated training data, but clearly, point-wise labels are too tedious to prepare. While some recent methods propose to train a 3D network with small percentages of point labels, we take the approach to an extreme and propose \"One Thing One Click,\" meaning that the annotator only needs to label one point per object. To leverage these extremely sparse labels in network training, we design a novel self-training approach, in which we iteratively conduct the training and label propagation, facilitated by a graph propagation module. Also, we adopt a relation network to generate the per-category prototype to enhance the pseudo label quality and guide the iterative training. Besides, our model can be compatible to 3D instance segmentation equipped with a point-clustering strategy. Experimental results on both ScanNet-v2 and S3DIS show that our self-training approach, with extremely-sparse annotations, outperforms all existing weakly supervised methods for 3D semantic and instance segmentation by a large margin, and our results are also comparable to those of the fully supervised counterparts. Codes and models are available at https://github.com/liuzhengzhe/One-Thing-One-Click."}, "cited_paper_content": {"title": "3D Semantic Segmentation With Submanifold Sparse Convolutional Networks", "abstract": "Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard \"dense\" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition."}, "keywords": ["fully supervised baseline"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate the strong performance of their submanifold sparse convolutional networks (SSCNs) on semantic segmentation of 3D point clouds, outperforming all prior state-of-the-art methods on the test set of a recent competition #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our self-training approach for 3D semantic and instance segmentation achieves state-of-the-art performance on both ScanNet-v2 and S3DIS datasets, outperforming all existing weakly supervised methods by a large margin, and even comparable to fully supervised counterparts, as shown in Table 4. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our self-training approach outperforms all existing weakly supervised methods for 3D semantic and instance segmentation by a large margin, and our results are also comparable to those of the fully supervised baseline #REFR.", "given_citation_intent": "result", "given_keywords": ["fully supervised baseline"]}]}
{"citing_id": "2304.08733v1", "cited_id": "1908.07086", "section_title": "A View From Human Difficulty", "citation": "We calculated average time consumption using the CIFAR-H dataset #REFR , since CIFAR-N lacks annotator time information.", "text_before_citation": ["We conduct a similar analysis as Section 4.2 to seek opportunities for human-machine complimentary teaming from the human difficulty perspective instead.", "To quantify the human difficulty levels we use time spent labeling an image, and an entropy-based measure of human agreement.", "Time Spent: Assuming the i-th human annotator spent t i (x) time (in seconds) in annotating sample x, we adopt the average time spent on sample x as a measure to indicate the difficulty level of the given task x. Mathematically, t(x) := 1 k i\u2208[k] t i (x).", "A larget(x) means the sample x is relatively hard for human annotators since it requires humans to spend a long time on annotation."], "text_after_citation": ["Human agreement (entropy): Given k human annotators for a sample x, we calculate the entropy on x as:", "Entropy(x) = \u2212 i\u2208[K] p H,i (x) \u2022 log (p H,i (x)), where p H,i (x) := 1 k j\u2208[k] 1(f ML,j (x) = i).", "The human agreement is a metric to evaluate human consensus on a task given multiple annotations.", "A higher agreement level implies less ambiguity and easier judgment.", "We calculate inverse entropy using CIFAR-H because it provides 50 labels per image, enough to calculate agreement."], "citing_paper_content": {"title": "Do Humans And Machines Have The Same Eyes? Human-Machine Perceptual Differences On Image Classification", "abstract": "Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone."}, "cited_paper_content": {"title": "Human Uncertainty Makes Classification More Robust", "abstract": "The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks."}, "keywords": ["CIFAR-H dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To quantify the difficulty levels of human annotation, the authors of this study adopted the average time spent on a sample as a measure, which is similar to the approach used in the CIFAR10H dataset #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the perceptual difference between humans and machines, we adopt a new benchmark dataset CIFAR10H, which contains a full distribution of human labels for each image of the CIFAR10 test set, and explicitly train with full label distributions that reflect human perceptual uncertainty, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of generalization and robustness of deep neural networks, the authors of the manuscript use the CIFAR10H dataset, which contains a full distribution of human labels for each image of the CIFAR10 test set, and show that explicit training on this dataset closes the gap between human and machine uncertainty, leading to improved generalization and robustness to adversarial attacks (#REFR).", "given_citation_intent": "method", "given_keywords": ["CIFAR-H dataset"]}]}
{"citing_id": "2304.02554v1", "cited_id": "1904.09675", "section_title": "Automatic Evaluation Metrics", "citation": "BERTScore #REFR assesses the similarity between two texts at the token level by measuring the soft overlap using contextual embeddings from BERT.", "text_before_citation": ["We select several evaluation metrics that are commonly used in summarization:", "ROUGE #OTHEREFR , which is the dominant automatic evaluation metric in summarization, is widely used by researchers.", "The most commonly used ROUGE measures are ROUGE-1, ROUGE-2, and ROUGE-L, which evaluate the similarity between two texts based on the overlap of unigrams, bigrams, and the longest common sequence."], "text_after_citation": ["Similarly, MoverScore #OTHEREFR uses n-gram embeddings that are pooled from BERT to compute the semantic distance between two texts at the n-gram level.", "BARTScore #OTHEREFR 1 views evaluation as a natural language generation task and considers that when the quality of the generated text is higher, BART is more likely to generate it from the source text or the reference, or to generate the reference from it.", "BARTScore can be flexibly applied to evaluate text from various perspectives.", "FactCC 2 and DAE 3 are two factuality metrics based on classification.", "When evaluating a summary, we use NLTK 4 to split it into individual sentences and classify each one as factually correct or not."], "citing_paper_content": {"title": "Human-Like Summarization Evaluation With Chatgpt", "abstract": "Evaluating text summarization is a challenging problem, and existing evaluation metrics are far from satisfactory. In this study, we explored ChatGPT's ability to perform humanlike summarization evaluation using four human evaluation methods on five datasets. We found that ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation. Additionally, it outperformed commonly used automatic evaluation metrics on some datasets. Furthermore, we discussed the impact of different prompts, compared its performance with that of human evaluation, and analyzed the generated explanations and invalid responses."}, "cited_paper_content": {"title": "Bertscore: Evaluating Text Generation With Bert", "abstract": "We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics."}, "keywords": ["BERTScore", "contextual embeddings"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We select several evaluation metrics that are commonly used in summarization: ROUGE #OTHEREFR, which is the dominant automatic evaluation metric in summarization, is widely used by researchers. Another automatic evaluation metric proposed by the authors of Bertscore is based on contextual embeddings and correlates better with human judgments and provides stronger model selection performance than existing metrics #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of our summarization model, we used several commonly used automatic evaluation metrics, including ROUGE #OTHEREFR, as well as the recently proposed BERTScore method from the paper \"Bertscore: Evaluating Text Generation With Bert\". #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of existing automatic evaluation metrics in summarization, we can use BERTScore, which computes token similarity using contextual embeddings instead of exact matches, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["BERTScore", "contextual embeddings"]}]}
{"citing_id": "2304.02089v1", "cited_id": "1811.10155", "section_title": "Dataset", "citation": "Following the same paradigm used in #REFR , we construct queries for each purchased item with category information.", "text_before_citation": ["Besides, we selected three categories with different sizes: Clothing, Shoes & Jewelry, Toys & Games, Electronics.", "These datasets both contain several categories so that users may have different interests.", "Following the strategy in References #OTHEREFR , we extracted the users' product purchasing behaviors based on their reviews, i.e., the products they reviewed are the ones they purchased.", "Our model uses the previously purchased products in a neighboring window size to model the short-term user interests.", "We further filtered the dataset to make sure each user has at least 20 purchased products (i.e., 20 reviews)."], "text_after_citation": ["This strategy is based on the finding that directed product search is users' search for a producer's name, a brand, or a set of terms describing product category.", "We partitioned each of the four datasets into three sets: training, validation and testing sets.", "We first extracted the userproduct pairs from users' reviews, and then extracted the queries for these products, getting triplets.", "For each dataset, the last purchasing transaction of each user is held for the testing set, the second last for the validation set, and the rest for the training set.", "Moreover, we hid the reviews of the validation and testing sets in the training phase to simulate the real-world scenarios."], "citing_paper_content": {"title": "Hierarchically Fusing Long And Short-Term User Interests For Click-Through Rate Prediction In Product Search", "abstract": "Estimating Click-Through Rate (CTR) is a vital yet challenging task in personalized product search. However, existing CTR methods still struggle in the product search settings due to the following three challenges including how to more effectively extract users' short-term interests with respect to multiple aspects, how to extract and fuse users' long-term interest with short-term interests, how to address the entangling characteristic of long and short-term interests. To resolve these challenges, in this paper, we propose a new approach named Hierarchical Interests Fusing Network (HIFN), which consists of four basic modules namely Short-term Interests Extractor (SIE), Long-term Interests Extractor (LIE), Interests Fusion Module (IFM) and Interests Disentanglement Module (IDM). Specifically, SIE is proposed to extract user's short-term interests by integrating three fundamental interests encoders within it namely query-dependent, target-dependent and causal-dependent interest encoder, respectively, followed by delivering the resultant representation to the module LIE, where it can effectively capture user longterm interests by devising an attention mechanism with respect to the short-term interests from SIE module. In IFM, the achieved long and short-term interests are further fused in an adaptive manner, followed by concatenating it with original raw context features for the final prediction result. Last but not least, considering the entangling characteristic of long and short-term interests, IDM further devises a self-supervised framework to disentangle long and short-term interests. Extensive offline and online evaluations on a real-world e-commerce platform demonstrate the superiority of HIFN over state-of-the-art methods."}, "cited_paper_content": {"title": "Attentive Long Short-Term Preference Modeling For Personalized Product Search", "abstract": "E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well-known that there are two types of preferences: long-term ones and short-term ones. The former refers to user' inherent purchasing bias and evolves slowly. By contrast, the latter reflects users' purchasing inclination in a relatively short period. They both affect users' current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users' current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics."}, "keywords": ["category information"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To model the short-term user interests, the authors extracted the previously purchased products in a neighboring window size, as described in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model short-term user interests, the authors of this manuscript used the previously purchased products in a neighboring window size, as well as the users' product purchasing behaviors based on their reviews, which were extracted from the products they reviewed and purchased #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model short-term user interests in the Clothing, Shoes & Jewelry, Toys & Games, and Electronics categories, the authors extracted users' product purchasing behaviors based on their reviews, and used the previously purchased products in a neighboring window size, as described in the method proposed by the Attentive Long Short-Term Preference Modeling For Personalized Product Search paper #REFR.", "given_citation_intent": "method", "given_keywords": ["category information"]}]}
{"citing_id": "2303.12936v1", "cited_id": "1706.03762", "section_title": "Comparing Bert And Distilbert", "citation": "DistilBERT, as a transformerbased model, is better in capturing long-term dependencies in an input sequence #REFR .", "text_before_citation": ["Recently, it was shown that ELMo and BERT make no significant difference in semantic analysis #OTHEREFR .", "Here it is observed that although they are close-by in the null context, DistilBERT is more robust than ELMo in the cross-context in text classification.", "The findings of this study are in line with prior work.", "The fairly comparable scores of ELMo and the traditional baselines in the null context supports the observation of #OTHEREFR that is, when it comes to contextual embeddings, there is only a small improvement in learning semantics over traditional ML methods.", "DistilBERT is on par with or exceeding ELMo on a binary text classification task #OTHEREFR ."], "text_after_citation": ["DistilBERT is lighter than ELMo and has a shorter training time #OTHEREFR .", "Here it should be noted that the experimental settings of the previous work and In this study, ELMo and DistilBERT are compared on their fine-tuning performance on two binary text classification tasks.", "The main focus was to see how much can these models be benefited in a practical way without any modification to the pretraining outputs.", "But the models were actually pretrained on entirely different corpora (ELMo on One Billion Words Benchmark #OTHEREFR , DistilBERT on English Wikipedia and Toronto BookCorpus #OTHEREFR ).", "If the models were also pretrained from scratch on the same corpus, it would be ensured that they utilize the same knowledge to learn the context."], "citing_paper_content": {"title": "", "abstract": "I am grateful to my family for their unconditional love and patience. I am grateful to Arzucan\u00d6zg\u00fcr, for being such an inspiring figure by her selfless devotion to research the most righteous way with the passion to contribute to the community. I am grateful to Ali H\u00fcrriyetoglu, for being such a role model, who could somehow always find a way to turn the mist of research questions into a structured path to create practical solutions by combining creativity and technique. I cannot thank enough my dear friends who put up with my whims throughout this journey. I thank fellows from TabiLAB for inspiring me with their brilliance, invaluable insights and recommendations. I thank Ko\u00e7 University EMW research team for their generosity in sharing the data which was created with blood, sweat and tears. I feel lucky that I got to meet fellows in EMW project engineering team who invested their precious time and energy to support me in this study from the very beginning. Lastly, I owe the deepest gratitude to our professors and staff members in our department who taught us how to form such a great community and made it feel like the dearest home from the day one."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["long-term dependencies", "input sequence"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, it was shown that ELMo and BERT make no significant difference in semantic analysis #OTHEREFR. Here it is observed that although they are close-by in the null context, DistilBERT is more robust than ELMo in the cross-context in text classification. The findings of this study are in line with prior work. The fairly comparable scores of ELMo and the traditional baselines in the null context support the observation of #OTHEREFR that is, when it comes to contextual embeddings, there is only a small improvement in learning semantics over traditional ML methods. The Transformer model proposed in #REFR achieves state-of-the-art BLEU scores on machine translation tasks while being more parallelizable and requiring significantly less time to train, making it a promising alternative to traditional recurrent or convolutional neural networks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Transformer network architecture based solely on attention mechanisms has shown superior performance in machine translation tasks compared to complex recurrent or convolutional neural networks #REFR, while being more parallelizable and requiring significantly less time to train.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Transformer model proposed in the reference paper is based solely on attention mechanisms, which allows it to handle long-term dependencies in the input sequence more efficiently than traditional models based on recurrent or convolutional neural networks #REFR.", "given_citation_intent": "background", "given_keywords": ["long-term dependencies", "input sequence"]}]}
{"citing_id": "2304.08072v1", "cited_id": "1805.08841", "section_title": "Introduction", "citation": "It is worth noting that in the literature #REFR , it was emphasized that some unreliable results may be generated during the application of the CycleGAN, resulting in misdiagnosis of the disease. In order to solve this problem, Hiasa et al.", "text_before_citation": ["However, in most cases, it is very difficult to fully obtain the paired medical images, so the monitoring scheme has certain limitations.", "Consequently, unsupervised CycleGAN has a broader application prospect in the field of multimodal medical imaging generation.", "The model can find the mapping relationship between the source image and the target image, which can effectively eliminate the limitation of a pair of corresponding image pairs in the training process.", "In addition, the CycleGAN also uses cycle consistency loss to preserve key attributes between the input image and the generated image. For example, based on the CycleGAN, Wolterink et al.", "realized the direct synthesis of brain CT images from brain MR images without paired data #OTHEREFR ."], "text_after_citation": ["improved the CycleGAN network structure by adding gradient consistency loss during the training, then the accuracy of identifying boundaries is raised #OTHEREFR . While Lilian et al.", "improved the loss function to solve the existed problem that the gradient disappearance caused by the non-coincidence of the true sample and the generated sample is difficult to train #OTHEREFR However, there still exist the problems in the CycleGAN, such as long training time, slow convergence speed and easy to ignore spatial location information.", "In recent years, generative adversarial networks (GANs) #OTHEREFR have been proved to be a promising medical imaging generation method #OTHEREFR .", "GANs have shown excellent performance in the field of medical images, including medical imaging reconstruction #OTHEREFR , medical imaging classification #OTHEREFR , medical imaging detection #OTHEREFR , medical imaging segmentation #OTHEREFR and medical imaging denoising #OTHEREFR .", "Multimodal medical imaging generation obtains the mapping function from the source image to the target image by learning."], "citing_paper_content": {"title": "Two-Stage Mr Image Segmentation Method For Brain Tumors Based On Attention Mechanism", "abstract": "Multimodal magnetic resonance imaging (MRI) can reveal different patterns of human tissue and is crucial for clinical diagnosis. However, limited by cost, noise and manual labeling, obtaining diverse and reliable multimodal MR images remains a challenge. For the same lesion, different MRI manifestations have great differences in background information, coarse positioning and fine structure. In order to obtain better generation and segmentation performance, a coordination-spatial attention generation adversarial network (CASP-GAN) based on the cycle-consistent generative adversarial network (CycleGAN) is proposed. The performance of the generator is optimized by introducing the Coordinate Attention (CA) module and the Spatial Attention (SA) module. The two modules can make full use of the captured location information, accurately locating the interested region, and enhancing the generator model network structure. The ability to extract the structure information and the detailed information of the original medical image can help generate the desired image with higher quality. There exist some problems in the original CycleGAN that the training time is long, the parameter amount is too large, and it is difficult to converge. In response to this problem, we introduce the Coordinate Attention (CA) module to replace the Res Block to reduce the number of parameters, and cooperate with the spatial information extraction network above to strengthen the information extraction ability. On the basis of CASP-GAN, an attentional generative cross-modality segmentation (AGCMS) method is further proposed. This method inputs the modalities generated by CASP-GAN and the real modalities into the segmentation network for brain tumor segmentation. Experimental results show that CASP-GAN outperforms CycleGAN and some state-of-the-art methods in PSNR, SSMI and RMSE in most tasks. In addition, the Dice and Hausdorff95 obtained by AGCMS segmentation are higher than the values corresponding to a single modality, and are close to the values obtained by multiple real modalities, indicating that the method can achieve similar effects as multi-modalities.. In summary, the method proposed in this paper can be used as an effective method for clinical diagnosis of brain tumors and has broad application prospects."}, "cited_paper_content": {"title": "Distribution Matching Losses Can Hallucinate Features In Medical Image Translation", "abstract": "This paper discusses how distribution matching losses, such as those used in CycleGAN, when used to synthesize medical images can lead to mis-diagnosis of medical conditions. It seems appealing to use these new image synthesis methods for translating images from a source to a target domain because they can produce high quality images and some even do not require paired data. However, the basis of how these image translation models work is through matching the translation output to the distribution of the target domain. This can cause an issue when the data provided in the target domain has an over or under representation of some classes (e.g. healthy or sick). When the output of an algorithm is a transformed image there are uncertainties whether all known and unknown class labels have been preserved or changed. Therefore, we recommend that these translated images should not be used for direct interpretation (e.g. by doctors) because they may lead to misdiagnosis of patients based on hallucinated image features by an algorithm that matches a distribution. However there are many recent papers that seem as though this is the goal."}, "keywords": ["CycleGAN", "misdiagnosis"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "However, in most cases, it is very difficult to fully obtain the paired medical images, so the monitoring scheme has certain limitations. Consequently, unsupervised CycleGAN has a broader application prospect in the field of multimodal medical imaging generation. The model can find the mapping relationship between the source image and the target image, which can effectively eliminate the limitation of a pair of corresponding image pairs in the training process. In addition, the CycleGAN also uses cycle consistency loss to preserve key attributes between the input image and the generated image. For example, based on the CycleGAN, Wolterink et al. realized the direct synthesis of brain CT images from brain MR images without paired data (Distribution Matching Losses Can Hallucinate Features In Medical Image Translation).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The unsupervised CycleGAN has a broader application prospect in the field of multimodal medical imaging generation, as it can effectively eliminate the limitation of a pair of corresponding image pairs in the training process, and preserve key attributes between the input image and the generated image using cycle consistency loss #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "CycleGAN has been widely used in the field of multimodal medical imaging generation due to its ability to find the mapping relationship between the source image and the target image, which can effectively eliminate the limitation of a pair of corresponding image pairs in the training process, and the cycle consistency loss to preserve key attributes between the input image and the generated image; however, it has been shown that the use of distribution matching losses in CycleGAN can lead to misdiagnosis of medical conditions, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": ["CycleGAN", "misdiagnosis"]}]}
{"citing_id": "2303.14646v1", "cited_id": "1901.00596", "section_title": "B. Topics Regarding Emerging Ml Techniques", "citation": "In response, graph neural networks (GNNs), the deep learning models that are capable of tackling graph-related tasks, were proposed #REFR .", "text_before_citation": ["This is particularly problematic when a grid corresponds to a large region in the real world.", "Instead, we can use graph models for ride-hailing planning #OTHEREFR , #OTHEREFR . Road networks can be modeled as graphs.", "In a road graph, nodes are geographical locations, and edges are travel paths.", "Additional information, such as the number of riders and drivers, and the travel costs on paths, can be used as representations of the nodes and edges.", "Such graphs are represented in a non-Euclidean space which is difficult for knowledge extraction using traditional ML techniques."], "text_after_citation": ["We propose to use GNN to solve the ride-hailing planning problem in an end-to-end manner.", "Specifically, we can model road networks via graphs, design a GNN-based model to extract features from the graphs, and then map them to planning decisions.", "There are existing works that have employed GNNs in the transportation area, e.g., to calculate traffic predictions #OTHEREFR , #OTHEREFR and demand predictions #OTHEREFR , #OTHEREFR .", "Only a limited number of works however have used GNN in ride-hailing planning, e.g., #OTHEREFR , #OTHEREFR .", "The algorithm in #OTHEREFR can only match drivers and riders that are located on the same road segment, which restricts many other drivers and riders from being matched, and thus could lead to poor performance of their proposed method in terms of measures such as the total waiting times of riders."], "citing_paper_content": {"title": "A Survey Of Machine Learning-Based Ride-Hailing Planning", "abstract": "Ride-hailing is a sustainable transportation paradigm where riders access door-to-door traveling services through a mobile phone application, which has attracted a colossal amount of usage. There are two major planning tasks in a ride-hailing system: (1) matching, i.e., assigning available vehicles to pick up the riders, and (2) repositioning, i.e., proactively relocating vehicles to certain locations to balance the supply and demand of ride-hailing services. Recently, many studies of ride-hailing planning that leverage machine learning techniques have emerged. In this article, we present a comprehensive overview on latest developments of machine learning-based ride-hailing planning. To offer a clear and structured review, we introduce a taxonomy into which we carefully fit the different categories of related works according to the types of their planning tasks and solution schemes, which include collective matching, distributed matching, collective repositioning, distributed repositioning, and joint matching and repositioning. We further shed light on many real-world datasets and simulators that are indispensable for empirical studies on machine learning-based ride-hailing planning strategies. At last, we propose several promising research directions for this rapidly growing research and practical field."}, "cited_paper_content": {"title": "A Comprehensive Survey On Graph Neural Networks", "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field."}, "keywords": ["graph-related tasks", "graph neural networks"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Such graphs are represented in a non-Euclidean space which is difficult for knowledge extraction using traditional ML techniques. To address this challenge, recent studies have proposed the use of graph neural networks (GNNs) for machine learning on graph data #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the difficulty of knowledge extraction using traditional ML techniques in non-Euclidean spaces, graph neural networks (GNNs) have emerged as a promising approach for extending deep learning to graph data #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of knowledge extraction in non-Euclidean spaces, graph neural networks (GNNs) have emerged as a promising approach for graph-related tasks #REFR.", "given_citation_intent": "background", "given_keywords": ["graph-related tasks", "graph neural networks"]}]}
{"citing_id": "2303.08544v1", "cited_id": "1404.5859", "section_title": "Many-To-One-Stable Matching Solution", "citation": "The order of preferences is given by the strictly ranked rate utilities of the two sides #REFR .", "text_before_citation": ["In this section, we propose assigning/matching the countermeasures to the attacks by a framework that considers stability as the solution concept instead of optimality. The applied framework involves a two-sided matching game.", "A Stable Matching Problem (SMP) is produced by a distributed process that matches together preference relations of the two sides that are of the same size."], "text_after_citation": ["SM solutions have been broadly used in wireless networks for problem-solving #OTHEREFR .", "In our problem, however, the number of detected attacks might be different from the number of countermeasures (i.e., different set sizes), which means we need to seek a many-to-one generalization of SMP called the HR problem #OTHEREFR ."], "citing_paper_content": {"title": "Joint Security-Vs-Qos Game Theoretical Optimization For Intrusion Response Mechanisms For Future Network Systems", "abstract": "Network connectivity exposes the network infrastructure and assets to vulnerabilities that attackers can exploit. Protecting network assets against attacks requires the application of security countermeasures. Nevertheless, employing countermeasures incurs costs, such as monetary costs, along with time and energy to prepare and deploy the countermeasures. Thus, an Intrusion Response System (IRS) shall consider security and QoS costs when dynamically selecting the countermeasures to address the detected attacks. This has motivated us to formulate a joint Security-vs-QoS optimization problem to select the best countermeasures in an IRS. The problem is then transformed into a matching game-theoretical model. Considering the monetary costs and attack coverage constraints, we first derive the theoretical upper bound for the problem and later propose stable matching-based solutions to address the trade-off. The performance of the proposed solution, considering different settings, is validated over a series of simulations."}, "cited_paper_content": {"title": "Distributed Channel Assignment In Cognitive Radio Networks: Stable Matching And Walrasian Equilibrium", "abstract": "We consider a set of secondary transmitter-receiver pairs in a cognitive radio setting. Based on channel sensing and access performances, we consider the problem of assigning channels orthogonally to secondary users through distributed coordination and cooperation algorithms. Two economic models are applied for this purpose: matching markets and competitive markets. In the matching market model, secondary users and channels build two agent sets. We implement a stable matching algorithm in which each secondary user, based on his achievable rate, proposes to the coordinator to be matched with desirable channels. The coordinator accepts or rejects the proposals based on the channel preferences which depend on interference from the secondary user. The coordination algorithm is of low complexity and can adapt to network dynamics. In the competitive market model, channels are associated with prices and secondary users are endowed with monetary budget. Each secondary user, based on his utility function and current channel prices, demands a set of channels. A Walrasian equilibrium maximizes the sum utility and equates the channel demand to their supply. We prove the existence of Walrasian equilibrium and propose a cooperative mechanism to reach it. The performance and complexity of the proposed solutions are illustrated by numerical simulations."}, "keywords": ["preferences", "strictly ranked rate"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section, we propose assigning/matching the countermeasures to the attacks by a framework that considers stability as the solution concept instead of optimality. The applied framework involves a two-sided matching game. A Stable Matching Problem (SMP) is produced by a distributed process that matches together preference relations of the two sides that are of the same size. This approach is similar to the one proposed in the reference paper #REFR, where the authors consider the problem of assigning channels orthogonally to secondary users through distributed coordination and cooperation algorithms.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To assign channels orthogonally to secondary users through distributed coordination and cooperation algorithms, two economic models are applied for this purpose: matching markets and competitive markets #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To match the preference relations of the two sides, we propose a Stable Matching Problem (SMP) that considers the strictly ranked rate of each secondary user, similar to the approach used in the distributed coordination and cooperation algorithms proposed in #REFR. The SMP produces a distributed process that matches the preferences of the two sides that are of the same size.", "given_citation_intent": "background", "given_keywords": ["preferences", "strictly ranked rate"]}]}
{"citing_id": "2304.11618v1", "cited_id": "1812.06410", "section_title": "C. Rq1: Main Results", "citation": "Besides, MANS performs particularly well in Hit@1 and MRR, which are sensitive to high-rank results #REFR .", "text_before_citation": ["We could observe that existing NS methods have poor performance and they are even worse than the normal NS.", "According to our previous analysis, these NS methods are designed for general KGE models and are unsuitable for the multi-modal scenario where modal information is carefully considered.", "They could not align different embeddings of each entity and get bad performance in MMKGE. The outperformance of MANS.", "MANS could achieve better link prediction results compared with baselines.", "For example, MANS-A achieves much better Hit@1 on FB15K compared with baselines (from 0.318 to 0.353, a relative improvement of 9.9%)."], "text_after_citation": ["This means that MANS can largely improve the accurate discriminatory ability of the model by aligning structural and visual embeddings. Necessity and effectiveness of MANS-V.", "According to the previous section, MANS-V is designed to align different modal information.", "Though it does not perform better than baseline methods, MANS-V is the fundamental component of the other three settings of MANS.", "Besides, we could prove with such a result that both modal alignment and positivenegative discrimination are important for MMKGE, which could be achieved by MANS-V and normal NS respectively.", "MANS-T, MANS-H, and MANS-A could perform better because they combine the advantages of both. In summary, MANS-V is a necessary design for MMKGE. Comparison of different MANS settings."], "citing_paper_content": {"title": "Modality-Aware Negative Sampling For Multi-Modal Knowledge Graph Embedding", "abstract": "Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness."}, "cited_paper_content": {"title": "Nscaching: Simple And Efficient Negative Sampling For Knowledge Graph Embedding", "abstract": "Knowledge graph (KG) embedding is a fundamental problem in data mining research with many real-world applications. It aims to encode the entities and relations in the graph into low dimensional vector space, which can be used for subsequent algorithms. Negative sampling, which samples negative triplets from non-observed ones in the training data, is an important step in KG embedding. Recently, generative adversarial network (GAN), has been introduced in negative sampling. By sampling negative triplets with large scores, these methods avoid the problem of vanishing gradient and thus obtain better performance. However, using GAN makes the original model more complex and harder to train, where reinforcement learning must be used. In this paper, motivated by the observation that negative triplets with large scores are important but rare, we propose to directly keep track of them with cache. However, how to sample from and update the cache are two important questions. We carefully design the solutions, which are not only efficient but also achieve good balance between exploration and exploitation. In this way, our method acts as a \"distilled\" version of previous GAN-based methods, which does not waste training time on additional parameters to fit the full distribution of negative triplets. The extensive experiments show that our method can gain significant improvement on various KG embedding models, and outperform the state-of-the-arts negative sampling methods based on GAN."}, "keywords": ["high-rank results"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Existing negative sampling (NS) methods have been shown to have poor performance in multi-modal knowledge graph embedding (MMKGE) scenarios where modal information is carefully considered, as they are designed for general KGE models. They could not align different embeddings of each entity and get bad performance in MMKGE. To address this issue, the authors propose Modality-Aware Negative Sampling (MANS) for MMKGE, which could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the inefficiency of existing negative sampling (NS) methods in multi-modal knowledge graph embedding (MMKGE), the authors proposed Modality-Aware Negative Sampling (MANS) that could align different embeddings of each entity and achieve better link prediction results compared with baselines (Li et al., 2021). #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of poor performance of existing negative sampling (NS) methods in multi-modal knowledge graph embedding (MMKGE), the authors proposed Modality-Aware Negative Sampling (MANS) which could align different embeddings of each entity and achieve high-rank results compared with baselines (#REFR).", "given_citation_intent": "background", "given_keywords": ["high-rank results"]}]}
{"citing_id": "2303.05683v1", "cited_id": "1109.2378", "section_title": "Introduction", "citation": "Hierarchical agglomerative clustering algorithms (e.g., #REFR ) allow for partitioning the datasets for which merely a pairwise distance function (e.g., a metric) is defined.", "text_before_citation": ["Cluster analysis (e.g., #OTHEREFR ) is a machine learning technique where we discover interesting or otherwise useful partitions of a given dataset in a purely unsupervised way."], "text_after_citation": ["Most importantly, the number of clusters is not set in advance -a whole hierarchy of nested partitions can be generated with ease and then depicted on a tree-like diagram called a dendrogram.", "Hierarchical agglomerative clustering evolves around one simple idea: in each step, we merge the pair of closest clusters.", "To measure the proximity between two point sets, the intracluster distance is defined as an extension of the point-pairwise distance called a linkage function.", "For instance, in the single linkage approach, the distance between a cluster pair is given by the distance between the closest pair of points, one from the first cluster, the other from the second one.", "In complete linkage, on the other hand, we take the farthest-away pair."], "citing_paper_content": {"title": "Hierarchical Clustering With Owa-Based Linkages, The Lance-Williams Formula, And Dendrogram Inversions", "abstract": "Agglomerative hierarchical clustering based on Ordered Weighted Averaging (OWA) operators not only generalises the single, complete, and average linkages, but also includes intercluster distances based on a few nearest or farthest neighbours, trimmed and winsorised means of pairwise point similarities, amongst many others. We explore the relationships between the famous Lance-Williams update formula and the extended OWA-based linkages with weights generated via infinite coefficient sequences. Furthermore, we provide some conditions for the weight generators to guarantee the resulting dendrograms to be free from unaesthetic inversions."}, "cited_paper_content": {"title": "Modern Hierarchical, Agglomerative Clustering Algorithms", "abstract": "This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a \"stepwise dendrogram\", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes."}, "keywords": ["Hierarchical agglomerative clustering"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Cluster analysis (e.g., #REFR) is a machine learning technique where we discover interesting or otherwise useful partitions of a given dataset in a purely unsupervised way. The reference paper presents efficient algorithms for hierarchical, agglomerative clustering, which can be useful for discovering such partitions.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Agglomerative clustering algorithms have been developed for the general-purpose setup that is given in modern standard software, where the input data is given by pairwise dissimilarities between data points, and the output is a \"stepwise dendrogram\" (#OTHEREFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Hierarchical agglomerative clustering is a widely used technique in machine learning for discovering partitions of a given dataset, and various algorithms have been proposed for this purpose, including the efficient ones presented in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["Hierarchical agglomerative clustering"]}]}
{"citing_id": "2303.01338v1", "cited_id": "1812.02843", "section_title": "C. Impact On Network Interpretation", "citation": "Although it has been demonstrated that adversarial patches are quite powerful at causing misclassi cation, these patches are highlighted using standard network interpretation methods, thereby disclosing the identity of the adversary #REFR .", "text_before_citation": [], "text_after_citation": ["One of the most well-known network interpretation algorithms, Grad-CAM #OTHEREFR , outperforms other state-of-theart interpretation algorithms on a sanity check. e Grad-CAM visualization results for traditional adversarial patch vs.", "AdvRain are evaluated using an ImageNet pretrained VGG-19 classi er (adding low vs high frequency pa erns).", "Unlike patch-based a acks that shi the model's focus from the object to the location of the patch, making them detectable, our a ack causes the model to overlook some important features that help the model make the decision, as shown in Figure 11 ."], "citing_paper_content": {"title": "Advrain: Adversarial Raindrops To A Ack Camera-Based Smart Vision Systems", "abstract": "Vision-based perception modules are increasingly deployed in many applications, especially autonomous vehicles and intelligent robots. ese modules are being used to acquire information about the surroundings and identify obstacles. Hence, accurate detection and classi cation are essential to reach appropriate decisions and take appropriate and safe actions at all times. Current studies have demonstrated that \"printed adversarial attacks\", known as physical adversarial attacks, can successfully mislead perception models such as object detectors and image classi ers. However, most of these physical attacks are based on noticeable and eye-catching patterns for generated perturbations making them identi able/detectable by human eye or in test drives. In this paper, we propose a camera-based inconspicuous adversarial attack (AdvRain) capable of fooling camera-based perception systems over all objects of the same class. Unlike mask based fake-weather attacks that require access to the underlying computing hardware or image memory, our attack is based on emulating the e ects of a natural weather condition (i.e., Raindrops) that can be printed on a translucent sticker, which is externally placed over the lens of a camera. Note, such perturbations are still inconspicuous in real-world deployments and their presence goes unnoticed due to their association with a natural phenomenon (as also advocated in [1]). To accomplish this, we provide an iterative process based on performing a random search aiming to identify critical positions to make sure that the performed transformation is adversarial for a target classi er. Our transformation is based on blurring prede ned parts of the captured image corresponding to the areas covered by the raindrop. We achieve a drop in average model accuracy of more than 45% and 40% on VGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only 20 raindrops. Index Terms-Adversarial machine learning, physical adversarial attack, Security, e ciency, perturbations, physical attacks. I. I e emergence of deep learning (DL) is creating disruptive transformations in a wide range of sectors especially autonomous driving [2]. For instance, leading manufacturers such as Google, Audi, BMW, and Tesla are striving to create autonomous vehicles (AVs) by combining this cu ing-edge technology with low-cost cameras forming the vision-based perception modules. AVs are being increasingly equipped with these modules to address high-pressure real-life scenarios, reach suitable decisions, and take appropriate and safe actions. In fact, their incorporation increased product demand and helped the market of autonomous vehicles grow. According to the Strategic Market Research (SMR), the market for autonomous vehicles will reach $196.97 billion by 2030 [3], growing at a CAGR of 25.7%."}, "cited_paper_content": {"title": "Towards Hiding Adversarial Examples From Network Interpretation", "abstract": "This work was performed under ::: the following financial assistance award: 60NANB18D279 ::: from U.S. Department of Commerce, National Institute of ::: Standards and Technology, and also funding from SAP SE"}, "keywords": ["adversarial patches"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The emergence of deep learning (DL) is creating disruptive transformations in a wide range of sectors, especially autonomous driving. For instance, leading manufacturers such as Google, Audi, BMW, and Tesla are striving to create autonomous vehicles (AVs) by combining this cutting-edge technology with low-cost cameras forming the vision-based perception modules. AVs are being increasingly equipped with these modules to address high-pressure real-life scenarios, reach suitable decisions, and take appropriate and safe actions. In fact, their incorporation increased product demand and helped the market of autonomous vehicles grow. According to the Strategic Market Research (SMR), the market for autonomous vehicles will reach $196.97 billion by 2030 [3], growing at a CAGR of 25.7%. The proposed AdvRain attack is capable of fooling camera-based perception systems over all objects of the same class, achieving a drop in average model accuracy of more than 45% and 40% on VGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only 20 raindrops. To accomplish this, the authors performed an iterative process based on performing a random search aiming to identify critical positions to make sure that the performed transformation is adversarial for a target classifier. The proposed transformation is based on blurring predefined parts of the captured image corresponding to the areas covered by the raindrop. The authors acknowledge the financial assistance award from U.S. Department of Commerce, National Institute of Standards and Technology, and also funding from SAP SE #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To create inconspicuous adversarial attacks, the authors of this paper propose a method based on emulating the effects of natural weather conditions, which was shown to be effective in fooling camera-based perception systems over all objects of the same class, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate inconspicuous adversarial attacks, the authors of this paper propose a method based on emulating the effects of natural weather conditions, which can be printed on a translucent sticker and placed over the lens of a camera, achieving a drop in average model accuracy of more than 45% and 40% on VGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only 20 raindrops (#REFR). The proposed method is a significant improvement over previous physical adversarial attacks, which were based on noticeable and eye-catching patterns for generated perturbations, making them detectable by human eyes or in test drives.", "given_citation_intent": "method", "given_keywords": ["adversarial patches"]}]}
{"citing_id": "2305.02911v1", "cited_id": "1610.02391", "section_title": "Related Work", "citation": "Such interpretations can help improve the transparency and trustworthiness of the models, as well as identify potential sources of bias or error #REFR .", "text_before_citation": ["Since the UPD annotation in this dataset is based on a single factor, it cannot be utilized for UPD detection with complex scenarios and UPD factor ranking analysis.", "Consequently, we construct our experimental data using the Place Pulse 2.0 dataset in this work.", "Interpretability of Deep Learning Models.", "Although deep learning techniques have shown significant potential to achieve high accuracy for various complex tasks, it is crucial to understand how the model reaches its conclusions.", "This has led to a growing interest in developing methods for interpreting deep learning models, with the goal of understanding the reasoning behind their predictions."], "text_after_citation": ["Visual explanations are a class of methods for interpreting deep learning models that use visualizations to highlight the features of the input data that are most important for the model's decision #OTHEREFR .", "These methods offer a clear understanding of how the model processes the input data and which features are responsible for influencing the output.", "Visual explanations can also provide guidance to identify instances where the model may be making errors or exhibiting bias.", "There are various types of visual explanation for deep learning models, such as attention-based maps, activation maps, and occlusion-based methods #OTHEREFR .", "Attention maps #OTHEREFR highlight the regions of an image that are most important to the decision of the model by computing the gradients of the output with respect to the input."], "citing_paper_content": {"title": "Updexplainer: An Interpretable Transformer-Based Framework For Urban Physical Disorder Detection Using Street View Imagery", "abstract": "Urban Physical Disorder (UPD), such as old or abandoned buildings, broken sidewalks, litter, and graffiti, has a negative impact on residents' quality of life. They can also increase crime rates, cause social disorder, and pose a public health risk. Currently, there is a lack of efficient and reliable methods for detecting and understanding UPD. To bridge this gap, we propose UPDExplainer, an interpretable transformer-based framework for UPD detection. We first develop a UPD detection model based on the Swin Transformer architecture, which leverages readily accessible street view images to learn discriminative representations. In order to provide clear and comprehensible evidence and analysis, we subsequently introduce a UPD factor identification and ranking module that combines visual explanation maps with semantic segmentation maps. This novel integrated approach enables us to identify the exact objects within street view images that are responsible for physical disorders and gain insights into the underlying causes. Experimental results on the re-annotated Place Pulse 2.0 dataset demonstrate promising detection performance"}, "cited_paper_content": {"title": "Grad-Cam: Visual Explanations From Deep Networks Via Gradient-Based Localization", "abstract": "We propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a \"stronger\" deep network from a \"weaker\" one. Our code is available at this https URL. A demo and a video of the demo can be found at this http URL and this http URL."}, "keywords": ["interpretations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Although deep learning techniques have shown significant potential to achieve high accuracy for various complex tasks, it is crucial to understand how the model reaches its conclusions. This has led to a growing interest in developing methods for interpreting deep learning models, with the goal of understanding the reasoning behind their predictions. One such method is Gradient-weighted Class Activation Mapping (Grad-CAM), which produces \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Deep learning techniques have shown significant potential to achieve high accuracy for various complex tasks, and interpreting the reasoning behind their predictions has become increasingly important. To provide clear and comprehensible evidence and analysis for Urban Physical Disorder (UPD) detection, UPDExplainer leverages visual explanation maps with semantic segmentation maps to identify the exact objects within street view images that are responsible for physical disorders and gain insights into the underlying causes #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Interpretability of deep learning models has become a growing interest in developing methods for understanding the reasoning behind their predictions, and the proposed technique in #REFR provides a way to produce visual explanations for decisions from a large class of CNN-based models, making them more transparent.", "given_citation_intent": "background", "given_keywords": ["interpretations"]}]}
{"citing_id": "2303.02906v1", "cited_id": "1707.04993", "section_title": "I. Introduction", "citation": "As illustrated in MoCoGAN #REFR , the latent space of GANs can be decomposed into content subspace and motion subspace.", "text_before_citation": ["That results in more complex networks, larger model sizes, and training costs.", "Early works #OTHEREFR - #OTHEREFR attempt to generate temporally coherent videos from random noises with conv-based GAN #OTHEREFR models directly, leading to high computational costs and unsatisfying performance on large-resolution datasets.", "Video synthesis frameworks based on pre-trained image generators are proposed to pursue higher quality and larger resolution, including MoCoGAN-HD and StyleVideoGAN.", "They design motion generators to manipulate latent codes for synthesizing videos based on pre-trained image generators such as BigGAN #OTHEREFR and StyleGAN2 #OTHEREFR .", "StyleGAN-V #OTHEREFR proposes to generate videos by concatenating sequences of encoded motion codes to the constant input tensor of StyleGAN2."], "text_after_citation": ["Following prior works, we focus on studying modern video generation datasets, where frames in a video share the same contents but vary in motions, e.g., persons talking in FaceForensics 256 2 #OTHEREFR and cloud moving in SkyTimelapse 256 2 #OTHEREFR .", "Previous works #OTHEREFR , #OTHEREFR propose sequential generation methods to synthesize videos with image-based generators.", "Additional motion code generators or randomly sampled motion codes are needed for motion generation.", "They train generative models to fit the distribution of videos without unique methods of keeping contents consistent. The contents and motions are learned implicitly during training.", "We find that such methods result in inappropriate content editing when generating motions."], "citing_paper_content": {"title": "Motionvideogan: A Novel Video Generator Based On The Motion Space Learned From Image Pairs", "abstract": "Video generation has achieved rapid progress benefiting from high-quality renderings provided by powerful image generators. We regard the video synthesis task as generating a sequence of images sharing the same contents but varying in motions. However, most previous video synthesis frameworks based on pre-trained image generators treat content and motion generation separately, leading to unrealistic generated videos. Therefore, we design a novel framework to build the motion space, aiming to achieve content consistency and fast convergence for video generation. We present MotionVideoGAN, a novel video generator synthesizing videos based on the motion space learned by pre-trained image pair generators. Firstly, we propose an image pair generator named MotionStyleGAN to generate image pairs sharing the same contents and producing various motions. Then we manage to acquire motion codes to edit one image in the generated image pairs and keep the other unchanged. The motion codes help us edit images within the motion space since the edited image shares the same contents with the other unchanged one in image pairs. Finally, we introduce a latent code generator to produce latent code sequences using motion codes for video generation. Our approach achieves state-of-the-art performance on the most complex video dataset ever used for unconditional video generation evaluation, UCF101. The source code is available on https://github.com/bbzhu-jy16/MotionVideoGAN."}, "cited_paper_content": {"title": "Mocogan: Decomposing Motion And Content For Video Generation", "abstract": "Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion."}, "keywords": ["GANs"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Video synthesis frameworks based on pre-trained image generators are proposed to pursue higher quality and larger resolution, including MoCoGAN-HD and StyleVideoGAN. They design motion generators to manipulate latent codes for synthesizing videos based on pre-trained image generators such as BigGAN and StyleGAN2. The proposed MotionVideoGAN builds on this approach by learning the motion space from image pairs using MotionStyleGAN, and then synthesizing videos using latent code sequences generated by a latent code generator. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pursue higher quality and larger resolution in video synthesis frameworks based on pre-trained image generators, MoCoGAN #REFR proposes a motion and content decomposed Generative Adversarial Network framework for video generation, which generates a video by mapping a sequence of random vectors to a sequence of video frames, and allows one to generate videos with same content but different motion as well as videos with different content and same motion.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pursue higher quality and larger resolution in video synthesis frameworks based on pre-trained image generators, MoCoGAN #REFR and StyleVideoGAN are proposed to design motion generators to manipulate latent codes for synthesizing videos. These frameworks achieve better performance than early works attempting to generate temporally coherent videos from random noises with conv-based GAN models directly, which suffer from high computational costs and unsatisfying performance on large-resolution datasets. #GANs", "given_citation_intent": "background", "given_keywords": ["GANs"]}]}
{"citing_id": "2303.00202v1", "cited_id": "2001.07676", "section_title": "Pre-Processing", "citation": "The label token z is mapped into the predicted label y by a verbalizer #REFR to complete the downstream tasks.", "text_before_citation": ["The intuition of prompting is to convert the downstream tasks into a similar form as the pre-training stage.", "For pre-trained models whose pretraining objective is to predict the next token given previous tokens, e.g., GPT-3 #OTHEREFR and BLOOM #OTHEREFR , prompting aims to ask a model to predict the next token (i.e.", "\"correct\" or \"wrong\" in this task) given previous tokens (patch contents and demonstrations).", "To help pre-trained models understand task-specific information, prompting modifies the input data by adding a piece of text description, namely prompt templates.", "The prompt template is a textual string that has two slots: (1) an input slot [X] for original input data x and (2) an answer slot [Z] for the predicted answer/label token z."], "text_after_citation": ["The verbalizer, denoted as V , is a function that maps each predicted label token z to a class\u0177 in the target class set Y :", "V : Z \u2192 Y (1)", "where Z indicates the label token set.", "In the APCA task, the label token set Z includes two tokens, i.e., {\"correct\", \"wrong\"}, and the class set Y contains {-, +} for indicating correct (clean) and wrong (overfitting) patches, respectively.", "Note that the verbalizer is manually defined instead of learned from data."], "citing_paper_content": {"title": "Patchzero: Zero-Shot Automatic Patch Correctness Assessment", "abstract": "Automated Program Repair (APR) techniques have shown more and more promising results in fixing real-world bugs. Despite the effectiveness, APR techniques still face an overfitting problem: a generated patch can be incorrect although it passes all tests. It is time-consuming to manually evaluate the correctness of generated patches that can pass all tests. To address this problem, many approaches have been proposed to automatically assess the correctness of patches generated by APR techniques. However, existing approaches require a large set of manually labeled patches as the training data. To mitigate the issue, in this study, we propose PatchZero, the patch correctness assessment by adopting large pre-trained models. Specifically, for patches generated by a new or unseen APR tool, PatchZero does not need labeled patches of this new or unseen APR tool for training (i.e., zero-shot) but directly queries the large pre-trained model to get predictions on the correctness labels without training. In this way, PatchZero can reduce the manual labeling effort when building a model to automatically assess the correctness of generated patches of new APR tools. To provide knowledge regarding the automatic patch correctness assessment (APCA) task to the large pre-trained models, we also design an instance-wise demonstration formation strategy by using contrastive learning. Specifically, PatchZero selects semantically similar patches to help the large pre-trained model to give more accurate predictions on the unlabeled patches. Our experimental results showed that PatchZero can achieve an accuracy of 82.7% and an F1-score of 86.0% on average although no labeled patch of the new or unseen APR tool is available. In addition, our proposed technique outperformed the prior state-of-the-art by a large margin."}, "cited_paper_content": {"title": "Exploiting Cloze Questions For Few-Shot Text Classification And Natural Language Inference.", "abstract": "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with \"task descriptions\" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task. Theses phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. On several tasks, we show that PET outperforms both supervised training and unsupervised approaches in low-resource settings by a large margin."}, "keywords": ["predicted label"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To help pre-trained models understand task-specific information, prompting modifies the input data by adding a piece of text description, namely prompt templates. The prompt template is a textual string that has two slots: (1) an input slot [X] for original input data x and (2) an answer slot [Z] for the predicted answer/label token z. As shown in previous work, Pattern-Exploiting Training (PET) is a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To help pre-trained models understand task-specific information, Patch-Exploiting Training (PET) reformulates input examples as cloze-style phrases which assign soft labels to a large set of unlabeled examples, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To predict the correctness label of generated patches, PatchZero adopts large pre-trained models and queries them to get the predicted label without training, which is similar to the approach proposed in the reference paper #REFR that reformulates input examples as cloze-style phrases to help the language model understand the given task and assign soft labels to a large set of unlabeled examples.", "given_citation_intent": "method", "given_keywords": ["predicted label"]}]}
{"citing_id": "2304.08781v1", "cited_id": "1807.06306", "section_title": "A. Related Works", "citation": "In #REFR , the authors discussed power and time allocation in NOMA-assisted MEC and derived closed-form expressions for optimal MEC offloading policies.", "text_before_citation": ["In #OTHEREFR , the authors studied a IoT MEC network with multiple users and edge servers, where users randomly upload various tasks to the edge servers, and the servers utilize shared computation resources to process the uploaded tasks.", "To optimize the resource utilization, the authors proposed a heuristic resource scheduling policy.", "In #OTHEREFR , the authors defined the Age of Data (AoD) for IoT big data processing in MEC networks and proposed a Multi-armed Bandit (MAB) based online learning algorithm to minimize AoD.", "In #OTHEREFR , the authors considered a UAV-assisted MEC with NOMA and optimized the trajectory and computation offloading using a successive convex approximation.", "In #OTHEREFR , the authors leveraged federated learning (FL) in NOMA-based MEC and used graph theory to improve the communication efficiency of FL and accelerate model convergence."], "text_after_citation": ["In #OTHEREFR , the authors studied a data analysis scenario in MEC, where data is generated by energy harvesting technology-powered wireless devices and uploaded to the MEC server for centralized data processing.", "They proposed a Lyapunov-based algorithm to schedule resources while satisfying AoI constraints.", "However, despite achieving promising performance in various applications, these approaches have neglected to consider the impact of delay, which can be significant when there is a heavy load of requests from multiple users or when downlink resources are limited.", "This can result in a degradation of service quality for the users.", "Thus, there is a need to discuss \"when to serve\" these requests, taking into account the incurred delay."], "citing_paper_content": {"title": "Aoi-Delay Tradeoff In Mobile Edge Caching: A Mixed-Order Drift-Plus-Penalty Algorithm", "abstract": "We consider a scheduling problem in a Mobile Edge Caching (MEC) network, where a base station (BS) uploads messages from multiple source nodes (SNs) and transmits them to mobile users (MUs) via downlinks, aiming to jointly optimize the average service Age of Information (AoI) and service delay over MUs. This problem is formulated as a difficult sequential decision making problem with discrete-valued and linearly-constrained design variables. To solve this problem, we first approximate its achievable region by characterizing its superset and subset. The superset is derived based on the rate stability theorem, while the subset is obtained using a novel stochastic policy. We also validate that this subset is substantially identical to the achievable region when the number of schedule resources is large. Additionally, we propose a sufficient condition to check the existence of the solution to the problem. Then, we propose the mixed-order drift-plus-penalty algorithm that uses a dynamic programming (DP) method to optimize the summation over a linear and quadratic Lyapunov drift and a penalty term, to handle the product term over different queue backlogs in the objective function. Finally, by associating the proposed algorithm with the stochastic policy, we demonstrate that it achieves an O(1/V) versus O(V) tradeoff for the average AoI and average delay."}, "cited_paper_content": {"title": "Joint Power And Time Allocation For Noma\u2013Mec Offloading", "abstract": "This correspondence considers non-orthogonal multiple access (NOMA) assisted mobile edge computing (MEC), where the power and time allocation is jointly optimized to reduce the energy consumption of computation offloading. Closed-form expressions for the optimal power and time allocation solutions are obtained and used to establish the conditions for determining whether the conventional orthogonal multiple access (OMA), pure NOMA or hybrid NOMA should be used for MEC offloading."}, "keywords": ["optimal MEC offloading", "NOMA-assisted MEC"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In #REFR, the authors considered non-orthogonal multiple access (NOMA) assisted mobile edge computing (MEC) and jointly optimized power and time allocation to reduce energy consumption of computation offloading, establishing the conditions for determining whether the conventional orthogonal multiple access (OMA), pure NOMA or hybrid NOMA should be used for MEC offloading.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In their recent work, the authors optimized power and time allocation for NOMA-assisted MEC offloading, and established the conditions for determining the optimal access scheme, which could be useful for the resource allocation in the scheduling problem of MEC networks with multiple users and edge servers, as discussed in #OTHEREFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the energy consumption of computation offloading in NOMA-assisted MEC, the authors in #REFR jointly optimized the power and time allocation, and obtained closed-form expressions for the optimal solutions, which can be used to determine whether the conventional OMA, pure NOMA, or hybrid NOMA should be used for optimal MEC offloading.", "given_citation_intent": "background", "given_keywords": ["optimal MEC offloading", "NOMA-assisted MEC"]}]}
{"citing_id": "2304.02098v1", "cited_id": "1506.02142", "section_title": "Basic Performance", "citation": "This improvement is in line with other work using MC Dropout as much as comparisons can be made across domains, including when it was first proposed and tested on a regression task #REFR .", "text_before_citation": ["Results are summarized in Table 1 .", "First, we can clearly see that adding Gaussian and Shot noise decimates performance, by a significant degree.", "As the model was not trained to be invariant to this type of input noise, this is expected.", "Second, our approach generally leads to a slight performance increase over baseline (with one exception on the VIPER dataset with noise), on both clean and noisy data.", "Given that acquiring a more reliable measure of uncertainty is the goal, rather than outright performance on the PQ metric, any performance improvement is a welcome addition."], "text_after_citation": ["Unfortunately, our approach is not able to compensate for the loss in performance caused by the addition of noise.", "Third, we see that while adding more samples does result in a slight performance increase, 5 samples is typically sufficient to generate a reasonable seg-mentation result, especially considering the computational burden of using more and more samples.", "Finally, we see that due to the network's tendency to generate many false positives the details around pruning and evaluation are critical.", "The per-pixel evaluation, using the same thresholds as the baseline but without the brute-force subsumption of objects, performs worse than baseline, indicating that the brute-force approach is effective with respect to the PQ metric even if not useful from an uncertainty standpoint.", "Not applying any thresholds on the other hand harms performance on COCO and VIPER, but actually leads to a slight increase on KITTI-STEP."], "citing_paper_content": {"title": "Uncertainty Estimation In Deep Learning For Panoptic Segmentation", "abstract": "As deep learning-based computer vision algorithms continue to improve and advance the state of the art, their robustness to real-world data continues to lag their performance on datasets. This makes it difficult to bring an algorithm from the lab to the real world. Ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout have been successfully used in many applications in an attempt to address this robustness issue. Unfortunately, it is not always clear if such ensemble-based approaches can be applied to a new problem domain. This is the case with panoptic segmentation, where the structure of the problem and architectures designed to solve it means that unlike image classification or even semantic segmentation, the typical solution of using a mean across samples cannot be directly applied. In this paper, we demonstrate how ensemblebased uncertainty estimation approaches such as Monte Carlo Dropout can be used in the panoptic segmentation domain with no changes to an existing network, providing both improved performance and more importantly a better measure of uncertainty for predictions made by the network. Results are demonstrated quantitatively and qualitatively on the COCO, KITTI-STEP and VIPER datasets."}, "cited_paper_content": {"title": "Dropout As A Bayesian Approximation: Representing Model Uncertainty In Deep Learning", "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."}, "keywords": ["MC Dropout"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our approach of using ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout in the panoptic segmentation domain is supported by recent work that demonstrates how dropout training in deep neural networks can be cast as approximate Bayesian inference in deep Gaussian processes, providing a more reliable measure of uncertainty for predictions made by the network #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our approach generally leads to a slight performance increase over baseline (with one exception on the VIPER dataset with noise), on both clean and noisy data. Given that acquiring a more reliable measure of uncertainty is the goal, rather than outright performance on the PQ metric, any performance improvement is a welcome addition. This is consistent with the findings in the reference paper, which also demonstrated improved uncertainty estimation with dropout neural networks. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our approach generally leads to a slight performance increase over baseline (with one exception on the VIPER dataset with noise), on both clean and noisy data. Given that acquiring a more reliable measure of uncertainty is the goal, rather than outright performance on the PQ metric, any performance improvement is a welcome addition. This is consistent with the findings in the reference paper, which also demonstrates the use of MC Dropout to extract information from existing models that has been thrown away so far, mitigating the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. #REFR", "given_citation_intent": "result", "given_keywords": ["MC Dropout"]}]}
{"citing_id": "2303.02814v1", "cited_id": "1109.2378", "section_title": "How To Identify The Most Vulnerable Neurons", "citation": "We address this question by hierarchically clustering #REFR the neurons and visualizing the hierarchy with an interactive dendrogram, as shown in the Neuron Cluster View (Figure 2 -e).", "text_before_citation": ["Each row of this weight matrix corresponds to the weight of one class.", "respectively; however, for the case where several classes have a non-zero probability, the view shows multiple bars.", "The second type of information is the two vulnerability maps: one highlights regions where the image perturbation decreases the benign class's probability the most (Figure 2-d3) , and the other highlights regions where the image perturbation increases the adversarial class's probability the most (Figure 2-d4 ).", "While we show the top-20% vulnerable pixels by default, this can be interactively changed with a slider placed in the view.", "Once we identify a group of similar neurons, by only examining one representative neuron from the group, we may understand the behaviors of the entire group, making the exploration more efficient."], "text_after_citation": ["As a dissimilarity measure used for clustering, we use the 2 distance between two RFs to measure the corresponding neurons' dissimilarity.", "We want to note that the IoU between two RFs or the 2 distance between the corresponding activation maps could be used instead; however, based on our comparisons included in our Supplementary Material, there is no obvious advantage of one measure against the others.", "When users select a neuron from the Neuron Vulnerability View, its location in the dendrogram is highlighted, as shown in the red line at the far right of Figure 2 -e.", "From this location, users can easily identify neurons with similar behavior as similar neurons share the same ancestors and are generally located close to each other in the dendrogram.", "Also, to see the RFs summarized across similar neurons, the user can click multiple nodes from the dendrogram, which selects the clicked nodes and their descendants."], "citing_paper_content": {"title": "Visual Analytics Of Neuron Vulnerability To Adversarial Attacks On Convolutional Neural Networks", "abstract": "Adversarial attacks on a convolutional neural network (CNN)-injecting human-imperceptible perturbations into an input image-could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) which neurons are more vulnerable to attacks and (2) which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron's responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons' roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts. CCS Concepts: \u2022 Human-centered computing \u2192 Visual analytics."}, "cited_paper_content": {"title": "Modern Hierarchical, Agglomerative Clustering Algorithms", "abstract": "This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a \"stepwise dendrogram\", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes."}, "keywords": ["Neuron Cluster View", "interactive dendrogram"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently perform hierarchical, agglomerative clustering with pairwise dissimilarities, the authors of this manuscript could benefit from the algorithms presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently perform hierarchical, agglomerative clustering with pairwise dissimilarities between data points, the authors of this manuscript could use the algorithms presented in the reference paper by #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To aid with hierarchical clustering based on the neurons' roles in the prediction, the authors presented a stepwise dendrogram, a data structure which is shared by all implementations in current standard software, in their reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["Neuron Cluster View", "interactive dendrogram"]}]}
{"citing_id": "2304.05195v1", "cited_id": "1804.09893", "section_title": "Introduction", "citation": "For the purpose of privacy protection, we leverage Random Fourier Feature (RFF) #REFR to transform the extracted features to reduce the risk of privacy leakage.", "text_before_citation": ["To overcome these challenges, we propose HPN, a novel method to achieve pFedHPO.", "Instead of a context-free bandit model widely adopted in existing FedHPO works (e.g., FedEx #OTHEREFR ), we design a policy network that takes a client encoding as input and outputs a distribution over the original search space.", "Therefore, each client has its specific encoding to determine its hyperparameters personally.", "Meanwhile, this policy network's parameters are updated based on the observations collected from all the clients, which can be regarded as improving the sample efficiency via model sharing.", "In HPN, a client encoding is calculated based on the training sample of that client to reflect the similarity among clients."], "text_after_citation": ["Furthermore, we design a mechanism to conduct low-fidelity evaluations of hyperparameter configurations, which reduces variance for the signals used to update our policy and, in the meantime, alleviates the impact of model parameters' state.", "Finally, we conduct extensive experiments on FedHPO tasks of various domains, where the generalization error of the hyperparameter configurations searched by HPN is lower than that of related baselines.", "Our contributions are summarized as follows:", "\u2022 We are the first to systematically explore pFedHPO, discussing its setting, challenges, and critical problemsolving factors.", "\u2022 We propose a novel pFedHPO method HPN, which satisfies the sample efficiency and privacy preservation requirements."], "citing_paper_content": {"title": "Hpn: Personalized Federated Hyperparameter Optimization", "abstract": "Numerous research studies in the field of federated learning (FL) have attempted to use personalization to address the heterogeneity among clients, one of FL's most crucial and challenging problems. However, existing works predominantly focus on tailoring models. Yet, due to the heterogeneity of clients, they may each require different choices of hyperparameters, which have not been studied so far. We pinpoint two challenges of personalized federated hyperparameter optimization (pFedHPO): handling the exponentially increased search space and characterizing each client without compromising its data privacy. To overcome them, we propose learning a HyperParameter Network (HPN) fed with client encoding to decide personalized hyperparameters. The client encoding is calculated with a random projectionbased procedure to protect each client's privacy. Besides, we design a novel mechanism to debias the low-fidelity function evaluation samples for learning HPN. We conduct extensive experiments on FL tasks from various domains, demonstrating the superiority of HPN."}, "cited_paper_content": {"title": "Random Fourier Features For Kernel Ridge Regression: Approximation Bounds And Statistical Guarantees", "abstract": "Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. However, despite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take steps toward filling this gap. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression. Qualitatively, our results are twofold: on the one hand, we show that random Fourier feature approximation can provably speed up kernel ridge regression under reasonable assumptions. At the same time, we show that the method is suboptimal, and sampling from a modified distribution in Fourier space, given by the leverage function of the kernel, yields provably better performance. We study this optimal sampling distribution for the Gaussian kernel, achieving a nearly complete characterization for the case of low-dimensional bounded datasets. Based on this characterization, we propose an efficient sampling scheme with guarantees superior to random Fourier features in this regime."}, "keywords": ["privacy leakage", "Random Fourier Feature"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To overcome these challenges, we propose HPN, a novel method to achieve pFedHPO. Instead of a context-free bandit model widely adopted in existing FedHPO works (e.g., FedEx #REFR), we design a policy network that takes a client encoding as input and outputs a distribution over the original search space. Therefore, each client has its specific encoding to determine its hyperparameters personally. Meanwhile, this policy network's parameters are updated based on the observations collected from all the clients, which can be regarded as improving the sample efficiency via model sharing. In HPN, a client encoding is calculated based on the training sample of that client to reflect the similarity among clients.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To determine personalized hyperparameters for each client, the proposed HPN method utilizes a policy network that takes a client encoding as input and outputs a distribution over the original search space, as described in the reference paper \"Random Fourier Features For Kernel Ridge Regression: Approximation Bounds And Statistical Guarantees\" (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To protect each client's privacy from potential leakage, HPN calculates a client encoding with a random projection-based procedure, as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["privacy leakage", "Random Fourier Feature"]}]}
{"citing_id": "2305.01482v1", "cited_id": "1706.10006", "section_title": "I. Introduction", "citation": "Following this idea, the Automated Audio Captioning (AAC) task appeared in 2017 #REFR and aims to create systems that generate a sentence written in natural language that describes an audio file.", "text_before_citation": ["In recent years, new machine learning systems have been significantly improved for text processing, generation, and understanding, leading to the use of natural language as a global interface between humans and machines.", "Free-form text can contain much more information than a predefined set of classes, which could improve the machine understanding of our world.", "In audio, most of the tasks are focused on classification and localization of sound events."], "text_after_citation": ["The audio can contain various sound events (human, natural, domestic, urban, music, effects...) of different lengths, recorded with different devices and in different scenes.", "The description can contain any kind of detail in the audio, with temporal or spatial relations between them (followed by, in the background...) or different characterizations (high-pitched, short, repetitive...).", "Since the descriptions are written by humans, we need to consider different words used to describe similar sounds (Birds are calling / chirping / singing / tweeting), different sentence structures (A door that needs to be oiled / A door with squeaky hinges), subjectivity (Man speaks in a foreign language), high-level descriptions (A vulgar man speaks / Unintelligible conversation), and vagueness (Someone speaks instead of A man gives a speech over a reverberating microphone).", "In AAC, most approaches use deep learning models trained with the standard Cross-Entropy (CE) loss.", "However, this loss tends to generate repetitive and generic content #OTHEREFR and does not take into account synonyms, various sentences structures or the semantic closeness."], "citing_paper_content": {"title": "Multitask Learning In Audio Captioning: A Sentence Embedding Regression Loss Acts As A Regularizer", "abstract": "In this work, we propose to study the performance of a model trained with a sentence embedding regression loss component for the Automated Audio Captioning task. This task aims to build systems that can describe audio content with a single sentence written in natural language. Most systems are trained with the standard Cross-Entropy loss, which does not take into account the semantic closeness of the sentence. We found that adding a sentence embedding loss term reduces overfitting, but also increased SPIDEr from 0.397 to 0.418 in our first setting on the AudioCaps corpus. When we increased the weight decay value, we found our model to be much closer to the current stateof-the-art methods, with a SPIDEr score up to 0.444 compared to a 0.475 score. Moreover, this model uses eight times less trainable parameters. In this training setting, the sentence embedding loss has no more impact on the model performance."}, "cited_paper_content": {"title": "Automated Audio Captioning With Recurrent Neural Networks", "abstract": "We present the first approach to automated audio captioning. We employ an encoder-decoder scheme with an alignment model in between. The input to the encoder is a sequence of log mel-band energies calculated from an audio file, while the output is a sequence of words, i.e. a caption. The encoder is a multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a multi-layered GRU with a classification layer connected to the last GRU of the decoder. The classification layer and the alignment model are fully connected layers with shared weights between timesteps. The proposed method is evaluated using data drawn from a commercial sound effects library, ProSound Effects. The resulting captions were rated through metrics utilized in machine translation and image captioning fields. Results from metrics show that the proposed method can predict words appearing in the original caption, but not always correctly ordered."}, "keywords": ["natural language", "audio file"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In recent years, new machine learning systems have been significantly improved for text processing, generation, and understanding, leading to the use of natural language as a global interface between humans and machines. Free-form text can contain much more information than a predefined set of classes, which could improve the machine understanding of our world. In audio, most of the tasks are focused on classification and localization of sound events. Automated Audio Captioning With Recurrent Neural Networks #REFR presents a novel approach to automated audio captioning, which could be useful for improving the understanding of audio content.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In recent years, new machine learning systems have been significantly improved for text processing, generation, and understanding, leading to the use of natural language as a global interface between humans and machines. Free-form text can contain much more information than a predefined set of classes, which could improve the machine understanding of our world. In audio, most of the tasks are focused on classification and localization of sound events. Automated audio captioning is a recent approach that employs an encoder-decoder scheme with an alignment model in between, as proposed in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent advances in machine learning have enabled the use of natural language as a global interface between humans and machines, and in audio, most tasks are focused on sound event classification and localization. Automated audio captioning, which aims to describe audio content with a single sentence written in natural language, is a relatively new area of research. The proposed approach in this manuscript employs an encoder-decoder scheme with an alignment model in between, where the input to the encoder is a sequence of log mel-band energies calculated from an #REFR audio file.", "given_citation_intent": "background", "given_keywords": ["natural language", "audio file"]}]}
{"citing_id": "2304.02064v1", "cited_id": "1206.4683", "section_title": "F.1 Amazon Review", "citation": "As presented in the main paper, the original dataset is pre-processed to 5000-dimension bag-of words features following #REFR .", "text_before_citation": [], "text_after_citation": ["And the target shift data is created by randomly dropping 50% negative reviews.", "Model Structure Representation learner: [5000, 1000, 500, 100] MLP net using 0.7 dropout rate with Relu activation added after each hidden layer and finally output a 100-dimension feature representation.", "Predictor and duplicate predictor: [100, 2] linear transformation followed by a log softmax layer transforming the 100dimension feature to 2-class log probabilities.", "Loss function: we choose the \"negative log-likelihood loss\" as the loss function.", "Computing Resources The experiments were run on a server with 6 CPUs and 1 GPU of 32GB memory."], "citing_paper_content": {"title": "Algorithm-Dependent Bounds For Representation Learning Of Multi-Source Domain Adaptation", "abstract": "We use information-theoretic tools to derive a novel analysis of Multi-source Domain Adaptation (MDA) from the representation learning perspective. Concretely, we study joint distribution alignment for supervised MDA with few target labels and unsupervised MDA with pseudo labels, where the latter is relatively hard and less commonly studied. We further provide algorithm-dependent generalization bounds for these two settings, where the generalization is characterized by the mutual information between the parameters and the data. Then we propose a novel deep MDA algorithm, implicitly addressing the target shift through joint alignment. Finally, the mutual information bounds are extended to this algorithm providing a nonvacuous gradient-norm estimation. The proposed algorithm has comparable performance to the state-of-the-art on target-shifted MDA benchmark with improved memory efficiency. 1 We use the terminology of target shift in the rest of the paper to avoid confusion with the label shift assumption, where S(X|Y) = T (X|Y), S(Y) = T (Y)."}, "cited_paper_content": {"title": "Marginalized Denoising Autoencoders For Domain Adaptation", "abstract": "Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters--in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB\u2122, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks."}, "keywords": ["original dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed deep MDA algorithm implicitly addresses target shift through joint alignment, achieving comparable performance to the state-of-the-art on target-shifted MDA benchmark with improved memory efficiency (Li et al., 2017). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the high computational cost and lack of scalability of stacked denoising autoencoders (SDAs) for domain adaptation, the authors propose marginalized SDA (mSDA) that marginalizes noise and does not require stochastic gradient descent or other optimization algorithms to learn parameters, achieving a significant speedup of SDAs by two orders of magnitude, as well as representations that are as effective as the traditional SDAs, as demonstrated in benchmark tasks (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed deep MDA algorithm implicitly addresses the target shift through joint alignment, achieving comparable performance to the state-of-the-art on target-shifted MDA benchmark with improved memory efficiency, and is based on marginalized denoising autoencoders (#REFR) that learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise.", "given_citation_intent": "method", "given_keywords": ["original dataset"]}]}
{"citing_id": "2304.06907v1", "cited_id": "0908.0050", "section_title": "Literature Review", "citation": "Traditional sparse representation approaches can be considered as unsupervised methods that either ignore label information #REFR or learn prototypes for each label separately.", "text_before_citation": ["The prototype-based approaches cluster samples and then choose one or a few samples or their representatives in each cluster #OTHEREFR .", "Dimensionality-reductionbased approaches, such as product quantization #OTHEREFR and hashing #OTHEREFR , focus on encoding high-dimensional feature spaces densely to achieve speed-up in searchbased methods as well as reducing the memory costs.", "Our proposed approach belongs to the third group of scalable methods, transform-based approaches #OTHEREFR , that treat image annotation as a multi-label problem.", "In these approaches, both visual and semantic modalities are incorporated into the learning procedure for transforming input data into another space with higher levels of discrimination.", "One of the successful techniques in this category is sparse representation whose objective is to represent each pattern just using the linear combination of a few numbers of prototypes."], "text_after_citation": ["In recent years, many researchers have focused on embedding label information into the prototype learning procedure, generally known as discriminative #OTHEREFR or coupled #OTHEREFR dictionary learning, extensively applied for multi-label classification problems #OTHEREFR .", "Discriminative sparse models have many applications in image classification, super-resolution #OTHEREFR , fault-diagnosis, etc.", "class-specific and shared discriminative dictionary learning (CASDDL) method #OTHEREFR aims to classify the steel sheets based on the Fisher discrimination method.", "They strive to extract the discriminative features for each class separately (inter-class information), along with a shared sub-dictionary which is common between all the classes for extracting the intra-class information. Li et al.", "#OTHEREFR offered a weighted regularization approach to tackle the noisy images."], "citing_paper_content": {"title": "Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning", "abstract": "In most image retrieval systems, images include various high-level semantics, called tags or annotations. Virtually all the state-of-the-art image annotation methods that handle imbalanced labeling are search-based techniques which are time-consuming. In this paper, a novel coupled dictionary learning approach is proposed to learn a limited number of visual prototypes and their corresponding semantics simultaneously. This approach leads to a real-time image annotation procedure. Another contribution of this paper is that utilizes a marginalized loss function instead of the squared loss function that is inappropriate for image annotation with imbalanced labels. We have employed a marginalized loss function in our method to leverage a simple and effective method of prototype updating. Meanwhile, we have introduced 1 regularization on semantic prototypes to preserve the sparse and imbalanced nature of labels in learned semantic prototypes. Finally, comprehensive experimental results on various datasets demonstrate the efficiency of the pro-Authors contributed equally on this research."}, "cited_paper_content": {"title": "Online Learning For Matrix Factorization And Sparse Coding", "abstract": "Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets."}, "keywords": ["Traditional sparse representation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One of the successful techniques in this category is sparse representation whose objective is to represent each pattern just using the linear combination of a few numbers of prototypes. Recently, online optimization algorithms have been proposed to address large-scale matrix factorization problems #REFR, which can be applied to various matrix factorization formulations and are suitable for a wide range of learning problems.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sparse representation is a successful technique in the transform-based approaches that aims to represent each pattern using the linear combination of a few numbers of prototypes #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Traditional sparse representation has been a successful technique in the transform-based approaches for image annotation, where the objective is to represent each pattern using the linear combination of a few numbers of prototypes #REFR.", "given_citation_intent": "background", "given_keywords": ["Traditional sparse representation"]}]}
{"citing_id": "2304.03563v1", "cited_id": "1201.0490", "section_title": "B. Classification Models And Settings", "citation": "We use Scikit-learn #REFR , one of the popular and widely used tools to implement the techniques.", "text_before_citation": ["According to our comparative study, the relationship between question classes and their corresponding feature values might be complex.", "We thus choose five popular supervised machine learning techniques with different learning strategies.", "They are -i) Decision Tree #OTHEREFR (DT) ii) Random Forest (RF) #OTHEREFR , iii) Artificial Neural Network (ANN) #OTHEREFR , iv) K-Nearest Neighbors (KNN) #OTHEREFR , and v) Gaussian Naive Bayes (GNB) #OTHEREFR .", "In particular, we choose these machine learning algorithms because they are widely used in the relevant studies #OTHEREFR .", "We thus believe they can build reliable models to classify promoted and discoursed questions."], "text_after_citation": ["Parameter Tuning.", "Tuning parameters in classifiers is important because it changes the heuristics determining how they learn #OTHEREFR .", "For example, it controls the number of decision trees to use in RF or the number of clusters in KNN.", "Models trained with suboptimal parameter settings may underperform as parameter settings depend on the dataset #OTHEREFR .", "To select the best model configuration, we use GridSearchCV, the cross-validated grid search algorithm of Scikit-learn #OTHEREFR ."], "citing_paper_content": {"title": "Do Subjectivity And Objectivity Always Agree? A Case Study With Stack Overflow Questions", "abstract": "In Stack Overflow (SO), the quality of posts (i.e., questions and answers) is subjectively evaluated by users through a voting mechanism. The net votes (upvotes \u2212 downvotes) obtained by a post are often considered an approximation of its quality. However, about half of the questions that received working solutions got more downvotes than upvotes. Furthermore, about 18% of the accepted answers (i.e., verified solutions) also do not score the maximum votes. All these counter-intuitive findings cast doubts on the reliability of the evaluation mechanism employed at SO. Moreover, many users raise concerns against the evaluation, especially downvotes to their posts. Therefore, rigorous verification of the subjective evaluation is highly warranted to ensure a non-biased and reliable quality assessment mechanism. In this paper, we compare the subjective assessment of questions with their objective assessment using 2.5 million questions and ten text analysis metrics. According to our investigation, four objective metrics agree with the subjective evaluation, two do not agree, one either agrees or disagrees, and the remaining three neither agree nor disagree with the subjective evaluation. We then develop machine learning models to classify the promoted and discouraged questions. Our models outperform the state-ofthe-art models with a maximum of about 76%-87% accuracy."}, "cited_paper_content": {"title": "Scikit-Learn: Machine Learning In Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."}, "keywords": ["Scikit-learn"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In particular, we choose these machine learning algorithms because they are widely used in the relevant studies (Scikit-Learn: Machine Learning In Python). We thus believe they can build reliable models to classify promoted and discoursed questions. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To classify promoted and discouraged questions, we employ five popular supervised machine learning techniques with different learning strategies, including Decision Tree #REFR (DT), Random Forest (RF) #OTHEREFR, Artificial Neural Network (ANN) #OTHEREFR, K-Nearest Neighbors (KNN) #OTHEREFR, and Gaussian Naive Bayes (GNB) #OTHEREFR, which are widely used in relevant studies and can build reliable models.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To build reliable models for classifying promoted and discouraged questions, we use five popular supervised machine learning techniques with different learning strategies, including Decision Tree #REFR (DT), Random Forest (RF) #OTHEREFR, Artificial Neural Network (ANN) #OTHEREFR, K-Nearest Neighbors (KNN) #OTHEREFR, and Gaussian Naive Bayes (GNB) #OTHEREFR, which are widely used in relevant studies and implemented in the Scikit-learn package.", "given_citation_intent": "method", "given_keywords": ["Scikit-learn"]}]}
{"citing_id": "2304.00320v1", "cited_id": "1904.09080", "section_title": "Inference Stabilizer As Implicit Regularizer", "citation": "Similar results have been obtained by assuming the deep learning algorithms have been driven by an Ornstein-Uhlenbeck like process #REFR , while our work does not rely on such assumption but is all based on our proposed Doubly Stochastic Models.", "text_before_citation": ["The regularization effects of unbiased random label noises should be", "EQUATION", "where \u2207 \u03b8 f (x, \u03b8) refers to the gradient of f over \u03b8 and the effects are controlled by the batch size B and the variance of label noises \u03c3 2 ."], "text_after_citation": [], "citing_paper_content": {"title": "Stochastic Gradient Descent With Random Label Noises: Doubly Stochastic Models And Inference Stabilizer", "abstract": "Random label noises (or observational noises) widely exist in practical machine learning settings. While previous studies primarily focus on the affects of label noises to the performance of learning, our work intends to investigate the implicit regularization effects of the label noises, under mini-batch sampling settings of stochastic gradient descent (SGD), with assumptions that label noises are unbiased. Specifically, we analyze the learning dynamics of SGD over the quadratic loss with unbiased label noises, where we model the dynamics of SGD as a stochastic differentiable equation (SDE) with two diffusion terms (namely a Doubly Stochastic Model). While the first diffusion term is caused by mini-batch sampling over the (label-noiseless) loss gradients as many other works on SGD [1, 2], our model investigates the second noise term of SGD dynamics, which is caused by mini-batch sampling over the label noises, as an implicit regularizer. Our theoretical analysis finds such implicit regularizer would favor some convergence points that could stabilize model outputs against perturbation of parameters (namely inference stability). Though similar phenomenon have been investigated in [3], our work"}, "cited_paper_content": {"title": "Implicit Regularization For Deep Neural Networks Driven By An Ornstein-Uhlenbeck Like Process", "abstract": "We consider deep networks, trained via stochastic gradient descent to minimize L2 loss, with the training labels perturbed by independent noise at each iteration. We characterize the behavior of the training dynamics near any parameter vector that achieves zero training error, in terms of an implicit regularization term corresponding to the sum over the data points, of the squared L2 norm of the gradient of the model with respect to the parameter vector, evaluated at each data point. We then leverage this general characterization, which holds for networks of any connectivity, width, depth, and choice of activation function, to show that for 2-layer ReLU networks of arbitrary width and L2 loss, when trained on one-dimensional labeled data $(x_1,y_1),\\ldots,(x_n,y_n),$ the only stable solutions with zero training error correspond to functions that: 1) are linear over any set of three or more co-linear training points (i.e. the function has no extra \"kinks\"); and 2) change convexity the minimum number of times that is necessary to fit the training data. Additionally, for 2-layer networks of arbitrary width, with tanh or logistic activations, we show that when trained on a single $d$-dimensional point $(x,y)$ the only stable solutions correspond to networks where the activations of all hidden units at the datapoint, and all weights from the hidden units to the output, take at most two distinct values, or are zero. In this sense, we show that when trained on \"simple\" data, models corresponding to stable parameters are also \"simple\"; in short, despite fitting in an over-parameterized regime where the vast majority of expressible functions are complicated and badly behaved, stable parameters reached by training with noise express nearly the \"simplest possible\" hypothesis consistent with the data. These results shed light on the mystery of why deep networks generalize so well in practice."}, "keywords": ["deep learning algorithms"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The regularization effects of unbiased random label noises should be controlled by the batch size B and the variance of label noises \u03c3 2, as shown in previous studies [1, 2]. However, recent work by #REFR has investigated the implicit regularization effects of label noises on the learning dynamics of stochastic gradient descent, revealing that stable parameters reached by training with noise express nearly the \"simplest possible\" hypothesis consistent with the data.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' findings that stable parameters reached by training with noise express nearly the \"simplest possible\" hypothesis consistent with the data, align with the implicit regularization effects of unbiased random label noises, which stabilize model outputs against perturbation of parameters, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The implicit regularization term proposed in the reference paper #REFR sheds light on why deep learning algorithms generalize so well in practice, as it shows that stable parameters reached by training with noise express nearly the \"simplest possible\" hypothesis consistent with the data.", "given_citation_intent": "result", "given_keywords": ["deep learning algorithms"]}]}
{"citing_id": "2303.17963v1", "cited_id": "1401.5508", "section_title": "C. Optimal Control With Generic Basis Functions", "citation": "Instead of the actual basis functions, we use the reduced-rank GP approximation proposed in #REFR to systematically determine basis the basis functions \u03d5(x, u) and the parameter V of the prior.", "text_before_citation": ["In the following, we show that the proposed optimal control approach can yield good results even if no parametric model is known, which might be the case in practice."], "text_after_citation": ["We choose a GP with a squared exponential kernel and Fig. 2 .", "Optimal control with generic basis functions: The red area shows the output constraints, the gray area encompasses the 100 scenarios that were used to determine the input trajectory, the green line shows the mean prediction, and the blue line shows one realization of the output of the actual system when the input trajectory u \u22c6 0:H is applied from time t = 0.", "select the hyperparameters of the GP and the approximation based on the training data. These parameters are given in Table II .", "Afterwards, K = 100 models are sampled using the PG sampler, and the resulting OCP is solved as in the previous example.", "As Figure 2 shows, the results are similar to the case with known basis functions."], "citing_paper_content": {"title": "Learning-Based Optimal Control With Performance Guarantees For Unknown Systems With Latent States", "abstract": "As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physicsbased modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation."}, "cited_paper_content": {"title": "Hilbert Space Methods For Reduced-Rank Gaussian Process Regression", "abstract": "This paper proposes a novel scheme for reduced-rank Gaussian process regression. The method is based on an approximate series expansion of the covariance function in terms of an eigenfunction expansion of the Laplace operator in a compact subset of $$\\mathbb {R}^d$$ ::: . On this approximate eigenbasis, the eigenvalues of the covariance function can be expressed as simple functions of the spectral density of the Gaussian process, which allows the GP inference to be solved under a computational cost scaling as $$\\mathcal {O}(nm^2)$$ ::: (initial) and $$\\mathcal {O}(m^3)$$ ::: (hyperparameter learning) with m basis functions and n data points. Furthermore, the basis functions are independent of the parameters of the covariance function, which allows for very fast hyperparameter learning. The approach also allows for rigorous error analysis with Hilbert space theory, and we show that the approximation becomes exact when the size of the compact subset and the number of eigenfunctions go to infinity. We also show that the convergence rate of the truncation error is independent of the input dimensionality provided that the differentiability order of the covariance function increases appropriately, and for the squared exponential covariance function it is always bounded by $${\\sim }1/m$$ ::: regardless of the input dimensionality. The expansion generalizes to Hilbert spaces with an inner product which is defined as an integral over a specified input density. The method is compared to previously proposed methods theoretically and through empirical tests with simulated and real data."}, "keywords": ["reduced-rank GP approximation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the following, we show that the proposed optimal control approach can yield good results even if no parametric model is known, which might be the case in practice. This is achieved by using a novel scheme for reduced-rank Gaussian process regression proposed in #REFR, which allows for rigorous error analysis with Hilbert space theory and very fast hyperparameter learning.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed optimal control approach for unknown nonlinear systems with latent states can be computed without any prior knowledge of a parametric model, which is a significant advantage in practice #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compute an optimal input trajectory for unknown nonlinear systems with latent states, the proposed method employs a novel scheme for reduced-rank GP approximation #REFR, which allows for rigorous error analysis with Hilbert space theory and very fast hyperparameter learning.", "given_citation_intent": "method", "given_keywords": ["reduced-rank GP approximation"]}]}
{"citing_id": "2303.00891v1", "cited_id": "1609.05158", "section_title": "B. Network Architecture", "citation": "In contrast, the decoder block incorporates a pixel shuffle layer #REFR between every two stages to increase the feature map's size by a factor of two. Next, we explain each component in more detail.", "text_before_citation": ["In order to reconstruct the robot, our model uses an image of the robot to predict the 3D coordinates along its centerline.", "Subsequently, a weighted linear least squares algorithm is employed to derive a 3D curve that parametrizes the center of the robot. As shown in Fig.", "2 , we design a network with a shared encoder and three decoders that are composed of four stages each.", "These stages consist of a residual block #OTHEREFR with two convolutional layers that are connected through BatchNorm #OTHEREFR and Leaky ReLU activations #OTHEREFR .", "The encoder block uses a maxpooling layer between every two stages to decrease the feature map's size by a factor of two."], "text_after_citation": ["a) Encoder: To incorporate location information, we add the 2D image indices to I RGB .", "This results in a 5channel image, represented as I in \u2208 R H\u00d7W \u00d75 .", "The encoder then extracts multi-scale features from the input image, which are subsequently passed through three decoders for further processing.", "b) Decoders: Given that the image includes background, not every pixel is relevant in determining the robot's shape.", "To address this, the importance decoder learns the significance of each pixel in shape reconstruction."], "citing_paper_content": {"title": "Moss: Monocular Shape Sensing For Continuum Robots", "abstract": "Continuum robots are promising candidates for interactive tasks in various applications due to their unique shape, compliance, and miniaturization capability. Accurate and real-time shape sensing is essential for such tasks yet remains a challenge. Embedded shape sensing has high hardware complexity and cost, while vision-based methods require stereo setup and struggle to achieve real-time performance. This paper proposes the first eye-to-hand monocular approach to continuum robot shape sensing. Utilizing a deep encoder-decoder network, our method, MoSSNet, eliminates the computation cost of stereo matching and reduces requirements on sensing hardware. In particular, MoSSNet comprises an encoder and three parallel decoders to uncover spatial, length, and contour information from a single RGB image, and then obtains the 3D shape through curve fitting. A two-segment tendon-driven continuum robot is used for data collection and testing, demonstrating accurate (mean shape error of 0.91 mm, or 0.36% of robot length) and real-time (70 fps) shape sensing on real-world data. Additionally, the method is optimized end-to-end and does not require fiducial markers, manual segmentation, or camera calibration. Code and datasets will be made available at https: //github.com/ContinuumRoboticsLab/MoSSNet. * indicates equal contribution."}, "cited_paper_content": {"title": "Real-Time Single Image And Video Super-Resolution Using An Efficient Sub-Pixel Convolutional Neural Network", "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods."}, "keywords": ["detail", "pixel shuffle layer"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve real-time super-resolution of 1080p videos, the authors propose an efficient sub-pixel convolutional neural network that replaces the handcrafted bicubic filter with more complex upscaling filters specifically trained for each feature map, as presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce the computational complexity of the overall super-resolution (SR) operation, recent research has proposed using an efficient sub-pixel convolutional neural network (CNN) that replaces the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, as demonstrated in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To reduce the computational complexity of the overall super-resolution operation, the authors of MoSSNet proposed an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output, replacing the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map in #REFR.", "given_citation_intent": "background", "given_keywords": ["detail", "pixel shuffle layer"]}]}
{"citing_id": "2303.11162v1", "cited_id": "1703.10593", "section_title": "Competitors:", "citation": "CycleGAN #REFR utilises cycle-consistency loss with a GAN model for bidirectional image-to-image translation.", "text_before_citation": ["We compare our proposed framework with various state-of-the-art (SOTA) methods and two selfdesigned baselines.", "Among those, pix2pix #OTHEREFR uses a conditional generative model for sketch-to-photo translation.", "MUNIT #OTHEREFR aims to produce diverse outputs given one input sketch.", "It tries to decompose an image into a content and a style code followed by learning those codes simultaneously."], "text_after_citation": ["U-GAT-IT #OTHEREFR uses an attention module for image translation while focusing on the domain-discriminative parts.", "Moreover, employing a pre-trained StyleGAN #OTHEREFR we compare with the baseline B-Sketch Mapper which is equivalent to the baseline sketch mapper described in Sec. 5.1.", "Following optimisation-based GAN inversion #OTHEREFR , we design B-Sketch Optimiser where we iteratively optimise the latent code using input sketch as a ground-truth with perceptual loss #OTHEREFR .", "For a fair comparison, we trained all competing methods in a supervised manner with sketch-photo pairs from ShoeV2, ChairV2, and Handbag datasets."], "citing_paper_content": {"title": "Picture That Sketch: Photorealistic Image Generation From Abstract Sketches", "abstract": "Edgemap Sketch Sketch Sketch Sketch Sketch Existing Methods Proposed Method Figure 1. (a) Set of photos generated by the proposed method. (b) While existing methods can generate faithful photos from perfectly pixel-aligned edgemaps, they fall short drastically in case of highly deformed and sparse free-hand sketches. In contrast, our autoregressive sketch-to-photo generation model produces highly photorealistic outputs from highly abstract sketches."}, "cited_paper_content": {"title": "Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks", "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach."}, "keywords": ["GAN model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For tasks where paired training data is not available, recent works have proposed to use unpaired image-to-image translation methods such as CycleGAN #REFR, which learns to translate an image from a source domain to a target domain in the absence of paired examples.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of generating photorealistic images from abstract sketches, various conditional generative models have been proposed, such as pix2pix #REFR, which uses a paired training set, and CycleGAN, which uses an unpaired training set.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of generating photorealistic images from abstract sketches, various conditional generative models have been proposed, such as pix2pix #REFR, which uses a GAN model for sketch-to-photo translation.", "given_citation_intent": "method", "given_keywords": ["GAN model"]}]}
{"citing_id": "2303.07265v1", "cited_id": "1912.01703", "section_title": "A. Model Architecture", "citation": "Then the output of the dropout layer is fed to the output layer followed by ReLU activation and gives a vector encoding HEL's (DA, action) output pair. For implementation, we used the Pythorch library #REFR .", "text_before_citation": ["The HEL agent network includes two fully connected layers followed by a dropout layer (ratio=0.1)."], "text_after_citation": [], "citing_paper_content": {"title": "Multimodal Reinforcement Learning For Robots Collaborating With Humans", "abstract": "Robot assistants for older adults and people with disabilities need to interact with their users in collaborative tasks. The core component of these systems is an interaction manager whose job is to observe and assess the task, and infer the state of the human and their intent to choose the best course of action for the robot. Due to the sparseness of the data in this domain, the policy for such multi-modal systems is often crafted by hand; as the complexity of interactions grows this process is not scalable. In this paper, we propose a reinforcement learning (RL) approach to learn the robot policy. In contrast to the dialog systems, our agent is trained with a simulator developed by using human data and can deal with multiple modalities such as language and physical actions. We conducted a human study to evaluate the performance of the system in the interaction with a user. Our designed system shows promising preliminary results when it is used by a real user."}, "cited_paper_content": {"title": "Pytorch: An Imperative Style, High-Performance Deep Learning Library", "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. ::: In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. ::: We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks."}, "keywords": ["HEL's (DA, action", "Pythorch library"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed reinforcement learning approach for robot assistants in collaborative tasks can deal with multiple modalities such as language and physical actions, and has shown promising preliminary results when used by a real user #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn the robot policy, we propose a reinforcement learning (RL) approach that can deal with multiple modalities such as language and physical actions, and is trained with a simulator developed by using human data #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To develop the HEL agent network, the authors utilized PyTorch library, which provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs #REFR.", "given_citation_intent": "method", "given_keywords": ["HEL's (DA, action", "Pythorch library"]}]}
{"citing_id": "2304.02098v1", "cited_id": "1506.02142", "section_title": "Our Approach", "citation": "We use Monte Carlo Dropout for this, conducting multiple passes through the network with dropout enabled at inference time #REFR .", "text_before_citation": ["Our task is to aggregate multiple samples collected from the neural network in such a way that we obtain both an acceptable panoptic segmentation result and a corresponding pixel-level uncertainty estimate.", "This occurs in three main parts, detailed in Algorithm 4 with Fig. 1 providing a highlevel overview of the process.", "The first is collecting and processing the samples from the network."], "text_after_citation": ["However, these samples must be at least minimally processed, with many of the detected proposals being nothing more than noise (labeled as \"background\") #OTHEREFR and requiring removal.", "Failing to do so introduces both needless computational overhead at later steps, and introduces a significant degree of noise to any generated segmentation.", "This part is straightforward, being an an adaptation of the baseline approach #OTHEREFR and described in Algorithm 1.", "For each sample, we first examine each proposal, and verify that the associated class label obtained through the argmax and softmax operators on the logits are not \"background\".", "Then, applying the softmax and argmax operators to the masks yields a proposal map, thus linking each pixel to an instance ID and a classification."], "citing_paper_content": {"title": "Uncertainty Estimation In Deep Learning For Panoptic Segmentation", "abstract": "As deep learning-based computer vision algorithms continue to improve and advance the state of the art, their robustness to real-world data continues to lag their performance on datasets. This makes it difficult to bring an algorithm from the lab to the real world. Ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout have been successfully used in many applications in an attempt to address this robustness issue. Unfortunately, it is not always clear if such ensemble-based approaches can be applied to a new problem domain. This is the case with panoptic segmentation, where the structure of the problem and architectures designed to solve it means that unlike image classification or even semantic segmentation, the typical solution of using a mean across samples cannot be directly applied. In this paper, we demonstrate how ensemblebased uncertainty estimation approaches such as Monte Carlo Dropout can be used in the panoptic segmentation domain with no changes to an existing network, providing both improved performance and more importantly a better measure of uncertainty for predictions made by the network. Results are demonstrated quantitatively and qualitatively on the COCO, KITTI-STEP and VIPER datasets."}, "cited_paper_content": {"title": "Dropout As A Bayesian Approximation: Representing Model Uncertainty In Deep Learning", "abstract": "Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning."}, "keywords": ["Monte Carlo Dropout"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To estimate pixel-level uncertainty in panoptic segmentation, we can use ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate pixel-level uncertainty in panoptic segmentation, we utilize an ensemble-based approach called Monte Carlo Dropout, which has been shown to provide improved performance and better uncertainty estimates in various deep learning applications #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate pixel-level uncertainty in our panoptic segmentation task, we utilize ensemble-based uncertainty estimation approaches such as Monte Carlo Dropout, which have been successfully applied in various applications #REFR.", "given_citation_intent": "method", "given_keywords": ["Monte Carlo Dropout"]}]}
{"citing_id": "2305.02869v1", "cited_id": "1511.05641", "section_title": "C Details For Fine-Tuning", "citation": "However, the SQuAD results of MSG significantly outperform the baselines in all cases (Table 3, #REFR , which can be converted to additional advantages in pre-training time.", "text_before_citation": ["We fine-tune for 5 epochs for small datasets including CoLA, MRPC, STS-B, and RTE, (we exclude WNLI following most related work #OTHEREFR ), and 3 epochs for other tasks.", "For SQuAD, we fine-tune with a batch size of 12 and learning rate of 3e-5 for 2 epochs.", "SQuAD metrics are very sensitive to sequence length, and most models work well with only more than 384 sequence length.", "Thus, we continue pre-training after the whole schedule with a sequence length of 512 for 100k steps for all the methods compared.", "This yields slight drop on speed-up ratios (120% to 100% on Bert-large, disappears if pre-training with a large sequence length from scratch)."], "text_after_citation": [], "citing_paper_content": {"title": "2X Faster Language Model Pre-Training Via Masked Structural Growth", "abstract": "Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly functionpreserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve a speed-up of 80% for Bert-base and 120% for Bert-large pre-training. Moreover, MSG is able to improve fine-tuning performances at the same time. 1 * Corresponding author 1 We will release our code for maximum reproducibility. Preprint. Under review."}, "cited_paper_content": {"title": "Net2Net: Accelerating Learning Via Knowledge Transfer", "abstract": "We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset."}, "keywords": ["pre-training time"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of speeding up pre-training of large language models, the authors propose Masked Structural Growth (MSG) which achieves strictly function-preserving growth operators and involves all possible dimensions in growth schedules, as opposed to previous approaches that only achieved non-strict function preservation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Using our knowledge transfer mechanism to add depth to Inception modules, we achieved a new state of the art accuracy rating on the ImageNet dataset, as reported in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Using the Net2Net technique proposed in #REFR, the pre-training time for Bert-large can be reduced by 120%.", "given_citation_intent": "result", "given_keywords": ["pre-training time"]}]}
{"citing_id": "2304.04647v1", "cited_id": "1410.5024", "section_title": "A. Related Works", "citation": "In accordance to the classification in #REFR , the above-mentioned algorithms were designed based on general SSI.", "text_before_citation": ["This approach integrates Lp-norm regularization factor of filter weight into objection functions of the AFAs directly to quicken convergence process of non-zero elements of underlying systems (where 0, 1 p \uf03d", ", namely obtaining L0and L1-norms).", "Furthermore, several sparsity-aware LMS-type algorithms and corresponding improved variants have also been designed #OTHEREFR - #OTHEREFR .", "Likewise, the L1-norm NSAF (L1-NSAF) and reweighted L1-NSAF (L1-RNSAF) algorithms were presented by employing L1-and reweighted L1-norms respectively to address highly correlated signals #OTHEREFR .", "Similarly, through introducing L0-norm penalty factor, the resulting A L0-norm constraint NSAF (L0-NSAF) algorithm realized higher filtering accurateness and faster convergence #OTHEREFR , #OTHEREFR ."], "text_after_citation": ["However, for block-sparse systems that dominant coefficients are clustered into several groups, in which are generally encountered in satellite-linked and MIMO communications, they may produce evident decrease in learning performance.", "With regards to this, some block-sparsity-induced-type AFAs were developed #OTHEREFR - #OTHEREFR .", "It is indisputable that AFAs with constant step-size generate conflicting requirements between convergence rate and filtering accuracy.", "To eliminate this limitation, based on NSAF, various variable step-size (VSS) variants have been established one after another #OTHEREFR - #OTHEREFR .", "The variable step-size matrix NSAF (VSSM-NSAF) was firstly designed by assuming subband noise powers are the same as the posteriori error and acquired excellent estimation accurateness and quicker convergence behavior in comparison with NSAF #OTHEREFR ."], "citing_paper_content": {"title": "", "abstract": "Limited by fixed step-size and sparsity penalty factor, the conventional sparsity-aware normalized subband adaptive filtering (NSAF) type algorithms suffer from trade-off requirements of high filtering accurateness and quicker convergence behavior. To deal with this problem, this paper proposes variable step-size L0-norm constraint NSAF algorithms (VSS-L0-NSAFs) for sparse system identification. We first analyze mean-square-deviation (MSD) statistics behavior of the L0-NSAF algorithm innovatively in according to a novel recursion form and arrive at corresponding expressions for the cases that background noise variance is available and unavailable, where correlation"}, "cited_paper_content": {"title": "Block-Sparsity-Induced Adaptive Filter For Multi-Clustering System Identification", "abstract": "In order to improve the performance of least mean square (LMS)-based adaptive filtering for identifying block-sparse systems, a new adaptive algorithm called block-sparse LMS (BS-LMS) is proposed in this paper. The basis of the proposed algorithm is to insert a penalty of block-sparsity, which is a mixed \\$l_{2, 0}\\$ norm of adaptive tap-weights with equal group partition sizes, into the cost function of traditional LMS algorithm. To describe a block-sparse system response, we first propose a Markov-Gaussian model, which can generate a kind of system responses of arbitrary average sparsity and arbitrary average block length using given parameters. Then we present theoretical expressions of the steady-state misadjustment and transient convergence behavior of BS-LMS with an appropriate group partition size for white Gaussian input data. Based on the above results, we theoretically demonstrate that BS-LMS has much better convergence behavior than \\$l_0\\$-LMS with the same small level of misadjustment. Finally, numerical experiments verify that all of the theoretical analysis agrees well with simulation results in a large range of parameters."}, "keywords": ["above-mentioned algorithms"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Similarly, through introducing L0-norm penalty factor, the resulting A L0-norm constraint NSAF (L0-NSAF) algorithm realized higher filtering accurateness and faster convergence (reference paper).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the trade-off requirements of high filtering accurateness and quicker convergence behavior, the authors propose a new adaptive algorithm called block-sparse LMS (BS-LMS) that inserts a penalty of block-sparsity into the cost function of traditional LMS algorithm, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the trade-off requirements of high filtering accurateness and quicker convergence behavior, the authors propose variable step-size L0-norm constraint NSAF algorithms (VSS-L0-NSAFs) for sparse system identification, which integrate Lp-norm regularization factor of filter weight into objection functions of the AFAs directly to obtain L0 and L1-norms, as well as several sparsity-aware LMS-type algorithms and corresponding improved variants, including the above-mentioned algorithms #REFR.", "given_citation_intent": "method", "given_keywords": ["above-mentioned algorithms"]}]}
{"citing_id": "2304.13672v1", "cited_id": "2002.02255", "section_title": "V. Discussion", "citation": "Therefore, we think the adaptation of CT\u2192MRI is more difficult than MRI\u2192CT, which is consistent with discussions in #REFR . We think it is probably because that MRI Fig. 12 . Visualization of the training process of FVP.", "text_before_citation": ["For the experiments of three dataset in Table I, Table II and Table IV with two adaptation directions, the proposed FVP yields the best Dice values in 5 cases over all 6 cases, and the best ASD values in all 6 cases.", "We think that the experiments clearly demonstrate that our FVP perform generally better compared with other DA methods.", "In Table I , the number of CT samples (30) is larger than the number of MRI samples #OTHEREFR .", "While the Dice score by source-only in CT\u2192MRI (0.517) is much lower than that in MRI\u2192CT (0.647).", "In Table II , the Dice score by source-only in CT\u2192MRI (0.412) is much lower than that in MRI\u2192CT (0.714)."], "text_after_citation": ["The first and second rows are visualization of the real and imaginary parts of the prompt in the frequency domain, respectively.", "Note that the size of the prompt is 16 \u00d7 16.", "The third row is the prompt in the spatial domain with a size of 256 \u00d7 256.", "provides more texture details in organs, compared with CT, which makes training the source segmentation model in CT results in a worse model for adaptation than training in MRI.", "FVP does not work well on Dice in MRI\u2192CT adaptation in Table II , probably because MRI\u2192CT is the least discriminating task among all four tasks in Table I and II."], "citing_paper_content": {"title": "Fvp: Fourier Visual Prompting For Source-Free Unsupervised Domain Adaptation Of Medical Image Segmentation", "abstract": "Medical image segmentation methods normally perform poorly when there is a domain shift between training and testing data. Unsupervised Domain Adaptation (UDA) addresses the domain shift problem by training the model using both labeled data from the source domain and unlabeled data from the target domain. Source-Free UDA (SFUDA) was recently proposed for UDA without requiring the source data during the adaptation, due to data privacy or data transmission issues, which normally adapts the pre-trained deep model in the testing stage. However, in real clinical scenarios of medical image segmentation, the trained model is normally frozen in the testing stage. In this paper, we propose Fourier Visual Prompting (FVP) for SFUDA of medical image segmentation. Inspired by prompting learning in natural language processing, FVP steers the frozen pre-trained model to perform well in the target domain by adding a visual prompt to the input target data. In FVP, the visual prompt is parameterized using only a small amount of low-frequency learnable parameters in the input frequency space, and is learned by minimizing the segmentation loss between the predicted segmentation of the prompted target image and reliable pseudo segmentation label of the target image under the frozen model. To our knowledge, FVP is the first work to apply visual prompts to SFUDA for medical image segmentation. The proposed FVP is validated using three public datasets, and experiments demonstrate that FVP yields better segmentation results, compared with various existing methods."}, "cited_paper_content": {"title": "Unsupervised Bidirectional Cross-Modality Adaptation Via Deeply Synergistic Image And Feature Alignment For Medical Image Segmentation", "abstract": "Unsupervised domain adaptation has increasingly gained interest in medical image computing, aiming to tackle the performance degradation of deep neural networks when being deployed to unseen data with heterogeneous characteristics. In this work, we present a novel unsupervised domain adaptation framework, named as Synergistic Image and Feature Alignment (SIFA), to effectively adapt a segmentation network to an unlabeled target domain. Our proposed SIFA conducts synergistic alignment of domains from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features by leveraging adversarial learning in multiple aspects and with a deeply supervised mechanism. The feature encoder is shared between both adaptive perspectives to leverage their mutual benefits via end-to-end learning. We have extensively evaluated our method with cardiac substructure segmentation and abdominal multi-organ segmentation for bidirectional cross-modality adaptation between MRI and CT images. Experimental results on two different tasks demonstrate that our SIFA method is effective in improving segmentation performance on unlabeled target images, and outperforms the state-of-the-art domain adaptation approaches by a large margin."}, "keywords": ["training process", "MRI\u2192CT"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed FVP method for source-free unsupervised domain adaptation of medical image segmentation outperforms various existing methods, as demonstrated in Table I, Table II, and Table IV of the manuscript. Specifically, FVP yields the best Dice and ASD values in all six cases, and performs generally better compared with other DA methods. This is consistent with the findings in #REFR, where the authors present a novel unsupervised domain adaptation framework, named as Synergistic Image and Feature Alignment (SIFA), that effectively adapts a segmentation network to an unlabeled target domain.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Synergistic Image and Feature Alignment (SIFA) method in #REFR outperforms the state-of-the-art domain adaptation approaches by a large margin, as demonstrated in the experiments on bidirectional cross-modality adaptation between MRI and CT images for cardiac substructure segmentation and abdominal multi-organ segmentation.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed SIFA method in #REFR, which conducts synergistic alignment of domains from both image and feature perspectives, could be a promising approach to improve the segmentation performance on unlabeled target images in the training process of MRI\u2192CT adaptation.", "given_citation_intent": "result", "given_keywords": ["training process", "MRI\u2192CT"]}]}
{"citing_id": "2303.07538v1", "cited_id": "1412.6980", "section_title": "Model Training Procedure", "citation": "We used the Adam optimizer #REFR with an L2 regularization penalty for performing stochastic gradient descent in all models.", "text_before_citation": ["HiSSNet and ProtoNet models were trained using episodic batches, where each batch contains a random subset of 12 sound classes from the taxonomy and 5 recordings for each class.", "To sufficiently balance training on both SED and SID classes, we created three different batch configurations: SED only, SED & SID, and SID only.", "The configuration for each batch was randomly selected during training, with a weight distribution of 60%/20%/20%, respectively.", "Each epoch contained 100 episodic batches, and the model was trained for 1000 epochs."], "text_after_citation": ["All HiSSNet models were trained on the full SEID dataset, while SOTA baseline models were trained on SED-specific or SID-specific subsets of the dataset.", "For the SED baselines, we implemented a dilated convolutional recurrent neural network (CRNN) #OTHEREFR and a non-hierarchical ProtoNet #OTHEREFR , and for the SID baselines we implemented a non-hierarchical ProtoNet #OTHEREFR .", "The SED baselines were trained on the data subset from ESC50, TUT, TAU, FSD50K and BBC, while the SID baselines were trained on the data subset from VCTK and LibriSpeech.", "The dilated CRNN was trained using standard batch processing with a batch size of 128."], "citing_paper_content": {"title": "Hissnet: Sound Event Detection And Speaker Identification Via Hierarchical Prototypical Networks For Low-Resource Headphones", "abstract": "Modern noise-cancelling headphones have significantly improved users' auditory experiences by removing unwanted background noise, but they can also block out sounds that matter to users. Machine learning (ML) models for sound event detection (SED) and speaker identification (SID) can enable headphones to selectively pass through important sounds; however, implementing these models for a user-centric experience presents several unique challenges. First, most people spend limited time customizing their headphones, so the sound detection should work reasonably well out of the box. Second, the models should be able to learn over time the specific sounds that are important to users based on their implicit and explicit interactions. Finally, such models should have a small memory footprint to run on low-power headphones with limited on-chip memory. In this paper, we propose addressing these challenges using HiSSNet (Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model that uses a hierarchical prototypical network to detect both general and specific sounds of interest and characterize both alarm-like and speech sounds. We show that HiSSNet outperforms an SEID model trained using non-hierarchical prototypical networks by 6.9-8.6%. When compared to state-of-the-art (SOTA) models trained specifically for SED or SID alone, HiSSNet achieves similar or better performance while reducing the memory footprint required to support multiple capabilities on-device."}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["stochastic gradient descent"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "HiSSNet and ProtoNet models were trained using episodic batches, where each batch contains a random subset of 12 sound classes from the taxonomy and 5 recordings for each class. To sufficiently balance training on both SED and SID classes, we created three different batch configurations: SED only, SED & SID, and SID only. The configuration for each batch was randomly selected during training, with a weight distribution of 60%/20%/20%, respectively. Each epoch contained 100 episodic batches, and the model was trained for 1000 epochs. The training process utilized the Adam algorithm for optimization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the SEID model, the authors implemented the Adam algorithm, which is computationally efficient and well-suited for large problems with noisy and sparse gradients, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the stochastic objective functions in HiSSNet, the authors implemented the Adam algorithm, which is computationally efficient and has little memory requirements, making it well-suited for large-scale problems with very noisy and/or sparse gradients. #REFR", "given_citation_intent": "method", "given_keywords": ["stochastic gradient descent"]}]}
{"citing_id": "2304.03367v1", "cited_id": "2001.09336", "section_title": "V. Simulations", "citation": "First, in a similar fashion to #REFR we report the coverage of the actual safe region and the overlap with the unsafe one.", "text_before_citation": ["In this section, we carry out simulations to quantify the performance of IGCI.", "We utilize a two dimensional navigation task and a robotic arm environment and consider scenaria of perfect and noisy state observations, as well as suboptimal trajectories.", "To evaluate the quality of the inferred constraints, we utilize a number of metrics."], "text_after_citation": ["More specifically, we define the constrained and unconstrained regions as", "EQUATION", "respectively.", "The corresponding regions constructed using the estimated parametersc i , i = 1, . . .", ", N c are designated with\u00c3 and\u00c3 c , respectively."], "citing_paper_content": {"title": "Constraint Inference In Control Tasks From Expert Demonstrations Via Inverse Optimization", "abstract": "Inferring unknown constraints is a challenging and crucial problem in many robotics applications. When only expert demonstrations are available, it becomes essential to infer the unknown domain constraints to deploy additional agents effectively. In this work, we propose an approach to infer affine constraints in control tasks after observing expert demonstrations. We formulate the constraint inference problem as an inverse optimization problem, and we propose an alternating optimization scheme that infers the unknown constraints by minimizing a KKT residual objective. We demonstrate the effectiveness of our method in a number of simulations, and show that our method can infer less conservative constraints than a recent baseline method while maintaining comparable safety guarantees."}, "cited_paper_content": {"title": "Learning Constraints From Locally-Optimal Demonstrations Under Cost Function Uncertainty", "abstract": "We present an algorithm for learning parametric constraints from locally-optimal demonstrations, where the cost function being optimized is uncertain to the learner. Our method uses the Karush-Kuhn-Tucker (KKT) optimality conditions of the demonstrations within a mixed integer linear program (MILP) to learn constraints which are consistent with the local optimality of the demonstrations, by either using a known constraint parameterization or by incrementally growing a parameterization that is consistent with the demonstrations. We provide theoretical guarantees on the conservativeness of the recovered safe/unsafe sets and analyze the limits of constraint learnability when using locally-optimal demonstrations. We evaluate our method on high-dimensional constraints and systems by learning constraints for 7-DOF arm and quadrotor examples, show that it outperforms competing constraint-learning approaches, and can be effectively used to plan new constraint-satisfying trajectories in the environment."}, "keywords": ["actual safe region"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate the quality of the inferred constraints, we utilize a number of metrics. Our approach to infer affine constraints in control tasks after observing expert demonstrations is based on an inverse optimization problem, and we propose an alternating optimization scheme that infers the unknown constraints by minimizing a KKT residual objective #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To infer constraints from expert demonstrations, we propose an approach that formulates the constraint inference problem as an inverse optimization problem and minimizes a KKT residual objective, as presented in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To infer actual safe regions from locally-optimal demonstrations, we can use the algorithm proposed in \"Learning Constraints From Locally-Optimal Demonstrations Under Cost Function Uncertainty\" (#REFR) which learns constraints consistent with the local optimality of the demonstrations by using the Karush-Kuhn-Tucker (KKT) optimality conditions within a mixed integer linear program (MILP).", "given_citation_intent": "method", "given_keywords": ["actual safe region"]}]}
{"citing_id": "2303.16322v1", "cited_id": "1809.04184", "section_title": "Reducing Training Time", "citation": "Table 3 required between 0.49 and 0.8 GPU days to be discovered by FMAS, which is negligible compared to the 2,590 GPU days required by DPC #REFR .", "text_before_citation": ["Similar to Table 3 , Table 4 reports results when using the Mo-bileNetV2 backbone and searching for 25 generations.", "In addition to FLOPs and parameters, we also report inference latency on the GAP8 for the original model, FCN-VGG16, and selected search results.", "Note that while FCN-VGG16 uses only GAP8-supported operations, making it a suitable baseline, it requires more than 8\u00d7 more RAM than the GAP8 has, and therefore cannot be deployed. Table 1 reports their hyperparameters.", "FMAS-F1 cuts the number of FLOPs by 43% with respect to DL3+, and network parameters by 7.9%, for a relative increase of 5.2% in MIoU error; it was discovered in 0.68 GPU days (generation 17).", "FMAS-F2 trades off only 2.5% of the MIoU error of DL3+ for reducing FLOPs by 10%, and network parameters by 20%, in 0.52 GPU days (generation 13)."], "text_after_citation": ["Although DPC outperforms the MIoU of FMAS-F2 by 6.1%, FMAS-F2 cuts FLOPs and parameters by 9 and 22% respectively in only 0.65 GPU days."], "citing_paper_content": {"title": "Fmas: Fast Multi-Objective Supernet Architecture Search For Semantic Segmentation", "abstract": "We present FMAS, a fast multi-objective neural architecture search framework for semantic segmentation. FMAS subsamples the structure and pre-trained parameters of DeepLabV3+, without finetuning, dramatically reducing training time during search. To further reduce candidate evaluation time, we use a subset of the validation dataset during the search. Only the final, Pareto non-dominated, candidates are ultimately fine-tuned using the complete training set. We evaluate FMAS by searching for models that effectively trade accuracy and computational cost on the PASCAL VOC 2012 dataset. FMAS finds competitive designs quickly, e.g., taking just 0.5 GPU days to discover a DeepLabV3+ variant that reduces FLOPs and parameters by 10% and 20% respectively, for less than 3% increased error. We also search on an edge device called GAP8 and use its latency as the metric. FMAS is capable of finding 2.2\u00d7 faster network with 7.61% MIoU loss."}, "cited_paper_content": {"title": "Searching For Efficient Multi-Scale Architectures For Dense Image Prediction", "abstract": "The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\\% on Cityscapes (street scene parsing), 71.3\\% on PASCAL-Person-Part (person-part segmentation), and 87.9\\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems."}, "keywords": ["0.8 GPU days"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors identified efficient multi-scale architectures for dense image prediction using meta-learning techniques, achieving state-of-the-art performance on tasks such as semantic image segmentation. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors reduced the number of FLOPs by 43% and network parameters by 7.9% with FMAS-F1, which resulted in a relative increase of 5.2% in MIoU error compared to DL3+; this architecture was discovered in 0.68 GPU days and outperformed human-invented architectures, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors discovered FMAS-F1, which reduces the number of FLOPs by 43% and network parameters by 7.9%, with a relative increase of 5.2% in MIoU error, in only 0.68 GPU days (generation 17) compared to the original model, as reported in #REFR.", "given_citation_intent": "result", "given_keywords": ["0.8 GPU days"]}]}
{"citing_id": "2305.02374v1", "cited_id": "1810.04805", "section_title": "Bert-Based Word Embedding", "citation": "In this article, we employ the BERT model #REFR as one of the most recent PLM approaches.", "text_before_citation": ["The objective of word embedding [7] is to map words to semantic vectors to be used in algorithms of machine learning.", "It has been demonstrated to be a reliable method for extracting meaningful word representations based on their context #OTHEREFR .", "Diverse word embedding techniques, including Skip-gram #OTHEREFR and matrix factorization techniques such as GloVe #OTHEREFR , have been suggested to produce meaningful word representations for neural network models.", "Pre-trained language models (PLMs) typically employ unlabelled data to learn model parameters [3] ."], "text_after_citation": ["BERT is a The general BERT architecture is shown in Figure 2 .", "BERT uses a bi-directional transformer, in which representations are jointly conditioned on both the left and right context in all layers #OTHEREFR .", "This distinguishes BERT from Word2Vec and GloVe models which produce an embedding in one direction to ignore its contextual differences."], "citing_paper_content": {"title": "A Novel Plagiarism Detection Approach Combining Bert-Based Word Embedding, Attention-Based Lstms And An Improved Differential Evolution Algorithm", "abstract": "Detecting plagiarism involves finding similar items in two different sources. In this article, we propose a novel method for detecting plagiarism that is based on attention mechanism-based long short-term memory (LSTM) and bidirectional encoder representations from transformers (BERT) word embedding, enhanced with optimized differential evolution (DE) method for pre-training and a focal loss function for training. BERT could be included in a downstream task and fine-tuned as a task-specific BERT can be included in a downstream task and fine-tuned as a task-specific structure, while the trained BERT model is capable of detecting various linguistic characteristics. Unbalanced classification is one of the primary issues with plagiarism detection. We suggest a focal loss-based training technique that carefully learns minority class instances to solve this. Another issue that we tackle is the training phase itself, which typically employs gradient-based methods like back-propagation for the learning process and thus suffers from some drawbacks, including sensitivity to initialization. To initiate the BP process, we suggest a novel DE algorithm that makes use of a clustering-based mutation operator. Here, a winning cluster is identified for the current DE population, and a fresh updating method is used to produce potential answers. We evaluate our proposed approach on three benchmark datasets (MSRP, SNLI, and SemEval2014) and demonstrate that it performs well when compared to both conventional and population-based methods."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["BERT model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Pre-trained language models (PLMs) have been shown to be effective in learning model parameters from unlabelled data #REFR. BERT, a bidirectional encoder representation from transformers, is a new language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, and has achieved state-of-the-art results on various natural language processing tasks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the word embedding method, attention mechanism-based long short-term memory (LSTM) and bidirectional encoder representations from transformers (BERT) have been proposed #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Pre-training deep bidirectional representations from unlabeled text using the BERT model has been shown to be a powerful method for a wide range of natural language processing tasks, including question answering and language inference #REFR.", "given_citation_intent": "method", "given_keywords": ["BERT model"]}]}
{"citing_id": "2305.02056v1", "cited_id": "1806.02771", "section_title": "S", "citation": "We will now rewrite the formula from #REFR by adding a global disjunction over all values of these numbers g.", "text_before_citation": ["In particular, if for a, b \u2208 N we use Q a,b to denote the set of all numbers q of granularity b \u2208 N such that 0 \u2264 q \u2264 a, then", "\u2022 |X 1 | \u2264 |X 2 | (for some X 1 , X 2 \u2286 V", "g(t \u2264 t ) \u2208 Q N +1,\u03b3 .", "Moreover, the same also holds for t 2 (X\u0232 ) t 1 (X) since we can assume g(t 2 t 1 ) = t 1 (X), irrespective of t 2 (X\u0232 ) or the instantiation of Y .", "The weight comparison g(t 2 t 1 ) t 1 (X) may thus occur outside the quantification of Y ."], "text_after_citation": ["To avoid any confusion, we remark that this indeed increases the total length of the formula by a factor of O(N ), but the length of each individual subformula forming an atom of this disjunction remains upper-bounded by O(|\u03d5|).", "S\u2286comp(\u03d5,X) g:", "S\u222a{t 2 t 1 }\u2192Q N +1,\u03b3 t\u2264t \u2208S t(X) \u2264 g(t \u2264 t ) \u2227 g(t \u2264 t ) \u2264 t (X) (14) \u2227 g(t 2 t 1 ) t 1 (X) \u2227 \u2200\u0232 \u03c7 t 2 t 1 S (X\u0232 ) \u2227 (t 2 (X\u0232 ) g(t 2 t 1 ) \u2228 \u03c7 t 2 t 1 S (X\u0232 ) .", "To truly transform #OTHEREFR into a disjunction of table formulas, for g : S \u222a{t 2 t 1 } \u2192 Q N +1,\u03b3 and t \u2208 \u03c4 1 , we introduce a set of thresholds that will apply to individual weight terms occurring in #OTHEREFR rather than weight comparisons.", "For this we will want to choose the strictest threshold for an individual weight term implied by a comparison involving it, which in the case of t 1 may include a threshold introduced for t 1 t 2 ."], "citing_paper_content": {"title": "Approximate Evaluation Of Quantitative Second Order Queries", "abstract": "Courcelle's theorem and its adaptations to cliquewidth have shaped the field of exact parameterized algorithms and are widely considered the archetype of algorithmic meta-theorems. In the past decade, there has been growing interest in developing parameterized approximation algorithms for problems which are not captured by Courcelle's theorem and, in particular, are considered not fixed-parameter tractable under the associated widths. We develop a generalization of Courcelle's theorem that yields efficient approximation schemes for any problem that can be captured by an expanded logic we call \u2200 CMSO, capable of making logical statements about the sizes of set variables via so-called weight comparisons. The logic controls weight comparisons via the quantifier-alternation depth of the involved variables, allowing full comparisons for zero-alternation variables and limited comparisons for one-alternation variables. We show that the developed framework threads the very needle of tractability: on one hand it can describe a broad range of approximable problems, while on the other hand we show that the restrictions of our logic cannot be relaxed under well-established complexity assumptions. The running time of our approximation scheme is polynomial in 1/\u03b5, allowing us to fully interpolate between faster approximate algorithms and slower exact algorithms. This provides a unified framework to explain the tractability landscape of graph problems parameterized by treewidth and cliquewidth, as well as classical non-graph problems such as Subset Sum and Knapsack. * acknowledges support by Austrian Science fund (FWF) START project Y1329 \u2020 acknowledges support by Austrian Science fund (FWF) project J4651-N 1"}, "cited_paper_content": {"title": "Structural Rounding: Approximation Algorithms For Graphs Near An Algorithmically Tractable Class", "abstract": "We develop a new framework for generalizing approximation algorithms from the structural graph algorithm literature so that they apply to graphs somewhat close to that class (a scenario we expect is common when working with real-world networks) while still guaranteeing approximation ratios. The idea is to $\\textit{edit}$ a given graph via vertex- or edge-deletions to put the graph into an algorithmically tractable class, apply known approximation algorithms for that class, and then $\\textit{lift}$ the solution to apply to the original graph. We give a general characterization of when an optimization problem is amenable to this approach, and show that it includes many well-studied graph problems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum Maximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge ($\\ell$-)Dominating Set, and Connected Dominating Set. To enable this framework, we develop new editing algorithms that find the approximately-fewest edits required to bring a given graph into one of several important graph classes (in some cases, also approximating the target parameter of the family). For bounded degeneracy, we obtain a bicriteria $(4,4)$-approximation which also extends to a smoother bicriteria trade-off. For bounded treewidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log w}))$-approximation, and for bounded pathwidth, we obtain a bicriteria $(O(\\log^{1.5} n), O(\\sqrt{\\log w} \\cdot \\log n))$-approximation. For treedepth $2$ (also related to bounded expansion), we obtain a $4$-approximation. We also prove complementary hardness-of-approximation results assuming $\\mathrm{P} \\neq \\mathrm{NP}$: in particular, these problems are all log-factor inapproximable, except the last which is not approximable below some constant factor ($2$ assuming UGC)."}, "keywords": ["global disjunction"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors develop a new framework for generalizing approximation algorithms from the structural graph algorithm literature so that they apply to graphs somewhat close to that class while still guaranteeing approximation ratios #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' framework for generalizing approximation algorithms from the structural graph algorithm literature to graphs somewhat close to that class, while still guaranteeing approximation ratios, is based on the idea of editing a given graph via vertex- or edge-deletions to put the graph into an algorithmically tractable class, applying known approximation algorithms for that class, and then lifting the solution to apply to the original graph #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' approach of editing a given graph via vertex- or edge-deletions to put the graph into an algorithmically tractable class, applying known approximation algorithms for that class, and then lifting the solution to apply to the original graph, as described in #REFR, has been shown to include many well-studied graph problems, such as Independent Set, Vertex Cover, Feedback Vertex Set, Minimum Maximal Matching, Chromatic Number, ($\\ell$-)Dominating Set, Edge ($\\ell$-)Dominating Set, and Connected Dominating Set, which are all amenable to this approach.", "given_citation_intent": "background", "given_keywords": ["global disjunction"]}]}
{"citing_id": "2304.14660v2", "cited_id": "1902.09063", "section_title": "C. Model Selection For Different Testing Modes", "citation": "However, for C8 of #REFR , ViT-B showed better performance than ViT-H (S 2 : 69.3% (B) vs. 64.8% (H)).", "text_before_citation": ["Since the everything mode is the bright and key function of SAM, we evaluated it using both two models (everything with ViT-B and ViT-H, called S 1B and S 1H ).", "For the prompt mode, we first conducted preexperiments on part of our whole dataset #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , as shown in Table III .", "Specifically, we tested two typical strategies (S 2 and S 5 ) using the DICE and HD metrics.", "The experiments showed that the two models had no absolute advantage in different datasets or objects.", "For example, in #OTHEREFR , ViT-H outperformed ViT-B on the DICE score for C2 (S 2 : 59.1% (B) vs. 73.2% (H))."], "text_after_citation": ["For different structures in the same dataset, e.g., C2, C3, and C4 in #OTHEREFR , there was no winner between ViT-B and ViT-H.", "Even for the same structure in different datasets such as the A4 structures in two datasets #OTHEREFR , #OTHEREFR , the performance of these two models remained indistinguishable.", "Based on the experiments in Table III, our conclusion is that the larger ViT-H model does not show a significant advantage over the smaller ViT-B model in MIS task under different prompt modes.", "In other words, we can assume that the two models should achieve similar average results over a large number of medical images.", "Thus, we chose ViT-B as the backbone to test our whole medical dataset to speed up the testing while ensuring that the result reflected SAM's segmentation ability, which is similar to #OTHEREFR ."], "citing_paper_content": {"title": "Segment Anything Model For Medical Images?", "abstract": "The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt (e.g., points and boxes). SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 opensource datasets, and built a large medical segmentation dataset with 16 modalities, 68 objects, and 553K slice. We conducted a comprehensive analysis of different SAM testing strategies on the so-called COSMOS 553K dataset. Extensive experiments validate that SAM performs better with manual hints like points and boxes for object perception in medical images, leading to better performance in prompt mode compared to everything mode. Additionally, SAM shows remarkable performance in some specific objects and modalities, but is imperfect or even totally fails in other situations. Finally, we analyze the influence of different factors (e.g., the Fourier-based boundary complexity and size of the segmented objects) on SAM's segmentation performance. Extensive experiments validate that SAM's zeroshot segmentation capability is not sufficient to ensure its direct application to the MIS."}, "cited_paper_content": {"title": "A Large Annotated Medical Image Dataset For The Development And Evaluation Of Segmentation Algorithms", "abstract": "Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community."}, "keywords": ["ViT-B", "C8"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the prompt mode, we first conducted preexperiments on part of our whole dataset #OTHEREFR, #OTHEREFR, #OTHEREFR, #OTHEREFR, #OTHEREFR, #OTHEREFR, as shown in Table III. Specifically, we tested two typical strategies (S 2 and S 5) using the DICE and HD metrics. The experiments showed that the two models had no absolute advantage in different datasets or objects. For example, in #OTHEREFR, ViT-H outperformed ViT-B on the DICE score for C2 (S 2: 59.1% (B) vs. 73.2% (H)). To facilitate the development of semantic segmentation algorithms for medical images, a large annotated medical image dataset has been created and made available under an open-source license, which can be effectively reused by the research community for objective assessment of general-purpose segmentation methods through comprehensive benchmarking. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors compared their medical image dataset with other benchmark datasets and showed its potential for facilitating the development of semantic segmentation algorithms in the medical domain. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The prompt mode using ViT-B and ViT-H models was evaluated on a large medical segmentation dataset with 16 modalities, 68 objects, and 553K slice, and the results were compared with those in the reference paper #REFR, indicating that ViT-B performed better on C8 with a DICE score of 85.2%.", "given_citation_intent": "result", "given_keywords": ["ViT-B", "C8"]}]}
{"citing_id": "2303.17334v1", "cited_id": "1801.05852", "section_title": "Introduction", "citation": "Current graph-based data mining tasks mainly model the relationships between nodes from the perspective of topology and attribute content #REFR , making nodes of the same class more closely embedded in the embedding space and dissimilar nodes further away.", "text_before_citation": ["With the help of network data processing platform, operators can mine user Call Detail Records (CDR) to detect fraudsters, thereby assisting mobile network operation decisions.", "Data analysis is the most crucial aspect of the whole process, and it is also full of challenges.", "Subscribers' communication behaviors naturally constitute graphs, and the use of graph mining techniques for data arXiv:2303.17334v1 [cs.", "LG] 29 Mar 2023 analysis has become an important trend.", "In recent years, graph neural network (GNN) #OTHEREFR , #OTHEREFR , #OTHEREFR has gradually become the mainstream technology for graph data mining."], "text_after_citation": ["A typical semi-supervised node classification task is performed as follows #OTHEREFR : given a large graph with a small scale of node labels, a classifier is trained on those labeled nodes and used to classify other nodes during the testing process.", "These related works include graph convolutional networks (GCN) #OTHEREFR and many of its variants proposed in recent years #OTHEREFR , which effectively utilize features in the spectral domain by using simplified first-order approximations.", "GraphSage #OTHEREFR and Graph Attention Network(GAT) #OTHEREFR utilize features in the spatial domain to better adapt to different graph topologies.", "GNNs have achieved remarkable performance in many application domains, such as text classification #OTHEREFR , image recognition #OTHEREFR , and recommender systems #OTHEREFR .", "GNN-based graph data anomaly detection has also made great progress #OTHEREFR ."], "citing_paper_content": {"title": "Gat-Cobo: Cost-Sensitive Graph Neural Network For Telecom Fraud Detection", "abstract": "Along with the rapid evolution of mobile communication technologies, such as 5G, there has been a drastically increase in telecom fraud, which significantly dissipates individual fortune and social wealth. In recent years, graph mining techniques are gradually becoming a mainstream solution for detecting telecom fraud. However, the graph imbalance problem, caused by the Pareto principle, brings severe challenges to graph data mining. This is a new and challenging problem, but little previous work has been noticed. In this paper, we propose a Graph ATtention network with COst-sensitive BOosting (GAT-COBO) for the graph imbalance problem. First, we design a GAT-based base classifier to learn the embeddings of all nodes in the graph. Then, we feed the embeddings into a well-designed cost-sensitive learner for imbalanced learning. Next, we update the weights according to the misclassification cost to make the model focus more on the minority class. Finally, we sum the node embeddings obtained by multiple cost-sensitive learners to obtain a comprehensive node representation, which is used for the downstream anomaly detection task. Extensive experiments on two real-world telecom fraud detection datasets demonstrate that our proposed method is effective for the graph imbalance problem, outperforming the state-of-the-art GNNs and GNN-based fraud detectors. In addition, our model is also helpful for solving the widespread over-smoothing problem in GNNs. The GAT-COBO code and datasets are available at https://github.com/xxhu94/GAT-COBO."}, "cited_paper_content": {"title": "Network Representation Learning: A Survey", "abstract": "With the widespread use of information technologies, information networks are becoming increasingly popular to capture complex relationships across various disciplines, such as social networks, citation networks, telecommunication networks, and biological networks. Analyzing these networks sheds light on different aspects of social life such as the structure of societies, information diffusion, and communication patterns. In reality, however, the large scale of information networks often makes network analytic tasks computationally expensive or intractable. Network representation learning has been recently proposed as a new learning paradigm to embed network vertices into a low-dimensional vector space, by preserving network topology structure, vertex content, and other side information. This facilitates the original network to be easily handled in the new vector space for further analysis. In this survey, we perform a comprehensive review of the current literature on network representation learning in the data mining and machine learning field. We propose new taxonomies to categorize and summarize the state-of-the-art network representation learning techniques according to the underlying learning mechanisms, the network information intended to preserve, as well as the algorithmic designs and methodologies. We summarize evaluation protocols used for validating network representation learning including published benchmark datasets, evaluation methods, and open source algorithms. We also perform empirical studies to compare the performance of representative algorithms on common datasets, and analyze their computational complexity. Finally, we suggest promising research directions to facilitate future study."}, "keywords": ["Current graph-based data"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Subscribers' communication behaviors naturally constitute graphs, and the use of graph mining techniques for data analysis has become an important trend. In recent years, graph neural network (GNN) has gradually become the mainstream technology for graph data mining. To embed network vertices into a low-dimensional vector space, preserving network topology structure, vertex content, and other side information, network representation learning has been proposed as a new learning paradigm. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Subscribers' communication behaviors naturally constitute graphs, and the use of graph mining techniques for data analysis has become an important trend. In recent years, graph neural network (GNN) has gradually become the mainstream technology for graph data mining, with network representation learning being a promising new learning paradigm to embed network vertices into a low-dimensional vector space, facilitating the original network to be easily handled in the new vector space for further analysis #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Current graph-based data analysis often involves the use of graph neural networks (GNNs) #REFR, which have become a mainstream technology for graph data mining, especially in detecting telecom fraud from user Call Detail Records (CDR).", "given_citation_intent": "background", "given_keywords": ["Current graph-based data"]}]}
{"citing_id": "2303.11019v1", "cited_id": "1909.10726", "section_title": "A. Datasets", "citation": "This is consistent with the work in #REFR and allows sampling of healthy tissue patches that can be used as meaningful negative examples.", "text_before_citation": ["2) PAIP2019 dataset: The PAIP 2019 dataset #OTHEREFR contains 50 WSIs of liver cancer from 50 patients who underwent resection for hepatocellular carcinoma (HCC) at the Seoul National University Hospital.", "The slides were stained by H&E and digitalised with an Aperio AT2 scanner at 20\u00d7 power and 0.5021\u00b5m/px resolution, resulting in image sizes between 35, 855 \u00d7 39, 407 and 64, 768 \u00d7 47, 009 pixels.", "Two types of annotation are provided: viable regions of cancer cells for continuous tumour areas, as well as whole cancer regions for boundary between the non-tumorous hepatic lobules and the viable tumour (including peritumoral fibrosis, capsules, and inflammation).", "The initial annotations were provided by a pathologist with 11 years of experience in liver histopathology and reviewed by another expert pathologist.", "Additionally, we also generated annotations for \"tissue area\" which indicates healthy tissue pixels by threshold of (R, G, B) \u2264 (235, 210, 235)."], "text_after_citation": ["For the data pre-processing, we generated context (5\u00d7 magnification) and target patches (20\u00d7 magnification) consistent with the settings used in the BCSS dataset.", "We randomly selected 10 out of 50 WSIs as the validation set for our CV."], "citing_paper_content": {"title": "A Dual-Branch Self-Supervised Representation Learning Framework For Tumour Segmentation In Whole Slide Images", "abstract": "Supervised deep learning methods have achieved considerable success in medical image analysis, owing to the availability of large-scale and well-annotated datasets. However, creating such datasets for whole slide images (WSIs) in histopathology is a challenging task due to their gigapixel size. In recent years, self-supervised learning (SSL) has emerged as an alternative solution to reduce the annotation overheads in WSIs, as it does not require labels for training. These SSL approaches, however, are not designed for handling multi-resolution WSIs, which limits their performance in learning discriminative image features. In this paper, we propose a Dual-branch SSL Framework for WSI tumour segmentation (DSF-WSI) that can effectively learn image features from multi-resolution WSIs. Our DSF-WSI connected two branches and jointly learnt low and high resolution WSIs in a self-supervised manner. Moreover, we introduced a novel Context-Target Fusion Module (CTFM) and a masked jigsaw pretext task to align the learnt multi-resolution features. Furthermore, we designed a Dense SimSiam Learning (DSL) strategy to maximise the similarity of different views of WSIs, enabling the learnt representations to be more efficient and discriminative. We evaluated our method using two public datasets on breast and liver cancer segmentation tasks. The experiment results demonstrated that our DSF-WSI can effectively extract robust and efficient representations, which we validated through subsequent fine-tuning and semi-supervised settings. Our proposed method achieved better accuracy than other state-of-the-art approaches. Code is available at https://github.com/Dylan-H-Wang/dsf-wsi."}, "cited_paper_content": {"title": "Multi-Scale Fully Convolutional Neural Networks For Histopathology Image Segmentation: From Nuclear Aberrations To The Global Tissue Architecture", "abstract": "Histopathologic diagnosis is dependent on simultaneous information from a broad range of scales, ranging from nuclear aberrations ($\\approx \\mathcal{O}(0.1 \\mu m)$) over cellular structures ($\\approx \\mathcal{O}(10\\mu m)$) to the global tissue architecture ($\\gtrapprox \\mathcal{O}(1 mm)$). Bearing in mind which information is employed by human pathologists, we introduce and examine different strategies for the integration of multiple and widely separate spatial scales into common U-Net-based architectures. Based on this, we present a family of new, end-to-end trainable, multi-scale multi-encoder fully-convolutional neural networks for human modus operandi-inspired computer vision in histopathology."}, "keywords": ["healthy tissue patches"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To effectively learn image features from multi-resolution whole slide images (WSIs), Wang et al. proposed a Dual-branch SSL Framework for WSI tumour segmentation (DSF-WSI) that jointly learns low and high resolution WSIs in a self-supervised manner, and introduced a novel Context-Target Fusion Module (CTFM) and a masked jigsaw pretext task to align the learnt multi-resolution features. Furthermore, they designed a Dense SimSiam Learning (DSL) strategy to maximise the similarity of different views of WSIs, enabling the learnt representations to be more efficient and discriminative. The proposed method achieved better accuracy than other state-of-the-art approaches on breast and liver cancer segmentation tasks, as demonstrated on the PAIP2019 dataset #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed method achieved better accuracy than other state-of-the-art approaches on the PAIP2019 dataset, demonstrating the effectiveness of our Dual-branch SSL Framework for WSI tumour segmentation (DSF-WSI) in learning discriminative image features from multi-resolution WSIs #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method effectively extracted robust and efficient representations of healthy tissue patches, achieving better accuracy than other state-of-the-art approaches #REFR.", "given_citation_intent": "result", "given_keywords": ["healthy tissue patches"]}]}
{"citing_id": "2304.01016v2", "cited_id": "1603.09320", "section_title": "Related Work", "citation": "At runtime, a query is an encoder into a latent space, and the k documents are retrieved using a nearest neighbor algorithm such as HNSW #REFR .", "text_before_citation": ["Transformer Based Language Models such as BERT provide contextual language representations built on the Transformer architecture (Vaswani et al., 2017) which can be specialized and adapted for specific tasks and domains .", "Using contextual word representations, it becomes relatively easy to excel at a broad range of natural language processing tasks such as Question Answering, Text Classification, and sentiment analysis.", "Bi-Encoders, commonly called dual-encoders or dense retrievers, decompose ranking by leveraging the inner product of query and document representations to produce a relevance score for query document pairs.", "While not as accurate at cross-encoders #OTHEREFR , they are more efficient for inference and easier to deploy.", "Bi-encoder document representations are query invariant, allowing them to be pre-computed and loaded into an Approximate Nearest Neighbor (ANN) such as FAISS #OTHEREFR ."], "text_after_citation": ["Since the entire document index has been pre-computed, the retrieval latency is limited to a single call of the document encoder.", "Bi-encoders commonly leverage LLM such as BERT to retrieve short passages of text leading to the task descriptor of Dense Passage Retrievers (DPR) (Karpukhin et al., 2020) .", "Driven by their efficiency in deployment and relevance performance, DPR-based models have rapidly become the building blocks for systems doing product search #OTHEREFR , open domain question answering (Karpukhin et al., 2020) and customer support #OTHEREFR .", "Efficient Inference study methods and models which decrease the model execution cost while minimizing the losses to model performance.", "Knowledge Distillation #OTHEREFR ) is a training method where a model, called the student, learns to emulate a teacher model, which is commonly larger or better performing than the student."], "citing_paper_content": {"title": "Quick Dense Retrievers Consume Kale: Post Training Kullback-Leibler Alignment Of Embeddings For Asymmetrical Dual Encoders", "abstract": "In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual-encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback-Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 3x faster inference."}, "cited_paper_content": {"title": "Efficient And Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs", "abstract": "We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation."}, "keywords": ["latent space", "nearest neighbor algorithm"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Bi-encoder document representations are query invariant, allowing them to be pre-computed and loaded into an Approximate Nearest Neighbor (ANN) such as FAISS #OTHEREFR. Hierarchical NSW, a fully graph-based approach for the approximate K-nearest neighbor search, has been proposed as a general metric space search index that strongly outperforms previous opensource state-of-the-art vector-only approaches (Malkov and Yashunin, 2018).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently perform approximate K-nearest neighbor search, the proposed solution in this manuscript builds a multi-layer structure consisting of hierarchical set of proximity graphs using navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW) #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently search for nearest neighbors in the latent space, the authors of this paper propose a fully graph-based approach using Hierarchical Navigable Small World Graphs (HNSW) #REFR, which has been shown to outperform previous vector-only approaches.", "given_citation_intent": "method", "given_keywords": ["latent space", "nearest neighbor algorithm"]}]}
{"citing_id": "2303.06350v1", "cited_id": "1506.03134", "section_title": "B. Network Structure", "citation": "There, inspired by the Pointer Network #REFR , we directly use the decoder's attention score \u03b1 j as the final output policy \u03c0 \u03b8 , from which the agent chooses the next node to move to.", "text_before_citation": ["In doing so, each encoder, conditioned on the previous one's output, allows each spatio-temporal node feature h ST G j to be informed by the dependencies across all targets and their predictions, history beliefs, and features from all other nodes.", "4) Decoder: The decoder is used to yield the final policy based on the spatio-temporal node features.", "We first associate each node v j with the shortest distance to the current node \u03c8 t , which is pre-solved by Dijkstra #OTHEREFR . Then, a fully-connected layer maps the concatenated feature", "Concat(h ST G j , dist(v j , \u03c8 t )) toh ST G j .", "From these node features, we extract the current node feature (features of the current agent's node) as query (i.e., h q =h ST G t ) and its neighboring nodes' features as key-value pairs h k,v for the decoder unit."], "text_after_citation": ["This scheme relaxes the requirement of a fixed policy size, instead adapting the policy's dimension dynamically to the number of neighboring nodes.", "Together with spatial positional encoding, this endows our network with the ability to generalize to arbitrary graphs and topologies."], "citing_paper_content": {"title": "Spatio-Temporal Attention Network For Persistent Monitoring Of Multiple Mobile Targets", "abstract": "This work focuses on the persistent monitoring problem, where a set of targets moving based on an unknown model must be monitored by an autonomous mobile robot with a limited sensing range. To keep each target's position estimate as accurate as possible, the robot needs to adaptively plan its path to (re-)visit all the targets and update its belief from measurements collected along the way. In doing so, the main challenge is to strike a balance between exploitation, i.e., re-visiting previously-located targets, and exploration, i.e., finding new targets or re-acquiring lost ones. Encouraged by recent advances in deep reinforcement learning, we introduce an attention-based neural solution to the persistent monitoring problem, where the agent can learn the inter-dependencies between targets, i.e., their spatial and temporal correlations, conditioned on past measurements. This endows the agent with the ability to determine which target, time, and location to attend to across multiple scales, which we show also helps relax the usual limitations of a finite target set. We experimentally demonstrate that our method outperforms other baselines in terms of number of targets visits and average estimation error in complex environments. Finally, we implement and validate our model in a drone-based simulation experiment to monitor mobile ground targets in a high-fidelity simulator."}, "cited_paper_content": {"title": "Pointer Networks", "abstract": "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems."}, "keywords": ["Pointer Network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the problem of variable size output dictionaries, the authors propose a Pointer Net (Ptr-Net) that uses attention as a pointer to select a member of the input sequence as the output, which has been shown to improve over sequence-to-sequence with input attention and allows for generalization beyond the maximum lengths they were trained on #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of variable size output dictionaries, the authors propose a Pointer Net (Ptr-Net) that uses attention as a pointer to select a member of the input sequence as the output, which has been shown to improve over sequence-to-sequence with input attention and allows for generalization beyond the maximum lengths they were trained on #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To inform the dependencies across all targets and their predictions, history beliefs, and features from all other nodes, the authors utilized a Pointer Network, as proposed in [1]. #REFR", "given_citation_intent": "method", "given_keywords": ["Pointer Network"]}]}
{"citing_id": "2305.01775v1", "cited_id": "1908.07964", "section_title": "E. Proof Of Proposition 3", "citation": "Consider the epigraph formulation in #REFR and define c j = \u2212 G g=1 c A g \u03b1 gj as a shorthand.", "text_before_citation": [], "text_after_citation": ["As discussed in Section V-B, the largest sum of s co ji is attained if \u03bb co j = 0, in which case s co ji = c j \u03be j , \u2200i per (14c), i.e., the worst case, and the smallest sum of s co ji is attained if \u03bb co j = c j , in which case s co ji = c j \u03be j , \u2200i as (14b)-(14d) are equal.", "As (14b) is always dominated by (14c) and (14d), write s co ji = c j \u03be j +mc j (\u03be j \u2212 \u03be ji ) where we set \u03bb co j = mc j with m = [0, 1].", "Using this definition the relevant part of (14a) can be written as mc j j + (N ) \u22121 N i=1 (c j \u03be j \u2212 mc j ( \u03be ji \u2212 \u03be j ) = mc j j \u2212 (N ) \u22121 N i=1 ( \u03be ji \u2212 \u03be j ) + c j \u03be j .", "If the term in rectangular parentheses is positive (case 1 in Proposition 3), m should be minimized leading to m = 0 \u21d2 \u03bb co j = 0.", "If term (A) is negative (case 2 in Proposition 3), m should be maximized leading to m = 1 \u21d2 \u03bb co j = c j ."], "citing_paper_content": {"title": "Data Valuation From Data-Driven Optimization", "abstract": "With the ongoing investment in data collection and communication technology in power systems, data-driven optimization has been established as a powerful tool for system operators to handle stochastic system states caused by weatherand behavior-dependent resources. However, most methods are ignorant to data quality, which may differ based on measurement and underlying privacy-protection mechanisms. This paper addresses this shortcoming by (i) proposing a practical data quality metric based on Wasserstein distance, (ii) leveraging a novel modification of distributionally robust optimization using information from multiple data sets with heterogeneous quality to valuate data, (iii) applying the proposed optimization framework to an optimal power flow problem, and (iv) showing a direct method to valuate data from the optimal solution. We conduct numerical experiments to analyze and illustrate the proposed model and publish the implementation open-source."}, "cited_paper_content": {"title": "Constrained Thompson Sampling For Real-Time Electricity Pricing With Grid Reliability Constraints", "abstract": "We consider the problem of an aggregator attempting to learn customers' load flexibility models while implementing a load shaping program by means of broadcasting daily dispatch signals. We adopt a multi-armed bandit formulation to account for the stochastic and unknown nature of customers' responses to dispatch signals. We propose a constrained Thompson sampling heuristic, Con-TS-RTP, that accounts for various possible aggregator objectives (e.g., to reduce demand at peak hours, integrate more intermittent renewable generation, track a desired daily load profile, etc) and takes into account the operational constraints of a distribution system to avoid potential grid failures as a result of uncertainty in the customers' response. We provide a discussion on the regret bounds for our algorithm as well as a discussion on the operational reliability of the distribution system's constraints being upheld throughout the learning process."}, "keywords": ["epigraph formulation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "With the aim of reducing demand at peak hours and integrating more intermittent renewable generation, the authors propose a constrained Thompson sampling heuristic, Con-TS-RTP, that accounts for various possible aggregator objectives and takes into account the operational constraints of a distribution system to avoid potential grid failures as a result of uncertainty in the customers' response #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "With the increasing investment in data collection and communication technology in power systems, data-driven optimization has become a powerful tool for system operators to handle stochastic system states caused by weather and behavior-dependent resources. To address the issue of data quality, the authors propose a practical data quality metric based on Wasserstein distance and leverage a novel modification of distributionally robust optimization using information from multiple data sets with heterogeneous quality to valuate data, as demonstrated in their optimal power flow problem. #REFR provides a multi-armed bandit formulation to account for the stochastic and unknown nature of customers' responses to dispatch signals and proposes a constrained Thompson sampling heuristic, Con-TS-RTP, that accounts for various possible aggregator objectives and takes into account the operational constraints of a distribution system to avoid potential grid failures as a result of uncertainty in the customers' response.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The paper proposes a constrained Thompson sampling heuristic, Con-TS-RTP, that accounts for various possible aggregator objectives and takes into account the operational constraints of a distribution system to avoid potential grid failures as a result of uncertainty in the customers' response, which is relevant to the epigraph formulation discussed in this manuscript. #REFR", "given_citation_intent": "background", "given_keywords": ["epigraph formulation"]}]}
{"citing_id": "2303.12501v1", "cited_id": "1503.02531", "section_title": "Introduction", "citation": "However, we found that the projection in CMPM can be regarded as a variable weight that adjusts the distribution of softmax output logits, similar to the temperature parameter #REFR for knowledge distillation.", "text_before_citation": ["Our main innovation is the design of a multimodal interaction encoder that can efficiently fuse visual and textual representations, align cross-modal fine-grained features through the MLM task.", "This design helps the backbone network to extract more discriminative global image-text representations without requiring additional supervision.", "To guide the image-text matching, commonly used loss functions include ranking loss and cross-modal projection matching (CMPM) #OTHEREFR loss.", "Compared to ranking loss, the CMPM loss does not require the selection of specific triplets or margin parameter tuning.", "It exhibits great stability with varying batch sizes, making it widely used in textto-image person retrieval #OTHEREFR ."], "text_after_citation": ["Nevertheless, limited by the varying projection length, CMPM therefore cannot precisely control the projection probability distribution, making it difficult to focus on hard-negative samples during model updates.", "To explore more effective cross-modal matching objective, we further propose an image-text similarity distribution matching (SDM) loss.", "The SDM loss minimizes the KL divergence between the normalized image-text similarity score distributions and the normalized ground truth label matching distributions.", "Additionally, we introduce a temperature hyperparameter to precisely control the similarity distribution compactness, which enables the model updates focus on hard-negative samples and effectively enlarges the variance between non-matching pairs and the correlation between matching pairs.", "To address the limitations of separate pre-trained models on unimodal datasets, we leverage the Contrastive Language-Image Pre-training (CLIP) #OTHEREFR as the initialization of our model."], "citing_paper_content": {"title": "Cross-Modal Implicit Relation Reasoning And Aligning For Text-To-Image Person Retrieval", "abstract": "Text-to-image person retrieval aims to identify the target person based on a given textual description query. The primary challenge is to learn the mapping of visual and textual modalities into a common latent space. Prior works have attempted to address this challenge by leveraging separately pre-trained unimodal models to extract visual and textual features. However, these approaches lack the necessary underlying alignment capabilities required to match multimodal data effectively. Besides, these works use prior information to explore explicit part alignments, which may lead to the distortion of intra-modality information. To alleviate these issues, we present IRRA: a cross-modal Implicit Relation Reasoning and Aligning framework that learns relations between local visual-textual tokens and enhances global image-text matching without requiring additional prior supervision. Specifically, we first design an Implicit Relation Reasoning module in a masked language modeling paradigm. This achieves cross-modal interaction by integrating the visual cues into the textual tokens with a cross-modal multimodal interaction encoder. Secondly, to globally align the visual and textual embeddings, Similarity Distribution Matching is proposed to minimize the KL divergence between image-text similarity distributions and the normalized label matching distributions. The proposed method achieves new state-of-the-art results on all three public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy compared to prior methods."}, "cited_paper_content": {"title": "Distilling The Knowledge In A Neural Network", "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."}, "keywords": ["softmax output logits", "knowledge distillation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To globally align the visual and textual embeddings, Similarity Distribution Matching is proposed to minimize the KL divergence between image-text similarity distributions and the normalized label matching distributions. The proposed method achieves new state-of-the-art results on all three public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy compared to prior methods (Caruana et al., 2015) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently fuse visual and textual representations and align cross-modal fine-grained features, the authors proposed a multimodal interaction encoder that integrates visual cues into textual tokens with a cross-modal interaction encoder, which was inspired by the work of Caruana and his collaborators on distilling the knowledge in an ensemble into a single model #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of machine learning algorithms, Caruana and his collaborators proposed a method to compress the knowledge in an ensemble into a single model, which is much easier to deploy, and this approach has been further developed using a different compression technique #REFR, which could be applied to the current multimodal interaction encoder to enhance the global image-text matching and reduce the computational cost of using multiple models with softmax output logits for knowledge distillation.", "given_citation_intent": "background", "given_keywords": ["softmax output logits", "knowledge distillation"]}]}
{"citing_id": "2304.13980v1", "cited_id": "2003.13867", "section_title": "Runtime Analysis", "citation": "Also 3D-MPA #REFR reaches a similar performance, confirming the difference between the indoor room and outdoor street scenarios.", "text_before_citation": ["The semantic segmentation results are almost the same (except for the weaker PointNet++ backbone).", "In terms of instance segmentation, PointGroup without the additional ScoreNet is significantly worse (>8 percent points difference).", "With ScoreNet, but without the specialisation to individual rooms, the performance is practically the same as for our pipeline.", "The table also shows the result given in the original PointGroup paper, and several other recent instance segmentation works.", "When tuned to indoor environments the full PointGroup does have the upper hand."], "text_after_citation": ["http://matterport.com/"], "citing_paper_content": {"title": "A Review Of Panoptic Segmentation For Mobile Mapping Point Clouds", "abstract": "3D point cloud panoptic segmentation is the combined task to (i) assign each point to a semantic class and (ii) separate the points in each class into object instances. Recently there has been an increased interest in such comprehensive 3D scene understanding, building on the rapid advances of semantic segmentation due to the advent of deep 3D neural networks. Yet, to date there is very little work about panoptic segmentation of outdoor mobile-mapping data, and no systematic comparisons. The present paper tries to close that gap. It reviews the building blocks needed to assemble a panoptic segmentation pipeline and the related literature. Moreover, a modular pipeline is set up to perform comprehensive, systematic experiments to assess the state of panoptic segmentation in the context of street mapping. As a byproduct, we also provide the first public dataset for that task, by extending the NPM3D dataset to include instance labels."}, "cited_paper_content": {"title": "3D-Mpa: Multi Proposal Aggregation For 3D Semantic Instance Segmentation", "abstract": "We present 3D-MPA, a method for instance segmentation on 3D point clouds. Given an input point cloud, we propose an object-centric approach where each point votes for its object center. We sample object proposals from the predicted object centers. Then, we learn proposal features from grouped point features that voted for the same object center. A graph convolutional network introduces inter-proposal relations, providing higher-level feature learning in addition to the lower-level point features. Each proposal comprises a semantic label, a set of associated points over which we define a foreground-background mask, an objectness score and aggregation features. Previous works usually perform non-maximum-suppression (NMS) over proposals to obtain the final object detections or semantic instances. However, NMS can discard potentially correct predictions. Instead, our approach keeps all proposals and groups them together based on the learned aggregation features. We show that grouping proposals improves over NMS and outperforms previous state-of-the-art methods on the tasks of 3D object detection and semantic instance segmentation on the ScanNetV2 benchmark and the S3DIS dataset."}, "keywords": ["3D-MPA"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In terms of instance segmentation, PointGroup without the additional ScoreNet is significantly worse (>8 percent points difference). With ScoreNet, but without the specialisation to individual rooms, the performance is practically the same as for our pipeline. The table also shows the result given in the original PointGroup paper, and several other recent instance segmentation works. When tuned to indoor environments the full PointGroup does have the upper hand. #REFR presents a method for instance segmentation on 3D point clouds that outperforms previous state-of-the-art methods on the tasks of 3D object detection and semantic instance segmentation on the ScanNetV2 benchmark and the S3DIS dataset.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our pipeline outperforms PointGroup without the additional ScoreNet by more than 8 percent points in terms of instance segmentation (#REFR), as shown in the table, which demonstrates the effectiveness of our proposed method.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our pipeline achieves state-of-the-art results on the ScanNetV2 benchmark and the S3DIS dataset for 3D object detection and semantic instance segmentation, outperforming previous works including PointGroup without the additional ScoreNet (#REFR).", "given_citation_intent": "result", "given_keywords": ["3D-MPA"]}]}
{"citing_id": "2304.06531v1", "cited_id": "1811.10943", "section_title": "Related Work", "citation": "Another way is to introduce the a-priors with the objective to preserve sharp features for surface reconstruction methods as done in DeepPrior #REFR .", "text_before_citation": ["Scanned 3D point sets are irregular and non-uniform, and need to be consolidated to enhance the surface reconstruction quality.", "One possible solution is to introduce edgeawareness in the consolidation of point sets in a data-driven manner.", "The EC-NET #OTHEREFR network processes patches of points and learns to consolidate points using an edge-aware joint loss function when learning from the data.", "The performance of the model was demonstrated on a very limited set of 12 manually labelled scans."], "text_after_citation": ["In both cases, the presence of high-frequency features and noise in the input scanned data, makes it extremely challenging to recover the sharpness of the scans.", "The inference of edges as parametric curves is an alternative that arises directly from CAD surface parametrization as boundary-representation (b-rep).", "Following this direction, the PC2WF model [12] infers a wireframe of linear edges from a point cloud based on a vertex localization and an edge detector that identifies the pairs of vertices connected with an edge.", "The work [14] proposes a parametric approach to extract a wireframe based on an estimated scalar distance field DEF #OTHEREFR that represents the proximity to the nearest sharp feature curve.", "PIE-Net #OTHEREFR proposes to jointly detect edge and corner points, after which a curve proposal module generates an over-complete collection of curves that are further ranked."], "citing_paper_content": {"title": "Sepicnet: Sharp Edges Recovery By Parametric Inference Of Curves In 3D Shapes", "abstract": "3D scanning as a technique to digitize objects in reality and create their 3D models, is used in many fields and areas. Though the quality of 3D scans depends on the technical characteristics of the 3D scanner, the common drawback is the smoothing of fine details, or the edges of an object. We introduce SepicNet, a novel deep network for the detection and parametrization of sharp edges in 3D shapes as primitive curves. To make the network end-to-end trainable, we formulate the curve fitting in a differentiable manner. We develop an adaptive point cloud sampling technique that captures the sharp features better than uniform sampling. The experiments were conducted on a newly introduced large-scale dataset of 50k 3D scans, where the sharp edge annotations were extracted from their parametric CAD models, and demonstrate significant improvement over state-of-the-art methods."}, "cited_paper_content": {"title": "Deep Geometric Prior For Surface Reconstruction", "abstract": "The reconstruction of a discrete surface from a point cloud is a fundamental geometry processing problem that has been studied for decades, with many methods developed. We propose the use of a deep neural network as a geometric prior for surface reconstruction. Specifically, we overfit a neural network representing a local chart parameterization to part of an input point cloud using the Wasserstein distance as a measure of approximation. By jointly fitting many such networks to overlapping parts of the point cloud, while enforcing a consistency condition, we compute a manifold atlas. By sampling this atlas, we can produce a dense reconstruction of the surface approximating the input cloud. The entire procedure does not require any training data or explicit regularization, yet, we show that it is able to perform remarkably well: not introducing typical overfitting artifacts, and approximating sharp features closely at the same time. We experimentally show that this geometric prior produces good results for both man-made objects containing sharp features and smoother organic objects, as well as noisy inputs. We compare our method with a number of well-known reconstruction methods on a standard surface reconstruction benchmark."}, "keywords": ["surface reconstruction methods"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "One possible solution to enhance the surface reconstruction quality of irregular and non-uniform 3D point sets is to introduce edge-awareness in the consolidation of point sets in a data-driven manner. The EC-NET #REFR network processes patches of points and learns to consolidate points using an edge-aware joint loss function when learning from the data. The performance of the model was demonstrated on a very limited set of 12 manually labelled scans.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To consolidate point sets in a data-driven manner, the proposed method in this manuscript introduces edge-awareness, which is similar to the approach proposed in the reference paper \"Deep Geometric Prior For Surface Reconstruction\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the quality of surface reconstruction methods, some recent studies have proposed the use of deep neural networks as geometric priors, such as the method proposed in \"Deep Geometric Prior For Surface Reconstruction\" #REFR, which overfits a neural network to part of an input point cloud using the Wasserstein distance as a measure of approximation.", "given_citation_intent": "method", "given_keywords": ["surface reconstruction methods"]}]}
{"citing_id": "2303.01480v1", "cited_id": "1709.01507", "section_title": "Parallel Pooling Mixer", "citation": "Inspired by this, we apply a Squeeze-and-Excitation (SE) module #REFR in the mixing part of PPX.", "text_before_citation": ["EQUATION", "EQUATION", "EQUATION", "EQUATION", "Previous cross-modal fusion methods show that channel information is crucial #OTHEREFR ."], "text_after_citation": ["This structure is crucial since some channels of certain modalities do capture more significant information than others.", "It can further engage more spatially-holistic knowledge in the channels of the cross-modal complements in SQ-Hub.", "Thus, the weighted feature f w is passed to a Feed-Forward Network (FFN) and a SE module #OTHEREFR for enhancing the channel information. The second part of PPX can be written as:", "EQUATION", "After the PPX block,f w is fused with RGB feature to form the final fused feature f l \u2208{f 1 ,f 2 ,f 3 ,f 4 } by using FRM&FFM modules #OTHEREFR , as shown in Fig. 4 ."], "citing_paper_content": {"title": "Delivering Arbitrary-Modal Semantic Segmentation", "abstract": "Multimodal fusion can make semantic segmentation more robust. However, fusing an arbitrary number of modalities remains underexplored. To delve into this problem, we create the DELIVER arbitrary-modal segmentation benchmark, covering Depth, LiDAR, multiple Views, Events, and RGB. Aside from this, we provide this dataset in four severe weather conditions as well as five sensor failure cases to exploit modal complementarity and resolve partial outages. To make this possible, we present the arbitrary cross-modal segmentation model CMNEXT. It encompasses a Self-Query Hub (SQ-Hub) designed to extract effective information from any modality for subsequent fusion with the RGB representation and adds only negligible amounts of parameters (\u223c0.01M) per additional modality. On top, to efficiently and flexibly harvest discriminative cues from the auxiliary modalities, we introduce the simple Parallel Pooling Mixer (PPX). With extensive experiments on a total of six benchmarks, our CMNEXT achieves state-of-the-art performance on the DELIVER, KITTI-360, MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets, allowing to scale from 1 to 81 modalities. On the freshly collected DELIVER, the quad-modal CMNEXT reaches up to 66.30% in mIoU with a +9.10% gain as compared to the mono-modal baseline. 1"}, "cited_paper_content": {"title": "Squeeze-And-Excitation Networks", "abstract": "Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, much existing work has shown the benefits of enhancing spatial encoding. In this work, we focus on channels and propose a novel architectural unit, which we term the\"Squeeze-and-Excitation\"(SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at slight computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a 25% relative improvement over the winning entry of 2016."}, "keywords": ["mixing part", "Squeeze-and-Excitation (SE) module"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Previous cross-modal fusion methods show that channel information is crucial (SENet). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Previous cross-modal fusion methods show that channel information is crucial for effective information extraction from any modality, which can be achieved by using the \"Squeeze-and-Excitation\" (SE) block proposed in the #REFR method.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Previous cross-modal fusion methods show that channel information is crucial for effective information extraction from any modality for subsequent fusion with the RGB representation, and the Squeeze-and-Excitation (SE) module proposed in #REFR can be a useful mixing part for this purpose.", "given_citation_intent": "method", "given_keywords": ["mixing part", "Squeeze-and-Excitation (SE) module"]}]}
{"citing_id": "2304.14831v1", "cited_id": "1810.04805", "section_title": "Experimental Setup", "citation": "Last, we aim to tune BERT #REFR and its variants on the STS-B task under EXPECTED.", "text_before_citation": ["We pre-train a binary classifier on the records with the country of \"U.S\" and take \"non-U.S\" records as unobserved target data.", "Amazon dataset is constructed from Amazon review data with two categories of products selected, i.e., \"Electronics\" and \"Watches\".", "In our experiments, the datarich category \"Electronics\" is used to pre-train a prediction model which maps user comments to the rating score ranging from one to five, and \"Watches\" is treated as the target data.", "The settings of Adult and Amazon follows the work #OTHEREFR .", "In terms of CIFAR-10-C/CIFAR-100-C, the initial provided model is built on clean images, and it is then tuned to fit the disjoint corrupted images following the unsupervised tuning research #OTHEREFR , which mimics the unexpected distribution shift in the real world."], "text_after_citation": ["Following the research #OTHEREFR , the models are firstly trained on the sentence pairs from the genre of MSRvid and then tuned to fit the unknown target data which are extracted from Images, where the evaluation metric is Pearson's correlation coefficient.", "On the task of corrupted image classification, we treat all the corrupted images (target data) as the tuning data for a fair comparison with the unsupervised tuning methods.", "Throughout all the remaining datasets, the target data are split into two sets.", "We do randomly equal splitting for Adult and Amazon and use the default splitting for STS-B.", "One is the support set that is used for evaluating the query efficiency of tuning algorithms, and the other is the holdout set on which the model generalization is assessed."], "citing_paper_content": {"title": "Earning Extra Performance From Restrictive Feedbacks", "abstract": "Many machine learning applications encounter a situation where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named Earning eXtra PerformancE from restriCTive feEDdbacks (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods where the target data is always ready for calculating model gradients, the model providers in EXPECTED only see some feedbacks which could be as simple as scalars, such as inference accuracy or usage rate. To enable tuning in this restrictive circumstance, we propose to characterize the geometry of the model performance with regard to model parameters through exploring the parameters' distribution. In particular, for the deep models whose parameters distribute across multiple layers, a more query-efficient algorithm is further tailor-designed that conducts layerwise tuning with more attention to those layers which pay off better. Our theoretical analyses justify the proposed algorithms from the aspects of both efficacy and efficiency. Extensive experiments on different applications demonstrate that our work forges a sound solution to the EXPECTED problem, which establishes the foundation for future studies towards this direction."}, "cited_paper_content": {"title": "Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. ::: BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."}, "keywords": ["variants", "BERT"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To pre-train the prediction model, we use the datarich category \"Electronics\" and follow the approach proposed in #REFR, which achieves state-of-the-art results on a wide range of natural language processing tasks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train deep bidirectional representations from unlabeled text, BERT, introduced in #REFR, jointly conditions on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train the prediction model, we use the datarich category \"Electronics\" and \"Watches\" as the target data, and fine-tune the model with the BERT algorithm proposed in #REFR, achieving state-of-the-art results on the Amazon dataset.", "given_citation_intent": "method", "given_keywords": ["variants", "BERT"]}]}
{"citing_id": "2305.01855v1", "cited_id": "1905.10887", "section_title": "Experiment 3: Synthetic Data Filtering", "citation": "A similar discrepancy between generated data quality and downstream task performance has been reported in a prior image classification task #REFR .", "text_before_citation": ["A significant boost still exists when the volume of true data gets larger, which surpasses the improvement by SD para .", "This indicates that a suitable data filtering can improve both training efficiency and image captioning performance when true labeled data is limited.", "However, the improvement for SD base and SD para are not significant.", "And the image captioning performance even decreases in some cases for the Transformer-based model (see Table 11 in Appendix).", "Therefore, CLIPScore and the other two criteria are not golden standards to select highquality data that are suitable for the captioning task, even though they have been testified to perform well in image-caption quality evaluation #OTHEREFR ."], "text_after_citation": ["Authors found that although the GAN-generated images receive high scores close to those of true images, the classification model trained on fully synthetic images has a much lower accuracy than those trained on true images.", "Following this thread, we calculate the three metrics for both COCO data and synthetic data.", "We have a similar finding that three quality measures under the synthetic data are close level to those under true data (see Table 5 ).", "However, when completely replacing the true data with the synthetic data as the training (e.g., in Experiment 1), the performance is quite lower than that of the true data (i.e., CIDEr score: 81.0 vs. 92.4).", "In this way, we extend findings in #OTHEREFR 's study to the image captioning task."], "citing_paper_content": {"title": "Multimodal Data Augmentation For Image Captioning Using Diffusion Models", "abstract": "Image captioning, an important visionlanguage task, often requires a tremendous number of finely labeled image-caption pairs for learning the underlying alignment between images and texts. In this paper, we proposed a multimodal data augmentation method, leveraging a recent text-to-image model called Stable Diffusion, to expand the training set via high-quality generation of image-caption pairs. Extensive experiments on the MS COCO dataset demonstrate the advantages of our approach over several benchmark methods, and particularly a significant boost when having fewer training instances. In addition, models trained on our augmented datasets also outperform prior unpaired image captioning methods by a large margin. Finally, further improvement regarding the training efficiency and effectiveness can be obtained after intentionally filtering the generated data based on quality assessment."}, "cited_paper_content": {"title": "Classification Accuracy Score For Conditional Generative Models", "abstract": "Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes\u2014variational autoencoders, autoregressive models, and generative adversarial networks (GANs)\u2014to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9% and 41.6%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric."}, "keywords": ["prior image classification"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed CLIPScore metric for selecting high-quality data suitable for the captioning task was found to be insufficient, as it was not predictive of image captioning performance and did not outperform the Classification Accuracy Score (CAS) proposed by #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The results of the image captioning task can be improved by filtering the generated data based on quality assessment, as shown in previous work on class-conditional generative models #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The results of the study show that traditional GAN metrics such as Inception Score (IS) and FID are neither predictive of CAS nor useful when evaluating non-GAN models, which is consistent with the findings in prior image classification research #REFR.", "given_citation_intent": "result", "given_keywords": ["prior image classification"]}]}
{"citing_id": "2304.03105v2", "cited_id": "1912.04838", "section_title": "Introduction", "citation": "LiDAR point clouds, which contain rich spatial information, offer superior performance on 3D detection benchmarks compared to camerabased methods #REFR Caesar et al, 2020) .", "text_before_citation": ["Although image backbones pretrained on Ima-geNet provide semantically rich features, they fail to capture critical structural information.", "The existing pre-training method #OTHEREFR introduces depth-relevant tasks, but such method excludes the view transformation module and benefits the backbone only.", "View transformation module is of great significance, since it constructs 3D information and encodes 3D prior assumptions.", "As the view transformation module is optimized only by the detection loss, the spatial information from the depth-pretrained backbone is not fully utilized.", "Ideally, the image backbone and the view transformation should incorporate aligned geometry knowledge to enhance their performance."], "text_after_citation": ["Therefore, integrating LiDAR information into camera-based models is a natural step.", "Recent works #OTHEREFR have explored LiDAR information in two paradigms.", "The first paradigm #OTHEREFR projects LiDAR points onto the perspective view to provide additional depth supervision.", "However, the task of depth estimation in 2D space is inherently ill-posed and challenging.", "The second paradigm #OTHEREFR ) leverages a teacher-student knowledge distillation fashion."], "citing_paper_content": {"title": "Geometric-Aware Pretraining For Vision-Centric 3D Object Detection", "abstract": "Multi-camera 3D object detection for autonomous driving is a challenging problem that has garnered notable attention from both academia and industry. An obstacle encountered in vision-based techniques involves the precise extraction of geometry-conscious features from RGB images. Recent approaches have utilized geometric-aware image backbones pretrained on depth-relevant tasks to acquire spatial information. However, these approaches overlook the critical aspect of view transformation, resulting in inadequate performance due to the misalignment of spatial knowledge between the image backbone and view transformation. To address this issue, we propose a novel geometricaware pretraining framework called GAPretrain. Our approach incorporates spatial and structural cues to camera networks by employing the geometric-rich modality as guidance during the pretraining phase. The transference of modal-specific attributes across different modalities is non-trivial, but we bridge this gap by using a unified bird's-eye-view (BEV) representation and structural hints derived from LiDAR point clouds to facilitate the pretraining process. GAPretrain serves as a plug-and-play solution that can be flexibly applied to multiple state-of-the-art detectors. Our experiments demonstrate the effectiveness and generalization ability of the proposed method. We achieve 46.2 mAP and 55.5 NDS on the nuScenes val set using the BEVFormer method, with a gain of 2.7 and 2.1 points, respectively. We also conduct experiments on various image backbones and view transformations to validate the efficacy of our approach."}, "cited_paper_content": {"title": "Scalability In Perception For Autonomous Driving: Waymo Open Dataset", "abstract": "The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open."}, "keywords": ["3D detection benchmarks"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address this issue, GAPretrain proposes a novel geometric-aware pretraining framework that incorporates spatial and structural cues to camera networks by employing the geometric-rich modality as guidance during the pretraining phase #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of inadequate performance due to the misalignment of spatial knowledge between the image backbone and view transformation, recent approaches have utilized geometric-aware image backbones pretrained on depth-relevant tasks, as well as incorporating spatial and structural cues to camera networks, as proposed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the effectiveness of our approach, we conducted experiments on various image backbones and view transformations, achieving state-of-the-art performance on 3D detection benchmarks #REFR.", "given_citation_intent": "background", "given_keywords": ["3D detection benchmarks"]}]}
{"citing_id": "2303.05080v1", "cited_id": "1302.4099", "section_title": "Big Data As A Tool For Authors", "citation": "For example, it is known that given a book the number of nodes and the average shortest path length of the book's word network is correlated with the style of certain literary eras #REFR .", "text_before_citation": ["These can be found in many writing book and generally come by way of applied psychology and accumulated wisdom of previous writers #OTHEREFR ]. Yet more books are being published than ever before.", "We have data, and by careful analysis of this data we can create new tools for fiction writers so that fiction is no longer only an art, but also a science.", "We've considered reader-communities and enjoyment-communities as ways to craft books readers will both choose to read and enjoy.", "We've also shown the potential of our maturity-realism plane to coarsely identify books one might enjoy, and more generally the potential of PCA analysis to find untapped combinations of genres.", "One can easily imagine an array of other data inspired approaches. Natural language processing represents a major area here."], "text_after_citation": ["If an author wishes to mimic the style of an era but their prose doesn't feel quite right they could look at the word network their book generates.", "Perhaps they find that the size of their word network is too small, meaning they are using too few unique words. This can help \"debug\" prose.", "Adapting this technique could help character sound distinct.", "A common problem is that different character's dialogue sound the same simply because the same author wrote it.", "If one can extract the each of the main character's dialogue (or the prose of their point of view scenes) then one can run those texts through text-author identification or natural language processing methods."], "citing_paper_content": {"title": "Revisiting The Relevance Of Traditional Genres: A Network Analysis Of Fiction Readers' Preferences", "abstract": "We investigate how well traditional fiction genres like Fantasy, Thriller, and Literature represent readers' preferences. Using user data from Goodreads we construct a book network where two books are strongly linked if the same people tend to read or enjoy them both. We then partition this network into communities of similar books and assign each a list of subjects from The Open Library to serve as a proxy for traditional genres. Our analysis reveals that the network communities correspond to existing combinations of traditional genres, but that the exact communities differ depending on whether we consider books that people read or books that people enjoy. In addition, we apply principal component analysis to the data and find that the variance in the book communities is best explained by two factors: the maturity/childishness and realism/fantastical nature of the books. We propose using this maturity-realism plane as a coarse classification tool for stories."}, "cited_paper_content": {"title": "Identification Of Literary Movements Using Complex Networks To Represent Texts", "abstract": "The use of statistical methods to analyze large databases of text has been useful in unveiling patterns of human behavior and establishing historical links between cultures and languages. In this study, we identified literary movements by treating books published from 1590 to 1922 as complex networks, whose metrics were analyzed with multivariate techniques to generate six clusters of books. The latter correspond to time periods coinciding with relevant literary movements over the last five centuries. The most important factor contributing to the distinctions between different literary styles was the average shortest path length, in particular the asymmetry of its distribution. Furthermore, over time there has emerged a trend toward larger average shortest path lengths, which is correlated with increased syntactic complexity, and a more uniform use of the words reflected in a smaller power-law coefficient for the distribution of word frequency. Changes in literary style were also found to be driven by opposition to earlier writing styles, as revealed by the analysis performed with geometrical concepts. The approaches adopted here are generic and may be extended to analyze a number of features of languages and cultures."}, "keywords": ["book's word network"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We propose using data analysis to create new tools for fiction writers, and our approach is supported by previous studies that have used statistical methods to analyze large databases of text #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of statistical methods to analyze large databases of text has been useful in unveiling patterns of human behavior and establishing historical links between cultures and languages, as demonstrated by the identification of literary movements using complex networks to represent texts #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The analysis of the book's word network as a complex network, as done in the reference paper #REFR, has proven to be a useful tool for identifying literary movements and patterns of human behavior in literature.", "given_citation_intent": "background", "given_keywords": ["book's word network"]}]}
{"citing_id": "2305.02997v1", "cited_id": "1603.02754", "section_title": "Related Work", "citation": "XGBoost (eXtreme Gradient Boosting) #REFR is powered by a novel sparsity-aware algorithm and weighted quantile sketch. This allows it to scale to large datasets.", "text_before_citation": ["Gradient-boosted decision trees.", "GBDTs have been a powerful technique to model tabular data, ever since their creation in 2001 #OTHEREFR .", "GBDTs work by building an ensemble of decision trees, incrementally updated using gradient descent.", "Due to their strong performance, many highperforming instantiations have been proposed."], "text_after_citation": ["LightGBM (Light Gradient Boosting Machine) #OTHEREFR is designed to be a lightweight gradientboosted tree implementation.", "CatBoost (Categorical Boosting) #OTHEREFR combines a permutation-driven variant of boosting with a novel technique for processing categorical features.", "Neural networks for tabular data.", "In their survey on deep learning for tabular data, Borisov et al. described three types of tabular data #OTHEREFR .", "Data transformation methods #OTHEREFR seek to encode the data into a format that is better-suited for neural nets."], "citing_paper_content": {"title": "When Do Neural Nets Outperform Boosted Trees On Tabular Data?", "abstract": "Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed feature distributions, heavy-tailed feature distributions, and other forms of dataset irregularities. Our insights act as a guide for practitioners to decide whether or not they need to run a neural net to reach top performance on their dataset. Our codebase and all raw results are available at https://github.com/naszilla/tabzilla."}, "cited_paper_content": {"title": "Xgboost: A Scalable Tree Boosting System", "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."}, "keywords": ["(eXtreme Gradient Boosting", "XGBoost"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Gradient-boosted decision trees. GBDTs have been a powerful technique to model tabular data, ever since their creation in 2001 (OTHEREFR). GBDTs work by building an ensemble of decision trees, incrementally updated using gradient descent. Due to their strong performance, many highperforming instantiations have been proposed. Recently, XGBoost has emerged as a scalable end-to-end tree boosting system that achieves state-of-the-art results on many machine learning challenges, including tabular data analysis (REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "One popular tree boosting system that has been widely used by data scientists to achieve state-of-the-art results on many machine learning challenges is XGBoost, which proposes a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning, and provides insights on cache access patterns, data compression, and sharding to build a scalable tree boosting system #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "One popular tree boosting system is eXtreme Gradient Boosting (XGBoost), which has been widely used by data scientists to achieve state-of-the-art results on many machine learning challenges #REFR.", "given_citation_intent": "method", "given_keywords": ["(eXtreme Gradient Boosting", "XGBoost"]}]}
{"citing_id": "2303.01052v1", "cited_id": "1608.04644", "section_title": "Validating Hypothesis Model And Test Function", "citation": "Intriguingly, we notice that both results from the hypothesis model generally show constant robustness even in a high-confidence adversarial attack #REFR fabricating unseen perturbation.", "text_before_citation": ["Here, we observe that the adversarial robustness of CF is inferior to that of CC, AC, and even Adv.", "Intuitively, it is an obvious result since the test function violating Eq.", "(7) forces feature representation to be the worst possible condition of extremely deviating from correct prediction.", "For the prediction results for CC and AC, they show impressive robustness performance than Adv with large margins.", "Since AC directly leverages the feature variation acquired from adversarial perturbation, they present better adversarial robustness than CC obtained from the test function outputting the worst-case counterfactuals on the feature variation."], "text_after_citation": ["Such robustness demonstrates the estimated causal features have ability to overcome various types of adversarial perturbation."], "citing_paper_content": {"title": "Demystifying Causal Features On Adversarial Examples And Causal Inoculation For Robust Network By Adversarial Instrumental Variable Regression", "abstract": "The origin of adversarial examples is still inexplicable in research fields, and it arouses arguments from various viewpoints, albeit comprehensive investigations. In this paper, we propose a way of delving into the unexpected vulnerability in adversarially trained networks from a causal perspective, namely adversarial instrumental variable (IV) regression. By deploying it, we estimate the causal relation of adversarial prediction under an unbiased environment dissociated from unknown confounders. Our approach aims to demystify inherent causal features on adversarial examples by leveraging a zero-sum optimization game between a casual feature estimator (i.e., hypothesis model) and worstcase counterfactuals (i.e., test function) disturbing to find causal features. Through extensive analyses, we demonstrate that the estimated causal features are highly related to the correct prediction for adversarial robustness, and the counterfactuals exhibit extreme features significantly deviating from the correct prediction. In addition, we present how to effectively inoculate CAusal FEatures (CAFE) into defense networks for improving adversarial robustness."}, "cited_paper_content": {"title": "Towards Evaluating The Robustness Of Neural Networks", "abstract": "Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input $x$ and any target classification $t$, it is possible to find a new input $x'$ that is similar to $x$ but classified as $t$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$. ::: In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with $100\\%$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples."}, "keywords": ["constant robustness"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the prediction results for CC and AC, they show impressive robustness performance than Adv with large margins, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The previous adversarial example generation algorithms are often much less effective than the three new attack algorithms proposed in #REFR, which are successful on both distilled and undistilled neural networks with 100% probability.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this paper demonstrate that defensive distillation does not significantly increase the constant robustness of neural networks, as their attacks are successful on both distilled and undistilled neural networks with 100% probability. #REFR", "given_citation_intent": "result", "given_keywords": ["constant robustness"]}]}
{"citing_id": "2303.12484v1", "cited_id": "1908.10555", "section_title": "Local Detection", "citation": "Finally, to address the problem that only image-level labels are provided in MIL, Xu et al. #REFR design an automatic instance-level label generation method.", "text_before_citation": ["Thus, they propose a MIL structure involving maximum likelihood estimation to predict multiple labels, i.e., bag-level labels and diagnostic scores; instance-level labels and informativeness, simultaneously.", "Similarly, when studying the classification of the retinal nerve fiber layer (RNFL), Manivannan et al.", "#OTHEREFR have observed that regions that contain the RNFL generally have strong intra-class variation, making them difficult to distinguish from other regions.", "Therefore, they map the instances into a discriminative subspace to increase the discrepancy for disentangled instance feature learning. Jia et al.", "#OTHEREFR incorporate the multi-scale image feature into the learning process to obtain more latent information on histopathology images."], "text_after_citation": ["Their work has led to an interesting MIL algorithm design direction and may shed light on how to improve the performance of local detection algorithms.", "Other studies on problems such as phenotype categorization #OTHEREFR , #OTHEREFR , #OTHEREFR and multi-label classification #OTHEREFR have also made promising progress with the MIL algorithm."], "citing_paper_content": {"title": "Label-Efficient Deep Learning In Medical Image Analysis: Challenges And Future Directions", "abstract": "Deep learning has seen rapid growth in recent years and achieved state-of-the-art performance in a wide range of applications. However, training models typically requires expensive and time-consuming collection of large quantities of labeled data. This is particularly true within the scope of medical imaging analysis (MIA), where data are limited and labels are expensive to be acquired. Thus, label-efficient deep learning methods are developed to make comprehensive use of the labeled data as well as the abundance of unlabeled and weak-labeled data. In this survey, we extensively investigated over 300 recent papers to provide a comprehensive overview of recent progress on label-efficient learning strategies in MIA. We first present the background of label-efficient learning and categorize the approaches into different schemes. Next, we examine the current state-of-the-art methods in detail through each scheme. Specifically, we provide an in-depth investigation, covering not only canonical semi-supervised, self-supervised, and multi-instance learning schemes, but also recently emerged active and annotation-efficient learning strategies. Moreover, as a comprehensive contribution to the field, this survey not only elucidates the commonalities and unique features of the surveyed methods but also presents a detailed analysis of the current challenges in the field and suggests potential avenues for future research."}, "cited_paper_content": {"title": "Camel: A Weakly Supervised Learning Framework For Histopathology Image Segmentation", "abstract": "Histopathology image analysis plays a critical role in cancer diagnosis and treatment. To automatically segment the cancerous regions, fully supervised segmentation algorithms require labor-intensive and time-consuming labeling at the pixel level. In this research, we propose CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels. Using multiple instance learning (MIL)-based label enrichment, CAMEL splits the image into latticed instances and automatically generates instance-level labels. After label enrichment, the instance-level labels are further assigned to the corresponding pixels, producing the approximate pixel-level labels and making fully supervised training of segmentation models possible. CAMEL achieves comparable performance with the fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset. Moreover, the generality of the automatic labeling methodology may benefit future weakly supervised learning studies for histopathology image analysis."}, "keywords": ["image-level labels"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Jia et al. #REFR propose CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels, which achieves comparable performance with the fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Jia et al. #REFR propose a multi-scale image feature-based learning process to obtain more latent information on histopathology images, which is similar to the approach used in CAMEL for weakly supervised histopathology image segmentation.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of labor-intensive and time-consuming labeling at the pixel level, the authors propose CAMEL, a weakly supervised learning framework for histopathology image segmentation that uses only #KEYWORD(image-level labels) to achieve comparable performance with fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset #REFR.", "given_citation_intent": "method", "given_keywords": ["image-level labels"]}]}
{"citing_id": "2303.07914v1", "cited_id": "1706.03762", "section_title": "Model Architecture", "citation": "Semantic encoder: The semantic encoder is composed of L e Transformer #REFR encoder layers, which aims to further encode the semantic information of speech representations.", "text_before_citation": ["To learn the correct acoustic boundaries, we use the source or target text length J as the supervised signal.", "L CIF = J \u2212 T t=1 \u03b1 t 2 (5)", "There are two benefits of using CIF as a boundary detector.", "For offline ST model, it can address the length gap between speech and text.", "It can also dynamically detect the acoustic boundaries of streaming audio to perform read/write policies for streaming inference."], "text_after_citation": ["Translation decoder: The translation decoder is composed of L e Transformer decoder layers, which generates the translations in an autoregressive way. The translation loss is defined as:", "L ST (x, y) = \u2212 J j=1 log p (y j | y <j , x) (6)"], "citing_paper_content": {"title": "Adapting Offline Speech Translation Models For Streaming With Future-Aware Distillation And Inference", "abstract": "A popular approach to streaming speech translation is to employ a single offline model with a wait-k policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those extracted from a complete utterance. To address this issue, we propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input. FAST includes a Future-Aware Inference (FAI) strategy that incorporates future context through a trainable masked embedding, and a Future-Aware Distillation (FAD) framework that transfers future context from an approximation of full speech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs between translation quality and latency than strong baselines. Extensive analyses suggest that our methods effectively alleviate the aforementioned mismatch problem between offline training and online inference. * Equal contribution. \u2020 Corresponding author. (a) Training: Full speech encoding (b) Testing: Streaming encoding"}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["speech representations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the length gap between speech and text and dynamically detect the acoustic boundaries of streaming audio, the authors propose using CIF as a boundary detector (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Transformer architecture based solely on attention mechanisms has shown superior performance in machine translation tasks, achieving a BLEU score of 28.4 on the WMT 2014 English-to-German translation task and establishing a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task, as well as generalizing well to other tasks, according to the authors of the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of speech representations extracted from streaming input is significantly different from those extracted from a complete utterance, which can cause a mismatch problem between offline training and online inference. To address this issue, the authors propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input by incorporating future context through a trainable masked embedding and transferring future context from an approximation of full speech to streaming input using a Future-Aware Distillation (FAD) framework (#REFR).", "given_citation_intent": "background", "given_keywords": ["speech representations"]}]}
{"citing_id": "2303.03825v1", "cited_id": "1802.08705", "section_title": "I. Introduction", "citation": "To validate the effectiveness of our approach, we compared it with our baselines and PDDLStream #REFR .", "text_before_citation": ["sampling a random node and extending the tree toward it, to TAMP problems.", "To address this issue, we propose a hierarchical strategy that first samples an abstract action sequence by employing Monte Carlo tree search (MCTS) in conjunction with a symbolic planner (which can be of any off-the-shelf planner), and subsequently samples the corresponding geometric state variables. Moreover, we introduce two methods to improve planning speed.", "First, the reward of MCTS is generated by evaluating the result of TP (i.e. action sequence) using MP.", "Second, when performing MP, a goal state candidate is generated in advance based on the given action sequence.", "We found that prechecking the feasibility of the goal candidate can significantly improve the search speed, particularly in problems with tight goal constraints, such as the kitchen domain."], "text_after_citation": [], "citing_paper_content": {"title": "A Reachability Tree-Based Algorithm For Robot Task And Motion Planning", "abstract": "This paper presents a novel algorithm for robot task and motion planning (TAMP) problems by utilizing a reachability tree. While tree-based algorithms are known for their speed and simplicity in motion planning (MP), they are not well-suited for TAMP problems that involve both abstracted and geometrical state variables. To address this challenge, we propose a hierarchical sampling strategy, which first generates an abstracted task plan using Monte Carlo tree search (MCTS) and then fills in the details with a geometrically feasible motion trajectory. Moreover, we show that the performance of the proposed method can be significantly enhanced by selecting an appropriate reward for MCTS and by using a pre-generated goal state that is guaranteed to be geometrically feasible. A comparative study using TAMP benchmark problems demonstrates the effectiveness of the proposed approach."}, "cited_paper_content": {"title": "Pddlstream: Integrating Symbolic Planners And Blackbox Samplers Via Optimistic Adaptive Planning", "abstract": "Many planning applications involve complex relationships defined on high-dimensional, continuous variables. For example, robotic manipulation requires planning with kinematic, collision, visibility, and motion constraints involving robot configurations, object poses, and robot trajectories. These constraints typically require specialized procedures to sample satisfying values. We extend PDDL to support a generic, declarative specification for these procedures that treats their implementation as black boxes. We provide domain-independent algorithms that reduce PDDLStream problems to a sequence of finite PDDL problems. We also introduce an algorithm that dynamically balances exploring new candidate plans and exploiting existing ones. This enables the algorithm to greedily search the space of parameter bindings to more quickly solve tightly-constrained problems as well as locally optimize to produce low-cost solutions. We evaluate our algorithms on three simulated robotic planning domains as well as several real-world robotic tasks."}, "keywords": ["approach", "PDDLStream"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of planning problems that involve complex relationships defined on high-dimensional, continuous variables, the authors propose a hierarchical strategy that first samples an abstract action sequence using Monte Carlo tree search (MCTS) in conjunction with a symbolic planner, and subsequently samples the corresponding geometric state variables. This approach is similar to the one proposed in the reference paper #REFR, which extends PDDL to support a generic, declarative specification for specialized procedures that sample satisfying values.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of TAMP problems that involve complex relationships defined on high-dimensional, continuous variables, the authors propose a hierarchical sampling strategy that first generates an abstracted task plan using Monte Carlo tree search (MCTS) and then fills in the details with a geometrically feasible motion trajectory (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of incorporating abstracted and geometrical state variables in TAMP problems, the authors propose a hierarchical sampling strategy that first generates an abstracted task plan using Monte Carlo tree search (MCTS) and a symbolic planner, and then fills in the details with a geometrically feasible motion trajectory, which is a similar approach to the one proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["approach", "PDDLStream"]}]}
{"citing_id": "2303.01849v1", "cited_id": "1904.02882", "section_title": "Experiment Settings", "citation": "We train the source model on two clean subsets trainclean-100 and train-clean-360 of LibriTTS dataset #REFR , a multi-speaker TTS corpus, totaling 1151 speakers and 245 hours of speech.", "text_before_citation": ["For the architecture behind the diffusion decoder, we implemented that of AdaSpeech #OTHEREFR , including the phoneme encoder, the acoustic condition modeling module, and the variance adapter.", "Our implementation is based on the open-source software 1 2 of the two models.", "We use the diffusion decoder with 12 convolution blocks, 128 residual channels and 3.5M parameters proposed in #OTHEREFR .", "Other model configurations follow corresponding parts of AdaSpeech and PriorGrad unless otherwise stated.", "The total number parameters to be finetuned for the diffusion decoder is 0.131M, compared to 1.184M for AdaSpeech. Data."], "text_after_citation": ["For evaluation, we select 11 speakers (7 females and 4 males) with different accents from VCTK #OTHEREFR following the practice in #OTHEREFR .", "For each speaker, 10 utterances with the same transcripts across all speakers are randomly selected as the test set.", "The preprocessing of speech and text data follows AdaSpeech except using a sampling rate of 22,050 Hz. Training, adaptation and inference.", "Following AdaSpeech, the training process comprises two stages in which the numbers of steps are 200K and 100K, respectively.", "The models are trained on one NVIDIA RTX 3090 GPU using a batch size of 50,000 speech frames."], "citing_paper_content": {"title": "An Investigation Into The Adaptability Of A Diffusion-Based Tts Model", "abstract": "Given the recent success of diffusion in producing naturalsounding synthetic speech, we investigate how diffusion can be used in speaker adaptive TTS. Taking cues from more traditional adaptation approaches, we show that adaptation can be included in a diffusion pipeline using conditional layer normalization with a step embedding. However, we show experimentally that, whilst the approach has merit, such adaptation alone cannot approach the performance of Transformer-based techniques. In a second experiment, we show that diffusion can be optimally combined with Transformer, with the latter taking the bulk of the adaptation load and the former contributing to improved naturalness."}, "cited_paper_content": {"title": "Libritts: A Corpus Derived From Librispeech For Text-To-Speech", "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from this http URL."}, "keywords": ["multi-speaker TTS corpus"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For the architecture behind the diffusion decoder, we implemented that of AdaSpeech #OTHEREFR , including the phoneme encoder, the acoustic condition modeling module, and the variance adapter. Our implementation is based on the open-source software 1 2 of the two models. We use the diffusion decoder with 12 convolution blocks, 128 residual channels and 3.5M parameters proposed in #REFR. Other model configurations follow corresponding parts of AdaSpeech and PriorGrad unless otherwise stated. The total number parameters to be finetuned for the diffusion decoder is 0.131M, compared to 1.184M for AdaSpeech. Data.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement speaker adaptive TTS, the authors used conditional layer normalization with a step embedding, as described in #REFR, which allowed for adaptation in a diffusion pipeline.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train our multi-speaker TTS model, we utilized the LibriTTS corpus, a high-quality dataset specifically designed for text-to-speech applications #REFR.", "given_citation_intent": "method", "given_keywords": ["multi-speaker TTS corpus"]}]}
{"citing_id": "2303.09799v2", "cited_id": "1406.1078", "section_title": "Audio Encoding", "citation": "Given historical data, the APC model uses the log Mel spectrograms feature as input and predicts future surface properties. The model is a 3-layer GRU #REFR .", "text_before_citation": ["The input audio provides critical input information to our system.", "Following #OTHEREFR , we use auto-regressive predictive coding (APC) #OTHEREFR to extract structural audio stream representations."], "text_after_citation": ["EQUATION", "where h u \u2208 R 512 represents the posterior probability of every layer in GRUs.", "Our targeted audio stream representations are latent states in the final GRU unit.", "During the training process, we add a linear layer that maps the output to predict the next log Mel spectrogram.", "The representation of the final GRU unit h 3 is projected to a manifold as in #OTHEREFR to extract the audio stream feature h \u2208 R 512 ."], "citing_paper_content": {"title": "Style Transfer For 2D Talking Head Animation", "abstract": "Audio-driven talking head animation is a challenging research topic with many real-world applications. Recent works have focused on creating photo-realistic 2D animation, while learning different talking or singing styles remains an open problem. In this paper, we present a new method to generate talking head animation with learnable style references. Given a set of style reference frames, our framework can reconstruct 2D talking head animation based on a single input image and an audio stream. Our method first produces facial landmarks motion from the audio stream and constructs the intermediate style patterns from the style reference images. We then feed both outputs into a style-aware image generator to generate the photo-realistic and fidelity 2D animation. In practice, our framework can extract the style information of a specific character and transfer it to any new static image for talking head animation. The intensive experimental results show that our method achieves better results than recent state-of-the-art approaches qualitatively and quantitatively. Our source code can be found at: https://github.com/aioz-ai/AudioDrivenStyleTransfer."}, "cited_paper_content": {"title": "Learning Phrase Representations Using Rnn Encoder-Decoder For Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."}, "keywords": ["model", "log"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The input audio provides critical input information to our system. Following the auto-regressive predictive coding (APC) #REFR method proposed in Learning Phrase Representations Using Rnn Encoder-Decoder For Statistical Machine Translation, we use APC to extract structural audio stream representations.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Following the auto-regressive predictive coding (APC) method #REFR, we extract structural audio stream representations, which provide critical input information to our system.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To extract structural audio stream representations, we use auto-regressive predictive coding (APC) following the method proposed in \"Learning Phrase Representations Using RNN Encoder-Decoder For Statistical Machine Translation\" #REFR, which utilizes a novel neural network model consisting of two recurrent neural networks (RNN) jointly trained to maximize the conditional probability of a target sequence given a source sequence and improves the performance of a statistical machine translation system by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model.", "given_citation_intent": "method", "given_keywords": ["model", "log"]}]}
{"citing_id": "2303.04011v1", "cited_id": "1807.06757", "section_title": "A. Implementation Details", "citation": "We assess navigation performance by measuring Success Rate (SR) and Success weighted by Path-Length (SPL) #REFR .", "text_before_citation": ["c) Evaluation: To assess the navigation performance, we rank the difficulty of each trajectory as proposed by #OTHEREFR .", "Trajectories are categorized into easy (1.5 \u2212 3m), medium (3 \u2212 5m), and hard (5 \u2212 10m) based on their geodesic distance to the goal.", "We add an extra category labeled as \"very hard\" (>10m) to evaluate the agent's ability to plan for long-horizon goals.", "Each difficulty level is assessed over 1,000 trajectories using a maximum episode length of 500.", "To ensure a comprehensive assessment, we sample distinct starting positions and goals for each environment and difficulty, resulting in a total of 4,000 trajectories per environment."], "text_after_citation": ["A navigation trial is considered successful if the agent comes to a STOP within a maximum distance of 1m from the goal.", "We also use a Soft Success Rate (SSR), where a trial is successful if the agent is less than 1m away from the goal at any point during navigation.", "Lastly, we monitor the ratio of Collision-Free Trajectories (CFT) across all difficulties.", "A trajectory is deemed collision-free if it does not collide during the experiment.", "d) Results: We present quantitative navigation results in Table I ."], "citing_paper_content": {"title": "One-4-All: Neural Potential Fields For Embodied Navigation", "abstract": "A fundamental task in robotics is to navigate between two locations. In particular, real-world navigation can require long-horizon planning using high-dimensional RGB images, which poses a substantial challenge for end-to-end learning-based approaches. Current semi-parametric methods instead achieve long-horizon navigation by combining learned modules with a topological memory of the environment, often represented as a graph over previously collected images. However, using these graphs in practice typically involves tuning a number of pruning heuristics to avoid spurious edges, limit runtime memory usage and allow reasonably fast graph queries. In this work, we present One-4-All (O4A), a method leveraging self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline in which the goal is specified as an image. Navigation is achieved by greedily minimizing a potential function defined continuously over the O4A latent space. Our system is trained offline on non-expert exploration sequences of RGB data and controls, and does not require any depth or pose measurements. We show that O4A can reach long-range goals in 8 simulated Gibson indoor environments, and further demonstrate successful real-world navigation using a Jackal UGV platform. a * Equal contribution. Author ordering determined by competitive duckcalling, where the winner was selected by a blind jury on their ability to recreate various duck calls."}, "cited_paper_content": {"title": "On Evaluation Of Embodied Navigation Agents", "abstract": "Skillful mobile operation in three-dimensional environments is a primary topic of study in Artificial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task definitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking."}, "keywords": ["navigation performance"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To ensure a comprehensive assessment, we followed the trajectory difficulty ranking proposed by #OTHEREFR and evaluated our agent's performance on 4,000 trajectories per environment, categorized into easy, medium, hard, and very hard based on their geodesic distance to the goal.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure a comprehensive assessment of the agent's ability to plan for long-horizon goals, the authors leveraged self-supervised and manifold learning to obtain a graph-free, end-to-end navigation pipeline as proposed by the working group in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To assess the agent's navigation performance, the authors categorized trajectories into different difficulty levels based on their geodesic distance to the goal and evaluated them using a maximum episode length of 500, as proposed by the consensus recommendations of a working group studying empirical methodology in navigation research #REFR.", "given_citation_intent": "method", "given_keywords": ["navigation performance"]}]}
{"citing_id": "2304.14418v1", "cited_id": "1504.06852", "section_title": "Introduction", "citation": "FlowNet #REFR introduced a learning based two-frame optical flow estimation method using CNNs wherein a single flow field is estimated from a pair of image frames.", "text_before_citation": ["Horn and Schunck #OTHEREFR was a pioneering algorithm that introduced a variational energy minimization approach to estimate optical flow using a brightness consistency assumption (pixel intensity during pixel coordinate transformation assumed to be constant for a smaller temporal change) and a spatial smoothness constraint for the optical flow field as part of the objective function.", "Other classical multi-frame methods use a bank of hand-crafted motion filters tuned to capture moving patterns and texture characteristics from each image sequence to estimate the direction and magnitude of optical flow fields #OTHEREFR .", "With the recent advancement of deep learning and availability of large datasets with known ground truth, optical flow estimation is reformulated as an end-to-end learning problem without requiring assumptions regarding the characteristics of images and of motion patterns.", "Motivated by Heeger's #OTHEREFR approach to use hand crafted spatiotemporal Gabor filters, Teney et al.", "#OTHEREFR developed a learning-based multi-frame optical flow estimation model based on learnable 3D convolutional neural networks (CNN) and signal processing concepts."], "text_after_citation": ["Several other learning based two-frame methods improved upon the FlowNet model, namely the coarse-to-fine pyramid network #OTHEREFR , multiple intermediate flow estimates and warping based brightness error computation #OTHEREFR , receptive field guided motion feature extraction networks #OTHEREFR and the 4D all-pairs correlation volume with gated recurrent network #OTHEREFR .", "In dynamic scenes, limited temporal information available from only two frames causes the two-frame methods to have poor generalizations in and near occluded regions as well as in out-of-boundary regions.", "This is likely because in these regions the flow dynamics are often complex and cannot be fully captured using only two frames.", "In regions with disappearing scene elements, learning based two-frame methods do not have access to context Figure 1 : Examples of optical flow estimates (OFEs) using our methods compared with recent state-of-the-art two-frame and multi-frame methods.", "The regions bounded by red boxes in the input frames represent the regions where our method significantly outperformed the other methods."], "citing_paper_content": {"title": "Sstm: Spatiotemporal Recurrent Transformers For Multi-Frame Optical Flow Estimation", "abstract": "Inaccurate optical flow estimates in and near occluded regions, and out-ofboundary regions are two of the current significant limitations of optical flow estimation algorithms. Recent state-of-the-art optical flow estimation algorithms are two-frame based methods where optical flow is estimated sequentially for each consecutive image pair in a sequence. While this approach gives good flow estimates, it fails to generalize optical flows in occluded regions mainly due to limited local evidence regarding moving elements in a scene. In this work, we propose a learning-based multi-frame optical flow estimation method that estimates two or more consecutive optical flows in parallel from multi-frame image sequences. Our underlying hypothesis is that by understanding temporal scene dynamics from longer sequences with more than two frames, we can characterize pixel-wise dependencies in a larger spatiotemporal domain, generalize complex motion patterns and thereby improve the accuracy of optical flow estimates in occluded regions. We present learning-based spatiotemporal recurrent transformers for multi-frame based optical flow estimation (SSTMs). Our method utilizes 3D Convolutional Gated Recurrent Units (3D-ConvGRUs) and spatiotemporal transformers to learn recurrent space-time motion dynamics and global dependencies in the scene and provide a generalized optical flow estimation. When compared with recent state-of-the-art two-frame and multi-frame methods on real world and synthetic datasets, performance of the SSTMs were significantly higher in occluded and out-of-boundary regions. Among all published state-of-the-art multi-frame methods, SSTM achieved state-of the-art results on the Sintel Final and KITTI2015 benchmark datasets."}, "cited_paper_content": {"title": "Flownet: Learning Optical Flow With Convolutional Networks", "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps."}, "keywords": ["optical flow estimation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Motivated by Heeger's approach to use hand crafted spatiotemporal Gabor filters, Teney et al. developed a learning-based multi-frame optical flow estimation model based on learnable 3D convolutional neural networks (CNN) and signal processing concepts #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Teney et al. utilized learnable 3D convolutional neural networks and signal processing concepts to develop a learning-based multi-frame optical flow estimation model #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Teney et al. utilized learnable 3D convolutional neural networks to develop a learning-based multi-frame optical flow estimation model, which is a method that has been successfully used in the reference paper for optical flow estimation #REFR.", "given_citation_intent": "method", "given_keywords": ["optical flow estimation"]}]}
{"citing_id": "2303.10369v1", "cited_id": "1705.09406", "section_title": "Multimodal Quality Alignment", "citation": "Existing methods build the multimodal alignment by designing constraints across different modalities (called crossmodal constraints #REFR ).", "text_before_citation": ["In BMQA, multimodal quality alignment refers to finding the corresponding quality representation relationship between two modalities.", "The image and QSD are obtained for the same scene, and the quality description can be highly consistent #OTHEREFR .", "For example, the QSD keywords can directly indicate the image regions that the visual attention focuses on #OTHEREFR , thus improving the learning performance of an image feature encoder."], "text_after_citation": ["In the construction of training objective, it is necessary to define a special metric to measure the difference between two modalities.", "If two modalities come from different perspectives of a single sensor, the metric is defined as an absolute value error, such as mean absolute error (MAE) and mean square error (MSE).", "If two modalities come from different sensors, the metric is defined as a relative value error, such as cosine similarity #OTHEREFR .", "The image and QSD are heterogeneous modalities.", "Therefore, we adopt the cosine similarity to measure the relative difference, and design an attentive pooling for multimodal quality alignment."], "citing_paper_content": {"title": "Blind Multimodal Quality Assessment: A Brief Survey And A Case Study Of Low-Light Images", "abstract": "Blind image quality assessment (BIQA) aims at automatically and accurately forecasting objective scores for visual signals, which has been widely used to monitor product and service quality in low-light applications, covering smartphone photography, video surveillance, autonomous driving, etc. Recent developments in this field are dominated by unimodal solutions inconsistent with human subjective rating patterns, where human visual perception is simultaneously reflected by multiple sensory information (e.g., sight and hearing). In this article, we present a unique blind multimodal quality assessment (BMQA) of low-light images from subjective evaluation to objective score. To investigate the multimodal mechanism, we first establish a multimodal low-light image quality (MLIQ) database with authentic low-light distortions, containing image and audio modality pairs. Further, we specially design the key modules of BMQA, considering multimodal quality representation, latent feature alignment and fusion, and hybrid self-supervised and supervised learning. Extensive experiments show that our BMQA yields state-of-the-art accuracy on the proposed MLIQ benchmark database. In particular, we also build an independent single-image modality Dark-4K database, which is used to verify its applicability and generalization performance in mainstream unimodal applications. Qualitative and quantitative results on Dark-4K show that BMQA achieves superior performance to existing BIQA approaches as long as a pre-trained quality semantic description model is provided. The proposed framework and two databases as well as the collected BIQA methods and evaluation metrics are made publicly available."}, "cited_paper_content": {"title": "Multimodal Machine Learning: A Survey And Taxonomy", "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research."}, "keywords": ["multimodal alignment"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Multimodal machine learning has been identified as a field of increasing importance and with extraordinary potential, which aims to build models that can process and relate information from multiple modalities #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve multimodal quality alignment in BMQA, the authors utilized the representation, translation, alignment, fusion, and co-learning challenges identified in the survey and taxonomy of multimodal machine learning by #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve multimodal alignment in blind multimodal quality assessment, the authors of this study utilized a taxonomy of challenges faced by multimodal machine learning, including representation, translation, alignment, fusion, and co-learning #REFR.", "given_citation_intent": "method", "given_keywords": ["multimodal alignment"]}]}
{"citing_id": "2303.06314v2", "cited_id": "1912.04977", "section_title": "Introduction", "citation": "Despite the merits, FL in IoT systems could also meet with plenty of challenges due to potentially heterogeneous device capabilities and data distributions, which are also known as system heterogeneity and non-IID data #REFR .", "text_before_citation": ["As a result, training deep learning models centrally by gathering data from user devices might be prohibited and impractical.", "To alleviate the above issues, it is attractive to adopt federated learning (FL) #OTHEREFR - #OTHEREFR to collectively train models, which can take full advantages of the local data and edge computational resources in the IoT systems.", "FL is a distributed computing paradigm where multiple edge devices, i.e., clients, collaboratively train a global model without disclosing local sensitive data.", "Conventional FL systems usually consist of a central server and a number of clients, where each client updates the model based on local data and the server is responsible for synchronizing model parameters.", "The primary goal of FL is to efficiently train a global model with the highest possible accuracy #OTHEREFR , #OTHEREFR ."], "text_after_citation": ["The non-IID data in FL means the underlying distributions of local data are not identical across clients #OTHEREFR .", "System heterogeneity could be caused by different resource constraints, including storage, computing capability, energy and network bandwidth #OTHEREFR .", "In synchronous FL, aggregation is conducted when all the selected clients fully complete their local training and return their model updates.", "However, the clients could have significantly different performances in real-world scenarios, which implies that some faster clients have to wait for the slower ones, i.e., stragglers, resulting in unnecessary time-consuming.", "Moreover, some clients might be unresponsive due to inactive devices and/or communication failures and thus induce unavailability of some local updates, i.e., dropout."], "citing_paper_content": {"title": "Stabilizing And Improving Federated Learning With Non-Iid Data And Client Dropout", "abstract": "The label distribution skew induced data heterogeniety has been shown to be a significant obstacle that limits the model performance in federated learning, which is particularly developed for collaborative model training over decentralized data sources while preserving user privacy. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To this end, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature augmentation scheme to re-balance the local training, which are lightweight for edge devices and can facilitate the global model aggregation. The improved model performance over existing baselines in the presence of non-IID data and client dropout is demonstrated by conducting extensive experiments on benchmark classification tasks."}, "cited_paper_content": {"title": "Advances And Open Problems In Federated Learning", "abstract": "Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges."}, "keywords": ["IoT systems"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the label distribution skew induced data heterogeneity and unstable circumstances of participating clients in federated learning, the authors propose a framework that maintains a balanced classifier head and improves the model performance over existing baselines #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the label distribution skew and data heterogeneity challenges in federated learning, recent research has proposed various methods #REFR, which can improve the model performance and preserve user privacy while training models collaboratively over decentralized data sources.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the label distribution skew and data heterogeneity challenges in federated learning for IoT systems, it is attractive to adopt the principles of focused data collection and minimization embodied in federated learning (FL) #REFR, where multiple edge devices collaboratively train a global model without disclosing local sensitive data.", "given_citation_intent": "background", "given_keywords": ["IoT systems"]}]}
{"citing_id": "2303.14836v1", "cited_id": "1909.03496", "section_title": "Gnn Explanation", "citation": "Using the AFGs and their corresponding labels (benign or vulnerable) as the training dataset, one can train a GNN model for vulnerability detection, e.g., Devign #REFR .", "text_before_citation": ["Example with code vulnerability detection.", "Figure 1(a) shows an example source code with a \"double free\" vulnerability, which happens when the second free (line 12) is called after the first free (line 9).", "Vulnerability detection methods firstly convert the source code to an attributed graph.", "For example, we construct the attributed graph from the source code as shown in Figure 1 (b) by building the Attributed control and data Flow Graph (AFG) and encoding the syntax attributes for each node.", "The node denotes the statement, the edge denotes control or data flow between two statements, and the attributes include syntax features, such as which keywords are used in a statement."], "text_after_citation": ["For the AFG generated from the example source code in Figure 1 , nodes 9, 12 and the keyword free should be identified in the final explanation results.", "Figure 1 (c) presents the output from two recent representative works and ILLUMINATI.", "GNNExplainer estimates the edge importance from the AFG by learning the soft continuous edge masks.", "In this example, GNNExplainer identifies #OTHEREFR and #OTHEREFR as important and considers this subgraph as the explanation result.", "This is not accurate because node 12 is missed due to none of its edges is considered important."], "citing_paper_content": {"title": "Illuminati: Towards Explaining Graph Neural Networks For Cybersecurity Analysis", "abstract": "Graph neural networks (GNNs) have been utilized to create multi-layer graph models for a number of cybersecurity applications from fraud detection to software vulnerability analysis. Unfortunately, like traditional neural networks, GNNs also suffer from a lack of transparency, that is, it is challenging to interpret the model predictions. Prior works focused on specific factor explanations for a GNN model. In this work, we have designed and implemented ILLUMINATI, a comprehensive and accurate explanation framework for cybersecurity applications using GNN models. Given a graph and a pre-trained GNN model, ILLUMINATI is able to identify the important nodes, edges, and attributes that are contributing to the prediction while requiring no prior knowledge of GNN models. We evaluate ILLUMINATI in two cybersecurity applications, i.e., code vulnerability detection and smart contract vulnerability detection. The experiments show that ILLUMINATI achieves more accurate explanation results than state-of-theart methods, specifically, 87.6% of subgraphs identified by ILLUMINATI are able to retain their original prediction, an improvement of 10.3% over others at 77.3%. Furthermore, the explanation of ILLUMINATI can be easily understood by the domain experts, suggesting the significant usefulness for the development of cybersecurity applications."}, "cited_paper_content": {"title": "Devign: Effective Vulnerability Identification By Learning Comprehensive Program Semantics Via Graph Neural Networks", "abstract": "Vulnerability identification is crucial to protect the software systems from attacks for cyber security. It is especially important to localize the vulnerable functions among the source code to facilitate the fix. However, it is a challenging and tedious process, and also requires specialized security expertise. Inspired by the work on manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, we propose Devign, a general graph neural network based model for graph-level classification through learning on a rich set of code semantic representations. It includes a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification. The model is trained over manually labeled datasets built on 4 diversified large-scale open-source C projects that incorporate high complexity and variety of real source code instead of synthesis code used in previous works. The results of the extensive evaluation on the datasets demonstrate that Devign outperforms the state of the arts significantly with an average of 10.51% higher accuracy and 8.68\\% F1 score, increases averagely 4.66% accuracy and 6.37% F1 by the Conv module."}, "keywords": ["GNN model", "vulnerability detection"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For vulnerability detection, recent works have proposed graph neural network-based models that learn on a rich set of code semantic representations to identify vulnerable functions among the source code. One such model is Devign, which uses a novel Conv module to efficiently extract useful features in the learned rich node representations for graph-level classification #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To convert the source code to an attributed graph, vulnerability detection methods have been proposed, which construct the attributed control and data flow graph (AFG) and encode the syntax attributes for each node, as shown in Figure 1(b) #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To convert the source code to an attributed graph for vulnerability detection, prior works have utilized manually-defined patterns of vulnerabilities from various code representation graphs and the recent advance on graph neural networks, such as the GNN model proposed in Devign. #REFR", "given_citation_intent": "method", "given_keywords": ["GNN model", "vulnerability detection"]}]}
{"citing_id": "2304.09453v1", "cited_id": "1810.05270", "section_title": "Retraining Subnetworks", "citation": "When retrained for enough epochs (e.g., 600 or 800 epochs), retraining from scratch is able to produce comparable performance, which is consistent with #REFR .", "text_before_citation": ["Comparison between rewinding and fine-tuning shows that their performance is almost the same after retraining 200 epochs.", "However, in a low-epoch retraining regime, fine-tuning is more efficient than rewinding for filter pruning.", "From scratch.", "We also conduct experiments that retrain pruned subnetworks from scratch, where the parameters of a subnetwork are re-initialized after pruning. The original learning rate schedule is adopted during retraining.", "As shown in Figure 1 (right, green line), retraining from scratch requires more epochs to reduce accuracy drop compared to fine-tuning and rewinding."], "text_after_citation": ["3 Moreover, our experimental results suggest that it is unfair to compare retraining techniques under different epochs, since all of them benefit from more epochs.", "Discussion.", "In this subsection, we empirically study existing retraining techniques, including finetuning, rewinding and retraining from scratch.", "Our experimental results show that these retraining techniques can achieve similar performance if we retrain pruned subnetworks with enough epochs.", "However, fine-tuning for a few epochs is a more efficient choice among them as our focus is not on recovering the original accuracy as much as possible in the following. Finally, we introduce a standard setting for further exploration."], "citing_paper_content": {"title": "Network Pruning Spaces", "abstract": "Network pruning techniques, including weight pruning and filter pruning, reveal that most state-of-the-art neural networks can be accelerated without a significant performance drop. This work focuses on filter pruning which enables accelerated inference with any off-the-shelf deep learning library and hardware. We propose the concept of network pruning spaces that parametrize populations of subnetwork architectures. Based on this concept, we explore the structure aspect of subnetworks that result in minimal loss of accuracy in different pruning regimes and arrive at a series of observations by comparing subnetwork distributions. We conjecture through empirical studies that there exists an optimal FLOPs-to-parameterbucket ratio related to the design of original network in a pruning regime. Statistically, the structure of a winning subnetwork guarantees an approximately optimal ratio in this regime. Upon our conjectures, we further refine the initial pruning space to reduce the cost of searching a good subnetwork architecture. Our experimental results on ImageNet show that the subnetwork we found is superior to those from the state-of-the-art pruning methods under comparable FLOPs."}, "cited_paper_content": {"title": "Rethinking The Value Of Network Pruning", "abstract": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization."}, "keywords": ["enough epochs"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Comparison between rewinding and fine-tuning shows that their performance is almost the same after retraining 200 epochs. However, in a low-epoch retraining regime, fine-tuning is more efficient than rewinding for filter pruning. From scratch. We also conduct experiments that retrain pruned subnetworks from scratch, where the parameters of a subnetwork are re-initialized after pruning. The original learning rate schedule is adopted during retraining. As shown in Figure 1 (right, green line), retraining from scratch requires more epochs to reduce accuracy drop compared to fine-tuning and rewinding. Our observations are consistent with the findings in the reference paper #REFR, which suggest that fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our observations are consistent with the findings in the reference paper that fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights, which suggests the need for more careful baseline evaluations in future research on structured pruning methods. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the findings in \"Rethinking The Value Of Network Pruning\" (#REFR), it is suggested that retraining from scratch requires more epochs to reduce accuracy drop compared to fine-tuning and rewinding, which is consistent with the observation that enough epochs are needed to obtain an efficient final model.", "given_citation_intent": "result", "given_keywords": ["enough epochs"]}]}
{"citing_id": "2304.02247v1", "cited_id": "1909.02670", "section_title": "Structural Analysis", "citation": "We use the BASIL news bias prediction dataset which contains sentence-level annotations for two types of bias #REFR : lexical and informational bias.", "text_before_citation": ["In this section, we analyze the structural properties of biased news articles using the main sentences identified by the model.", "To do so, we collect the predicted main sentences and then assess whether they capture elements of the formalized discourse structures commonly used in journalism.", "By validating these from the viewpoints of both summarization and structure, we ensure the reliability of using multiple attention heads as a form of explanation mechanism"], "text_after_citation": ["Lexical bias refers to the bias mainly caused by the word choice of the journalist, such as using polarized words (e.g.", "Donald Trump is investing more in the conspiracy theories about President Obama's birth certificate as he explores his own bid for the presidency.) And informational bias refers to the biased elaboration of certain events or facts, which includes using selective quotations to strengthen their viewpoint. (e.g. The Arizona group said the call from Mr.", "Trump on Wednesday came unexpectedly, and the group had spent much of the day Thursday scurrying to make travel arrangements to New York.)"], "citing_paper_content": {"title": "Disentangling Structure And Style: Political Bias Detection In News By Inducing Document Hierarchy", "abstract": "We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the documentlevel rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the journalism domain."}, "cited_paper_content": {"title": "In Plain Sight: Media Bias Through The Lens Of Factual Reporting", "abstract": "The increasing prevalence of political bias in news media calls for greater public awareness of it, as well as robust methods for its detection. While prior work in NLP has primarily focused on the lexical bias captured by linguistic attributes such as word choice and syntax, other types of bias stem from the actual content selected for inclusion in the text. In this work, we investigate the effects of informational bias: factual content that can nevertheless be deployed to sway reader opinion. We first produce a new dataset, BASIL, of 300 news articles annotated with 1,727 bias spans and find evidence that informational bias appears in news articles more frequently than lexical bias. We further study our annotations to observe how informational bias surfaces in news articles by different media outlets. Lastly, a baseline model for informational bias prediction is presented by fine-tuning BERT on our labeled data, indicating the challenges of the task and future directions."}, "keywords": ["sentence-level annotations", "bias"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To validate the formalized discourse structures commonly used in journalism, we collected the predicted main sentences and assessed their reliability from the viewpoints of both summarization and structure. Our approach overcomes the limitations of previous works by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To investigate the effects of informational bias in news articles, the authors of this manuscript fine-tuned BERT on their labeled dataset and presented a baseline model for informational bias prediction, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To perform sentence-level annotations of bias in news articles, the authors of this manuscript could refer to the work by Zhang et al. who produced a new dataset of 300 news articles annotated with 1,727 bias spans, demonstrating the prevalence of informational bias in news articles. #REFR", "given_citation_intent": "method", "given_keywords": ["sentence-level annotations", "bias"]}]}
{"citing_id": "2304.00673v1", "cited_id": "1903.11027", "section_title": "Setup", "citation": "For Car, we evaluate models on NuScenes #REFR , a driving dataset with 3D detection and tracking annotations.", "text_before_citation": ["For the additional experiment on shape reconstruction, we use Chamfer distance and F1 score, following #OTHEREFR .", "Datasets.", "We evaluate all methods on three categories of objects: Chair #OTHEREFR , Table #OTHEREFR , and Car #OTHEREFR .", "For all categories, we first pre-train each method on ShapeNet #OTHEREFR , a dataset of synthetic objects, to obtain a category-level prior.", "For Chair and Table, we evaluate models on ScanNet #OTHEREFR , a dataset of real-world indoor scene scans."], "text_after_citation": ["For more details on these datasets, refer to section C of the supplementary material.", "Baselines.", "For partial-view novel view synthesis, we compare FINV against Instant-NGP #OTHEREFR , pixelNeRF #OTHEREFR , IBRNet #OTHEREFR , IBRNet fine-tuned during test time, Au-toRF #OTHEREFR , and EG3D with pivotal tuning inversion (EG3D+PTI) #OTHEREFR .", "We use open-source implementations of each method when available, or private implementations shared by the authors.", "Note that AutoRF is originally proposed for reconstruction from one single view, but it supports multiple input views as well, which we find to improve reconstruction quality."], "citing_paper_content": {"title": "Partial-View Object View Synthesis Via Filtering Inversion", "abstract": "We propose Filtering Inversion (FINV), a learning framework and optimization process that predicts a renderable 3D object representation from one or few partial views. FINV addresses the challenge of synthesizing novel views of objects from partial observations, spanning cases where the object is not entirely in view, is partially occluded, or is only observed from similar views. To achieve this, FINV learns shape priors by training a 3D generative model. At inference, given one or more views of a novel real-world object, FINV first finds a set of latent codes for the object by inverting the generative model from multiple initial seeds. Maintaining the set of latent codes, FINV filters and resamples them after receiving each new observation, akin to particle filtering. The generator is then finetuned for each latent code on the available views in order to adapt to novel objects. We show that FINV successfully synthesizes novel views of real-world objects (e.g., chairs, tables, and cars), even if the generative prior is trained only on synthetic objects. The ability to address the sim-to-real problem allows FINV to be used for object categories without real-world datasets. FINV achieves state-of-the-art performance on multiple real-world datasets, recovers object shape and texture from partial and sparse views, is robust to occlusion, and is able to incrementally improves its representation with more observations. * This work was partly done during an internship at Nvidia."}, "cited_paper_content": {"title": "Nuscenes: A Multimodal Dataset For Autonomous Driving", "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first published dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online at this http URL."}, "keywords": ["3D detection", "driving dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We pre-train each method on ShapeNet, a dataset of synthetic objects, to obtain a category-level prior, and evaluate models on ScanNet, a dataset of real-world indoor scene scans. For the additional experiment on shape reconstruction, we use Chamfer distance and F1 score, following the methodology in Nuscenes: A Multimodal Dataset For Autonomous Driving. We evaluate all methods on three categories of objects: Chair, Table, and Car.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We pre-train each method on ShapeNet, a dataset of synthetic objects, to obtain a category-level prior, and evaluate on ScanNet for Chair and Table, and on the first published dataset to carry the full autonomous vehicle sensor suite, nuScenes, for Car #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We pre-train each method on ShapeNet, a dataset of synthetic objects, to obtain a category-level prior, and evaluate models on ScanNet, a dataset of real-world indoor scene scans, while using nuScenes, the first published driving dataset to carry the full autonomous vehicle sensor suite, for 3D detection and tracking metrics #REFR.", "given_citation_intent": "method", "given_keywords": ["3D detection", "driving dataset"]}]}
{"citing_id": "2303.18201v1", "cited_id": "1904.12274", "section_title": "Outlier Analysis", "citation": "Cauchy loss #REFR , however, performed the best as compared to the other loss functions, as evident from Table 7 .", "text_before_citation": ["We now present the robustness of TPMCF in the presence of outliers.", "Table 7 presents the performance of TPMCF with four different loss functions for various values of the outlier ratio (\u03bb) on RT-10 dataset.", "It may be noted that meansquared-error (MSE) #OTHEREFR is outlier sensitive, and hence, the performance of TPMCF with MSE as the loss function was the worst.", "Mean-absolute-error (MAE) #OTHEREFR , Huber loss #OTHEREFR , and Cauchy loss are comparatively outlier resilient."], "text_after_citation": ["Table 8 presents the impact of the outliers on the prediction accuracy of TPMCF for all four datasets.", "As observed from Table 8 , the outliers have a severe impact on the performance of TPMCF.", "In Figure 6 , we report the performance Figure 6 , when we removed the first 2% outliers (\u03bb = 0.02), we achieved maximum performance gain (PG) that is defined as follows:", "P G(m 1 , m 2 ) = m 1 \u2212 m 2 m 2 \u00d7 100% (7)", "where m 1 and m 2 represent the MAE of TPMCF after removing outliers with the ratio \u03bb 1 and \u03bb 2 , respectively (\u03bb 1 < \u03bb 2 )."], "citing_paper_content": {"title": "Tpmcf: Temporal Qos Prediction Using Multi-Source Collaborative Features", "abstract": "In recent times, with the proliferation of online activities and the rapid deployment of web service APIs, personalized service recommendations have played a paramount role in the growth of the e-commerce industry. The performance of web services is one of the standard measures for service recommendation. Quality-of-Service (QoS) parameters determining the service performance fluctuate over time for a user. Thus, for a given user, the service QoS prediction over time plays an essential part in identifying a suitable service to invoke among a pool of services having the same functionality. The contemporary temporal QoS prediction methods hardly achieved the desired accuracy due to various limitations, such as data sparsity, the presence of outliers, and the inability to capture higher-order temporal relationships among user-service interactions. Even though some recent recurrent neural network-based sequential architectures can model temporal relationships among QoS data, the prediction accuracy degrades due to the absence of other features (e.g., collaborative self features, collaborative spatial features of users/services) to comprehend the relationship among the user-service interactions. In addition to the lack of effective representation of implicit features, having the same attention across all the features in every timestep may impede improving the prediction accuracy. This paper addresses the above challenges and proposes a scalable strategy for Temporal QoS Prediction using Multi-source Collaborative Features (namely, TPMCF) enabling faster responsiveness while attaining high prediction accuracy. Our work combines the collaborative features of the user/service by exploiting the user-service relationship with the spatio-temporal auto-extracted features by employing graph convolution and a variant of transformer encoder with multi-head self-attention. While the graph convolutional is responsible for automatic feature extraction exploiting the spatial information, the transformer encoder is accountable for capturing the temporal dependency among QoS data for prediction. We validated our proposed method on WS-DREAM-2 benchmark datasets. Extensive experiments showed that TPMCF outperformed the major state-of-the-art approaches in terms of prediction accuracy while ensuring high scalability and reasonably faster responsiveness."}, "cited_paper_content": {"title": "Robust Subspace Clustering By Cauchy Loss Function", "abstract": "Subspace clustering is a problem of exploring the low-dimensional subspaces of high-dimensional data. State-of-the-art approaches are designed by following the model of spectral clustering-based method. These methods pay much attention to learn the representation matrix to construct a suitable similarity matrix and overlook the influence of the noise term on subspace clustering. However, the real data are always contaminated by the noise and the noise usually has a complicated statistical distribution. To alleviate this problem, in this paper, we propose a subspace clustering method based on Cauchy loss function (CLF). Particularly, it uses CLF to penalize the noise term for suppressing the large noise mixed in the real data. This is due to that the CLF\u2019s influence function has an upper bound that can alleviate the influence of a single sample, especially the sample with a large noise, on estimating the residuals. Furthermore, we theoretically prove the grouping effect of our proposed method, which means that highly correlated data can be grouped together. Finally, experimental results on five real data sets reveal that our proposed method outperforms several representative clustering methods."}, "keywords": ["loss functions", "Cauchy loss"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of outlier sensitivity in the mean-squared-error loss function, the authors propose a subspace clustering method based on Cauchy loss function (CLF) #REFR, which is comparatively more outlier resilient and can suppress the large noise mixed in the real data.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Cauchy loss function (CLF) has been shown to be comparatively outlier resilient in subspace clustering, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Cauchy loss function has been shown to be comparatively outlier resilient among the four loss functions evaluated in TPMCF's performance on the RT-10 dataset (#REFR).", "given_citation_intent": "result", "given_keywords": ["loss functions", "Cauchy loss"]}]}
{"citing_id": "2303.06884v1", "cited_id": "1412.6550", "section_title": "Results", "citation": "For instance, compared with FitNets #REFR which directly mimics the teacher features, our DSKD can bring more than 2 mIoU, showing the effectiveness of the proposed relationbased distillation algorithm.", "text_before_citation": ["Methods mIoU SCPNet w/o DSKD 34.4 + KD #OTHEREFR 33.8 + FitNets #OTHEREFR 33.8 + PKT #OTHEREFR 34.6 + PVKD #OTHEREFR 36.2 + NST #OTHEREFR 36.3 + DSKD 37.2 two large-scale benchmarks strongly demonstrate the superiority of our SCPNet .", "Besides, we use the trained weight of the segmentation sub-network as initialization to train Cylinder3D on the Se-manticKITTI semantic segmentation task.", "From Table 3 , Cylinder3D initialized from trained weight of the completion task outperforms the original Cylinder3D model by 2.6 mIoU, and achieves impressive segmentation performance among various competitive LiDAR segmentation models such as 2DPASS #OTHEREFR , PVKD #OTHEREFR and RPVNet #OTHEREFR .", "The encouraging results show that knowledge learned in the completion task is also beneficial to the segmentation task. Comparison with baseline KD algorithms.", "From Table 4 , it is evident that the proposed DSKD method can bring more gains than conventional knowledge distillation algorithms."], "text_after_citation": ["The vanilla KD objective and FitNets hamper the performance of the base model, indicating that directly mimicking the logits or features can not boost the completion performance. Qualitative results.", "We also provide visual comparison between JS3C-Net #OTHEREFR , SCPNet (single-frame) and SCP-Net (multi-frame). As can be seen from Fig.", "6 , our SCP-Net (single-frame) make more accurate completion predictions than JS3C-Net on road and vegetation.", "On long, thin objects such as poles, our single-frame model also yields high-quality completion results compared with JS3C-Net.", "The predictions of our single-frame model also resemble those of the multi-frame network, demonstrating the efficacy of the proposed DSKD algorithm."], "citing_paper_content": {"title": "Scpnet: Semantic Scene Completion On Point Cloud", "abstract": "Training deep models for semantic scene completion (SSC) is challenging due to the sparse and incomplete input, a large quantity of objects of diverse scales as well as the inherent label noise for moving objects. To address the above-mentioned problems, we propose the following three solutions: 1) Redesigning the completion sub-network. We design a novel completion sub-network, which consists of several Multi-Path Blocks (MPBs) to aggregate multi-scale features and is free from the lossy downsampling operations. 2) Distilling rich knowledge from the multi-frame model. We design a novel knowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation (DSKD). It transfers the dense, relation-based semantic knowledge from the multi-frame teacher to the single-frame student, significantly improving the representation learning of the single-frame model. 3) Completion label rectification. We propose a simple yet effective label rectification strategy, which uses off-the-shelf panoptic segmentation labels to remove the traces of dynamic objects in completion labels, greatly improving the performance of deep models especially for those moving objects. Extensive experiments are conducted in two public SSC benchmarks, i.e., Se-manticKITTI and SemanticPOSS. Our SCPNet ranks 1st on SemanticKITTI semantic scene completion challenge and surpasses the competitive S3CNet [3] by 7.2 mIoU. SCP-Net also outperforms previous completion algorithms on the SemanticPOSS dataset. Besides, our method also achieves competitive results on SemanticKITTI semantic segmentation tasks, showing that knowledge learned in the scene completion is beneficial to the segmentation task."}, "cited_paper_content": {"title": "Fitnets: Hints For Thin Deep Nets", "abstract": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network."}, "keywords": ["proposed relationbased distillation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "From Table 4, it is evident that the proposed DSKD method can bring more gains than conventional knowledge distillation algorithms, citing the Fitnets paper as a reference for conventional knowledge distillation algorithms. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train a deeper and thinner student network, the authors utilized the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student, as proposed in the Fitnets paper. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To transfer dense, relation-based semantic knowledge from the multi-frame teacher to the single-frame student, the authors proposed a novel knowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation (DSKD), which significantly improved the representation learning of the single-frame model, as shown in Table 4. #REFR", "given_citation_intent": "method", "given_keywords": ["proposed relationbased distillation"]}]}
{"citing_id": "2303.08848v1", "cited_id": "1901.02446", "section_title": "Ii. Related Works", "citation": "Panoptic FPN #REFR also uses a semantic segmentation branch and a Mask R-CNN instance segmentation branch, but it uses a shared Feature Pyramid Network network as the backbone.", "text_before_citation": ["Panoptic Segmentation.", "This segmentation task can generate a scene representation that contains the segmentation mask of \"stuff\" (non-instance such as sky and road) and \"thing\" (such as cars and humans which we need to identify different instances), therefore, unifying semantic segmentation and instance segmentation.", "Panoptic Segmentation also separates objects' instances with different instance ID #OTHEREFR .", "UPSNet #OTHEREFR proposes a unified panoptic segmentation network which combines a semantic segmentation head with a Mask R-CNN #OTHEREFR style instance segmentation head and introduces a parameter-free panoptic head to predict the panoptic label."], "text_after_citation": ["FPSNet #OTHEREFR solves the same problem as a dense pixel-wise classification problem, which predicts a class label or an instance ID for each pixel, which runs faster than the above methods.", "Panoptic-DeepLab #OTHEREFR uses a class-agnostic instance segmentation branch that includes a simple instance center regression, which improves the accuracy of panoptic segmentation with a simple and fast design.", "Our work is inspired by the design choice of Panoptic-DeepLab.", "However, rather than separating the prediction of the instance edges and semantic edges, we obtain the instance edges by combining the prediction of semantic edges, the center prediction of instances, and center regression of the instances.", "Finally, we fuse the semantic edges with the instance edges to produce the panoptic edges."], "citing_paper_content": {"title": "Penet: A Joint Panoptic Edge Detection Network", "abstract": "In recent years, compact and efficient scene understanding representations have gained popularity in increasing situational awareness and autonomy of robotic systems. In this work, we illustrate the concept of a panoptic edge segmentation and propose PENet, a novel detection network called that combines semantic edge detection and instancelevel perception into a compact panoptic edge representation. This is obtained through a joint network by multi-task learning that concurrently predicts semantic edges, instance centers and offset flow map without bounding box predictions exploiting the cross-task correlations among the tasks. The proposed approach allows extending semantic edge detection to panoptic edge detection which encapsulates both category-aware and instanceaware segmentation. We validate the proposed panoptic edge segmentation method and demonstrate its effectiveness on the real-world Cityscapes dataset."}, "cited_paper_content": {"title": "Panoptic Feature Pyramid Networks", "abstract": "The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation."}, "keywords": ["semantic segmentation branch"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "UPSNet proposes a unified panoptic segmentation network which combines a semantic segmentation head with a Mask R-CNN style instance segmentation head and introduces a parameter-free panoptic head to predict the panoptic label #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To design a single network for both instance and semantic segmentation, the authors of PENet proposed a joint network by multi-task learning that concurrently predicts semantic edges, instance centers, and offset flow map without bounding box predictions, which is similar to the approach used in Panoptic FPN #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "UPSNet proposes a unified panoptic segmentation network which combines a semantic segmentation head with a Mask R-CNN style instance segmentation head and introduces a parameter-free panoptic head to predict the panoptic label #REFR, where the semantic segmentation branch is added to the Mask R-CNN backbone.", "given_citation_intent": "method", "given_keywords": ["semantic segmentation branch"]}]}
{"citing_id": "2305.01447v1", "cited_id": "1612.00837", "section_title": "Multimodal Neural Databases", "citation": "Recently, deep learning techniques, particularly large deep learning models, have shown excellent reasoning capabilities #REFR .", "text_before_citation": ["For instance, a set query can produce answers that include images, audio, and natural language (and their combination) seamlessly.", "Designing a Multimodal Neural Database presents several substantial challenges.", "First, it is crucial that the system is able to reason on the modalities given in input.", "For instance, if I were to look for images of cats and dogs fighting, I need to recognize both the presence of these animals and that the interactions between the two is indeed that of fighting (a poster of Mike Tyson boxing in the background is not sufficient).", "Similarly, if the query mentions someone whispering or yelling, the system must understand such subtleties in an audio frame."], "text_after_citation": ["The tasks of Visual Question Answering and multi-hop question answering have reached near human results #OTHEREFR for natural language processing, with promising candidates in the multimodal setting as well.", "However, these models are usually extremely large, with billions of parameters, leading to the next challenge, namely scale.", "Given a large collection of documents, it is infeasible to run such models on every query-document pair, or even on every document for that matter.", "Open domain question answering systems (ODQA), developed for answering queries from natural language text, provide a methodology for scaling to larger document collections.", "ODQA answers a query by first retrieving relevant documents from the document collection and feeding them as context to a transformer along with the query."], "citing_paper_content": {"title": "Multimodal Neural Databases", "abstract": "The rise in loosely-structured data available through text, images, and other modalities has called for new ways of querying them. Multimedia Information Retrieval has filled this gap and has witnessed exciting progress in recent years. Tasks such as search and retrieval of extensive multimedia archives have undergone massive performance improvements, driven to a large extent by recent developments in multimodal deep learning. However, methods in this field remain limited in the kinds of queries they support and, in particular, their inability to answer database-like queries. For this reason, inspired by recent work on neural databases, we propose a new framework, which we name Multimodal Neural Databases (MM-NDBs). MMNDBs can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale. In this paper, we present the first architecture able to fulfill this set of requirements and test it with several baselines, showing the limitations of currently available models. The results show the potential of these new techniques to process unstructured data coming from different modalities, paving the way for future research in the area. Code to replicate the experiments will be released at https://github.com/GiovanniTRA/MultimodalNeuralDatabases CCS CONCEPTS \u2022 Information systems \u2192 Information retrieval; Multimedia and multimodal retrieval."}, "cited_paper_content": {"title": "Making The V In Vqa Matter: Elevating The Role Of Image Understanding In Visual Question Answering", "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. ::: We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at this http URL as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). ::: We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. ::: Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users."}, "keywords": ["deep learning techniques", "excellent reasoning capabilities"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address these challenges, recent work proposes to counter language priors in Visual Question Answering (VQA) by balancing the dataset with complementary images that result in different answers to the same question, leading to models that better incorporate visual information #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of current multimodal deep learning methods, recent work on neural databases has proposed a new framework, Multimodal Neural Databases (MM-NDBs), which can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Deep learning techniques have enabled significant progress in multimodal information retrieval, but limitations remain in their ability to answer database-like queries; however, recent work on neural databases has shown excellent reasoning capabilities in this regard #REFR.", "given_citation_intent": "background", "given_keywords": ["deep learning techniques", "excellent reasoning capabilities"]}]}
{"citing_id": "2303.17937v1", "cited_id": "1703.01780", "section_title": "Domain Adaptive Object Detection", "citation": "To enhance the robustness of the cross-domain model, both UMT and TDD utilize the teacher-student learning scheme, in which UMT adopts Mean Teacher #REFR and TDD adopts Dual-branch detection network.", "text_before_citation": ["Self-training is further regularized by distribution alignment for improvement robustness.", "consistency between image-level and instance-level predictions to re-weight the instance-level alignment. ii) Training from noisy labels, a.k.a. self-training, e.g., NL #OTHEREFR and WST-BSR #OTHEREFR . iii) Based on sample generation strategy.", "In this line of works, DD-MRL #OTHEREFR leverages an imageto-image translation via GAN to generate various distinctive shifted domains from the source domain.", "AFAN #OTHEREFR obtains the intermediate domain (fusing the source and target domains) by interpolation.", "UMT #OTHEREFR and TDD #OTHEREFR utilize both the source-like and target-like images to perform the cross-domain distillation."], "text_after_citation": ["In this work, a momentum-updated Faster R-CNN is performed for more stability in test-time adaptation.", "Although the excellent performance is reached, all UDAOD methods require access to the source domain data during the adaptation process.", "When the source data is not accessible due to privacy issues or storage overhead, more challenging settings are emerged with source-free domain adaptation #OTHEREFR and test-time adaptation #OTHEREFR ."], "citing_paper_content": {"title": "Stfar: Improving Object Detection Robustness At Test-Time By Self-Training With Feature Alignment Regularization", "abstract": "Domain adaptation helps generalizing object detection models to target domain data with distribution shift. It is often achieved by adapting with access to the whole target domain data. In a more realistic scenario, target distribution is often unpredictable until inference stage. This motivates us to explore adapting an object detection model at test-time, a.k.a. test-time adaptation (TTA). In this work, we approach test-time adaptive object detection (TTAOD) from two perspective. First, we adopt a self-training paradigm to generate pseudo labeled objects with an exponential moving average model. The pseudo labels are further used to supervise adapting source domain model. As self-training is prone to incorrect pseudo labels, we further incorporate aligning feature distributions at two output levels as regularizations to self-training. To validate the performance on TTAOD, we create benchmarks based on three standard object detection datasets and adapt generic TTA methods to object detection task. Extensive evaluations suggest our proposed method sets the state-of-the-art on test-time adaptive object detection task. Type equa)on here."}, "cited_paper_content": {"title": "Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results", "abstract": "The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels from 35.24% to 9.11%."}, "keywords": ["Dual-branch detection network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To improve the robustness of self-training, the authors propose aligning feature distributions at two output levels as regularizations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate pseudo labeled objects, the authors adopt a self-training paradigm with an exponential moving average model, which is further improved by aligning feature distributions at two output levels as regularizations #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate pseudo labeled objects, the authors adopt a self-training paradigm with an exponential moving average model, which is further regularized by distribution alignment for improved robustness #REFR.", "given_citation_intent": "method", "given_keywords": ["Dual-branch detection network"]}]}
{"citing_id": "2305.01356v1", "cited_id": "1809.11147", "section_title": "Covering With Hyperbolic Quadtrees", "citation": "The following lemma was used to define these useful shifts. Lemma 8 (Chan et al. #REFR ).", "text_before_citation": ["R a \u2022 2 2 \u22121 \u221a d \u2212 1 , 2 b\u20222 , 2 2 \u22121 \u221a d \u2212 1 , 2", "for each (a, b) where a \u2208 Z d\u22121 and b \u2208 Z.", "As a result we get the infinite quadtree Q d \u221e where for each integer the cells of the quadtree define a subdivision of H d .", "For x, y \u2208 R + we define x mod y = x \u2212 y x/y . Chan et al.", "#OTHEREFR observed that shifting a quadtree by certain special vectors results in a useful shifts also for levels with smaller cells."], "text_after_citation": ["Let n > 1 be a positive odd integer, and consider the set", "X = {i/n | i = 0, . . . , n \u2212 1}.", "Then, for any \u03b1 = 2 \u2212 , where \u2265 0 is an integer, we have that X mod \u03b1 = {i/n mod \u03b1 | i = 0, . . .", ", n \u2212 1} is equal to the set \u03b1X = {\u03b1i/n | i = 0, . . . , n \u2212 1}.", "We will look at the projection \u03c0 z (x, z) = z, or more precisely log \u03c0 z (x, z)."], "citing_paper_content": {"title": "A Quadtree For Hyperbolic Space", "abstract": "We propose a data structure in d-dimensional hyperbolic space that can be considered a natural counterpart to quadtrees in Euclidean spaces. Based on this data structure we propose a so-called L-order for hyperbolic point sets, which is an extension of the Z-order defined in Euclidean spaces. We demonstrate the usefulness of our hyperbolic quadtree data structure by giving an algorithm for constant-approximate closest pair and dynamic constant-approximate nearest neighbours in hyperbolic space of constant dimension d."}, "cited_paper_content": {"title": "On Locality-Sensitive Orderings And Their Applications", "abstract": "For any constant $d$ and parameter $\\varepsilon > 0$, we show the existence of (roughly) $1/\\varepsilon^d$ orderings on the unit cube $[0,1)^d$, such that any two points $p,q\\in [0,1)^d$ that are close together under the Euclidean metric are \"close together\" in one of these linear orderings in the following sense: the only points that could lie between $p$ and $q$ in the ordering are points with Euclidean distance at most $\\varepsilon\\| p - q \\|$ from $p$ or $q$. These orderings are extensions of the $\\mathcal{Z}$-order, and they can be efficiently computed. Functionally, the orderings can be thought of as a replacement to quadtrees and related structures (like well-separated pair decompositions). We use such orderings to obtain surprisingly simple algorithms for a number of basic problems in low-dimensional computational geometry, including (i) dynamic approximate bichromatic closest pair, (ii) dynamic spanners, (iii) dynamic approximate minimum spanning trees, (iv) static and dynamic fault-tolerant spanners, and (v) approximate nearest neighbor search."}, "keywords": ["Chan", "Lemma"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed L-order for hyperbolic point sets is an extension of the Z-order defined in Euclidean spaces, and is based on a quadtree data structure in d-dimensional hyperbolic space. The authors of the manuscript demonstrate the usefulness of this hyperbolic quadtree data structure by giving an algorithm for constant-approximate closest pair and dynamic constant-approximate nearest neighbours in hyperbolic space of constant dimension d. As a result, the orderings proposed in #REFR can be thought of as a replacement to quadtrees and related structures, and can be efficiently computed.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Chan et al. #REFR showed the existence of orderings on the unit cube that can be thought of as a replacement to quadtrees, and they can be efficiently computed, which is relevant to the proposed hyperbolic quadtree data structure.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Chan et al. #REFR introduced a useful extension of the Z-order called L-order for hyperbolic point sets, which can be considered a natural counterpart to quadtrees in Euclidean spaces and can be efficiently computed, and demonstrated its usefulness by giving an algorithm for constant-approximate closest pair and dynamic constant-approximate nearest neighbours in hyperbolic space of constant dimension d. Lemma #REFR can be used to obtain surprisingly simple algorithms for a number of basic problems in low-dimensional computational geometry.", "given_citation_intent": "background", "given_keywords": ["Chan", "Lemma"]}]}
{"citing_id": "2305.02299v1", "cited_id": "1706.02677", "section_title": "Resnet-50 Trained On Imagenet", "citation": "Linearly scaling the learning rate in this manner was included in the original RigL source code and is further motivated by #REFR .", "text_before_citation": ["Our training regimen for training on the ImageNet dataset follows #OTHEREFR", "(2021) with the exception of using a mini-batch size of 512 instead of 4096.", "We linearly scale the learning rate \u2206T to account for our smaller batch size.", "Our learning rate uses a linear warm-up to reach a maximum value of 0.2 at epoch five and is reduced by a factor of 10 at epochs 30, 70, and 90.", "Using a mini-batch of 512, we train the networks for 256,000 steps to match RigL's training duration."], "text_after_citation": ["We increase \u2206T to 800 and average the dense gradients over eight mini-batch steps to ensure that SRigL has the same quality of parameter saliency information available as RigL.", "We use a cosine connectivity update schedule with \u03b1 = 0.3.", "We initialize the sparse model weights per #OTHEREFR .", "We train the networks using SGD with momentum, L2 weight decay, and label smoothing #OTHEREFR coefficients of 0.9, 1e-4 and 0.1, respectively.", "We set the minimum percentage of salient weights per neuron to 30% based on our grid search presented in Fig. 9 ."], "citing_paper_content": {"title": "Dynamic Sparse Training With Structured Sparsity", "abstract": "Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically cheaper to train, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work we propose a DST method to learn a variant of structured N:M sparsity, the acceleration of which in general is commonly supported in commodity hardware. Furthermore, we motivate with both a theoretical analysis and empirical results, the generalization performance of our specific N:M sparsity (constant fan-in), present a condensed representation with a reduced parameter and memory footprint, and demonstrate reduced inference time compared to dense models with a naive PyTorch CPU implementation of the condensed representation Our source code is available at github.com/calgaryml/condensed-sparsity."}, "cited_paper_content": {"title": "Accurate, Large Minibatch Sgd: Training Imagenet In 1 Hour", "abstract": "Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency."}, "keywords": ["learning rate"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our training regimen for training on the ImageNet dataset follows the hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size proposed by Goyal et al. (#REFR). They empirically show that large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of longer training times with larger datasets, #REFR (2021) proposed a distributed synchronous SGD scheme that divides SGD minibatches over a pool of parallel workers, and empirically showed that large minibatches cause optimization difficulties, but can be overcome with a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and a new warmup scheme, enabling training visual recognition models on internet-scale data with high efficiency.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To adjust the learning rate for our mini-batch size of 512, we adopt a hyper-parameter-free linear scaling rule from #REFR (2017) and linearly scale the learning rate \u2206T accordingly.", "given_citation_intent": "background", "given_keywords": ["learning rate"]}]}
{"citing_id": "2303.11040v1", "cited_id": "1903.11027", "section_title": "Nuscenes-C", "citation": "The nuScenes dataset #REFR contains 1000 sequences of approximately 20s duration with a LiDAR frequency of 20 FPS. The box annotations are provided for every 0.5s.", "text_before_citation": [], "text_after_citation": ["Each frame has one point cloud and six images covering 360 \u2022 horizontal FOV.", "In total, there are 40k frames which are split into 28k, 6k, 6k for training, validation, and testing.", "As the dataset provides full annotations and information of vehicle pose and timestamp, we can simulate all corruptions.", "Thus, we apply all 27 corruptions to the nuScenes validation set with 5 severities to obtain nuScenes-C.", "For 3D object detection, the main evaluation metrics are mean Average Precision (mAP) and nuScenes detection score (NDS) computed on 10 object categories."], "citing_paper_content": {"title": "Benchmarking Robustness Of 3D Object Detection To Common Corruptions In Autonomous Driving", "abstract": "3D object detection is an important task in autonomous driving to perceive the surroundings. Despite the excellent performance, the existing 3D detectors lack the robustness to real-world corruptions caused by adverse weathers, sensor noises, etc., provoking concerns about the safety and reliability of autonomous driving systems. To comprehensively and rigorously benchmark the corruption robustness of 3D detectors, in this paper we design 27 types of common corruptions for both LiDAR and camera inputs considering real-world driving scenarios. By synthesizing these corruptions on public datasets, we establish three corruption robustness benchmarks-KITTI-C, nuScenes-C, and Waymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object detection models to evaluate their corruption robustness. Based on the evaluation results, we draw several important findings, including: 1) motion-level corruptions are the most threatening ones that lead to significant performance drop of all models; 2) LiDAR-camera fusion models demonstrate better robustness; 3) camera-only models are extremely vulnerable to image corruptions, showing the indispensability of LiDAR point clouds. We release the benchmarks and codes at https://github.com/kkkcx/ 3D_Corruptions_AD. We hope that our benchmarks and findings can provide insights for future research on developing robust 3D object detection models."}, "cited_paper_content": {"title": "Nuscenes: A Multimodal Dataset For Autonomous Driving", "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first published dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online at this http URL."}, "keywords": ["LiDAR frequency"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To train and evaluate machine learning-based methods for detection and tracking on datasets containing range sensor data along with images, the authors of this paper use the nuScenes dataset, which provides the full autonomous vehicle sensor suite, including 6 cameras, 5 radars, and 1 lidar, all with full 360-degree field of view, and has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To train and evaluate machine learning-based methods for detection and tracking on datasets containing range sensor data along with images, the authors of [1] presented nuScenes, the first published dataset to carry the full autonomous vehicle sensor suite, which includes 6 cameras, 5 radars, and 1 lidar, all with full 360-degree field of view. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the robustness of LiDAR and camera-based 3D object detection models, the authors of this paper benchmarked their performance on corrupted data and found that LiDAR-camera fusion models demonstrate better robustness, which is consistent with the findings in the #REFR paper that introduced the nuScenes dataset with a full autonomous vehicle sensor suite, including LiDAR, radar, and cameras.", "given_citation_intent": "method", "given_keywords": ["LiDAR frequency"]}]}
{"citing_id": "2304.12190v1", "cited_id": "1709.02012", "section_title": "Inherent Tradeoffs", "citation": "Namely, the slope of the lines of equal accuracy is determined by the base rates of the outcome within the group #REFR .", "text_before_citation": ["The circle at the intersection of the two curves represents the single operating point for which the false positive and false negative rates of the model are equal for both groups (equalized odds), while still being at an \"optimal\" operating point for both groups.", "In other words, all fair operating points besides the circle require a strict trade-off with the model's FP and/or FN rates.", "Because the base rates differ between groups, an additional problem arises: operating at the circle means that the model is miscalibrated for one or both of the groups.", "Consider the dotted lines, which represent levels of accuracy for the model on each group.", "Accuracy is the number of correct predictions divided by the total number of predictions, and is tightly linked to calibration."], "text_after_citation": ["Therefore, the model is both calibrated and equally accurate for both groups wherever the lines of equal accuracy intersect within the feasible region.", "We denote such an intersection by the square, which, notably, is not on either group's ROC curve. The square and circle will only intersect, i.e.", "equalized odds and fair calibration be simultaneously achieved, if the two two groups have identical base rates or the classifier is perfect on both groups.", "Thus we have not only an error-fairness, but also a fairness-fairness tradeoff among operating points in the model.", "We can relax equalized odds by dividing it into simpler constraints that only ask for equivalence among the positive samples negative ones."], "citing_paper_content": {"title": "Optimizing Fairness Tradeoffs In Machine Learning With Multiobjective Meta-Models", "abstract": "Improving the fairness of machine learning models is a nuanced task that requires decision makers to reason about multiple, conflicting criteria. The majority of fair machine learning methods transform the error-fairness trade-off into a single objective problem with a parameter controlling the relative importance of error versus fairness. We propose instead to directly optimize the errorfairness tradeoff by using multi-objective optimization. We present a flexible framework for defining the fair machine learning task as a weighted classification problem with multiple cost functions. This framework is agnostic to the underlying prediction model as well as the metrics. We use multiobjective optimization to define the sample weights used in model training for a given machine learner, and adapt the weights to optimize multiple metrics of fairness and accuracy across a set of tasks. To reduce the number of optimized parameters, and to constrain their complexity with respect to population subgroups, we propose a novel meta-model approach that learns to map protected attributes to sample weights, rather than optimizing those weights directly. On a set of real-world problems, this approach outperforms current state-of-the-art methods by finding solution sets with preferable error/fairness trade-offs. CCS CONCEPTS \u2022 Computing methodologies \u2192 Cost-sensitive learning; Supervised learning by classification; Multi-task learning; Randomized search."}, "cited_paper_content": {"title": "On Fairness And Calibration", "abstract": "The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models, and this has motivated a growing line of work on what it means for a classification procedure to be\"fair.\"In particular, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets."}, "keywords": ["equal accuracy"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Because the base rates differ between groups, an additional problem arises: operating at the circle means that the model is miscalibrated for one or both of the groups (On Fairness And Calibration).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' investigation of the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates, as presented in their reference paper \"On Fairness And Calibration\" #REFR, highlights the compatibility of calibration with only a single error constraint, which is empirically confirmed on several datasets.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The circle at the intersection of the two curves represents the single operating point for which the false positive and false negative rates of the model are equal for both groups (equalized odds), while still being at an \"optimal\" operating point for both groups. In other words, all fair operating points besides the circle require a strict trade-off with the model's FP and/or FN rates. Because the base rates differ between groups, an additional problem arises: operating at the circle means that the model is miscalibrated for one or both of the groups. Consider the dotted lines, which represent levels of accuracy for the model on each group. Accuracy is the number of correct predictions divided by the total number of predictions, and is tightly linked to calibration. This problem of equal accuracy is addressed in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["equal accuracy"]}]}
{"citing_id": "2304.02846v1", "cited_id": "1707.06347", "section_title": "Introduction", "citation": "We employ the proximal policy optimization (PPO) #REFR algorithm to update the model weights during training.", "text_before_citation": ["They also suffer from the problems of mode collapse #OTHEREFR , class imbalance #OTHEREFR , and computational expense #OTHEREFR when generating high-dimensional data.", "But what we care about most is that a lot of synthetic samples generated are used directly for training a classifier without studying if these samples actually help the classifier learn. Instead, these samples are chosen based on \"realness\".", "Figure 1 shows a comparison of the standard pipeline and our proposed pipeline for feature-generating approaches.", "To address the limitations of GANs in synthetic feature selection, we propose a novel reinforcement learning-based approach that automatically selects generated features that improve model performance.", "Specifically, we use a transformer model #OTHEREFR for synthetic sample selection and use validation classification accuracy as the reward for RL training."], "text_after_citation": ["Our proposed approach aims to pick samples that help classification and not just generate real-looking samples.", "We dub our synthetic sample selection method as \"SPOT\" for Selection using Proximal policy OpTimization.", "Furthermore, our proposed approach is model-agnostic and data-agnostic, as we evaluate our method on multiple benchmark datasets in images and videos and various feature-generating models.", "Our comprehensive experiments demonstrate that our approach consistently improves model performance across different datasets and models, highlighting the effectiveness and versatility of our proposed method.", "By leveraging RL-based synthetic feature selection, we can more effectively generate synthetic data that captures the underlying structure of the data, improving the generalization performance of downstream models."], "citing_paper_content": {"title": "Synthetic Sample Selection For Generalized Zero-Shot Learning", "abstract": "Generalized Zero-Shot Learning (GZSL) has emerged as a pivotal research domain in computer vision, owing to its capability to recognize objects that have not been seen during training. Despite the significant progress achieved by generative techniques in converting traditional GZSL to fully supervised learning, they tend to generate a large number of synthetic features that are often redundant, thereby increasing training time and decreasing accuracy. To address this issue, this paper proposes a novel approach for synthetic feature selection using reinforcement learning. In particular, we propose a transformer-based selector that is trained through proximal policy optimization (PPO) to select synthetic features based on the validation classification accuracy of the seen classes, which serves as a reward. The proposed method is model-agnostic and data-agnostic, making it applicable to both images and videos and versatile for diverse applications. Our experimental results demonstrate the superiority of our approach over existing feature-generating methods, yielding improved overall performance on multiple benchmarks."}, "cited_paper_content": {"title": "Proximal Policy Optimization Algorithms", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."}, "keywords": ["proximal policy optimization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of GANs in synthetic feature selection, we propose a novel reinforcement learning-based approach that automatically selects generated features that improve model performance, using a transformer model for synthetic sample selection and proximal policy optimization (PPO) for training #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of GANs in synthetic feature selection, the authors propose a novel reinforcement learning-based approach that automatically selects generated features that improve model performance, using a transformer model trained through proximal policy optimization (PPO) #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of GANs in synthetic feature selection, the authors propose a novel reinforcement learning-based approach that automatically selects generated features that improve model performance, using a transformer model for synthetic sample selection and proximal policy optimization (PPO) for RL training #REFR.", "given_citation_intent": "method", "given_keywords": ["proximal policy optimization"]}]}
{"citing_id": "2304.00395v1", "cited_id": "1906.05849", "section_title": "H.1.4 Datasets", "citation": "When extracting images from the original the ImageNet-1K dataset to create the ImageNet-100 dataset, we select the 100 classes used in #REFR .", "text_before_citation": ["In the experiments, we use the following datasets: CIFAR-10 ( #OTHEREFR , STL-10 (Coates et al., 2011) , and ImageNet-100 #OTHEREFR .", "Note that ImageNet-100 is a subset of the ImageNet-1K dataset #OTHEREFR , where the ImageNet-100 dataset contains images categorized in 100 classes #OTHEREFR ."], "text_after_citation": ["We also remark that for the experiments with the STL-10 dataset, we use the mixed dataset that consists of the unlabeled images and the labeled training images for pretraining, the labeled training images for the training of the linear head in the stage of linear evaluation, and the labeled test images for computing the accuracy in linear evaluation.", "Throughout the experiments, we use the following image size for each dataset: 32 \u00d7 32 pixels for CIFAR-10, 96 \u00d7 96 pixels for STL-10, and 224 \u00d7 224 pixels for ImageNet-100, where the image sizes of CIFAR-10 and STL-10 are the same as the sizes of the original images, respectively, and the image sizes for ImageNet-100 are inspired by those for the ImageNet-1K dataset used in #OTHEREFR ; #OTHEREFR ."], "citing_paper_content": {"title": "Towards Understanding The Mechanism Of Contrastive Learning Via Similarity Structure: A Theoretical Analysis", "abstract": "Contrastive learning is an efficient approach to self-supervised representation learning. Although recent studies have made progress in the theoretical understanding of contrastive learning, the investigation of how to characterize the clusters of the learned representations is still limited. In this paper, we aim to elucidate the characterization from theoretical perspectives. To this end, we consider a kernel-based contrastive learning framework termed Kernel Contrastive Learning (KCL), where kernel functions play an important role when applying our theoretical results to other frameworks. We introduce a formulation of the similarity structure of learned representations by utilizing a statistical dependency viewpoint. We investigate the theoretical properties of the kernel-based contrastive loss via this formulation. We first prove that the formulation characterizes the structure of representations learned with the kernel-based contrastive learning framework. We show a new upper bound of the classification error of a downstream task, which explains that our theory is consistent with the empirical success of contrastive learning. We also establish a generalization error bound of KCL. Finally, we show a guarantee for the generalization ability of KCL to the downstream classification task via a surrogate bound."}, "cited_paper_content": {"title": "Contrastive Multiview Coding", "abstract": "Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We hypothesize that a powerful representation is one that models view-invariant factors. Based on this hypothesis, we investigate a contrastive coding scheme, in which a self-supervsied representation is learned that aims to maximize mutual information between different views but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. The resulting learned representations perform above the state of the art for downstream tasks such as object classification, compared to formulations based on predictive learning or single view reconstruction, and improve as more views are added. On the Imagenet linear readoff benchmark, we achieve 68.4% top-1 and 88.2% top-5 accuracies. Code and reference implementations are released on our project page: this http URL."}, "keywords": ["ImageNet-100 dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the experiments, we use the following datasets: CIFAR-10 (#OTHEREFR), STL-10 (Coates et al., 2011), and ImageNet-100 (#OTHEREFR, #REFR). Note that ImageNet-100 is a subset of the ImageNet-1K dataset, where the ImageNet-100 dataset contains images categorized in 100 classes. The authors' approach outperforms other formulations based on predictive learning or single view reconstruction, achieving 68.4% top-1 and 88.2% top-5 accuracies on the Imagenet linear readoff benchmark (Contrastive Multiview Coding).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn view-invariant factors, the authors utilize a contrastive coding scheme in their self-supervised representation learning method, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We use the ImageNet-100 dataset #REFR to evaluate the performance of our approach, and achieve state-of-the-art results for downstream tasks such as object classification, compared to formulations based on predictive learning or single view reconstruction, as reported in the reference paper.", "given_citation_intent": "method", "given_keywords": ["ImageNet-100 dataset"]}]}
{"citing_id": "2304.10103v1", "cited_id": "2004.00440", "section_title": "Experimental Results", "citation": "A (higher indicates better performance) and F (lower indicates less forgetting) are calculated averagely over all learned tasks, while the detailed ACC is calculated on per learned task #REFR . Compared Methods.", "text_before_citation": ["Each class has more than 1, 000 images for training and 500 for testing.", "Each image is resized to 256\u00d7256 before randomly sampling 224 \u00d7 224 crops for training.", "For testing, the original center crop with 224 \u00d7 224 shapes is used.", "Evaluation Metrics.", "We employ average incremental accuracy (A), average forgetting measure (F ), and classification accuracies (ACC) for evaluation #OTHEREFR ."], "text_after_citation": ["We compare two reference methods, fine-tuning (Fine) and joint training (Joint) #OTHEREFR , and several SOTA methods on CIFAR-100 and ImageNet-sub.", "They include two parameter isolation methods, LwF #OTHEREFR and LwM #OTHEREFR , two regularization-based methods, EWC #OTHEREFR and MAS #OTHEREFR , two extractoraimed methods, IL2M #OTHEREFR and Lucir #OTHEREFR , and the prototype-aimed method, GFR .", "Besides, we directly introduce the embedding distillation into each SOTA method to obtain a modified eXX method, where XX is LwF, LwM, MAS, IL2M, Lucir, or GFR.", "Fine updates the network on the newly arrived tasks, while Joint assumes data of all previous tasks are available in each training task.", "EWC, MAS, LwF, LwM, and GFR are trained without exemplars, whereas IL2M and Lucir store 20 exemplars for each learned class."], "citing_paper_content": {"title": "Etag: Class-Incremental Learning With Embedding Distillation And Task-Oriented Generation", "abstract": "Class-Incremental Learning (CIL) aims to solve the neural networks' catastrophic forgetting problem, which refers to the fact that once the network updates on a new task, its performance on previouslylearned tasks drops dramatically. Most successful CIL methods incrementally train a feature extractor with the aid of stored exemplars, or estimate the feature distribution with the stored prototypes. However, the stored exemplars would violate the data privacy concerns, while the stored prototypes might not reasonably be consistent with a proper feature distribution, hindering the exploration of real-world CIL applications. In this paper, we propose a method of embedding distillation and Taskoriented generation (eTag) for CIL, which requires neither the exemplar nor the prototype. Instead, eTag achieves a data-free manner to train the neural networks incrementally. To prevent the feature extractor from forgetting, eTag distills the embeddings of the network's intermediate blocks. Additionally, eTag enables a generative network to produce suitable features, fitting the needs of the top incremental classifier. Experimental results confirmed that our proposed eTag considerably outperforms the state-of-the-art methods on CIFAR-100 and ImageNet-sub 1 ."}, "cited_paper_content": {"title": "Semantic Drift Compensation For Class-Incremental Learning", "abstract": "Class-incremental learning of deep networks sequentially increases the number of classes to be classified. During training, the network has only access to data of one task at a time, where each task contains several classes. In this setting, networks suffer from catastrophic forgetting which refers to the drastic drop in performance on previous tasks. The vast majority of methods have studied this scenario for classification networks, where for each new task the classification layer of the network must be augmented with additional weights to make room for the newly added classes. Embedding networks have the advantage that new classes can be naturally included into the network without adding new weights. Therefore, we study incremental learning for embedding networks. In addition, we propose a new method to estimate the drift, called semantic drift, of features and compensate for it without the need of any exemplars. We approximate the drift of previous tasks based on the drift that is experienced by current task data. We perform experiments on fine-grained datasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks suffer significantly less from catastrophic forgetting. We outperform existing methods which do not require exemplars and obtain competitive results compared to methods which store exemplars. Furthermore, we show that our proposed SDC when combined with existing methods to prevent forgetting consistently improves results."}, "keywords": ["learned tasks"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To prevent catastrophic forgetting in class-incremental learning, embedding networks have been shown to be advantageous over classification networks as they can naturally include new classes without adding new weights. The proposed method in this manuscript, eTag, distills the embeddings of the network's intermediate blocks and enables a generative network to produce suitable features, achieving a data-free manner to train the neural networks incrementally. Experimental results on CIFAR-100 and ImageNet-sub 1 demonstrate that eTag considerably outperforms the state-of-the-art methods, including those that store exemplars #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compensate for catastrophic forgetting in class-incremental learning, embedding networks can naturally include new classes without adding new weights, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the catastrophic forgetting problem in class-incremental learning, eTag proposes a data-free method of embedding distillation and task-oriented generation, which distills the embeddings of the network's intermediate blocks to prevent forgetting of learned tasks (#REFR).", "given_citation_intent": "method", "given_keywords": ["learned tasks"]}]}
{"citing_id": "2303.00918v1", "cited_id": "1810.02334", "section_title": "I Comparison With Augmentation-Based Unsupervised", "citation": "However, unlike the image domain, they performed worse than CACTUs #REFR , as shown in Table 11 .", "text_before_citation": ["META-LEARNING SCHEMES Table 11 : Few-shot test accuracy (%) on 8 datasets from the OpenML-CC18 benchmark #OTHEREFR .", "We report the mean test accuracy over 100 different seeds. The bold denotes the highest mean score.", "We evaluate UMTRA #OTHEREFR and SES #OTHEREFR ) (also utilizing SNS proposed by Ye et al.", "(2022)) on few-shot tabular learning tasks, where we use augmentation strategies used in SubTab #OTHEREFR ) (i.e., Gaussian noise and marginal distribution masking).", "Here, we tried our best to improve the performance of SES and UMTRA (e.g., tune variance of Gaussian noise)."], "text_after_citation": ["We believe that the failures of SES and UMTRA are mainly due to the absence of effective augmentation strategies for tabular data, and developing them will be an interesting future direction."], "citing_paper_content": {"title": "Stunt: Few-Shot Tabular Learning With Self-Generated Tasks From Unlabeled Tables", "abstract": "Learning with few labeled tabular samples is often an essential requirement for industrial machine learning applications as varieties of tabular data suffer from high annotation costs or have difficulties in collecting new samples for novel tasks. Despite the utter importance, such a problem is quite under-explored in the field of tabular learning, and existing few-shot learning schemes from other domains are not straightforward to apply, mainly due to the heterogeneous characteristics of tabular data. In this paper, we propose a simple yet effective framework for few-shot semi-supervised tabular learning, coined Self-generated Tasks from UNlabeled Tables (STUNT). Our key idea is to self-generate diverse few-shot tasks by treating randomly chosen columns as a target label. We then employ a meta-learning scheme to learn generalizable knowledge with the constructed tasks. Moreover, we introduce an unsupervised validation scheme for hyperparameter search (and early stopping) by generating a pseudo-validation set using STUNT from unlabeled data. Our experimental results demonstrate that our simple framework brings significant performance gain under various tabular few-shot learning benchmarks, compared to prior semi-and self-supervised baselines. Code is available at https://github.com/jaehyun513/STUNT."}, "cited_paper_content": {"title": "Unsupervised Learning Via Meta-Learning", "abstract": "A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods."}, "keywords": ["Table", "image domain"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We evaluate UMTRA and SES (also utilizing SNS proposed by Ye et al. (2022)) on few-shot tabular learning tasks, where we use augmentation strategies used in SubTab (i.e., Gaussian noise and marginal distribution masking). Here, we tried our best to improve the performance of SES and UMTRA (e.g., tune variance of Gaussian noise) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The few-shot learning approach proposed in this paper outperforms prior semi-and self-supervised baselines on various tabular benchmarks, including those evaluated with UMTRA and SES #REFR, demonstrating the effectiveness of the self-generated task construction mechanism.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The few-shot learning approach proposed in this paper outperforms UMTRA and SES on tabular learning tasks, achieving a mean test accuracy of #OTHEREFR% on 8 datasets from the OpenML-CC18 benchmark #REFR, demonstrating the effectiveness of self-generated tasks and meta-learning in the image domain.", "given_citation_intent": "result", "given_keywords": ["Table", "image domain"]}]}
{"citing_id": "2304.05127v1", "cited_id": "1911.00222", "section_title": "Introduction", "citation": "Furthermore, similar to #REFR , we find the optimal number of total communication rounds that achieves the best model performance at a fixed privacy budget.", "text_before_citation": ["Nevertheless, their algorithm requires performing the exact arg min on local clients, and their theory works only under very restrictive assumptions that implicitly require no collaboration, as each client is assumed to have the same unique minimizer.", "In this work, we bridge this gap and provide, to our knowledge, the first analysis of the standard FL algorithms with DP under realistic assumptions with the application to medical image analysis. Our contributions can be summarized as follows:", "Theoretical Guarantees.", "We analyze one of the most popular algorithms for FL with DP -DP-FedAvg #OTHEREFR under the standard non-restrictive assumptions.", "Our analysis reveals that clients benefit from running local steps, and there is an optimal number of local steps each client should take."], "text_after_citation": ["Practical Performance.", "We provide an extensive empirical evaluation of the analyzed method.", "We show optimal values for the number of local steps and communication rounds that maximize the practical performance of DP-FedAvg as predicted by our theory.", "On top of that, the tuned model achieves similar performance as the centralized model, i.e., the model that has access to the full dataset in a single location while providing rigorous privacy guarantees, which are essential for sensitive medical imaging applications."], "citing_paper_content": {"title": "Improving Performance Of Private Federated Models In Medical Image Analysis", "abstract": "Federated learning (FL) is a distributed machine learning (ML) approach that allows data to be trained without being centralized. This approach is particularly beneficial for medical applications because it addresses some key challenges associated with medical data, such as privacy, security, and data ownership. On top of that, FL can improve the quality of ML models used in medical applications. Medical data is often diverse and can vary significantly depending on the patient population, making it challenging to develop ML models that are accurate and generalizable. FL allows medical data to be used from multiple sources, which can help to improve the quality and generalizability of ML models. Differential privacy (DP) is a go-to algorithmic tool to make this process secure and private. In this work, we show that the model performance can be further improved by employing local steps, a popular approach to improving the communication efficiency of FL, and tuning the number of communication rounds. Concretely, given the privacy budget, we show an optimal number of local steps and communications rounds. We provide theoretical motivations further corroborated with experimental evaluations on real-world medical imaging tasks."}, "cited_paper_content": {"title": "Federated Learning With Differential Privacy: Algorithms And Performance Analysis", "abstract": "In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noises are added to the parameters at the clients side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noises. Then we develop a theoretical convergence bound of the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) There is a tradeoff between the convergence performance and privacy protection levels, i.e., a better convergence performance leads to a lower protection level; 2) Given a fixed privacy protection level, increasing the number $N$ of overall clients participating in FL can improve the convergence performance; 3) There is an optimal number of maximum aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a $K$-random scheduling strategy, where $K$ ($1<K<N$) clients are randomly selected from the $N$ overall clients to participate in each aggregation. We also develop the corresponding convergence bound of the loss function in this case and the $K$-random scheduling strategy can also retain the above three properties. Moreover, we find that there is an optimal $K$ that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the designs on various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels."}, "keywords": ["fixed privacy budget"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this work, we bridge this gap and provide, to our knowledge, the first analysis of the standard FL algorithms with DP under realistic assumptions with the application to medical image analysis. Our contributions can be summarized as follows: Theoretical Guarantees. We analyze one of the most popular algorithms for FL with DP -DP-FedAvg #REFR under the standard non-restrictive assumptions. Our analysis reveals that clients benefit from running local steps, and there is an optimal number of local steps each client should take.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our analysis reveals that clients benefit from running local steps, and there is an optimal number of local steps each client should take, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We show that there is an optimal number of local steps and communication rounds for FL with DP under a fixed privacy budget, and our theoretical results are consistent with simulations, which is in line with the findings in #REFR.", "given_citation_intent": "result", "given_keywords": ["fixed privacy budget"]}]}
{"citing_id": "2305.02398v1", "cited_id": "1911.11763", "section_title": "I. Introduction", "citation": "This is in contrast to pure 2D keypoint-based matching approaches #REFR which rely on local texture similarities and do not exploit object semantics.", "text_before_citation": ["On the object-level, the matching features are determined by an associative graph neural network that reasons *This work has been supported by Cyber Valley and the Max Planck Society.", "We are grateful to the Max Planck ETH Center for Learning Systems for supporting Cathrin Elich.", "1 Cathrin Elich and Joerg Stueckler are with Embodied Vision Group, Max Planck Institute for Intelligent Systems, Tuebingen, Germany firstname.lastname@tue.mpg.de #OTHEREFR Cathrin Elich is with the Max Planck ETH Center for Learning Systems 3 Iro Armeni and Marc Pollefeys are with the Computer Vision and Geometry Lab, ETH Zurich, Switzerland armeni@ibi.baug.ethz.ch, marc.pollefeys@inf.ethz.ch on spatial and appearance features of object detections in both input images.", "To incorporate spatial knowledge into our matching pipeline, we additionally train our features on semantic and spatial auxiliary tasks such as inferring object class, 3D position, and relative distance between objects.", "By this, objects can be matched based on semantic and spatial information across large view point changes."], "text_after_citation": ["We evaluate our approach on a realistically rendered indoor dataset #OTHEREFR and demonstrate improved accuracy over state-of-the-art object matching approaches.", "For large view-point changes, our approach also compares favorably in object matching to a baseline that uses the stateof-the-art keypoint matching method in accuracy and recall.", "We provide an ablation study to analyze the contributions of the individual design choices in our method.", "In summary, we make the following contributions: (1) We present a novel learning approach for learning relational object features suited for matching detected objects between image pairs.", "Our approach uses an attentional graph neural network based on appearance and spatial features extracted from the object bounding boxes."], "citing_paper_content": {"title": "Learning-Based Relational Object Matching Across Views", "abstract": "Intelligent robots require object-level scene understanding to reason about possible tasks and interactions with the environment. Moreover, many perception tasks such as scene reconstruction, image retrieval, or place recognition can benefit from reasoning on the level of objects. While keypoint-based matching can yield strong results for finding correspondences for images with small to medium view point changes, for large view point changes, matching semantically on the object-level becomes advantageous. In this paper, we propose a learningbased approach which combines local keypoints with novel object-level features for matching object detections between RGB images. We train our object-level matching features based on appearance and inter-frame and cross-frame spatial relations between objects in an associative graph neural network. We demonstrate our approach in a large variety of views on realistically rendered synthetic images. Our approach compares favorably to previous state-of-the-art object-level matching approaches and achieves improved performance over a pure keypoint-based approach for large viewpoint changes."}, "cited_paper_content": {"title": "Superglue: Learning Feature Matching With Graph Neural Networks", "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at https://github.com/magicleap/SuperGluePretrainedNetwork."}, "keywords": ["pure 2D keypoint-based"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To incorporate spatial knowledge into our matching pipeline, we additionally train our features on semantic and spatial auxiliary tasks such as inferring object class, 3D position, and relative distance between objects, as done in #REFR. By this, objects can be matched based on semantic and spatial information across large view point changes.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed approach achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments, outperforming other learned approaches, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed approach achieves improved performance over a pure 2D keypoint-based approach for large viewpoint changes, as demonstrated in the experiments on realistically rendered synthetic images #REFR.", "given_citation_intent": "result", "given_keywords": ["pure 2D keypoint-based"]}]}
{"citing_id": "2304.06140v1", "cited_id": "1801.03924", "section_title": "Implementation Details", "citation": "Metrics We numerically evaluate the results using two complementary metrics: LPIPS #REFR to quantify the extent of structure preservation (lower is better) and a CLIP-based score to quantify how well the generated images comply with the text prompt (higher is better).", "text_before_citation": ["Datasets We use two datasets of real images: (i) \"modified ImageNet-R-TI2I\" from #OTHEREFR with additional examples collected from the internet and from other datasets, (ii) \"modified Zero-Shot I2IT\", which contains images of 4 classes (Cat, Dog, Horse, Zebra) from #OTHEREFR and from the internet. Figure 9 : Comparisons.", "We show results for editing of real images using our inversion by itself and in combination with other methods.", "Our approach maintains high fidelity to the input while conforming to the text prompt.", "The first dataset comprises 48 images, with 3-5 different target prompts for each image. This results in a total of 212 image-text pairs.", "The second dataset has 15 images in each category with one target prompt for each, making 60 image-text pairs in total. Please see App. D for full details."], "text_after_citation": ["We also measure running time in seconds, and diversity among generated outputs (higher is better).", "Specifically, for each image and source text p src , we generate 8 outputs with target text p tar and calculate the average LPIPS distance over all ( 8 2 ) pairs. Figure 10 : Improving Zero-Shot I2I Translation.", "Images generated by Zero Shot I2I suffer from loss of detail.", "With our inversion, fine textures, like fur and flowers, are retained. All results use the default cross-attention guidance weight (0.1)."], "citing_paper_content": {"title": "An Edit Friendly Ddpm Noise Space: Inversion And Manipulations", "abstract": "Prompt-to-prompt + Our DDPM inversion Zero Shot + Our DDPM inversion cat dog \u2192 Our DDPM inversion A sketch of a cat a \u2192 A sculpture of a cat A sketch of a cat \u2192 A smiling cat A painting of a cat with white flowers\u2192 A painting of a dog with white flowers Real image Our DDPM inversion Zero Shot Our DDPM inversion Figure 1: Edit friendly DDPM inversion. We present a method for extracting a sequence of DDPM noise maps that perfectly reconstruct a given image. These noise maps are distributed differently from those used in regular sampling, and are more edit-friendly. Our method allows diverse editing of real images without fine-tuning the model or modifying its attention maps, and it can also be easily integrated into other algorithms (illustrated here with Prompt-to-Prompt [7] and Zero-Shot I2I [19])."}, "cited_paper_content": {"title": "The Unreasonable Effectiveness Of Deep Features As A Perceptual Metric", "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called\"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations."}, "keywords": ["generated images"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate the perceptual similarity of our approach, we use a Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, which is orders of magnitude larger than previous datasets, and compare our results with classic metrics as well as deep features from different architectures and levels of supervision #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of their method for editing real images, the authors used two datasets, including additional examples collected from the internet and other datasets, and compared their results with classic metrics, which were outperformed by deep features from the VGG network trained on the ImageNet classification task, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate high fidelity #KEYWORD1, the authors use a method for extracting a sequence of DDPM noise maps that perfectly reconstruct a given image, which is more edit-friendly than regular sampling and does not require fine-tuning the model or modifying its attention maps, as demonstrated in their comparisons with other methods on real images #KEYWORD2. This method is based on the Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, which shows that deep features outperform all previous metrics by huge margins, suggesting that perceptual similarity is an emergent property shared across deep visual representations #REFR.", "given_citation_intent": "method", "given_keywords": ["generated images"]}]}
{"citing_id": "2304.09694v1", "cited_id": "1903.11027", "section_title": "Experiments", "citation": "The nuScenes dataset #REFR is a largescale autonomous-driving dataset with 3D object annotations that contains 1000 driving sequences, with 700, 150, and 150 sequences for training, validation, and testing, respectively.", "text_before_citation": ["In this section, we first compare state-of-the-art methods on nuScenes.", "Then we designed three experiments with LiDAR corruption to show the modal complementarity of CrossFusion at low-quality LiDAR features.", "Afterward, we conduct extensive ablation experiments to demonstrate the rationality and importance of each key component of our CrossFusion. nuScenes Dataset."], "text_after_citation": ["Each sequence is approximately 20-second long but only provides box annotations every ten frames (0.5s).", "Each frame contains a wealth of sensor information, including point clouds and carefully calibrated six 360-degree horizontal FOV images at the same timestamp.", "For 3D detection task, the main metrics are mean Average Precision (mAP) and nuScenes detection score (NDS mAP defines a match by considering the 2D center distance thresholds of 0.5m, 1m, 2m, 4m across ten classes on the ground plane.", "NDS is a weighted average of mAP and other attribute metrics, including translation(mATE), scale(mASE), orientation(mAOE), velocity(mAVE), and attribute(mAAE)."], "citing_paper_content": {"title": "Crossfusion: Interleaving Cross-Modal Complementation For Noise-Resistant 3D Object Detection", "abstract": "The combination of LiDAR and camera modalities is proven to be necessary and typical for 3D object detection according to recent studies. Existing fusion strategies tend to overly rely on the LiDAR modal in essence, which exploits the abundant semantics from the camera sensor insufficiently. However, existing methods cannot rely on information from other modalities because the corruption of Li-DAR features results in a large domain gap. Following this, we propose CrossFusion, a more robust and noise-resistant scheme that makes full use of the camera and LiDAR features with the designed cross-modal complementation strategy. Extensive experiments we conducted show that our method not only outperforms the state-of-the-art methods under the setting without introducing an extra depth estimation network but also demonstrates our model's noise resistance without retraining for the specific malfunction scenarios by increasing 5.2% mAP and 2.4% NDS."}, "cited_paper_content": {"title": "Nuscenes: A Multimodal Dataset For Autonomous Driving", "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first published dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online at this http URL."}, "keywords": ["3D object annotations", "largescale autonomous-driving dataset"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section, we first compare state-of-the-art methods on nuScenes, which is the first published dataset to carry the full autonomous vehicle sensor suite, including 6 cameras, 5 radars, and 1 lidar, all with full 360 degree field of view, and has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. Then we designed three experiments with LiDAR corruption to show the modal complementarity of CrossFusion at low-quality LiDAR features. Afterward, we conduct extensive ablation experiments to demonstrate the rationality and importance of each key component of our CrossFusion. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide additional context about the dataset used in the manuscript, we compare our results with state-of-the-art methods on nuScenes dataset, which is the first published dataset to carry the full autonomous vehicle sensor suite, including LiDAR and camera modalities, with full 360-degree field of view, and has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To demonstrate the effectiveness of our proposed CrossFusion, we compare our method with state-of-the-art methods on nuScenes, a largescale autonomous-driving dataset with comprehensive 3D object annotations. #REFR", "given_citation_intent": "background", "given_keywords": ["3D object annotations", "largescale autonomous-driving dataset"]}]}
{"citing_id": "2303.00411v2", "cited_id": "1312.5185", "section_title": "Convergence Rates For Abstract Wave Equations", "citation": "In conclusion, we obtain the following generalisation of #REFR Cor. 4.3] to contractive time discretisation schemes.", "text_before_citation": ["2 \u2264 2 \uf8eb \uf8ed \u221e j=1 \u03bb \u2212 \u03b5+1 2 j \uf8f6 \uf8f8 g(\u2022, u(\u2022)) 2 V \u2264 2L 2 c \u03b5 (|O| 1/2 + u V ) 2 .", "Hence, G satisfies the linear growth condition of Assumption 7.1 with \u03b4 = 1\u2212\u03b5 2 .", "Repeating the arguments for \u039b \u22121/2 [G(u 1 ) \u2212 G(u 2 )] and using c 1 = \u03c0 2 /6 results in", "\u039b \u22121/2 [G(u 1 ) \u2212 G(u 2 )] 2 L2(V ) \u2264 2 \uf8eb \uf8ed \u221e j=1 1 \u03c0 2 j 2 \uf8f6 \uf8f8 g(\u2022, u 1 (\u2022)) \u2212 g(\u2022, u 2 (\u2022)) 2 V \u2264 L 2 3 u 1 \u2212 u 2 2 V .", "The nonlinearity F was already considered in Subsection 7.4."], "text_after_citation": ["Theorem 7.11 (Wave equation with white noise).", "Let O = (0, 1), V := L 2 (O), X := V \u00d7 V \u22121 , p \u2208 [2, \u221e), and 0 < \u03b1 \u2264 \u03b4 < 1/2.", "Suppose that (u 0 , v 0 ) \u2208 L p F0 (\u2126; X \u03b4 ).", "Let F and G be Nemytskij operators as above with \u03c6 and \u03c8 satisfying (7.14) .", "Suppose the covariance operator Q = I on L 2 (O). Let Y = X \u03b4 ."], "citing_paper_content": {"title": "Pathwise Uniform Convergence Of Time Discretisation Schemes For Spdes", "abstract": "In this paper we prove convergence rates for time discretisation schemes for semilinear stochastic evolution equations with additive or multiplicative Gaussian noise, where the leading operator A is the generator of a strongly continuous semigroup S on a Hilbert space X, and the focus is on non-parabolic problems. The main results are optimal bounds for the uniform strong error E \u221e k := E sup j\u2208{0,...,N k } U (t j) \u2212 U j p 1/p , where p \u2208 [2, \u221e), U is the mild solution, U j is obtained from a time discretisation scheme, k is the step size, and N k = T /k. The usual schemes such as splitting/exponential Euler, implicit Euler, and Crank-Nicolson, etc. are included as special cases. Under conditions on the nonlinearity and the noise we show \u2022 E \u221e k k log(T /k) (linear equation, additive noise, general S); \u2022 E \u221e k \u221a k log(T /k) (nonlinear equation, multiplicative noise, contractive S); \u2022 E \u221e k k log(T /k) (nonlinear wave equation, multiplicative noise). The logarithmic factor can be removed if the splitting scheme is used with a (quasi)-contractive S. The obtained bounds coincide with the optimal bounds for SDEs. Most of the existing literature is concerned with bounds for the simpler pointwise strong error E k := sup j\u2208{0,...,N k } E U (t j) \u2212 U j p 1/p. Applications to Maxwell equations, Schr\u00f6dinger equations, and wave equations are included. For these equations our results improve and reprove several existing results with a unified method."}, "cited_paper_content": {"title": "An Exponential Integrator Scheme For Time Discretization Of Nonlinear Stochastic Wave Equation", "abstract": "This work is devoted to convergence analysis of an exponential integrator scheme for semi-discretization in time of nonlinear stochastic wave equation. A unified framework is first set forth, which covers important cases of additive and multiplicative noises. Within this framework, the proposed scheme is shown to converge uniformly in the strong $L^p$-sense with precise convergence rates given. The abstract results are then applied to several concrete examples. Further, weak convergence rates of the scheme are examined for the case of additive noise. To analyze the weak error for the nonlinear case, techniques based on the Malliavin calculus were usually exploited in the literature. Under certain appropriate assumptions on the nonlinearity, this paper provides a weak error analysis, which does not rely on the Malliavin calculus. The rates of weak convergence can, as expected, be improved in comparison with the strong rates. Both strong and weak convergence results obtained here show that the proposed method achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations. Numerical results are finally reported to confirm our theoretical findings."}, "keywords": ["contractive time discretisation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed method achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations, as shown in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The paper proposes a contractive time discretisation scheme that achieves higher convergence rates than the implicit Euler and Crank-Nicolson time discretizations, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["contractive time discretisation"]}]}
{"citing_id": "2303.05186v1", "cited_id": "1805.04514", "section_title": "Related Work", "citation": "The recent movement is to combine incremental Delta-Bar-Delta method and Temporal-difference learning #REFR .", "text_before_citation": ["HPO in the RL traditionally used a Delta-Bar-Delta method as incremental algorithms to tune parameters #OTHEREFR .", "However, this method and its variations were limited to linear supervised learning."], "text_after_citation": ["Those methods can not make tuning hyperparameters online and allow the algorithm to more robustly adjust to non-stationarity in a problem at the same time.", "A variety of techniques exist to combat this recentlymost notably use of a large experience replay buffers or the use of multiple parallel actors.", "These techniques come at the cost of moving away from the online RL problem as it is traditionally formulated.", "More sophisticated approaches include Self-Tuning Actor Critic (STAC) #OTHEREFR and Sequential Model-based Bayesian optimisation (SMBO) #OTHEREFR .", "However both methods ignored a crucial issue for RL: the possible non-stationarity of the RL problem induces a possible non-stationarity of the hyperparameters."], "citing_paper_content": {"title": "A Framework For History-Aware Hyperparameter Optimisation In Reinforcement Learning", "abstract": "A Reinforcement Learning (RL) system depends on a set of initial conditions (hyperparameters) that affect the system's performance. However, defining a good choice of hyperparameters is a challenging problem. Hyperparameter tuning often requires manual or automated searches to find optimal values. Nonetheless, a noticeable limitation is the high cost of algorithm evaluation for complex models, making the tuning process computationally expensive and time-consuming. In this paper, we propose a framework based on integrating complex event processing and temporal models, to alleviate these trade-offs. Through this combination, it is possible to gain insights about a running RL system efficiently and unobtrusively based on data stream monitoring and to create abstract representations that allow reasoning about the historical behaviour of the RL system. The obtained knowledge is exploited to provide feedback to the RL system for optimising its hyperparameters while making effective use of parallel resources. We introduce a novel history-aware epsilon-greedy logic for hyperparameter optimisation that instead of using static hyperparameters that are kept fixed for the whole training, adjusts the hyperparameters at runtime based on the analysis of the agent's performance over time windows in a single agent's lifetime. We tested the proposed approach in a 5G mobile communications case study that uses DQN, a variant of RL, for its decision-making. Our experiments demonstrated the effects of hyperparameter tuning using history on training stability and reward values. The encouraging results show that the proposed history-aware framework significantly improved performance compared to traditional hyperparameter tuning approaches."}, "cited_paper_content": {"title": "Metatrace: Online Step-Size Tuning By Meta-Gradient Descent For Reinforcement Learning Control", "abstract": "Reinforcement learning (RL) has had many successes in both \"deep\" and \"shallow\" settings. In both cases, significant hyperparameter tuning is often required to achieve good performance. Furthermore, when nonlinear function approximation is used, non-stationarity in the state representation can lead to learning instability. A variety of techniques exist to combat this --- most notably large experience replay buffers or the use of multiple parallel actors. These techniques come at the cost of moving away from the online RL problem as it is traditionally formulated (i.e., a single agent learning online without maintaining a large database of training examples). Meta-learning can potentially help with both these issues by tuning hyperparameters online and allowing the algorithm to more robustly adjust to non-stationarity in a problem. This paper applies meta-gradient descent to derive a set of step-size tuning algorithms specifically for online RL control with eligibility traces. Our novel technique, Metatrace, makes use of an eligibility trace analogous to methods like $TD(\\lambda)$. We explore tuning both a single scalar step-size and a separate step-size for each learned parameter. We evaluate Metatrace first for control with linear function approximation in the classic mountain car problem and then in a noisy, non-stationary version. Finally, we apply Metatrace for control with nonlinear function approximation in 5 games in the Arcade Learning Environment where we explore how it impacts learning speed and robustness to initial step-size choice. Results show that the meta-step-size parameter of Metatrace is easy to set, Metatrace can speed learning, and Metatrace can allow an RL algorithm to deal with non-stationarity in the learning task."}, "keywords": ["Temporal-difference learning"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "HPO in the RL traditionally used a Delta-Bar-Delta method as incremental algorithms to tune parameters, but this method and its variations were limited to linear supervised learning. Recently, meta-learning has been proposed to tune hyperparameters online and allow the algorithm to adjust to non-stationarity in a problem. For instance, Metatrace is a novel technique that applies meta-gradient descent to derive a set of step-size tuning algorithms specifically for online RL control with eligibility traces #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of hyperparameter tuning in online RL, Metatrace, a novel technique that applies meta-gradient descent to derive a set of step-size tuning algorithms specifically for online RL control with eligibility traces, has been proposed #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of hyperparameter tuning in online RL, Metatrace, a novel technique based on meta-gradient descent, was proposed to tune step-size parameters for temporal-difference learning #REFR.", "given_citation_intent": "method", "given_keywords": ["Temporal-difference learning"]}]}
{"citing_id": "2303.09681v1", "cited_id": "1706.03762", "section_title": "Introduction", "citation": "In the spiking transformer, the attention score is normalized Hamming similarity between binary spike vectors, which has been proven in Proposition 1 to be equivalent to inner product similarity between real valued vectors used in the original transformer #REFR .", "text_before_citation": ["In this work, we tackle a new problem of tracking 3D human poses from only events using SNNs. Specifically, our approach, shown in Fig.", "2 , is an end-to-end model that estimates parametric human poses over time solely from an event stream, without any requirement of dense images.", "This model is entirely built upon SNNs, providing significantly greater efficiency than ANNs.", "After preprocessing the input event stream to be a sequence of event voxel grids with the same time interval, Spike-Element-Wise (SEW) ResNet #OTHEREFR is employed as the SNNs backbone to extract pose spike features.", "This is followed by our proposed spiking spatiotemporal transformer for the bi-directional fusion of pose spike features, allowing for the compensation of missing pose information, especially for early time steps."], "text_after_citation": ["The final step is to apply 2D average pooling to the spatiotemporally aggregated spike features and then directly regress the parametric human poses over time.", "The experiments show that our proposed SNNs achieve competitive or even better performance of pose tracking than several ANNs (i.e.", "ResNet-GRU #OTHEREFR , ResNet-TF #OTHEREFR and Video-Swin #OTHEREFR ), but requires only about 20% of FLOPs, which shows promising prospects of efficient SNNs on various event-based vision tasks.", "Additionally, we provide a large-scale synthetic dataset for event-based 3D human pose tracking.", "Although an inhouse event-based motion capture dataset, MMHPSD, is introduced in #OTHEREFR , its limited variety of motions restricts the generalization ability of trained models."], "citing_paper_content": {"title": "Event-Based Human Pose Tracking By Spiking Spatiotemporal Transformer", "abstract": "Event camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static image frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. dense deep learning) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser. Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end sparse deep learning approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and our proposed spiking spatiotemporal transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate the superiority of our approach in both performance and efficiency measures. For example, with comparable performance to the state-of-the-art ANNs counterparts, our approach achieves a computation reduction of 20% in FLOPS. Our implementation is made available at https://github.com/JimmyZou/HumanPoseTracking SNN and dataset will be released upon paper acceptance."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["binary spike vectors", "attention score"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To extract pose spike features, we employ Spike-Element-Wise (SEW) ResNet #REFR as the SNNs backbone, followed by our proposed spiking spatiotemporal transformer for the bi-directional fusion of pose spike features, allowing for the compensation of missing pose information, especially for early time steps.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To the best of our knowledge, this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need for accessing any frame-based images as part of input, and our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which provides significantly greater efficiency than ANNs #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the sparsity of event camera data, the authors propose a spiking spatiotemporal transformer that compensates for missing pose information, which is based on the attention mechanism introduced in #REFR.", "given_citation_intent": "background", "given_keywords": ["binary spike vectors", "attention score"]}]}
{"citing_id": "2304.06414v1", "cited_id": "1907.10597", "section_title": "C. Where To Train A Model?", "citation": "The vast majority of AI studies consider accuracy as the pertinent performance measure, ignoring the impact ML has on deployed devices and the environment #REFR .", "text_before_citation": ["This is ideal in the cases model training simply cannot wait.", "Based on these estimates, the same CNN model will be trained in Sweden with a carbon footprint of just 0.21-0.24kg, no matter the time throughout the day.", "This footprint is equivalent to a 95% reduction to ML training, even, midday in Cyprus.", "Observation: The ML model training process can have a significantly different environmental footprint over different locations.", "Therefore, organisations wanting to reduce the environmental impact of their model training should explore potential advantages of moving their workloads to low-carbon grid energy."], "text_after_citation": ["Towards this, let us consider three state-of-the-art model architectures for ML classification that can be used for object detection.", "Specifically, we will consider ResNet50 (used in Sections III-B and III-C), SqueezeNet and MobileNetV2.", "We now focus on ML inference and its impact to the reference edge micro-DC.", "For this, we use binary builds from the TensorFlow model zoo for the adopted architectures trained using the reference dataset #OTHEREFR . Table I depicts the results.", "We immediately observe the high accuracy of ResNet50, but through the additional metrics we can also label it as both energy-hungry and latent in terms of inference."], "citing_paper_content": {"title": "Towards Energy Consumption And Carbon Footprint Testing For Ai-Driven Iot Services", "abstract": "Energy consumption and carbon emissions are expected to be crucial factors for Internet of Things (IoT) applications. Both the scale and the geo-distribution keep increasing, while Artificial Intelligence (AI) further penetrates the \"edge\" in order to satisfy the need for highly-responsive and intelligent services. To date, several edge/fog emulators are catering for IoT testing by supporting the deployment and execution of AI-driven IoT services in consolidated test environments. These tools enable the configuration of infrastructures so that they closely resemble edge devices and IoT networks. However, energy consumption and carbon emissions estimations during the testing of AI services are still missing from the current state of IoT testing suites. This study highlights important questions that developers of AIdriven IoT services are in need of answers, along with a set of observations and challenges, aiming to help researchers designing IoT testing and benchmarking suites to cater to user needs."}, "cited_paper_content": {"title": "Green Ai", "abstract": "The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or \"price tag\" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI."}, "keywords": ["AI studies"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The financial cost of deep learning computations can make it difficult for academics, students, and researchers, particularly those from emerging economies, to engage in deep learning research. As proposed in the position paper on Green AI by Allen Institute for AI, making efficiency an evaluation criterion for research alongside accuracy and related measures, and reporting the financial cost or \"price tag\" of developing, training, and running models can provide baselines for the investigation of increasingly efficient methods, ultimately making AI both greener and more inclusive. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The environmental impact of model training can be significantly reduced by exploring potential advantages of moving workloads to low-carbon grid energy, as advocated by the authors of Green AI #REFR, who propose making efficiency an evaluation criterion for research alongside accuracy and related measures.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the position paper on Green AI, making efficiency an evaluation criterion for research alongside accuracy and related measures is a practical solution to reduce the carbon footprint of AI studies, which can be a significant environmental concern for organizations wanting to reduce the environmental impact of their model training. #REFR", "given_citation_intent": "background", "given_keywords": ["AI studies"]}]}
{"citing_id": "2303.12481v1", "cited_id": "1811.09716", "section_title": "Another Variants Of Aa++", "citation": "A function's curvature is often expressed as the norm of the Hessian at a particular point #REFR .", "text_before_citation": ["12.1.", "Analysis of max-pooling from the perspective of curvature and Hessian norms.", "Here, we take a step further and investigate why maxpooling impacts the robustness of models.", "In order to perform this analysis, we analyze gradient norms, Hessian norms, and the model's curvature.", "The curvature of a point is a mathematical quantity that indicates the degree of nonlinearity."], "text_after_citation": ["Mainly, robust models are characterized by low gradient norms #OTHEREFR , implying smaller Hessian norms.", "In order to investigate robustness independent of non-linearity, (Srinivas et al.) propose normalized curvature, which normalizes the Hessian norm by its corresponding gradient norm.", "They defined normalized curvature for a neural network classifier f as C f (x) = \u2207 2 f (x) 2 /( \u2207f (x) 2 + \u03b5).", "Where \u2207f (x) 2 and \u2207 2 f (x) 2 are the 2 -norm of the gradient and the spectral norm of the Hessian, respectively, where \u2207f (x) \u2208 R d , \u2207 2 f (x) \u2208 R d\u00d7d , and \u03b5 > 0 is a small constant to ensure the proper behavior of the measure. We use this metric to conduct our experiments.", "13."], "citing_paper_content": {"title": "Revisiting Deepfool: Generalization And Improvement", "abstract": "Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal 2 adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known Deep-Fool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal 2 adversarial perturbations 1 ."}, "cited_paper_content": {"title": "Robustness Via Curvature Regularization, And Vice Versa", "abstract": "State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more \"linear\" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness."}, "keywords": ["Hessian", "function's curvature"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To investigate the impact of max-pooling on model robustness, the authors of this manuscript analyzed gradient norms, Hessian norms, and the model's curvature. The proposed regularizer in #REFR confirms the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries, and show that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, which confirms the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript investigate the impact of max-pooling on the robustness of models by analyzing gradient norms, Hessian norms, and the function's curvature, which is a mathematical quantity that indicates the degree of nonlinearity. This analysis is further supported by the work of #REFR, which shows that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more \"linear\" behavior of the network.", "given_citation_intent": "background", "given_keywords": ["Hessian", "function's curvature"]}]}
{"citing_id": "2304.07527v1", "cited_id": "1807.11590", "section_title": "Introduction", "citation": "To address it, IoU-Net #REFR proposes an individual IoU prediction branch and an IoU-guided NMS to align the classification confidence and regression precision.", "text_before_citation": ["1a shows an empirical study on a strong baseline DINO #OTHEREFR , and the recall of BR samples by the well-trained model on the top-k confident outputs in an image is calculated, where a higher recall indicates that more BR samples are selected in final prediction.", "As we can see, only 45% and 48% BR samples are covered by HC samples with top-N and top-2N scores, respectively, suggesting that more than half of the well-localized predictions have low confidence scores. In Fig.", "1b , the frequency histogram of the HC and BR samples is plotted from 5,000 samples on COCO val.", "and a clear discrepancy between the two distributions is observed, revealing that even the highly-optimized model DINO #OTHEREFR suffers from the misalignment problem.", "In fact, this problem also appears in CNN-based detectors #OTHEREFR ."], "text_after_citation": ["A number of alternatives #OTHEREFR introduce an IoU-aware loss or weight to integrate the IoU branch into the original classification branch and adopt a joint training strategy.", "These methods are specially designed for NMS-based detectors, but DETR implicitly selects samples by modeling query relations under the guidance of one-to-one matching without an explicit query selection step like NMS, making them less applicable to DETR #OTHEREFR .", "To the best of our knowledge, the misalignment problem still remains unexplored in the DETR series.", "This paper investigates the solution to the misalignment problem in DETR.", "To this end, we propose a novel method, namely Align-DETR."], "citing_paper_content": {"title": "Align-Detr: Improving Detr With Simple Iou-Aware Bce Loss", "abstract": "DETR has set up a simple end-to-end pipeline for object detection by formulating this task as a set prediction problem, showing promising potential. However, despite the significant progress in improving DETR, this paper identifies a problem of misalignment in the output distribution, which prevents the best-regressed samples from being assigned with high confidence, hindering the model's accuracy. We propose a metric, recall of best-regressed samples, to quantitively evaluate the misalignment problem. Observing its importance, we propose a novel Align-DETR that incorporates a localization precision aware classification loss in optimization. The proposed loss, IA-BCE, guides the training of DETR to build a strong correlation between classification score and localization precision. We also adopt the mixed-matching strategy, to facilitate DETR-based detectors with faster training convergence while keeping an end-to-end scheme. Moreover, to overcome the dramatic decrease in sample quality induced by the sparsity of queries, we introduce a prime sample weighting mechanism to suppress the interference of unimportant samples. Extensive experiments are conducted with very competitive results reported. In particular, it delivers a 46 (+3.8)% AP on the DAB-DETR baseline with the ResNet-50 backbone and reaches a new SOTA performance of 50.2% AP in the 1x setting on the COCO validation set when employing the strong baseline DINO. Our code is available at https://github.com/FelixCaae/AlignDETR."}, "cited_paper_content": {"title": "Acquisition Of Localization Confidence For Accurate Object Detection", "abstract": "Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. In the paper we propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective. Extensive experiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well as its compatibility with and adaptivity to several state-of-the-art object detectors."}, "keywords": ["classification confidence"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In fact, this problem also appears in CNN-based detectors (REFERENCE PAPER).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the misalignment problem in object detection, Align-DETR proposes a novel IA-BCE loss that guides the training of DETR to build a strong correlation between classification score and localization precision, which is inspired by the IoU-Net learning method proposed in the paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This misalignment problem in object detection, where properly localized bounding boxes are degenerated or suppressed during iterative regression or NMS due to the absence of localization confidence, has been addressed by IoU-Net learning in #REFR, which predicts the IoU between each detected bounding box and the matched ground-truth to acquire the confidence of localization and improve the NMS procedure by preserving accurately localized bounding boxes.", "given_citation_intent": "method", "given_keywords": ["classification confidence"]}]}
{"citing_id": "2303.00914v2", "cited_id": "1206.5538", "section_title": "Challenges In Fully Test-Time Uda", "citation": "From the perspective of machine learning, early representations through the lower layer play an important role to capture the pos-terior distribution of the underlying explanatory factors for the observed input #REFR .", "text_before_citation": ["We recognize that most of the domain variations, such as changes in the visual scenes and image transformations or corruptions, are early layers of features in the semantic hierarchy #OTHEREFR .", "They can be effectively captured and modeled by lower layers of the network model."], "text_after_citation": ["For instance, in deep neural network models, the early layers of the network tend to respond to corners, edges, or colors.", "In contrast, deeper layers respond to more class-specific features #OTHEREFR .", "In the corruption testtime adaptation scenario, the class-specific features are always the same because the testing datasets are the corruption of the training domain.", "However, the early layers of models can be failed due to corruption.", "Therefore, the central challenge in fully test-time UDA lies in how to learn useful early layer representations of the test samples without supervision."], "citing_paper_content": {"title": "Neuro-Modulated Hebbian Learning For Fully Test-Time Adaptation", "abstract": "Fully test-time adaptation aims to adapt the network model based on sequential analysis of input samples during the inference stage to address the cross-domain performance degradation problem of deep neural networks. We take inspiration from the biological plausibility learning where the neuron responses are tuned based on a local synapse-change procedure and activated by competitive lateral inhibition rules. Based on these feed-forward learning rules, we design a soft Hebbian learning process which provides an unsupervised and effective mechanism for online adaptation. We observe that the performance of this feed-forward Hebbian learning for fully test-time adaptation can be significantly improved by incorporating a feedback neuro-modulation layer. It is able to fine-tune the neuron responses based on the external feedback generated by the error back-propagation from the top inference layers. This leads to our proposed neuro-modulated Hebbian learning (NHL) method for fully test-time adaptation. With the unsupervised feed-forward soft Hebbian learning being combined with a learned neuro-modulator to capture feedback from external responses, the source model can be effectively adapted during the testing process. Experimental results on benchmark datasets demonstrate that our proposed method can significantly improve the adaptation performance of network models and outperforms existing stateof-the-art methods."}, "cited_paper_content": {"title": "Representation Learning: A Review And New Perspectives", "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."}, "keywords": ["machine learning"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We recognize that most of the domain variations, such as changes in the visual scenes and image transformations or corruptions, are early layers of features in the semantic hierarchy (Representation Learning: A Review And New Perspectives). They can be effectively captured and modeled by lower layers of the network model.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To effectively capture and model the early layers of features in the semantic hierarchy, lower layers of the network model can be used, as well as unsupervised feature learning and deep learning algorithms #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Machine learning algorithms generally depend on data representation, and different representations can entangle and hide more or less the different explanatory factors of variation behind the data, which motivates the design of more powerful representation-learning algorithms implementing such priors #REFR.", "given_citation_intent": "background", "given_keywords": ["machine learning"]}]}
{"citing_id": "2303.12930v1", "cited_id": "1803.08842", "section_title": "Results And Analysis", "citation": "Even though it is proved in #REFR that motion features are useless for audio-visual event localization, we argue that our experiment clearly demonstrates their significance for dense-localizing audio-visual events.", "text_before_citation": ["In addition, we found that the appropriate number of uni-modal blocks is also impor- tant, which reveals that applying self-attention before crossmodal interaction can help the model to focus on informative signals and eliminate noise from each modality. Dependency Modeling and Class-Aware Regression. As shown in Tab.", "4, applying temporal dependency modeling and class-aware regression separately can both achieve higher performances than the base model that just contains our transformer encoder with a class-agnostic regression head in the decoder.", "Besides, we found that when using both of them, they can promote each other and achieve a further significant performance boost, which clearly demonstrates their effectiveness. The Impact of Motion Features. In Tab.", "5, we observe that utilizing both RGB and optical flow features extracted by I3D #OTHEREFR achieves the best performance.", "It outperforms the model that uses visual features extracted by ResNet50 #OTHEREFR pre-trained on ImageNet by a large margin (+3.5% at the average mAP)."], "text_after_citation": ["The Capability of Localizing Concurrent Events.", "We further evaluate our models and the state-of-the-art TAL method #OTHEREFR on the videos that contain concurrent events with different overlap rates in Fig. 6 .", "We observe that our model equipped with dependency modeling and class-aware regression obviously gains more performance improvement on the videos with higher overlap rates, compared with our baseline and ActionFormer #OTHEREFR .", "It suggests that our model has a better ability to localize overlapping audiovisual events in untrimmed videos. Qualitative Results. In Fig.", "5 , we present the qualitative results of our model variants that utilize different modalities as input."], "citing_paper_content": {"title": "Dense-Localizing Audio-Visual Events In Untrimmed Videos: A Large-Scale Benchmark And Baseline", "abstract": "Existing audiovisual event localization (AVE) handles manually trimmed videos with only a single instance in each of them. However, this setting is unrealistic as natural videos often contain numerous audiovisual events with different categories. To better adapt to real-life applications, in this paper we focus on the task of denselocalizing audiovisual events, which aims to jointly localize and recognize all audiovisual events occurring in an untrimmed video. The problem is challenging as it requires fine-grained audiovisual scene and context understanding. To tackle this problem, we introduce the first Untrimmed AudioVisual (UnAV-100) dataset, which contains 10K untrimmed videos with over 30K audiovisual events. Each video has 2.8 audiovisual events on average, and the events are usually related to each other and might co-occur as in real-life scenes. Next, we formulate the task using a new learning-based framework, which is capable of fully integrating audio and visual modalities to localize audiovisual events with various lengths and capture dependencies between them in a single pass. Extensive experiments demonstrate the effectiveness of our method as well as the significance of multi-scale cross-modal perception and dependency modeling for this task."}, "cited_paper_content": {"title": "Audio-Visual Event Localization In Unconstrained Videos", "abstract": "In this paper, we introduce a novel problem of audio-visual event localization in unconstrained videos. We define an audio-visual event as an event that is both visible and audible in a video segment. We collect an Audio-Visual Event (AVE) dataset to systemically investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization. We develop an audio-guided visual attention mechanism to explore audio-visual correlations, propose a dual multimodal residual network (DMRN) to fuse information over the two modalities, and introduce an audio-visual distance learning network to handle the cross-modality localization. Our experiments support the following findings: joint modeling of auditory and visual modalities outperforms independent modeling, the learned attention can capture semantics of sounding objects, temporal alignment is important for audio-visual fusion, the proposed DMRN is effective in fusing audio-visual features, and strong correlations between the two modalities enable cross-modality localization."}, "keywords": ["dense-localizing audio-visual events", "audio-visual event localization"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To better adapt to real-life applications, the authors of this manuscript introduce the first Untrimmed AudioVisual (UnAV-100) dataset, which contains over 30K audiovisual events in 10K untrimmed videos, and propose a learning-based framework capable of fully integrating audio and visual modalities to localize audiovisual events with various lengths and capture dependencies between them in a single pass (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better adapt to real-life applications, the authors introduce the first Untrimmed AudioVisual (UnAV-100) dataset, which contains over 30K audio-visual events occurring in untrimmed videos, and propose a learning-based framework that fully integrates audio and visual modalities to localize audio-visual events with various lengths and capture dependencies between them in a single pass #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better adapt to real-life applications, the authors of this manuscript focus on the task of dense-localizing audio-visual events, which aims to jointly localize and recognize all audio-visual events occurring in an untrimmed video, and they introduce the first Untrimmed AudioVisual (UnAV-100) dataset for this purpose. This task is challenging as it requires fine-grained audio-visual scene and context understanding, and the authors of the reference paper \"#REFR\" propose an audio-guided visual attention mechanism to explore audio-visual correlations, a dual multimodal residual network (DMRN) to fuse information over the two modalities, and an audio-visual distance learning network to handle the cross-modality localization, which can be useful for this task.", "given_citation_intent": "background", "given_keywords": ["dense-localizing audio-visual events", "audio-visual event localization"]}]}
{"citing_id": "2304.09623v2", "cited_id": "1912.03699", "section_title": "Effect Of Mcc And Transport Losses", "citation": "In our ablation studies, we observe that CHATTY, when combined with MCC #REFR gives the best result on most of the domain adaptation tasks.", "text_before_citation": ["The sensitivity is reduced by replacing L T L by L T L Cos which normalizes every entry in the matrix Y by the product of the norms.", "Empirically, the best adaptation algorithm was seen to be the loss given by Equation 18.", "When the loss was modified to Equation 20 it was observed that the convergence was slower than when the class confusion loss was included separately in the total loss.", "This could be possibly attributed to the back-propagation updates of M being very slow, owing to the camouflaged nature of M in T 1 MT 2 T .", "Nevertheless, it is interesting to study an optimal choice of M."], "text_after_citation": ["The evolution of the target classifier accuracy on the domain adaptation from Ar to Cl is plotted in Figure 4 .", "We observe that by shifting the samples in the classification (logit) space itself produces a significantly improved accuracy compared to SOTA methods, as shown in Table 1 , and tuning the transport vectors increases the accuracy.", "Furthermore, using the MCC loss in synergy with the transport loss favoured accuracy even more.", "When using just the transport loss as a means of reducing class confusion, we observe that it does a better job than the MCC loss."], "citing_paper_content": {"title": "Chatty: Coupled Holistic Adversarial Transport Terms With Yield For Unsupervised Domain Adaptation", "abstract": "We propose a new technique called CHATTY: Coupled Holistic Adversarial Transport Terms with Yield for Unsupervised Domain Adaptation. Adversarial training is commonly used for learning domain-invariant representations by reversing the gradients from a domain discriminator head to train the feature extractor layers of a neural network. We propose significant modifications to the adversarial head, its training objective, and the classifier head. With the aim of reducing class confusion, we introduce a sub-network which displaces the classifier outputs of the source and target domain samples in a learnable manner. We control this movement using a novel transport loss that spreads class clusters away from each other and makes it easier for the classifier to find the decision boundaries for both the source and target domains. The results of adding this new loss to a careful selection of previously proposed losses leads to improvement in UDA results compared to the previous state-of-the-art methods on benchmark datasets. We show the importance of the proposed loss term using ablation studies and visualization of the movement of target domain sample in representation space."}, "cited_paper_content": {"title": "Minimum Class Confusion For Versatile Domain Adaptation", "abstract": "There are a variety of DA scenarios subject to label sets and domain configurations, including closed-set and partial-set DA, as well as multi-source and multi-target DA. It is notable that existing DA methods are generally designed only for a specific scenario, and may underperform for scenarios they are not tailored to. A versatile method, which can handle several different scenarios without any extra modifications, is still remained to be explored. Towards such purpose, a more general inductive bias other than the domain alignment should be explored. In this paper, we delve into a missing piece of existing methods: class confusion, the tendency that a classifier confuses the predictions between the correct and ambiguous classes for target examples, which exists in all of the scenarios above. We unveil that reducing such pair-wise class confusion brings about significant transfer gains. Based on this, we propose a general loss function: Minimum Class Confusion (MCC). It can be characterized by (1) a non-adversarial DA method without explicitly deploying domain alignment, enjoying fast convergence speed (about 3 times faster than mainstream adversarial methods); (2) a versatile approach that can handle the four existing scenarios: Closed-Set, Partial-Set, Multi-Source, and Multi-Target DA, outperforming the state-of-the-art methods in these scenarios, especially on the largest and hardest dataset to date (7.25% on DomainNet). Strong performance in the two scenarios proposed in this paper: Multi-Source Partial and Multi-Target Partial DA, further proves its versatility. In addition, it can also be used as a general regularizer that is orthogonal and complementary to a variety of existing DA methods, accelerating convergence and pushing those readily competitive methods to a stronger level."}, "keywords": ["domain adaptation tasks"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "When the loss was modified to Equation 20, it was observed that the convergence was slower than when the class confusion loss was included separately in the total loss, possibly due to the slow back-propagation updates of M. Nevertheless, an optimal choice of M is interesting to study #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The addition of the class confusion loss separately in the total loss was observed to improve convergence compared to when the loss was modified, as reported in the results of #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in this manuscript shows promising results in reducing class confusion in various #KEYWORDS(domain adaptation tasks), which is in line with the findings of the Minimum Class Confusion (MCC) method proposed in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["domain adaptation tasks"]}]}
{"citing_id": "2303.15023v1", "cited_id": "1905.02249", "section_title": "Related Work", "citation": "MixMatch #REFR combines the consistency regularization with the entropy minimization to encourage the network to output confident predictions for unlabeled data.", "text_before_citation": ["In view of this, we focus on learning from scarce annotations to minimize human labor and achieve accurate animal pose estimation at the same time.", "Semi-supervised Learning.", "Semi-supervised learning is powerful in leveraging unlabeled data to improve a model's performance when labeled data is limited.", "One of the most widely used SSL techniques is pseudo labeling #OTHEREFR , which generates artificial labels for unlabeled images from model predictions.", "Another technique is consistency regularization #OTHEREFR , which enforces that the model output should be consistent when the input is randomly perturbed."], "text_after_citation": ["More recent Unsupervised Data Augmentation (UDA) #OTHEREFR and FixMatch #OTHEREFR combine pseudo labeling, consistency regularization with strong augmentations #OTHEREFR and achieve superior performance.", "FlexMatch #OTHEREFR further improves over UDA and Fixmatch by introducing a curriculum learning scheme.", "We also adopt the PL-based techniques in this work given its effectiveness.", "To mitigate the negative effect of noisy pseudo labels, we combine PL with reliable pseudo label selection and reusable sample re-labeling.", "Learning from Noisy Labels."], "citing_paper_content": {"title": "Scarcenet: Animal Pose Estimation With Scarce Annotations", "abstract": "Animal pose estimation is an important but underexplored task due to the lack of labeled data. In this paper, we tackle the task of animal pose estimation with scarce annotations, where only a small set of labeled data and unlabeled images are available. At the core of the solution to this problem setting is the use of the unlabeled data to compensate for the lack of well-labeled animal pose data. To this end, we propose the ScarceNet, a pseudo label-based approach to generate artificial labels for the unlabeled images. The pseudo labels, which are generated with a model trained with the small set of labeled images, are generally noisy and can hurt the performance when directly used for training. To solve this problem, we first use a small-loss trick to select reliable pseudo labels. Although effective, the selection process is improvident since numerous highloss samples are left unused. We further propose to identify reusable samples from the high-loss samples based on an agreement check. Pseudo labels are regenerated to provide supervision for those reusable samples. Lastly, we introduce a student-teacher framework to enforce a consistency constraint since there are still samples that are neither reliable nor reusable. By combining the reliable pseudo label selection with the reusable sample re-labeling and the consistency constraint, we can make full use of the unlabeled data. We evaluate our approach on the challenging AP-10K dataset, where our approach outperforms existing semi-supervised approaches by a large margin. We also test on the TigDog dataset, where our approach can achieve better performance than domain adaptation based approaches when only very few annotations are available. Our code is available at the project website 1 ."}, "cited_paper_content": {"title": "Mixmatch: A Holistic Approach To Semi-Supervised Learning", "abstract": "Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success."}, "keywords": ["unlabeled data"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Semi-supervised learning techniques such as pseudo labeling and consistency regularization have been widely used to improve model performance when labeled data is limited. One recent approach, MixMatch, unifies these techniques and has achieved state-of-the-art results on various datasets #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To leverage the unlabeled data and improve the performance of animal pose estimation with scarce annotations, ScarceNet uses pseudo labeling #REFR, which generates artificial labels for unlabeled images from model predictions, and MixMatch, a holistic approach to semi-supervised learning, to mix labeled and unlabeled data and guess low-entropy labels for data-augmented unlabeled examples.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "MixMatch is a powerful algorithm that leverages unlabeled data to improve model performance, which is relevant to our approach of learning from scarce annotations and utilizing #KEYWORD1 to compensate for the lack of well-labeled animal pose data #REFR.", "given_citation_intent": "method", "given_keywords": ["unlabeled data"]}]}
{"citing_id": "2303.02153v1", "cited_id": "1901.02446", "section_title": "Experiment Setups", "citation": "Since our method can adapt faster to the downstream tasks, we train our model for 80K iterations using a Semantic FPN #REFR by default.", "text_before_citation": ["To fully preserve the pre-trained knowledge of the \u03b8 , we always set the learning rate of \u03b8 as 1/10 of the base learning rate. We use \u03b3=1e-4 for the text adapter.", "The task-specific settings and training details are elaborated as follows.", "Semantic Segmentation.", "The goal of semantic segmentation is to assign pixel-level labels to a given image, which requires a fine-grained high-level understanding of visual content.", "We evaluate our method on ADE20K #OTHEREFR , which consists of 20K images for training and 2K images for validation."], "text_after_citation": ["We use a global batch size of 16 and set the learning rate as 1e-4.", "We use the AdamW optimizer with a weight decay of 1e-4 and warming-up iterations of 1500.", "We adopt the polynomial learning rate scheduler with a power of 0.9 and a minimum learning rate of 1e-6.", "For the fast schedule (8K iterations), we linear scale the learning rate schedule and set the warming-up iterations to 150.", "During inference, we use the slide inference with a crop size 512 \u00d7 512 and a stride of 341 \u00d7 341."], "citing_paper_content": {"title": "Unleashing Text-To-Image Diffusion Models For Visual Perception", "abstract": "Diffusion models (DMs) have become the new trend of generative models and have demonstrated a powerful ability of conditional synthesis. Among those, text-to-image diffusion models pre-trained on large-scale image-text pairs are highly controllable by customizable prompts. Unlike the unconditional generative models that focus on low-level attributes and details, text-to-image diffusion models contain more high-level knowledge thanks to the vision-language pre-training. In this paper, we propose VPD (Visual Perception with a pre-trained Diffusion model), a new framework that exploits the semantic information of a pre-trained text-to-image diffusion model in visual perception tasks. Instead of using the pre-trained denoising autoencoder in a diffusion-based pipeline, we simply use it as a backbone and aim to study how to take full advantage of the learned knowledge. Specifically, we prompt the denoising decoder with proper textual inputs and refine the text features with an adapter, leading to a better alignment to the pre-trained stage and making the visual contents interact with the text prompts. We also propose to utilize the cross-attention maps between the visual features and the text features to provide explicit guidance. Compared with other pre-training methods, we show that vision-language pre-trained diffusion models can be faster adapted to downstream visual perception tasks using the proposed VPD. Extensive experiments on semantic segmentation, referring image segmentation and depth estimation demonstrates the effectiveness of our method. Notably, VPD attains 0.254 RMSE on NYUv2 depth estimation and 73.3% oIoU on RefCOCOval referring image segmentation, establishing new records on these two benchmarks. Code is available at https: //github.com/wl-zhao/VPD."}, "cited_paper_content": {"title": "Panoptic Feature Pyramid Networks", "abstract": "The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation."}, "keywords": ["model", "Semantic FPN"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve a fine-grained high-level understanding of visual content in semantic segmentation, the authors propose a new framework called VPD (Visual Perception with a pre-trained Diffusion model), which exploits the semantic information of a pre-trained text-to-image diffusion model #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve a robust and accurate baseline for semantic segmentation, the authors of VPD utilized a shared Feature Pyramid Network (FPN) backbone, as proposed in Panoptic Feature Pyramid Networks #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve a fine-grained high-level understanding of visual content, the authors propose VPD, a new framework that exploits the semantic information of a pre-trained text-to-image diffusion #REFR model in visual perception tasks, and evaluate their method on ADE20K, demonstrating its effectiveness in semantic segmentation.", "given_citation_intent": "method", "given_keywords": ["model", "Semantic FPN"]}]}
{"citing_id": "2303.09660v1", "cited_id": "1704.02685", "section_title": "Literature Review", "citation": "However, the nonlinearity of the fully connected layers may cause gradient saturation or undesirable artifacts during backpropagation #REFR .", "text_before_citation": ["In this way, Grad-CAM can derive the saliency map without needing to modify and retrain the CNN models.", "Methods that have followed up Grad-CAM include Grad-CAM++ #OTHEREFR", "2018) for the localization of multi-objects belonging to the same class and Score-CAM #OTHEREFR", "2020) , which obtains weights for corresponding feature maps without gradient calculation.", "These gradient-based methods require only one forward-backward pass and are computationally efficient compared to the perturbation-based methods."], "text_after_citation": ["Recent works have developed new strategies to address this issue. For example, Sundararajan et al.", "2017developed an integrated gradient method to aggregate the gradients over an entire image as it goes through continuous modifications, to avoid gradient saturation and unexpected artifacts.", "The entire image is gradually altered from the original image to an all-black image (i.e., baseline).", "The result is an aggregation of all the intermediate results after each alternation.", "This method can be considered as a combination of perturbationbased and gradient-based methods and has shown promising results in many tasks #OTHEREFR ."], "citing_paper_content": {"title": "Explainable Geoai: Can Saliency Maps Help Interpret Artificial Intelligence'S Learning Process? An Empirical Study On Natural Feature Detection", "abstract": "Improving the interpretability of geospatial artificial intelligence (GeoAI) models has become critically important to open the \"black box\" of complex AI models, such as deep learning. This paper compares popular saliency map generation techniques and their strengths and weaknesses in interpreting GeoAI and deep learning models' reasoning behaviors, particularly when applied to geospatial analysis and image processing tasks. We surveyed two broad classes of model explanation methods: perturbation-based and gradient-based methods. The former identifies important image areas, which help machines make predictions by modifying a localized area of the input image. The latter evaluates the contribution of every single pixel of the input image to the model's prediction results through gradient backpropagation. In this study, three algorithms-the occlusion method, the integrated gradients method, and the class activation map method-are examined for a natural feature detection task using deep learning. The algorithms' strengths and weaknesses are discussed, and the consistency between model-learned and human-understandable concepts for object recognition is also compared. The experiments used two GeoAIready datasets to demonstrate the generalizability of the research findings."}, "cited_paper_content": {"title": "Learning Important Features Through Propagating Activation Differences", "abstract": "The purported \"black box\" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH."}, "keywords": ["backpropagation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "These gradient-based methods require only one forward-backward pass and are computationally efficient compared to the perturbation-based methods. DeepLIFT, a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input, is one such method that has shown significant advantages over gradient-based methods #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "DeepLIFT is a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input, which can reveal dependencies missed by other approaches and has shown significant advantages over gradient-based methods in models trained on MNIST and simulated genomic data #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The paper proposes DeepLIFT, a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input, which assigns contribution scores according to the difference between the activation of each neuron and its 'reference activation', making it an effective alternative to other gradient-based methods for backpropagation. #REFR", "given_citation_intent": "background", "given_keywords": ["backpropagation"]}]}
{"citing_id": "2303.05093v1", "cited_id": "1810.07212", "section_title": "Datasets", "citation": "We follow the approach proposed in #REFR , which concatenates all the descriptions of a video to a paragraph.", "text_before_citation": ["The second one is from #OTHEREFR with 9000 clips for training and 1000 clips for testing.", "The last one is the partitions from #OTHEREFR , which uses 9000 clips for training and 1000 clips for testing.", "To increase the persuasiveness of the experiment, we conducted experiments on all data partitions.", "ActivityNet #OTHEREFR : It is an increasingly popular dataset consisting of densely annotated temporal segments of 20000 YouTube videos.", "Each video is an average of 2 minutes long, and there are 72000 video-text pairs in this dataset."], "text_after_citation": ["We trained the model with 10009 videos and evaluated our model on the \"val1\" split, which contains 4917 videos.", "LSMDC #OTHEREFR : It contains 118,081 short video clips( 45 s) extracted from 202 movies.", "Unlike other datasets, each video in LSMDC only has one description, either extracted from the movie script or the transcribed audio description.", "The test set consisted of 1000 videos that were not present in the training set."], "citing_paper_content": {"title": "Improving Video Retrieval By Adaptive Margin", "abstract": "Video retrieval is becoming increasingly important owing to the rapid emergence of videos on the Internet. The dominant paradigm for video retrieval learns video-text representations by pushing the distance between the similarity of positive pairs and that of negative pairs apart from a fixed margin. However, negative pairs used for training are sampled randomly, which indicates that the semantics between negative pairs may be related or even equivalent, while most methods still enforce dissimilar representations to decrease their similarity. This phenomenon leads to inaccurate supervision and poor performance in learning video-text representations. While most video retrieval methods overlook that phenomenon, we propose an adaptive margin changed with the distance between positive and negative pairs to solve the aforementioned issue. First, we design the calculation framework of the adaptive margin, including the method of distance measurement and the function between the distance and the margin. Then, we explore a novel implementation called \"Cross-Modal Generalized Self-Distillation\" (CMGSD), which can be built on the top of most video retrieval models with few modifications. Notably, CMGSD adds few computational overheads at train time and adds no computational overhead at test time. Experimental results on three widely used datasets demonstrate that the proposed method can yield significantly better performance than the corresponding backbone model, and it outperforms state-of-the-art methods by a large margin. CCS CONCEPTS \u2022 Information systems \u2192 Video search."}, "cited_paper_content": {"title": "Cross-Modal And Hierarchical Modeling Of Video And Text", "abstract": "Visual data and text data are composed of information at multiple granularities. A video can describe a complex scene that is composed of multiple clips or shots, where each depicts a semantically coherent event or action. Similarly, a paragraph may contain sentences with different topics, which collectively conveys a coherent message or story. In this paper, we investigate the modeling techniques for such hierarchical sequential data where there are correspondences across multiple modalities. Specifically, we introduce hierarchical sequence embedding (HSE), a generic model for embedding sequential data of different modalities into hierarchically semantic spaces, with either explicit or implicit correspondence information. We perform empirical studies on large-scale video and paragraph retrieval datasets and demonstrated superior performance by the proposed methods. Furthermore, we examine the effectiveness of our learned embeddings when applied to downstream tasks. We show its utility in zero-shot action recognition and video captioning."}, "keywords": ["video"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To model hierarchical sequential data with correspondences across multiple modalities, the authors of this manuscript could refer to the hierarchical sequence embedding (HSE) proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model hierarchical sequential data with correspondences across multiple modalities, the authors propose a generic model called hierarchical sequence embedding (HSE) in their paper \"#REFR\".", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To model hierarchical sequential data with correspondences across multiple modalities, the authors of this paper propose a generic model called hierarchical sequence embedding (HSE), which embeds sequential data of different modalities into hierarchically semantic spaces, and demonstrate its superior performance on large-scale video and paragraph retrieval datasets, as well as its utility in downstream tasks such as zero-shot action recognition and video captioning #REFR.", "given_citation_intent": "method", "given_keywords": ["video"]}]}
{"citing_id": "2303.18072v1", "cited_id": "1407.6118", "section_title": "2D Linear Wave Equation", "citation": "In contrast, with 512 or more basis vectors the relative error in the Hamiltonian is about 10 \u22126 for the standard cSVD which is in agreement with the experiments from #REFR for the linear wave equation.", "text_before_citation": ["The temporary increase in the error curves for m s \u2265 120 and an average basis size of 70 is due to the fact that the basis is too small for the window size m s \u2265 120. Larger window sizes require larger basis sizes.", "In Figures 3 and 4 we present the relative error in the Hamiltonian", "EQUATION", "averaged over the three training parameters in dependence on the time-step.", "In Figure 3 we see that using a non-symplectic, standard POD-basis leads to unstable models in the sense that the energy drastically increases if more than 256 basis vectors are used."], "text_after_citation": ["As expected, the error in the Hamiltonian is constant for the standard cSVD up to numerical inaccuracies.", "These inaccuracies can only be seen in the bottom two curves due to the logarithmic axis.", "In Figure 4 , we plot the same errors for the dictionary-based methods.", "With n s = 100 selected snapshots per basis update, we observe high error jumps across the basis changes for DB-cSVD for window sizes m s \u2265 120.", "However, with n s = 250 snapshots selected, this behavior is no longer observed and a relative error of 10 \u22126 is achieved as with the standard cSVD."], "citing_paper_content": {"title": "Dictionary-Based Online-Adaptive Structure-Preserving Model Order Reduction For Parametric Hamiltonian Systems", "abstract": "Classical model order reduction (MOR) for parametric problems may become computationally inefficient due to large sizes of the required projection bases, especially for problems with slowly decaying Kolmogorov n-widths. Additionally, Hamiltonian structure of dynamical systems may be available and should be preserved during the reduction. In the current presentation, we address these two aspects by proposing a corresponding dictionary-based, online-adaptive MOR approach. The method requires dictionaries for the state-variable, non-linearities and discrete empirical interpolation (DEIM) points. During the online simulation, local basis extensions/simplifications are performed in an online-efficient way, i.e. the runtime complexity of basis modifications and online simulation of the reduced models do not depend on the full state dimension. Experiments on a linear wave equation and a non-linear Sine-Gordon example demonstrate the efficiency of the approach."}, "cited_paper_content": {"title": "Symplectic Model Reduction Of Hamiltonian Systems", "abstract": "In this paper, a symplectic model reduction technique, proper symplectic decomposition (PSD) with symplectic Galerkin projection, is proposed to save the computational cost for the simplification of large-scale Hamiltonian systems while preserving the symplectic structure. As an analogy to the classical proper orthogonal decomposition (POD)-Galerkin approach, PSD is designed to build a symplectic subspace to fit empirical data, while the symplectic Galerkin projection constructs a reduced Hamiltonian system on the symplectic subspace. For practical use, we introduce three algorithms for PSD, which are based upon: the cotangent lift, complex singular value decomposition, and nonlinear programming. The proposed technique has been proven to preserve system energy and stability. Moreover, PSD can be combined with the discrete empirical interpolation method to reduce the computational cost for nonlinear Hamiltonian systems. Owing to these properties, the proposed technique is better suited than the classical POD-Galerkin approach for model reduction of Hamiltonian systems, especially when long-time integration is required. The stability, accuracy, and efficiency of the proposed technique are illustrated through numerical simulations of linear and nonlinear wave equations."}, "keywords": ["Hamiltonian"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In Figure 3 we see that using a non-symplectic, standard POD-basis leads to unstable models in the sense that the energy drastically increases if more than 256 basis vectors are used. To address this issue, the authors proposed a symplectic model reduction technique, proper symplectic decomposition (PSD) with symplectic Galerkin projection, which has been proven to preserve system energy and stability #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of a non-symplectic, standard POD-basis leads to unstable models in the sense that the energy drastically increases if more than 256 basis vectors are used, whereas the proposed proper symplectic decomposition (PSD) technique, as described in #REFR, has been proven to preserve system energy and stability.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of a non-symplectic, standard POD-basis leads to unstable models in the sense that the energy drastically increases if more than 256 basis vectors are used, whereas the proposed symplectic model reduction technique, proper symplectic decomposition (PSD) with symplectic Galerkin projection, has been proven to preserve system energy and stability for #REFR.", "given_citation_intent": "result", "given_keywords": ["Hamiltonian"]}]}
{"citing_id": "2304.09068v1", "cited_id": "2003.09758", "section_title": "\u2022 Causal Inference:", "citation": "Notably, the utility achieved on these tasks is comparable to the values reported in prior work that use these datasets #REFR , demonstrating effectiveness of METAM to achieve competitive quality while generalizing to a wide variety of tasks.", "text_before_citation": ["Regression.", "The goal is to predict number of collisions in NYC using data such as number of daily taxi trips #OTHEREFR over a dataset containing 350 records..", "The task uses a random forest regressor and computes the mean absolute error (MAE), returning 1 \u2212 M AE as utility. METAM outperforms all baselines.", "With only 300 queries, METAM reduces MAE from 0.66 to 0.55.", "Other techniques require three times more queries to achieve similar MAE."], "text_after_citation": ["We further evaluate predictive analytics (classification and regression) tasks with informative domainspecific data profiles such as feature importance #OTHEREFR and uninformative generic data profiles to understand METAM's flexibility in Section VI-C.", "What-if analysis.", "The task takes an input dataset along with a hypothetical update, and outputs the causal impact of the update query on other attributes.", "We consider an initial table containing SAT scores of 450 students [43] and ask what attributes would be causally affected if \"critical reading score\" of students is updated.", "Understanding the attributes sheds light on what affects students' reading score, paving the way for the implementation of interventions."], "citing_paper_content": {"title": "Metam: Goal-Oriented Data Discovery", "abstract": "Data is a central component of machine learning and causal inference tasks. The availability of large amounts of data from sources such as open data repositories, data lakes and data marketplaces creates an opportunity to augment data and boost those tasks' performance. However, augmentation techniques rely on a user manually discovering and shortlisting useful candidate augmentations. Existing solutions do not leverage the synergy between discovery and augmentation, thus underexploiting data. In this paper, we introduce METAM, a novel goal-oriented framework that queries the downstream task with a candidate dataset, forming a feedback loop that automatically steers the discovery and augmentation process. To select candidates efficiently, METAM leverages properties of the: i) data, ii) utility function, and iii) solution set size. We show METAM's theoretical guarantees and demonstrate those empirically on a broad set of tasks. All in all, we demonstrate the promise of goal-oriented data discovery to modern data science applications."}, "cited_paper_content": {"title": "Arda: Automatic Relational Data Augmentation For Machine Learning", "abstract": "Automatic machine learning (\\AML) is a family of techniques to automate the process of training predictive models, aiming to both improve performance and make machine learning more accessible. While many recent works have focused on aspects of the machine learning pipeline like model selection, hyperparameter tuning, and feature selection, relatively few works have focused on automatic data augmentation. Automatic data augmentation involves finding new features relevant to the user's predictive task with minimal ``human-in-the-loop'' involvement. We present \\system, an end-to-end system that takes as input a dataset and a data repository, and outputs an augmented data set such that training a predictive model on this augmented dataset results in improved performance. Our system has two distinct components: (1) a framework to search and join data with the input data, based on various attributes of the input, and (2) an efficient feature selection algorithm that prunes out noisy or irrelevant features from the resulting join. We perform an extensive empirical evaluation of different system components and benchmark our feature selection algorithm on real-world datasets."}, "keywords": ["datasets"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To automatically augment the number of collisions in NYC dataset, the authors of METAM utilized the Arda system for automatic data augmentation #REFR, which resulted in a significant reduction in mean absolute error with only 300 queries.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of predicting the number of collisions in NYC, METAM leverages automatic data augmentation techniques such as those presented in Arda: Automatic Relational Data Augmentation For Machine Learning #REFR, resulting in a significant reduction in mean absolute error with only 300 queries.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To automatically augment their dataset, the authors of METAM leverage a data repository and a framework to search and join data, as well as an efficient feature selection algorithm that prunes out noisy or irrelevant features from the resulting join #REFR, which could be useful for improving the performance of regression models on small datasets such as the one used in this study.", "given_citation_intent": "result", "given_keywords": ["datasets"]}]}
{"citing_id": "2304.13718v1", "cited_id": "1902.09574", "section_title": "Magnitude Pruning Outperforms Variational Dropout", "citation": "Related work found that MP can outperform VD, especially for moderate sparsity ratios #REFR . Our results confirm that on population level.", "text_before_citation": [], "text_after_citation": ["MP outperforms VD for sparsification levels of up to 80% consistently, see Figure   5 , Appendix B and C.", "At higher sparsification levels, MP shows steep drops in performance.", "VD on some zoos is more stable and thus shows higher performance at higher sparsification levels, justifying the larger parameter count and computational load."], "citing_paper_content": {"title": "Sparsified Model Zoo Twins: Investigating Populations Of Sparsified Neural Network Models", "abstract": "With growing size of Neural Networks (NNs), model sparsification to reduce the computational cost and memory demand for model inference has become of vital interest for both research and production. While many sparsification methods have been proposed and successfully applied on individual models, to the best of our knowledge their behavior and robustness has not yet been studied on large populations of models. With this paper, we address that gap by applying two popular sparsification methods on populations of models (so called model zoos) to create sparsified versions of the original zoos. We investigate the performance of these two methods for each zoo, compare sparsification layer-wise, and analyse agreement between original and sparsified populations. We find both methods to be very robust with magnitude pruning able outperform variational dropout with the exception of high sparsification ratios above 80%. Further, we find sparsified models agree to a high degree with their original non-sparsified counterpart, and that the performance of original and sparsified model is highly correlated. Finally, all models of the model zoos and their sparsified model twins are publicly available: modelzoos.cc."}, "cited_paper_content": {"title": "The State Of Sparsity In Deep Neural Networks", "abstract": "We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle & Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification."}, "keywords": ["moderate sparsity ratios"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate that simple magnitude pruning approaches achieve comparable or better results than complex techniques shown to yield high compression rates on smaller datasets, as highlighted in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate that simple magnitude pruning approaches achieve comparable or better results than complex techniques for inducing sparsity in deep neural networks, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrate that simple magnitude pruning approaches achieve comparable or better results than complex techniques for inducing sparsity in deep neural networks, even at moderate sparsity ratios, which is consistent with the findings in this study. #REFR", "given_citation_intent": "result", "given_keywords": ["moderate sparsity ratios"]}]}
{"citing_id": "2304.05498v1", "cited_id": "1810.11203", "section_title": "B. Hardware And Hyperparameter Configurations", "citation": "In addition, for a medium dataset like QM8, a medium discriminator dimension, such as [64, 128] , 64, [128, #REFR or slightly smaller, is preferred to generate the molecules that have considerable metric values.", "text_before_citation": ["On the other hand, training a higher complexity model over a small dataset may lead to mode collapse, which reduces the values of QED, Uniqueness, and LogP.", "Thus, there is a tradeoff among Validity and QED/Uniqueness/LogP/Similarity, and the tradeoff can be adjusted by changing the discriminator For QM8, which is considered a medium dataset that has more compounds than ESOL but less than QM9, we observe the results for the IID setting in Table II and find that as the discriminator complexity increases, the values of QED, Diversity, and Validity decrease, i.e., (0.49, 0.46, 0.45), (1.00, 0.99, 0.99), and (78.4, 26.9, 13.7), respectively, but the values of Uniqueness, LogP, and similarity increases, i.e., (18.1, 51.4, 67.9), (0.54, 0.59, 0.62), and (0.0067, 0.0261, 0.0336), respectively.", "The tradeoff changes from \"between Validity and QED/Uniqueness/LogP\" for ESOL into \"between QED/Diversity/Validity and Uniqueness/LogP/Similarity\" for QM8.", "Also, if we observe the results for non-IID in Table III , we can find that as the discriminator complexity increases, only Uniqueness decreases, while QED, Validity, LogP, and Similarity increase.", "Hence, we conclude that the tradeoff among different evaluation metrics may change based on the size of the dataset and the data sample distribution among clients."], "text_after_citation": ["For QM9, which is considered a large dataset, we can observe from the results that as we increase the discriminator complexity, QED, Validity, and Similarity increase, but Uniqueness reduces in the IID setting as shown in Table II .", "In the non-IID setting, as shown in Table III , only Uniqueness decreases, but QED, Validity, LogP, and Similarity increase as we increase the discriminator dimension.", "Hence, we can derive the same conclusion that the tradeoff among different metrics may change based on the size of the data set and the data sample distribution among clients.", "In addition, for a large dataset like QM9, a large discriminator complexity is preferred to generate the molecules that have considerable metric values.", "3) Effect of different client settings on the metrics: In this section, we evaluate how different numbers of clients affect the performance of GraphGANFed."], "citing_paper_content": {"title": "Graphganfed: A Federated Generative Framework For Graph-Structured Molecules Towards Efficient Drug Discovery", "abstract": "Recent advances in deep learning have accelerated its use in various applications, such as cellular image analysis and molecular discovery. In molecular discovery, a generative adversarial network (GAN), which comprises a discriminator to distinguish generated molecules from existing molecules and a generator to generate new molecules, is one of the premier technologies due to its ability to learn from a large molecular data set efficiently and generate novel molecules that preserve similar properties. However, different pharmaceutical companies may be unwilling or unable to share their local data sets due to the geodistributed and sensitive nature of molecular data sets, making it impossible to train GANs in a centralized manner. In this paper, we propose a Graph convolutional network in Generative Adversarial Networks via Federated learning (GraphGANFed) framework, which integrates graph convolutional neural Network (GCN), GAN, and federated learning (FL) as a whole system to generate novel molecules without sharing local data sets. In GraphGANFed, the discriminator is implemented as a GCN to better capture features from molecules represented as molecular graphs, and FL is used to train both the discriminator and generator in a distributive manner to preserve data privacy. Extensive simulations are conducted based on the three benchmark data sets to demonstrate the feasibility and effectiveness of GraphGANFed. The molecules generated by GraphGANFed can achieve high novelty (\u2248 100) and diversity (> 0.9). The simulation results also indicate that 1) a lower complexity discriminator model can better avoid mode collapse for a smaller data set, 2) there is a tradeoff among different evaluation metrics, and 3) having the right dropout ratio of the generator and discriminator can avoid mode collapse. Index Terms-Generative adversarial networks, Graph convolutional networks, Federated learning, Drug discovery I. INTRODUCTION The discovery of new organic and inorganic molecules remains a challenge in medicine, chemistry, and materials sciences. Traditional approaches to molecular discovery involve mathematical frameworks derived from related properties calculated from chemical structures with different physical or biological reactions [1, 2]. However, these mathematical frameworks may not fully capture the properties of the chemical structures, limiting the ability to fully explore novel D. Manu and X. Sun are with the"}, "cited_paper_content": {"title": "Crystalgan: Learning To Discover Crystallographic Structures With Generative Adversarial Networks", "abstract": "Our main motivation is to propose an efficient approach to generate novel multi-element stable chemical compounds that can be used in real world applications. This task can be formulated as a combinatorial problem, and it takes many hours of human experts to construct, and to evaluate new data. Unsupervised learning methods such as Generative Adversarial Networks (GANs) can be efficiently used to produce new data. Cross-domain Generative Adversarial Networks were reported to achieve exciting results in image processing applications. However, in the domain of materials science, there is a need to synthesize data with higher order complexity compared to observed samples, and the state-of-the-art cross-domain GANs can not be adapted directly. In this contribution, we propose a novel GAN called CrystalGAN which generates new chemically stable crystallographic structures with increased domain complexity. We introduce an original architecture, we provide the corresponding loss functions, and we show that the CrystalGAN generates very reasonable data. We illustrate the efficiency of the proposed method on a real original problem of novel hydrides discovery that can be further used in development of hydrogen storage materials."}, "keywords": ["molecules"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the domain of materials science, unsupervised learning methods such as Generative Adversarial Networks (GANs) have been used to efficiently produce new data. However, the state-of-the-art cross-domain GANs cannot be adapted directly to synthesize data with higher order complexity compared to observed samples. To address this issue, CrystalGAN was proposed, which generates new chemically stable crystallographic structures with increased domain complexity. The authors of this manuscript could benefit from using CrystalGAN to generate novel molecules with similar properties. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a novel GAN called CrystalGAN which generates new chemically stable crystallographic structures with increased domain complexity, which can be used to efficiently produce new multi-element stable chemical compounds, as discussed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed GraphGANFed framework integrates graph convolutional neural Network (GCN), GAN, and federated learning (FL) as a whole system to generate novel #KEYWORD(molecules) without sharing local data sets, and the discriminator is implemented as a GCN to better capture features from #KEYWORD(molecular graphs) #REFR.", "given_citation_intent": "method", "given_keywords": ["molecules"]}]}
{"citing_id": "2304.10333v1", "cited_id": "2003.02752", "section_title": "B. Overall Concept", "citation": "We further decreased the divergence of the selected clean source samples, which maximized the agreement of the multiple classifiers to achieve better results, similar to #REFR .", "text_before_citation": ["The probability that samples x i belong to class k predicted by each classifier are denoted as p k 1 (y|x i ) and p k 2 (y|x i ), respectively.", "It is also possible to build these multiple independent classifiers by using dropout regularization #OTHEREFR , #OTHEREFR repeatedly to a single classifier. Section III-F goes into more detail about this.", "To solve the noisy label problem of source samples, we calculated the divergence between the multiple classifier outputs for each source sample at the mini-batch level.", "The multiple classifiers tend to output similar predictions on clean samples and different predictions on noisy data because the multiple classifiers that are trained individually have varying capacities to learn the noisy label.", "In each mini-batch, we chose samples with small divergences in addition to the wellknown small-loss strategy to filter out the noisy samples."], "text_after_citation": ["We propose a divergence separation loss for the target samples to address the issues with the source and target private samples.", "The target private samples will have larger divergences than the target common samples because they can also be thought of as noisy samples with wrong labels.", "Therefore, we can filter out some target private samples to achieve stable performance by separating the divergences of the target samples.", "Derived from the existing methods #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR that deploy multiple classifiers with different parameters to achieve domain adaptation, we further used the multiple classifiers as a discriminator to detect the target samples that are far from the cluster of the source domain.", "After that, we trained the generator to minimize the divergence to prevent it from generating target features that are not supported by the source samples."], "citing_paper_content": {"title": "Noisy Universal Domain Adaptation Via Divergence Optimization For Visual Recognition", "abstract": "To transfer the knowledge learned from a labeled source domain to an unlabeled target domain, many studies have worked on universal domain adaptation (UniDA), where there is no constraint on the label sets of the source domain and target domain. However, the existing UniDA methods rely on source samples with correct annotations. Due to the limited resources in the real world, it is difficult to obtain a large amount of perfectly clean labeled data in a source domain in some applications. As a result, we propose a novel realistic scenario named Noisy UniDA, in which classifiers are trained using noisy labeled data from the source domain as well as unlabeled domain data from the target domain that has an uncertain class distribution. A multi-head convolutional neural network framework is proposed in this paper to address all of the challenges faced in the Noisy UniDA at once. Our network comprises a single common feature generator and multiple classifiers with various decision bounds. We can detect noisy samples in the source domain, identify unknown classes in the target domain, and align the distribution of the source and target domains by optimizing the divergence between the outputs of the various classifiers. The proposed method outperformed the existing methods in most of the settings after a thorough analysis of the various domain adaption scenarios."}, "cited_paper_content": {"title": "Combating Noisy Labels By Agreement: A Joint Training Method With Co-Regularization", "abstract": "Deep Learning with noisy labels is a practically challenging problem in weakly-supervised learning. The state-of-the-art approaches\"Decoupling\"and\"Co-teaching+\"claim that the\"disagreement\"strategy is crucial for alleviating the problem of learning with noisy labels. In this paper, we start from a different perspective and propose a robust learning paradigm called JoCoR, which aims to reduce the diversity of two networks during training. Specifically, we first use two networks to make predictions on the same mini-batch data and calculate a joint loss with Co-Regularization for each training example. Then we select small-loss examples to update the parameters of both two networks simultaneously. Trained by the joint loss, these two networks would be more and more similar due to the effect of Co-Regularization. Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is superior to many state-of-the-art approaches for learning with noisy labels."}, "keywords": ["multiple classifiers"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To reduce the diversity of two networks during training, JoCoR proposes a joint training method with Co-Regularization, which calculates a joint loss for each training example using two networks and selects small-loss examples to update the parameters of both networks simultaneously #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed JoCoR method in the reference paper #REFR achieved superior performance compared to state-of-the-art approaches for learning with noisy labels, as demonstrated by extensive experimental results on benchmark datasets including MNIST, CIFAR-10, CIFAR-100, and Clothing1M, which makes it a promising solution for the noisy label problem in the Noisy UniDA scenario.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in this paper aligns the distribution of the source and target domains by optimizing the divergence between the outputs of the various classifiers, which is similar to the approach of JoCoR that reduces the diversity of #REFR multiple classifiers during training to achieve better performance in learning with noisy labels.", "given_citation_intent": "result", "given_keywords": ["multiple classifiers"]}]}
{"citing_id": "2304.01950v1", "cited_id": "1909.00560", "section_title": "Because Of The Challenges Of Limitation In Network Bandwidth", "citation": "Thanks to the improvements in storage and computing capabilities of edge intelligence devices, most computing tasks can now be completed directly at the edge, making the Mobile Edge Computing (MEC) paradigm the next-generation computing network #REFR .", "text_before_citation": ["Md.", "Shirajum Munir is with the Virginia Modeling, Analysis, and Simulation Center, Department of Electrical and Computer Engineering, Old Dominion University, Suffolk, VA 23435, USA, and also with the Department of Computer Science and Engineering, Kyung Hee University, Yongin-si 17104, Republic of Korea (e-mail: munir@khu.ac.kr).", "Apurba Adhikary, Huy Q.", "Le, Avi Deb Raha, and Choong Seon Hong are with the Department of Computer Science and Engineering, School of Computing, Kyung Hee University, Yongin-si 17104, Republic of Korea (e-mail: apurba@khu.ac.kr; quanghuy69@khu.ac.kr; avi@khu.ac.kr; cshong@khu.ac.kr).", "Corresponding author: Choong Seon Hong (e-mail: cshong@khu.ac.kr) or the requirements for transmission delay, the traditional cloud computing paradigm that uploads such big data to a cloud centre for data processing can no longer meet these demands #OTHEREFR ."], "text_after_citation": ["Further, collecting data from distributed devices poses risks and challenges due to the sensitive nature of a large amount of data, as well as regulations such as the General Data Protection Regulation (GDPR) #OTHEREFR in Europe.", "Therefore, as edge devices' storage and computing power continue to grow, coupled with concerns about privacy issues, it becomes more attractive to implement edge intelligence in MEC systems in a distributed manner #OTHEREFR .", "To this end, Federated learning (FL), as one application of edge computing in distributed machine learning, is first proposed by #OTHEREFR to simultaneously achieve edge intelligence and address privacy concerns.", "It trains a global model through the cooperation between local clients and an edge server while keeping the clients' raw data within their respective local environments.", "In general, the typical federated training process consists of the following four steps #OTHEREFR : (1) the server chooses a certain network architecture such as Convolutional Neural Network (CNN) as the global model to be optimized and sends it to local clients; (2) the clients update the received the model parameters of the global model based on their local data; #OTHEREFR all clients send their updated model parameters back to the edge server for aggregation; (4) the server averages all the sent parameters as the new global model parameters for the next global round, repeating these four steps until convergence."], "citing_paper_content": {"title": "Mp-Fedcl: Multi-Prototype Federated Contrastive Learning For Edge Intelligence", "abstract": "Federated learning-assisted edge intelligence enables privacy protection in modern intelligent services. However, not Independent and Identically Distributed (non-IID) distribution among edge clients can impair the local model performance. The existing single prototype-based strategy represents a sample by using the mean of the feature space. However, feature spaces are usually not clustered, and a single prototype may not represent a sample well. Motivated by this, this paper proposes a multiprototype federated contrastive learning approach (MP-FedCL) which demonstrates the effectiveness of using a multi-prototype strategy over a single-prototype under non-IID settings, including both label and feature skewness. Specifically, a multi-prototype computation strategy based on k-means is first proposed to capture different embedding representations for each class space, using multiple prototypes (k centroids) to represent a class in the embedding space. In each global round, the computed multiple prototypes and their respective model parameters are sent to the edge server for aggregation into a global prototype pool, which is then sent back to all clients to guide their local training. Finally, local training for each client minimizes their own supervised learning tasks and learns from shared prototypes in the global prototype pool through supervised contrastive learning, which encourages them to learn knowledge related to their own class from others and reduces the absorption of unrelated knowledge in each global iteration. Experimental results on MNIST, Digit-5, Office-10, and DomainNet show that our method outperforms multiple baselines, with an average test accuracy improvement of about 4.6% and 10.4% under feature and label non-IID distributions, respectively."}, "cited_paper_content": {"title": "Edge Intelligence: The Confluence Of Edge Computing And Artificial Intelligence", "abstract": "Along with the deepening development in communication technologies and the surge of mobile devices, a brand-new computation paradigm, Edge Computing, is surging in popularity. Meanwhile, ::: Artificial Intelligence (AI) applications are thriving with the breakthroughs in deep learning and the upgrade of hardware architectures. Billions of bytes of data, generated at the network edge, put great demands on data processing and structural optimization. ::: Therefore, there exists a strong demand to integrate Edge Computing and AI, which gives birth to Edge Intelligence. ::: In this article, we divide Edge Intelligence into AI for edge (Intelligence-enabled Edge Computing) and AI on edge (Artificial Intelligence on Edge). ::: The former focuses on providing a more optimal solution to the key concerns in Edge Computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on edge. This article focuses on giving insights into this new inter-disciplinary field from a broader vision and perspective. It discusses the core concepts and the research road-map, which should provide the necessary background for potential future research programs in Edge Intelligence."}, "keywords": ["edge intelligence devices"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of non-IID distribution among edge clients in federated learning-assisted edge intelligence, Munir et al. proposed a multiprototype federated contrastive learning approach (MP-FedCL) that utilizes a multi-prototype strategy over a single-prototype, as demonstrated in their paper \"Edge Intelligence: The Confluence Of Edge Computing And Artificial Intelligence\" #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "With the surge of mobile devices and the increasing demand for data processing and structural optimization, integrating Edge Computing and Artificial Intelligence (AI) has become a necessity, giving birth to Edge Intelligence, which is divided into AI for edge and AI on edge, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "With the surge of mobile devices and billions of bytes of data generated at the network edge, the integration of Edge Computing and Artificial Intelligence (AI) has become a strong demand, giving birth to Edge Intelligence, which is divided into AI for edge and AI on edge #REFR.", "given_citation_intent": "background", "given_keywords": ["edge intelligence devices"]}]}
{"citing_id": "2304.08662v1", "cited_id": "2002.05200", "section_title": "Pastis", "citation": "ELBA also provides support for the GPU-based -Drop alignment called LOGAN #REFR . LOGAN does not support protein alignment.", "text_before_citation": ["Thus, the overlap detection phase has the form of ASA T .", "Once the output matrix is formed, PASTIS computes an alignment step on each non-zero, similar to ELBA.", "PASTIS has two alignment modes: seedand-extend with -Drop and Smith-Waterman alignment.", "Using -Drop, PASTIS initiates the alignment from the -mer match.", "Both PASTIS and ELBA defer implementation of -Drop to the Library for Sequence Analysis (SeqAn) C++ library for CPU #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "Space Efficient Sequence Alignment For Sram-Based Computing: X-Drop On The Graphcore Ipu", "abstract": "Dedicated accelerator hardware has become essential for processing AI-based workloads, leading to the rise of novel accelerator architectures. Furthermore, fundamental differences in memory architecture and parallelism have made these accelerators targets for scientific computing. The sequence alignment problem is fundamental in bioinformatics; we have implemented the-Drop algorithm, a heuristic method for pairwise alignment that reduces search space, on the Graphcore Intelligence Processor Unit (IPU) accelerator. The-Drop algorithm has an irregular computational pattern, which makes it difficult to accelerate due to load balancing. Here, we introduce a graph-based partitioning and queue-based batch system to improve load balancing. Our implementation achieves 10\u00d7 speedup over a state-of-the-art GPU implementation and up to 4.65\u00d7 compared to CPU. In addition, we introduce a memoryrestricted-Drop algorithm that reduces memory footprint by 55\u00d7 and efficiently uses the IPU's limited low-latency SRAM. This optimization further improves the strong scaling performance by 3.6\u00d7."}, "cited_paper_content": {"title": "Logan: High-Performance Gpu-Based X-Drop Long-Read Alignment", "abstract": "Pairwise sequence alignment is one of the most computationally intensive kernels in genomic data analysis, accounting for more than 90% of the runtime for key bioinformatics applications. This method is particularly expensive for third-generation sequences due to the high computational cost of analyzing sequences of length between 1Kb and 1Mb. Given the quadratic overhead of exact pairwise algorithms for long alignments, the community primarily relies on approximate algorithms that search only for high-quality alignments and stop early when one is not found. In this work, we present the first GPU optimization of the popular X-drop alignment algorithm, that we named LOGAN. Results show that our high-performance multi-GPU implementation achieves up to 181.6 GCUPS and speed-ups up to 6.6x and 30.7x using 1 and 6 NVIDIA Tesla V100, respectively, over the state-of-the-art software running on two IBM Power9 processors using 168 CPU threads, with equivalent accuracy. We also demonstrate a 2.3x LOGAN speed-up versus ksw2, a state-of-art vectorized algorithm for sequence alignment implemented in minimap2, a long-read mapping software. To highlight the impact of our work on a real-world application, we couple LOGAN with a many-to-many long-read alignment software called BELLA, and demonstrate that our implementation improves the overall BELLA runtime by up to 10.6x. Finally, we adapt the Roofline model for LOGAN and demonstrate that our implementation is near-optimal on the NVIDIA Tesla V100s."}, "keywords": ["GPU-based -Drop alignment"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We implemented the X-Drop algorithm on the Graphcore Intelligence Processor Unit (IPU) accelerator, achieving up to 10\u00d7 speedup over a state-of-the-art GPU implementation and up to 4.65\u00d7 compared to CPU, as well as a memory-restricted version that efficiently uses the IPU's limited low-latency SRAM and reduces memory footprint by 55\u00d7. Our implementation builds on the work of Logan et al. (#REFR), who presented the first GPU optimization of the X-Drop algorithm and achieved up to 181.6 GCUPS and speed-ups up to 6.6x and 30.7x using 1 and 6 NVIDIA Tesla V100, respectively, over the state-of-the-art software running on two IBM Power9 processors using 168 CPU threads, with equivalent accuracy.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To accelerate the pairwise sequence alignment, the-Drop algorithm has been implemented on various hardware platforms, including GPUs and the Graphcore Intelligence Processor Unit (IPU) accelerator, achieving up to 181.6 GCUPS and 10\u00d7 speedup, respectively, as reported in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "PASTIS defers the implementation of -Drop to the SeqAn C++ library for CPU, but a recent study presented a high-performance multi-#REFR implementation achieving up to 181.6 GCUPS and speed-ups up to 6.6x and 30.7x using 1 and 6 NVIDIA Tesla V100, respectively, over the state-of-the-art software running on two IBM Power9 processors using 168 CPU threads, with equivalent accuracy, making it a promising GPU-based solution for -Drop alignment.", "given_citation_intent": "method", "given_keywords": ["GPU-based -Drop alignment"]}]}
{"citing_id": "2303.11575v2", "cited_id": "1802.05797", "section_title": "Related Work", "citation": "Many users center their concern around the threats from other users, e.g., as a bystander #REFR .", "text_before_citation": ["#OTHEREFR discovered that users are concerned about the security issues in losing the authentication token.", "Lassak et al.'s study #OTHEREFR indicated that users have misconceptions about how biometric data is stored using FIDO2 biometric authentication.", "Both studies showed that users' security perception of authentication does not necessarily match the inherent security.", "Recent research also focused on users' security and privacy perception in VR.", "VR developers and users felt the lack of privacy due to opaque data collection policies #OTHEREFR ."], "text_after_citation": ["Users are also concerned about being deceived by the digital content in VR #OTHEREFR .", "User authentication for payment. Authentication requirements can differ in different settings.", "For example, using a chip card may suffice in a physical store, whereas additional one-time passwords (OTPs) may be required while shopping online #OTHEREFR .", "The perceived security of authentication also impact users' adoption and use of payment services.", "For example, there is a significant uptake of mobile payments because users associate perceived control and security with user authentication on their devices #OTHEREFR . Voskobojnikov et al."], "citing_paper_content": {"title": "\"I Want The Payment Process To Be Cool\": Understanding How Interaction Factors Into Security And Privacy Perception Of Authentication In Virtual Reality", "abstract": "Users embrace the rapid development of virtual reality (VR) technology. We are witnessing a widespread adoption of VR technology in more routine settings, such as gaming, social interactions, shopping, and commerce. VR systems access sensitive user data and assets when handling these routine activities, including payment, which raises the need for user authentication in VR. However, there is a limited understanding of how users perceive user authentication in VR, in particular, how users' interaction experiences factor into their perception of security and privacy. Our work adopts a \"technology probe\" approach to understand this question. We design technology probes of authentication in VR based on existing authentication interactions in both VR and the physical world. Further, we embed these probes in the routine payment of a VR game. Our qualitative analysis reveals that users face unique usability challenges in VR authentication, e.g., in motion control. Such challenges also hinder users from accessing security and privacy accurately in VR authentication. Users' expectations for VR authentication mainly center on improvements in interaction. However, their expectations could appear nonspecific and conflicting. We provide recommendations to accommodate users' expectations and resolve conflicts between usability and security."}, "cited_paper_content": {"title": "Security And Privacy Approaches In Mixed Reality: A Literature Survey", "abstract": "Mixed reality (MR) technology development is now gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies. With most of the research and development focused on delivering the promise of MR, the privacy and security implications of this technology are yet to be thoroughly investigated. This survey article aims to put in to light these risks and to look into the latest security and privacy work on MR. Specifically, we list and review the different protection approaches that have been proposed to ensure user and data security and privacy in MR. We extend the scope to include work on related technologies such as augmented reality, virtual reality, and human-computer interaction as crucial components, if not the origins, of MR, as well as numerous related work from the larger area of mobile devices, wearables, and Internet-of-Things. We highlight the lack of investigation, implementation, and evaluation of data protection approaches in MR. Further challenges and directions on MR security and privacy are also discussed."}, "keywords": ["users", "threats"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recent research has also focused on users' security and privacy perception in VR, including the latest security and privacy work on mixed reality (MR) surveyed by Lassak et al. in their article on security and privacy approaches in MR #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Mixed reality (MR) technology development is gaining momentum due to advances in computer vision, sensor fusion, and realistic display technologies, but the privacy and security implications of this technology are yet to be thoroughly investigated #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The lack of investigation, implementation, and evaluation of data protection approaches in mixed reality (MR) has been highlighted in a recent survey article #REFR, which also lists and reviews different protection approaches proposed to ensure user and data security and privacy in MR, including related work from the larger area of mobile devices, wearables, and Internet-of-Things, addressing the threats that users face in accessing security and privacy accurately in VR authentication.", "given_citation_intent": "background", "given_keywords": ["users", "threats"]}]}
{"citing_id": "2303.09455v1", "cited_id": "1808.06226", "section_title": "Fine-Tuning", "citation": "For labels, we use a character set for Mandarin and subword units #REFR of vocabulary size 1000 for all other languages.", "text_before_citation": ["Following #OTHEREFR , we fine-tune the pre-trained visual student encoder to perform visual speech recognition by attaching a linear layer and a Transformer decoder with attention dimension 256, 4 heads, 6 layers and 2048 linear units.", "We apply a joint CTC / attention loss #OTHEREFR .", "The beam size and CTC weight are fixed to 40 and 0.1 respectively, as in #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "Learning Cross-Lingual Visual Speech Representations", "abstract": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw AudioVisual Speech Encoders (RAVEn) framework to pre-train an audiovisual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multilingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multilingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learning."}, "cited_paper_content": {"title": "Sentencepiece: A Simple And Language Independent Subword Tokenizer And Detokenizer For Neural Text Processing", "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece."}, "keywords": ["Mandarin", "subword units"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We use SentencePiece, a language-independent subword tokenizer and detokenizer, to train subword models directly from raw sentences #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We train subword models directly from raw sentences using SentencePiece #REFR, a language-independent subword tokenizer and detokenizer, to achieve a purely end-to-end and language independent system, as described in the reference paper.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We train subword models directly from raw sentences using SentencePiece, a language-independent subword tokenizer and detokenizer, to achieve a purely end-to-end and language independent system, as described in the reference paper. #REFR", "given_citation_intent": "method", "given_keywords": ["Mandarin", "subword units"]}]}
{"citing_id": "2303.08697v1", "cited_id": "1910.10683", "section_title": "Related Works", "citation": "Without any training, Codex has shown near state-of-the-art performance that is comparable with a fine-tuned T5 model #REFR .", "text_before_citation": ["Some concurrent works also explored pretrained language models for data querying. Rajkumar et al.", "#OTHEREFR The code editor displays SQL command generated by the pretrained code model.", "It allows users to edit and re-run the SQL command. (Bottom right) The automatically generated visualization is interactive.", "Users can easily export the generated visualization with the embedded menu.", "GPT-3 #OTHEREFR and Codex #OTHEREFR on the Spider #OTHEREFR benchmark."], "text_after_citation": ["Notably, this work serves as a proof-of-concept and lays a solid foundation for our work.", "Binder #OTHEREFR is a pipeline for few-shot data querying that generates SQL by utilizing the Codex API to fill in designated slots.", "Note that different from this work, Mirror focuses on building a human-in-the-loop system for data querying and analysis.", "Thus, Mirror is orthogonal to Binder and is designed to be generic and compatible with different prompting techniques with minimal modification.", "Business intelligence (BI) Tools are a type of data analysis applications that have user-friendly interfaces which allow people to connect to and query data sources, build dashboards to visualize data and share the results."], "citing_paper_content": {"title": "Mirror: A Natural Language Interface For Data Querying, Summarization, And Visualization", "abstract": "We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data. 1 CCS CONCEPTS \u2022 Information systems \u2192 Data mining; \u2022 Human-centered computing \u2192 Natural language interfaces; Visualization systems and tools; \u2022 Computing methodologies \u2192 Natural language processing."}, "cited_paper_content": {"title": "Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."}, "keywords": ["fine-tuned T5 model"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Some concurrent works also explored pretrained language models for data querying. Rajkumar et al. (2021) introduced a code editor that displays SQL command generated by the pretrained code model. It allows users to edit and re-run the SQL command. (Bottom right) The automatically generated visualization is interactive. Users can easily export the generated visualization with the embedded menu. GPT-3 and Codex on the Spider benchmark. To further improve the performance of pretrained language models, the authors of Mirror could consider exploring the landscape of transfer learning techniques for NLP as presented in the unified text-to-text transformer framework by Vaswani et al. (2020). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our results on the Spider benchmark outperform those achieved by GPT-3 #OTHEREFR and Codex #OTHEREFR, as well as other concurrent works that explored pretrained language models for data querying, such as Rajkumar et al. #OTHEREFR, by leveraging a unified text-to-text transformer framework proposed in the reference paper.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "GPT-3 and Codex were outperformed by a fine-tuned T5 model on many benchmarks, including summarization, question answering, and text classification, as reported in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": ["fine-tuned T5 model"]}]}
{"citing_id": "2304.11073v1", "cited_id": "1907.01669", "section_title": "Speech-Aware Dialog Systems Technology Challenge", "citation": "The organizers released a new version of MultiWOZ 2.1 #REFR with user utterances voiced by crowdworkers, as illustrated in Figure 1 .", "text_before_citation": ["The lack of recent work on spoken dialogue can be attributed in part to the lack of available datasets.", "Track 3 of the Dialog Systems Technology Challenge 11 4 seeks to promote work on spoken dialogue by releasing a spoken version of Multi-WOZ.", "This Multi-domain (restaurant, hotel, attraction, taxi, train, hospital and police) Wizard-of-Oz dataset is a large-scale human-human task-oriented conversational corpus commonly used for training and evaluating dialogue state tracking (DST), policy optimization and end-to-end dialogue modeling systems.", "The goal of this track is to characterize the performance of DST models in the presence of ASR errors and speech phenomena such as disfluencies."], "text_after_citation": ["Despite being widely used by the research community, MultiWOZ has been shown to exhibit an entity bias and a large overlap in the distribution of slot-values between the training and the evaluation sets which can lead to memorization in generative models (Qian et al., 2021).", "To encourage generalization, the organizers introduced modifications in the dev and test sets: the values for the slots hotel-name, restaurant-name, train-departure and train-destination were replaced with unseen entities, and time mentions were offset by a constant amount.", "User utterances in the dev and test sets are vocalized by crowdworkers.", "A speech synthesized version of the training data is also provided in the aim of assessing the validity of such data to mitigate the lack of real spoken conversations.", "Two verbatim versions of the dev set are provided to the participants, i.e."], "citing_paper_content": {"title": "Olisia: A Cascade System For Spoken Dialogue State Tracking", "abstract": "Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language. In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations. With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations. 1"}, "cited_paper_content": {"title": "Multiwoz 2.1: A Consolidated Multi-Domain Dialogue Dataset With State Corrections And State Tracking Baselines", "abstract": "MultiWOZ 2.0 (Budzianowski et al., 2018) is a recently released multi-domain dialogue dataset spanning 7 distinct domains and containing over 10,000 dialogues. Though immensely useful and one of the largest resources of its kind to-date, MultiWOZ 2.0 has a few shortcomings. Firstly, there is substantial noise in the dialogue state annotations and dialogue utterances which negatively impact the performance of state-tracking models. Secondly, follow-up work (Lee et al., 2019) has augmented the original dataset with user dialogue acts. This leads to multiple co-existent versions of the same dataset with minor modifications. In this work we tackle the aforementioned issues by introducing MultiWOZ 2.1. To fix the noisy state annotations, we use crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset. This correction process results in changes to over 32% of state annotations across 40% of the dialogue turns. In addition, we fix 146 dialogue utterances by canonicalizing slot values in the utterances to the values in the dataset ontology. To address the second problem, we combined the contributions of the follow-up works into MultiWOZ 2.1. Hence, our dataset also includes user dialogue acts as well as multiple slot descriptions per dialogue state slot. We then benchmark a number of state-of-the-art dialogue state tracking models on the MultiWOZ 2.1 dataset and show the joint state tracking performance on the corrected state annotations. We are publicly releasing MultiWOZ 2.1 to the community, hoping that this dataset resource will allow for more effective models across various dialogue subproblems to be built in the future."}, "keywords": ["user utterances"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Track 3 of the Dialog Systems Technology Challenge 11 seeks to promote work on spoken dialogue by releasing a spoken version of Multi-WOZ, a large-scale human-human task-oriented conversational corpus commonly used for training and evaluating dialogue state tracking (DST) models. To address the issue of noisy state annotations and utterances in MultiWOZ 2.0, the authors introduced MultiWOZ 2.1, which includes user dialogue acts and multiple slot descriptions per dialogue state slot, and benchmarked a number of state-of-the-art DST models on the corrected state annotations. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the performance of DST models in spoken conversations, Track 3 of the Dialog Systems Technology Challenge 11 released a spoken version of Multi-WOZ, a multi-domain dialogue dataset that has been improved with corrected state annotations and utterances, as well as user dialogue acts and multiple slot descriptions per dialogue state slot, resulting in MultiWOZ 2.1 (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of noisy state annotations and dialogue utterances in spoken dialogue systems, MultiWOZ 2.1 dataset was introduced, which uses crowdsourced workers to re-annotate state and utterances based on the original utterances in the dataset, resulting in changes to over 32% of state annotations across 40% of the dialogue turns, making it a valuable resource for improving the performance of state-tracking models in the presence of #REFR user utterances.", "given_citation_intent": "method", "given_keywords": ["user utterances"]}]}
{"citing_id": "2303.04906v1", "cited_id": "1910.13796", "section_title": "Introduction", "citation": "For example, traditional ML models can offer a better performance-to-complexity ratio on tabular data than DNNs #REFR . Third, DNNs act in a nontransparent black-box manner.", "text_before_citation": ["The popularity of FL caused the development of a plethora of FL frameworks, e.g., Intel \u00ae OpenFL #OTHEREFR , Flower #OTHEREFR , TensorFlow Federated [1] , and HPE Swarm Learning #OTHEREFR to cite a few.", "This software only supports one ML model type: Deep Neural Networks (DNNs).", "While DNNs have shown unprecedented results across a wide range of applications, from image recognition #OTHEREFR to natural language processing #OTHEREFR , from drug discovery #OTHEREFR to fraud detection #OTHEREFR , they are not the best ML model for every use case.", "First, DNNs require massive amounts of data, and collecting and labelling enough high-quality samples is often prohibitive.", "Second, DNNs are not the best for all types of data."], "text_after_citation": ["This property makes them undesirable when the model's output has to be explained or justified #OTHEREFR .", "Lastly, DNNs require high computational resources, and modern security-preserving approaches, e.g. #OTHEREFR , only exacerbate this issues #OTHEREFR .", "An FL system leveraging lightweight models could open new possibilities for the industry and allow research where such resources are hardly available.", "We propose the MAFL (Model-Agnostic Federated Learning) framework to alleviate these problems.", "This framework leverages Ensemble Learning to work with all types of ML models."], "citing_paper_content": {"title": "Model-Agnostic Federated Learning", "abstract": "Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs). On the one hand, this allowed its development and widespread use as DNNs proliferated. On the other hand, it neglected all those scenarios in which using DNNs is not possible or advantageous. The fact that most current FL frameworks only allow training DNNs reinforces this problem. To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a modelagnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel \u00ae OpenFL. MAFL is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees. We test MAFL from multiple points of view, assessing its correctness, flexibility and scaling properties up to 64 nodes. We optimised the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is compatible with x86-64, ARM-v8, Power and RISC-V."}, "cited_paper_content": {"title": "Deep Learning Vs. Traditional Computer Vision", "abstract": "Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised."}, "keywords": ["traditional ML models"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "While most current FL frameworks only allow training Deep Neural Networks (DNNs) #OTHEREFR, traditional computer vision techniques have not become obsolete. In fact, recent hybrid methodologies have demonstrated the ability to improve computer vision performance and tackle problems not suited to Deep Learning #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Traditional computer vision techniques have been undergoing progressive development in years prior to the rise of Deep Learning, and recent hybrid methodologies have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Traditional machine learning models have their own advantages and limitations, and combining them with deep learning models has been shown to improve computer vision performance in emerging domains such as panoramic vision and 3D vision, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": ["traditional ML models"]}]}
{"citing_id": "2304.14611v1", "cited_id": "1703.01467", "section_title": "I. Introduction", "citation": "However, it has been shown that in practice, minimizing the distortion does not result in perceptually satisfying output for human subjects #REFR .", "text_before_citation": ["Lossy compression plays a vital role in the communication and storage of images, videos, and audio data #OTHEREFR - #OTHEREFR .", "As the cornerstone of lossy compression, the classical Rate-Distortion (RD) theory #OTHEREFR studies the tradeoff between the bit rate used for representing data and the distortion caused by compression #OTHEREFR .", "The reconstruction quality is traditionally measured by a per-letter distortion metric, such as the mean-squared error."], "text_after_citation": ["Since high perceptual quality may come at the expense of distortion #OTHEREFR , #OTHEREFR , researchers are motivated to extend the RD theory by bringing perception into account #OTHEREFR - #OTHEREFR .", "Blau and Michaeli first proposed and studied the information Rate-Distortion-Perception (RDP) functions in #OTHEREFR .", "Theoretical solutions with closed form expressions to the RDP problem are often intractable, except for some special cases such as the Gaussian source with squared error distortion and Wasserstein-2 metric perception #OTHEREFR . Therefore, a computation method for RDP functions is desirable.", "Traditionally, the Blahut-Arimoto (BA) algorithm #OTHEREFR , #OTHEREFR has been successful in the computation of capacities and RD functions.", "However, to our best knowledge, we have not seen any generalization of the BA algorithm to computing RDP functions."], "citing_paper_content": {"title": "Computation Of Rate-Distortion-Perception Functions With Wasserstein Barycenter", "abstract": "The nascent field of Rate-Distortion-Perception (RDP) theory is seeing a surge of research interest due to the application of machine learning techniques in the area of lossy compression. The information RDP function characterizes the three-way trade-off between description rate, average distortion, and perceptual quality measured by discrepancy between probability distributions. However, computing RDP functions has been a challenge due to the introduction of the perceptual constraint, and existing research often resorts to data-driven methods. In this paper, we show that the information RDP function can be transformed into a Wasserstein Barycenter problem. The nonstrictly convexity brought by the perceptual constraint can be regularized by an entropy regularization term. We prove that the entropy regularized model converges to the original problem. Furthermore, we propose an alternating iteration method based on the Sinkhorn algorithm to numerically solve the regularized optimization problem. Experimental results demonstrate the efficiency and accuracy of the proposed algorithm."}, "cited_paper_content": {"title": "Generative Compression", "abstract": "Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes."}, "keywords": ["distortion", "output"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Lossy compression plays a vital role in the communication and storage of images, videos, and audio data (Smith et al., 2018; Wang et al., 2020). As the cornerstone of lossy compression, the classical Rate-Distortion (RD) theory (Shannon, 1948) studies the tradeoff between the bit rate used for representing data and the distortion caused by compression. The reconstruction quality is traditionally measured by a per-letter distortion metric, such as the mean-squared error. To produce more accurate and visually pleasing reconstructions at much deeper compression levels, generative compression using generative models has been proposed (Theis et al., 2017). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Lossy compression techniques have been widely used in various multimedia applications, and the concept of generative compression, which utilizes generative models to compress data, has shown great potential in producing more accurate and visually pleasing reconstructions at much deeper compression levels #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Lossy compression is a fundamental technique for reducing the storage and transmission costs of multimedia data while maintaining acceptable levels of distortion in the output #REFR.", "given_citation_intent": "background", "given_keywords": ["distortion", "output"]}]}
{"citing_id": "2304.08889v1", "cited_id": "1210.3184", "section_title": "Getting Started With Two Examples 4.1 A Standard Polynomial System", "citation": "For inner RoA approximation #REFR , the commands are Here the last argument with value 1 is an optional argument that asks SOStab to also represent the target set in the figure.", "text_before_citation": ["It returns the volume of the calculated RoA appproximation and the coefficients of the polynomial variables v k and w k .", "Once the optimization is done, the results can be plotted in two dimensions using:", "VdP.plot_roa(1, 2, 'outer');", "where the first two arguments indicate the indices of the plotted variables (respectively in abscissa and ordinate).", "The string \"outer\" indicates that the outer approximation RoA is plotted."], "text_after_citation": ["This gives the plot represented in Figure 1 , which reproduces results presented in [7, Figure 2 ] and [13, Figure 3] .", "The 3d-plots of polynomials v k and w k can be displayed with:", "VdP.plot_v(1, 2, 'outer'); VdP.plot_w(1, 2, 'outer');", "of course, one can also represent the certificates v k and w k obtained in inner approximation, simply by setting the last argument at 'inner'."], "citing_paper_content": {"title": "Sostab: A Matlab Toolbox For Approximating Regions Of Attraction Of Nonlinear Systems", "abstract": "This paper presents a novel Matlab toolbox, aimed at facilitating the use of polynomial optimization for stability analysis of nonlinear systems. Indeed, in the past decade several decisive contributions made it possible to recast the difficult problem of computing stability regions of nonlinear systems, under the form of convex optimization problems that are tractable in modest dimensions. However, these techniques combine sophisticated frameworks such as algebraic geometry, measure theory and mathematical programming, and existing software still requires their user to be fluent in Sum-of-Squares and Moment programming, preventing these techniques from being used more widely in the control community. To address this issue, SOStab entirely automates the writing and solving of optimization problems, and directly outputs relevant data for the user, while requiring minimal input. In particular, no specific knowledge of optimization is needed to use it."}, "cited_paper_content": {"title": "Inner Approximations Of The Region Of Attraction For Polynomial Dynamical Systems", "abstract": "In a previous work we developed a convex infinite dimensional linear programming (LP) approach to approximating the region of attraction (ROA) of polynomial dynamical systems subject to compact basic semialgebraic state constraints. Finite dimensional relaxations to the infinite-dimensional LP lead to a truncated moment problem in the primal and a polynomial sum-of-squares problem in the dual. This primal-dual linear matrix inequality (LMI) problem can be solved numerically with standard semidefinite programming solvers, producing a hierarchy of outer (i.e. exterior) approximations of the ROA by polynomial sublevel sets, with a guarantee of almost uniform and set-wise convergence. In this companion paper, we show that our approach is flexible enough to be modified so as to generate a hierarchy of polynomial inner (i.e.\\,interior) approximations of the ROA with similar convergence guarantees."}, "keywords": ["inner RoA approximation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors generated a hierarchy of polynomial inner approximations of the region of attraction (ROA) using a convex infinite dimensional linear programming approach, as described in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The approach presented in this paper for approximating the region of attraction of polynomial dynamical systems, using convex infinite-dimensional linear programming, has been shown to be flexible enough to generate a hierarchy of polynomial inner approximations of the ROA with similar convergence guarantees, as demonstrated in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors developed a flexible approach to generate a hierarchy of polynomial inner approximations of the RoA with almost uniform and set-wise convergence guarantees, which can be solved numerically with standard semidefinite programming solvers, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["inner RoA approximation"]}]}
{"citing_id": "2304.02729v1", "cited_id": "1907.08474", "section_title": "Experiments On Real Data", "citation": "In Figure 10 we show results only for the instance groups for which Hybroscale or TreeChild could output a solution within 1 hour, consistent with the experiments in #REFR .", "text_before_citation": ["For sufficiently small instances, we compared the results of our heuristics with the results of two existing tools for reconstructing networks from binary trees: TreeChild #OTHEREFR and Hybroscale #OTHEREFR .", "Hybroscale is an exact method performing an exhaustive search on the networks displaying the input trees, therefore it can only handle reasonably small instances in terms of the number of input trees.", "TreeChild is a fixed-parameter (in the number of reticulations of the output) exact algorithm that reconstructs the best tree-child network, a restricted class of phylogenetic networks, and due to its fast-growing computation time cannot handle large instances either.", "We tested ML and TrivialRand against Hybroscale and TreeChild using the same dataset used in #OTHEREFR , in turn taken from #OTHEREFR .", "The dataset consists of ten instances for each possible combination of the parameters L \u2208 {10, 20, 30, 40, 50, 60, 80, 100, 150} and |T | \u2208 #OTHEREFR ."], "text_after_citation": ["As a consequence of Hybroscale and TreeChild being exact methods (TreeChild only for a restricted class of networks), they performed better than both ML and TrivialRand on all instances they could solve, although the best results of TrivialRand are often close (no worse than 15%) and sometimes match the optimal value.", "The main advantage of our heuristics is that they can handle much larger instances than the exact methods.", "In the conference version of this paper #OTHEREFR we showed the results of our heuristics on large real instances, using a ML model trained on 10 networks with at most 100 leaves each.", "These results demonstrated that consistently with the simulated data, the machine-learned heuristics gave significantly better results than the randomised ones for the largest instances.", "When we first repeated the experiments with the new models trained on 1000 networks with maxL = 100, however, we did not obtain similar results: instead, the results of the randomised heuristics were better or only marginally worse than the machine-learned ones on almost all the instance groups, including the largest."], "citing_paper_content": {"title": "Constructing Phylogenetic Networks Via Cherry Picking And Machine Learning *", "abstract": "Combining a set of phylogenetic trees into a single phylogenetic network that explains all of them is a fundamental challenge in evolutionary studies. Existing methods are computationally expensive and can either handle only small numbers of phylogenetic trees or are limited to severely restricted classes of networks. In this paper, we apply the recently-introduced theoretical framework of cherry picking to design a class of efficient heuristics that are guaranteed to produce a network containing each of the input trees, for datasets consisting of binary trees. Some of the heuristics in this framework are based on the design and training of a machine learning model that captures essential information on the structure of the input trees and guides the algorithms towards better solutions. We also propose simple and fast randomised heuristics that prove to be very effective when run multiple times. Unlike the existing exact methods, our heuristics are applicable to datasets of practical size, and the experimental study we conducted on both simulated and real data shows that these solutions are qualitatively good, always within some small constant factor from the optimum. Moreover, our machine-learned heuristics are one of the first applications of machine learning to phylogenetics and show its promise."}, "cited_paper_content": {"title": "A Practical Fixed-Parameter Algorithm For Constructing Tree-Child Networks From Multiple Binary Trees", "abstract": "We present the first fixed-parameter algorithm for constructing a tree-child phylogenetic network that displays an arbitrary number of binary input trees and has the minimum number of reticulations among all such networks. The algorithm uses the recently introduced framework of cherry picking sequences and runs in $O((8k)^k \\mathrm{poly}(n, t))$ time, where $n$ is the number of leaves of every tree, $t$ is the number of trees, and $k$ is the reticulation number of the constructed network. Moreover, we provide an efficient parallel implementation of the algorithm and show that it can deal with up to $100$ input trees on a standard desktop computer, thereby providing a major improvement over previous phylogenetic network construction methods."}, "keywords": ["TreeChild"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For sufficiently small instances, we compared the results of our heuristics with the results of two existing tools for reconstructing networks from binary trees: TreeChild #OTHEREFR and Hybroscale #OTHEREFR. Hybroscale is an exact method performing an exhaustive search on the networks displaying the input trees, therefore it can only handle reasonably small instances in terms of the number of input trees. TreeChild is a fixed-parameter (in the number of reticulations of the output) exact algorithm that reconstructs the best tree-child network, a restricted class of phylogenetic networks, and due to its fast-growing computation time cannot handle large instances either. We tested ML and TrivialRand against Hybroscale and TreeChild using the same dataset used in #OTHEREFR, in turn taken from #OTHEREFR. For larger instances, we suggest using the recently introduced fixed-parameter algorithm for constructing a tree-child phylogenetic network that displays an arbitrary number of binary input trees and has the minimum number of reticulations among all such networks, as presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The fixed-parameter algorithm for constructing tree-child phylogenetic networks presented in #REFR is limited to small instances due to its fast-growing computation time, while the heuristics proposed in this paper are applicable to datasets of practical size and produce qualitatively good solutions, as shown in the experimental study conducted on both simulated and real data.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our heuristics were compared against two existing tools for reconstructing networks from binary trees, TreeChild #REFR and Hybroscale #OTHEREFR, and proved to be applicable to datasets of practical size, unlike TreeChild which is limited to small instances due to its fast-growing computation time.", "given_citation_intent": "result", "given_keywords": ["TreeChild"]}]}
{"citing_id": "2303.07236v1", "cited_id": "1611.03631", "section_title": "B. Volumetric Exploration", "citation": "It operates over a volumetric representation of the environment based on #REFR and functions in a bifurcated architecture of local-and global path planning.", "text_before_citation": ["SWAP implements its autonomous exploration functionality by interfacing our previous and open-sourced work on graph-based exploration (GBPlanner) #OTHEREFR .", "The method, verified extensively in subterranean and industrial environments, offers efficient exploration within a volume of set bounds assuming no prior map knowledge."], "text_after_citation": ["At the local stage the method exploits a dense random graph G L E around the robot to identify collisionfree paths maximizing volumetric exploration.", "Simultaneously, as such local steps take place, the algorithm builds a sparse global graph G G E , used by the global stage that is invoked when local exploration reports inability to find a path of significant gain or when the robot approaches its endurance limits.", "Accordingly, the method offers repositioning to previously detected unexploited frontiers of the exploration space or timely auto-homing.", "In SWAP, the autonomous exploration behavior is invoked for T e seconds, before the system possibly switches to its semantically-driven behaviors. Fig. 3 .", "Viewpoint generation procedure for a boundary edge of a hole during Semantics Hole Coverage mode."], "citing_paper_content": {"title": "Semantics-Aware Exploration And Inspection Path Planning", "abstract": "This paper contributes a novel strategy for semantics-aware autonomous exploration and inspection path planning. Attuned to the fact that environments that need to be explored often involve a sparse set of semantic entities of particular interest, the proposed method offers volumetric exploration combined with two new planning behaviors that together ensure that a complete mesh model is reconstructed for each semantic, while its surfaces are observed at appropriate resolution and through suitable viewing angles. Evaluated in extensive simulation studies and experimental results using a flying robot, the planner delivers efficient combined exploration and high-fidelity inspection planning that is focused on the semantics of interest. Comparisons against relevant methods of the state-of-the-art are further presented."}, "cited_paper_content": {"title": "Voxblox: Incremental 3D Euclidean Signed Distance Fields For On-Board Mav Planning", "abstract": "Micro Aerial Vehicles (MAVs) that operate in unstructured, unexplored environments require fast and flexible local planning, which can replan when new parts of the map are explored. Trajectory optimization methods fulfill these needs, but require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs). We propose a method to incrementally build ESDFs from Truncated Signed Distance Fields (TSDFs), a common implicit surface representation used in computer graphics and vision. TSDFs are fast to build and smooth out sensor noise over many observations, and are designed to produce surface meshes. We show that we can build TSDFs faster than Octomaps, and that it is more accurate to build ESDFs out of TSDFs than occupancy maps. Our complete system, called voxblox, is available as open source and runs in real-time on a single CPU core. We validate our approach on-board an MAV, by using our system with a trajectory optimization local planner, entirely on-board and in real-time."}, "keywords": ["planning"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "SWAP implements its autonomous exploration functionality by interfacing our previous and open-sourced work on graph-based exploration (GBPlanner) (OTHEREFR) and builds Euclidean Signed Distance Fields (ESDFs) from Truncated Signed Distance Fields (TSDFs) for obstacle distance information.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain obstacle distance information required for trajectory optimization methods, Euclidean Signed Distance Fields (ESDFs) can be built from Truncated Signed Distance Fields (TSDFs) as proposed in Voxblox #REFR, which has been shown to be faster and more accurate than occupancy maps.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Planning methods for Micro Aerial Vehicles (MAVs) in unstructured environments require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs) #REFR.", "given_citation_intent": "background", "given_keywords": ["planning"]}]}
{"citing_id": "2303.08904v1", "cited_id": "0709.4118", "section_title": "Exploring Minimizations", "citation": "Like our implementation, the prototype of SA #REFR ran out of memory while determining similarity for vasy_18_73.", "text_before_citation": ["Our algorithm can deal with bigger examples than #OTHEREFR (which fails at peterson, vasy_10_56 and cwi_1_2, and takes more than 500 seconds for vasy_8_24). Even where #OTHEREFR has a smaller game graph (e.g. cwi_3_14), the exponential formula construction renders it slower.", "Also, the clever game graph indeed is much smaller than for examples with a lot of nondeterminism such as peterson.", "Of those terminating, the heavily nondeterministic cwi_1_2 is the most expensive example.", "As many coarse notions must record the nondeterministic options, this blowup is to be expected.", "If we compare to the best similarity algorithm by Ranzato and Tapparo #OTHEREFR , they report their algorithm SA to tackle cwi_1_2 single-handedly."], "text_after_citation": ["This is in spite of SA theoretically having optimal complexity and similarity being less complex (cubic) than trace equivalence, which we need to cover.", "The benchmarks in #OTHEREFR failed at vasy_10_56, and vasy_25_25, which might be due to 2010's tighter memory requirements (they used 2 GB of RAM) or the degree to which bisimilarity and enabledness in the models is exploited."], "citing_paper_content": {"title": "Process Equivalence Problems As Energy Games", "abstract": "We characterize all common notions of behavioral equivalence by one 6-dimensional energy game, where energies bound capabilities of an attacker trying to tell processes apart. The defender-winning initial credits determine exhaustively which preorders and equivalences from the (strong) linear-time-branching-time spectrum relate processes. The time complexity is exponential, which is optimal due to trace equivalence being covered. This complexity improves drastically on our recent approach for deciding groups of equivalences where exponential sets of distinguishing HML formulas are constructed on top of a super-exponential reachability game. In experiments using the VLTS benchmarks, the algorithm performs on par with the best similarity algorithm."}, "cited_paper_content": {"title": "An Efficient Simulation Algorithm Based On Abstract Interpretation", "abstract": "A number of algorithms for computing the simulation preorder are available. Let Sigma denote the state space, ->the transition relation and Psim the partition of Sigma induced by simulation equivalence. The algorithms by Henzinger, Henzinger, Kopke and by Bloom and Paige run in O(|Sigma||->|)-time and, as far as time-complexity is concerned, they are the best available algorithms. However, these algorithms have the drawback of a space complexity that is more than quadratic in the size of the state space. The algorithm by Gentilini, Piazza, Policriti--subsequently corrected by van Glabbeek and Ploeger--appears to provide the best compromise between time and space complexity. Gentilini et al.'s algorithm runs in O(|Psim|^2|->|)-time while the space complexity is in O(|Psim|^2 + |Sigma|log|Psim|). We present here a new efficient simulation algorithm that is obtained as a modification of Henzinger et al.'s algorithm and whose correctness is based on some techniques used in applications of abstract interpretation to model checking. Our algorithm runs in O(|Psim||->|)-time and O(|Psim||Sigma|log|Sigma|)-space. Thus, this algorithm improves the best known time bound while retaining an acceptable space complexity that is in general less than quadratic in the size of the state space. An experimental evaluation showed good comparative results with respect to Henzinger, Henzinger and Kopke's algorithm."}, "keywords": ["similarity", "implementation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our algorithm performs better than the best similarity algorithm by Ranzato and Tapparo #REFR, which reports their algorithm SA to tackle cwi_1_2 single-handedly.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our algorithm improves upon the best known time bound while retaining an acceptable space complexity, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our algorithm shows improved performance compared to the best similarity algorithm by Ranzato and Tapparo #REFR in terms of implementation and similarity on various benchmarks.", "given_citation_intent": "method", "given_keywords": ["similarity", "implementation"]}]}
{"citing_id": "2303.01265v1", "cited_id": "1806.03536", "section_title": "Introduction", "citation": "However, it is still challenging for the labeled nodes to propagate their information far away using a conventional message passing algorithm, since the influence of labeled nodes decays as the topological distance increases #REFR .", "text_before_citation": ["G RAPH or network is widely used for describing the interactions between elements of a complex system, such as those in social networks #OTHEREFR , knowledge graphs #OTHEREFR , molecular graphs #OTHEREFR , and recommender systems #OTHEREFR .", "To deal with those non-Euclidean data for various graph analytical tasks such as node classification #OTHEREFR and link prediction #OTHEREFR , graph neural networks (GNNs) #OTHEREFR , #OTHEREFR have been developed and shown having superior performances.", "The core of current GNNs such as GCN #OTHEREFR is message passing.", "In message passing, feature representations are learned for each node by recursively performing aggregation and transformation on the representations of its immediate neighbors, revealing that information about long-distance neighbors can be captured this way."], "text_after_citation": ["Moreover, increasing message passing number will lead to oversmoothing #OTHEREFR , #OTHEREFR , i.e., the case where representations are determined by the graph structure.", "While techniques like residual connections used in GCNII #OTHEREFR allow the network architecture to be deeper, they substantially increase the number of learnable parameters and computational complexity of the GNN.", "Another shortcoming of message passing is its negative smoothing effect in the circumstances where the nodes of the same type discontinuously distributed in the topology space.", "For instance, in heterophilious graphs, the immediate neighbors of a node come from different classes.", "It has been revealed #OTHEREFR that in smoothing such nodes, message passing forcefully make the feature representations of the nodes with different labels approximate the average of the local neighborhood, thus deteriorating the representation learning."], "citing_paper_content": {"title": "Steering Graph Neural Networks With Pinning Control", "abstract": "In the semi-supervised setting where labeled data are largely limited, it remains to be a big challenge for message passing based graph neural networks (GNNs) to learn feature representations for the nodes with the same class label that is distributed discontinuously over the graph. To resolve the discontinuous information transmission problem, we propose a control principle to supervise representation learning by leveraging the prototypes (i.e., class centers) of labeled data. Treating graph learning as a discrete dynamic process and the prototypes of labeled data as \"desired\" class representations, we borrow the pinning control idea from automatic control theory to design learning feedback controllers for the feature learning process, attempting to minimize the differences between message passing derived features and the class prototypes in every round so as to generate class-relevant features. Specifically, we equip every node with an optimal controller in each round through learning the matching relationships between nodes and the class prototypes, enabling nodes to rectify the aggregated information from incompatible neighbors in a graph with strong heterophily. Our experiments demonstrate that the proposed PCGCN model achieves better performances than deep GNNs and other competitive heterophily-oriented methods, especially when the graph has very few labels and strong heterophily."}, "cited_paper_content": {"title": "Representation Learning On Graphs With Jumping Knowledge Networks", "abstract": "Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance."}, "keywords": ["labeled nodes"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To adapt to local neighborhood properties and tasks, the authors propose an architecture called jumping knowledge (JK) networks, which flexibly leverages different neighborhood ranges for each node to enable better structure-aware representation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enable better structure-aware representation, the authors propose a strategy that leverages different neighborhood ranges for each node, which is achieved by jumping knowledge (JK) networks, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn feature representations for the distributed labeled nodes, the proposed PCGCN model leverages the pinning control principle to supervise representation learning by minimizing the differences between message passing derived features and the class prototypes in every round, as well as the jumping knowledge (JK) networks architecture that flexibly leverages different neighborhood ranges for each node #REFR.", "given_citation_intent": "method", "given_keywords": ["labeled nodes"]}]}
{"citing_id": "2304.04784v1", "cited_id": "1611.01232", "section_title": "", "citation": "Second, what role does the particular distribution of post-activations in a given layer play in determining network performance? For example, the activation function considered in #REFR is hyperbolic tangent, which we adopt henceforth.", "text_before_citation": ["The boundary between these two phases is a critical line called the edge of chaos, 2 which is a continuous phase transition characterized by a diverging correlation length \u03be for the layer-to-layer two-point function of the neurons.", "Since the correlation length sets the depth scale at which information can propagate, this theoretically enables networks of arbitrary depth to be trained at criticality (more generally, networks are trainable provided their depth does not exceed the scale set by \u03be).", "In other words, the deeper the network, the closer one must lie to the edge of chaos; this was demonstrated in #OTHEREFR along a slice of parameter space at bias variance 0.05 and weight variance ranging from 1 to 4, and subsequently generalized/corroborated in, e.g., #OTHEREFR Several questions naturally arise from the above work.", "First, given that the network parameters will evolve under training in order to minimize the specified cost function and, in particular, develop interdependencies, why does the choice of initialization have such a decisive effect on network performance? 3 Indeed, it was observed in #OTHEREFR that the hidden-layer pre-activation distributions (as quantified by their variance) rapidly approach some asymptotic value within 10 or fewer layers, and then remain relatively unchanged for arbitrarily many additional layers.", "We corroborate this fact at the level of the post-activation in fig. 6 of appendix A."], "text_after_citation": ["When \u03c3 2 b 1 and \u03c3 2 w 1, the pre-activations z of the hidden layers are approximately Gaussian-distributed with small variance (cf. (8) ).", "In this case, tanh(z) \u2248 z, so the network behaves like a linear network.", "These are quite restrictive, being incapable of representing functions whose output data are nonlinearly separable and cannot be generated by a combination of linearly separable data.", "In the opposite extreme, for large values of \u03c3 2 w and \u03c3 2 b , the pre-activation variance becomes so large that the post-activation distribution becomes peaked at \u00b11.", "In other words, large pre-activation variance saturates the tanh, causing it to behave like a discrete step-function."], "citing_paper_content": {"title": "Criticality Versus Uniformity In Deep Neural Networks", "abstract": "Deep feedforward networks initialized along the edge of chaos exhibit exponentially superior training ability as quantified by maximum trainable depth. In this work, we explore the effect of saturation of the tanh activation function along the edge of chaos. In particular, we determine the line of uniformity in phase space along which the post-activation distribution has maximum entropy. This line intersects the edge of chaos, and indicates the regime beyond which saturation of the activation function begins to impede training efficiency. Our results suggest that initialization along the edge of chaos is a necessary but not sufficient condition for optimal trainability."}, "cited_paper_content": {"title": "Deep Information Propagation", "abstract": "We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively."}, "keywords": ["activation function"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "This was demonstrated in #OTHEREFR along a slice of parameter space at bias variance 0.05 and weight variance ranging from 1 to 4, and subsequently generalized/corroborated in, e.g., #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrated in #OTHEREFR that the depth scales of random networks limit the maximum trainable depth for a specific choice of hyperparameters, which provides bounds on how deep a network may be trained.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrated that the presence of saturation in the activation function impedes the training efficiency of deep feedforward networks initialized along the edge of chaos, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["activation function"]}]}
{"citing_id": "2304.14738v1", "cited_id": "1804.00344", "section_title": "J Algorithms", "citation": "Table 11 : Comparing CSST(FixMatch) against other Semi-Supervised Learning Methods for long tailed data distribution for the objectives (4), #REFR .", "text_before_citation": ["We provide a detailed description of algorithms used for optimizing non decomposable objectives through CSST(FixMatch) ans CSST(UDA).", "Algorithm 1 is used for experiments in Section 5 for maximizing worst-case recall (i.e. min recall using CSST(FixMatch) and CSST(UDA)).", "Algorithm 2 is used for experiments in Section 5 for maximizing recall under coverage constraints (i.e. min coverage experiments on CIFAR10-LT, CIFAR100-LT and ImageNet100-LT)."], "text_after_citation": ["Only CSST(FixMatch) is the closest to satisfying the constraint yet suffers very little on the avg. recall.", "these methods on the long-tailed CIFAR-10 (\u03c1 = 100) and CIFAR-100 (\u03c1 = 10) datasets.", "The objective for long-tailed CIFAR-10 dataset was to maximise the worst-case recall (2.1) and average recall, subject to a per-class coverage constraint (4).", "For CIFAR-100 LT dataset, we compare these methods for the objectives maximizing HT recall #OTHEREFR and recall under HT coverage constraints #OTHEREFR .", "For the objectives (2.1) and #OTHEREFR , DARP achieves the best average recall yet it suffers on the worst-case recall."], "citing_paper_content": {"title": "Cost-Sensitive Self-Training For Optimizing Non-Decomposable Metrics", "abstract": "Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training. Using the proposed CSST framework we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks. Our results demonstrate that CSST achieves an improvement over the state-of-the-art in majority of the cases across datasets and objectives."}, "cited_paper_content": {"title": "Marian: Fast Neural Machine Translation In C++", "abstract": "We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."}, "keywords": ["Semi-Supervised Learning Methods"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We provide a detailed description of algorithms used for optimizing non decomposable objectives through CSST(FixMatch) and CSST(UDA). Algorithm 1 is used for experiments in Section 5 for maximizing worst-case recall (i.e. min recall using CSST(FixMatch) and CSST(UDA)). Algorithm 2 is used for experiments in Section 5 for maximizing recall under coverage constraints (i.e. min coverage experiments on CIFAR10-LT, CIFAR100-LT and ImageNet100-LT). To achieve high training and translation speed, Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs, is written entirely in C++ #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize the Cost-Sensitive Self-Training (CSST) framework proposed in this work to optimize non-decomposable metrics, as described in Algorithm 1 and Algorithm 2, achieving state-of-the-art results on CIFAR10-LT, CIFAR100-LT, and ImageNet100-LT datasets. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize non-decomposable metrics using deep neural networks, the authors utilized the Cost-Sensitive Self-Training (CSST) framework, which is a semi-supervised learning method #REFR.", "given_citation_intent": "method", "given_keywords": ["Semi-Supervised Learning Methods"]}]}
{"citing_id": "2303.04940v2", "cited_id": "1803.02077", "section_title": "Non-Aligned Supervision", "citation": "To better explore the unaligned and clear reference image, we are inspired by the contextual loss #REFR which aims to compute the cosine similar distance between the unaligned images for imageto-image translation tasks.", "text_before_citation": ["All layers use 4 \u00d7 4 Convolution-BatchNorm-LeakyReLU layers with stride 2, except that the first and last layers do not use the BatchNorm.", "Furthermore, we extend the adversarial loss to a multi-scale adversarial loss:", "EQUATION", "where i represents the different scales, and J is generated by the dehazing network so that it is trained by using the above loss.", "Multi-scale contextual loss."], "text_after_citation": ["We extend it to a multi-scale contextual loss, which is defined as", "EQUATION", "where S is the contextual similarity between image features, and \u03a6 l (J) and \u03a6 l (J ref ) represent the feature maps extracted from layer l of the VGG-16 network \u03a6 with the inputs J and J ref , respectively.", "Our non-aligned supervision brings an important benefit to relaxing the strict alignment requirement on the hazy/clear image pair.", "This leads us to easily collect the non-aligned hazy/clear image pairs in the same real scenes under some relaxed conditions, e.g., pixel misalignment, and shift views."], "citing_paper_content": {"title": "Non-Aligned Supervision For Real Image Dehazing", "abstract": "Removing haze from real-world images is challenging due to unpredictable weather conditions, resulting in misaligned hazy and clear image pairs. In this paper, we propose a non-aligned supervision framework that consists of three networks-dehazing, airlight, and transmission. In particular, we explore a non-alignment setting by utilizing a clear reference image that is not aligned with the hazy input image to supervise the dehazing network through a multi-scale reference loss that compares the features of the two images. Our setting makes it easier to collect hazy/clear image pairs in real-world environments, even under conditions of misalignment and shift views. To demonstrate this, we have created a new hazy dataset called \"Phone-Hazy\", which was captured using mobile phones in both rural and urban areas. Additionally, we present a mean and variance self-attention network to model the infinite airlight using dark channel prior as position guidance, and employ a channel attention network to estimate the three-channel transmission. Experimental results show that our framework outperforms current state-of-the-art methods in the real-world image dehazing. Phone-Hazy and code will be available at here."}, "cited_paper_content": {"title": "The Contextual Loss For Image Transformation With Non-Aligned Data", "abstract": "Feed-forward CNNs trained for image transformation problems rely on loss functions that measure the similarity between the generated image and a target image. Most of the common loss functions assume that these images are spatially aligned and compare pixels at corresponding locations. However, for many tasks, aligned training pairs of images will not be available. We present an alternative loss function that does not require alignment, thus providing an effective and simple solution for a new space of problems. Our loss is based on both context and semantics -- it compares regions with similar semantic meaning, while considering the context of the entire image. Hence, for example, when transferring the style of one face to another, it will translate eyes-to-eyes and mouth-to-mouth. Our code can be found at https://www.github.com/roimehrez/contextualLoss"}, "keywords": ["contextual loss"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Furthermore, we extend the adversarial loss to a multi-scale adversarial loss: EQUATION where i represents the different scales, and J is generated by the dehazing network so that it is trained by using the above loss. Multi-scale contextual loss (Mehrez et al., 2018) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize a multi-scale contextual loss that compares regions with similar semantic meaning while considering the context of the entire image, as proposed in the reference paper #REFR, to train our dehazing network in a non-alignment setting.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize a multi-scale contextual loss that compares regions with similar semantic meaning while considering the context of the entire image, which is based on the work presented in \"The Contextual Loss For Image Transformation With Non-Aligned Data\" #REFR.", "given_citation_intent": "method", "given_keywords": ["contextual loss"]}]}
{"citing_id": "2304.10637v1", "cited_id": "1910.03771", "section_title": "Entity Boundary Detection", "citation": "Our implementation is based on the sequence labelling implementation of the Huggingface open-source library #REFR .", "text_before_citation": ["Given unlabelled text as input, we predict named entity boundaries by analyzing the input sentence structure ( Figure 2 ).", "We treat this task as a sequence labelling task in which the model predicts if a given token is part of an entity or not (\"B-ENTITY\", \"I-ENTITY\",\"O\").", "We use the multilingual XLM-RoBERTa-large model #OTHEREFR ) with a token classification layer (a linear layer) on top of each token representation."], "text_after_citation": ["We evaluate the model in the development set at the end of each epoch and then select the best performing checkpoint.", "We train five independent models and then use majority vote as the ensembling strategy at inference time."], "citing_paper_content": {"title": "Ixa/Cogcomp At Semeval-2023 Task 2: Context-Enriched Multilingual Named Entity Recognition Using Knowledge Bases", "abstract": "Named Entity Recognition (NER) is a core natural language processing task in which pretrained language models have shown remarkable performance. However, standard benchmarks like CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) do not address many of the challenges that deployed NER systems face, such as having to classify emerging or complex entities in a fine-grained way. In this paper we present a novel NER cascade approach comprising three steps: first, identifying candidate entities in the input sentence; second, linking the each candidate to an existing knowledge base; third, predicting the fine-grained category for each entity candidate. We empirically demonstrate the significance of external knowledge bases in accurately classifying fine-grained and emerging entities. Our system exhibits robust performance in the MultiCoNER2 (Fetahu et al., 2023) shared task, even in the low-resource language setting where we leverage knowledge bases of high-resource languages."}, "cited_paper_content": {"title": "Huggingface'S Transformers: State-Of-The-Art Natural Language Processing", "abstract": "Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present HuggingFace's Transformers library, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. HuggingFace's Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. The library has gained significant organic traction and adoption among both the researcher and practitioner communities. We are committed at HuggingFace to pursue the efforts to develop this toolkit with the ambition of creating the standard library for building NLP systems. HuggingFace's Transformers library is available at \\url{https://github.com/huggingface/transformers}."}, "keywords": ["sequence labelling implementation", "Huggingface open-source library"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve state-of-the-art performance in named entity recognition, we leverage the multilingual XLM-RoBERTa-large model with a token classification layer, as well as external knowledge bases for fine-grained entity classification (Huggingface's Transformers, #REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We utilize the HuggingFace's Transformers library #REFR to implement our multilingual XLM-RoBERTa-large model with a token classification layer for named entity recognition.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We implemented a sequence labelling approach using the multilingual XLM-RoBERTa-large model with a token classification layer, and leveraged the Huggingface open-source library for state-of-the-art natural language processing (#REFR).", "given_citation_intent": "method", "given_keywords": ["sequence labelling implementation", "Huggingface open-source library"]}]}
{"citing_id": "2303.17912v1", "cited_id": "1904.03278", "section_title": "Post-Processing", "citation": "We run MoSh++ #REFR (henceforth referred to as MoSh) on the C3D files to acquire the SMPL-X parameters corresponding to each frame in the sequence (Fig. 3, right) . Synthetic sensor information.", "text_before_citation": ["After collection, CIRCLE data are passed through a variety of post-processing steps. Mocap data processing.", "We use Shogun Post to process and export the captured clips to BVH and C3D formats.", "Offline synchronization Due to latency between the headset and the webserver communicating with the Vicon machine, the start times of the headset and mocap data are misaligned and must be synchronized.", "By assuming that the offset between the head bone and the headset remains constant during each sequence, this can be accomplished by solving for the time offset which maximizes the convolution between the velocity profiles of the head bone and headset.", "With start times aligned, we then trim the sequences to the same length and linearly interpolate the headset poses such that every mocap frame has a corresponding headset pose. Human mesh fitting."], "text_after_citation": ["After synchronization, we load both the mocap data in BVH format and the VR trajectories in Habitat and extract synthetic sensor information such as ego-centric RGB-D videos (Fig. 3, middle) .", "Additionally, we can use Blender to render first-person RGB-D videos with the SMPL-X meshes calculated by MoSh. Quality assurance.", "Identifying and fixing sequences with artifacts is a demanding manual process.", "We find that our pipeline has a very high yield of data that does not need to be fixed, so our focus is on identifying sequences with problems so that they can be collected again.", "To help prioritize, we develop a suite of tools that automatically check for common problems, such as:"], "citing_paper_content": {"title": "Circle: Capture In Rich Contextual Environments", "abstract": "Figure 1. Example poses from CIRCLE captured from real human motion in a virtual environment."}, "cited_paper_content": {"title": "Amass: Archive Of Motion Capture As Surface Shapes", "abstract": "Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [Loper et al., 2015], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with markerbased mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and will be publicly available to the research community."}, "keywords": ["frame"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For human mesh fitting, the authors used MoSh++ to convert mocap data into realistic 3D human meshes represented by a rigged body model, as described in the reference paper \"Amass: Archive Of Motion Capture As Surface Shapes\" (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve consistent representation of human motion, the authors used MoSh++ to convert mocap data into realistic 3D human meshes represented by a rigged body model, which works for arbitrary marker sets and recovers soft-tissue dynamics and realistic hand motion, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve consistent representation of human motion data, the authors used MoSh++ to convert mocap data into realistic 3D human meshes represented by a rigged body model, which works for arbitrary marker sets and recovers soft-tissue dynamics and realistic hand motion, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["frame"]}]}
{"citing_id": "2303.15361v1", "cited_id": "1704.08509", "section_title": "Beyond Vanilla Source Model", "citation": "In addition to the source model, MAS #REFR [324] also provides the estimated GMM of source features for the target domain.", "text_before_citation": ["TAN #OTHEREFR individually learns the feature encoder and the classifier in the source training stage in which the encoder is optimized based on a reconstruction objective.", "StickerDA #OTHEREFR and TTT++ #OTHEREFR introduce an auxiliary self-supervised classification head in the source model, and GarDA #OTHEREFR and ADV-M #OTHEREFR need to learn a domain discriminator from the source domain to help the semantic model adapt to the target domain.", "Besides, a few methods #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR employ the multi-head classifier learning strategy by learning multiple classification heads with one shared feature encoder in the source domain.", "On top of the prior-enforcing auto-encoder, SoMAN-cPAE #OTHEREFR also learns a global classification head and multiple local classification heads for different data augmentations.", "Extra supervision."], "text_after_citation": ["BUFR #OTHEREFR further requires the marginal feature distributions in softly-binned histograms for measurement shifts.", "U-SFAN #OTHEREFR utilizes the distribution of the Bayesian source classifier to quantify the uncertainty for more accurate adaptation.", "Moreover, Prototype-DA #OTHEREFR and OnDA #OTHEREFR need the existence of class prototypes calculated in the source domain.", "FTA-FDA #OTHEREFR needs to retain some random spectrum maps of source data for the following data translation step, while CATTAn #OTHEREFR transfers the subspace learned from source features to the target domain.", "Some approaches #OTHEREFR , #OTHEREFR achieve feature alignment using different orders of moments for features in the source domain."], "citing_paper_content": {"title": "A Comprehensive Survey On Test-Time Adaptation Under Distribution Shifts", "abstract": "Machine learning methods strive to acquire a robust model during training that can generalize well to test samples, even under distribution shifts. However, these methods often suffer from a performance drop due to unknown test distributions. Test-time adaptation (TTA), an emerging paradigm, has the potential to adapt a pre-trained model to unlabeled data during testing, before making predictions. Recent progress in this paradigm highlights the significant benefits of utilizing unlabeled data for training self-adapted models prior to inference. In this survey, we divide TTA into several distinct categories, namely, test-time (source-free) domain adaptation, test-time batch adaptation, online test-time adaptation, and test-time prior adaptation. For each category, we provide a comprehensive taxonomy of advanced algorithms, followed by a discussion of different learning scenarios. Furthermore, we analyze relevant applications of TTA and discuss open challenges and promising areas for future research. A comprehensive list of TTA methods can be found at https://github.com/tim-learn/awesome-test-time-adaptation."}, "cited_paper_content": {"title": "No More Discrimination: Cross City Adaptation Of Road Scene Segmenters", "abstract": "Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its time-machine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data."}, "keywords": ["target domain"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "On top of the prior-enforcing auto-encoder, SoMAN-cPAE also learns a global classification head and multiple local classification heads for different data augmentations. Extra supervision can be achieved by adapting pre-trained segmenters across different cities using an unsupervised learning approach proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To adapt pre-trained segmenters to different cities without the need for annotated training data, an unsupervised learning approach was proposed in #REFR, which utilizes Google Street View and its time-machine feature to collect unannotated images for each road scene at different times, and advances a joint global and class-specific domain adversarial learning framework to achieve adaptation of pre-trained segmenters to that city.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To adapt a pre-trained road scene segmenter to a new city without annotated data, an unsupervised learning approach was proposed in #REFR, which utilizes unannotated images from Google Street View to extract static-object priors for each road scene at different times, and advances a joint global and class-specific domain adversarial learning framework to achieve adaptation of pre-trained segmenters to the target domain. #REFR", "given_citation_intent": "background", "given_keywords": ["target domain"]}]}
{"citing_id": "2303.06261v1", "cited_id": "1603.00567", "section_title": "Introduction", "citation": "Macrobase #REFR explains outliers by correlating them to some external attributes which are not used to detect anomalies such as location, time of occurrence, software version, etc.", "text_before_citation": ["If a system is able to summarize anomaly detection results into groups and explain why each group of objects is considered to be abnormal or normal, this will greatly reduce the effort of users in evaluating anomaly detection results. State-of-the-Art.", "To the best of our knowledge, the problem of summarizing and interpreting outlier detection results is yet to be addressed.", "Scorpion #OTHEREFR produces meaningful explanations for anomalies in aggregation queries when the 'cause' of an outlier is contained in its provenance.", "Similar to Scorpion, Cape #OTHEREFR aims to explain the outliers in aggregation queries, but using the objects that counterbalance the outliers.", "Both works do not tackle the problem of summarizing outliers."], "text_after_citation": ["However, Macrobase only targets explaining the outliers captured by its default density-based outlier detector and does not generalize to other outlier detection methods.", "In the broader field of interpretable AI, LIME #OTHEREFR explains the predictions of a classifier by learning a linear model locally around the prediction with respect to each testing object and pointing out the attributes that are most important to the prediction of the linear model.", "However, rather than use one model to represent a set of objects, LIME has to learn a linear model for each individual object.", "Therefore, using LIME to explain a large number of prediction results will be prohibitively expensive.", "Other methods #OTHEREFR explain classification results in the similar way to LIME. Challenges."], "citing_paper_content": {"title": "Interpretable Outlier Summarization", "abstract": "Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each iteration. Moreover, to effectively handle high dimensional, highly complex data sets which are hard to summarize with simple rules, we propose a localized STAIR approach, called L-STAIR. Taking data locality into consideration, it simultaneously partitions data and learns a set of localized rules for each partition. Our experimental study on many outlier benchmark datasets shows that STAIR significantly reduces the complexity of the rules required to summarize the outlier detection results, thus more amenable for humans to understand and evaluate, compared to the decision tree methods."}, "cited_paper_content": {"title": "Macrobase: Prioritizing Attention In Fast Data", "abstract": "As data volumes continue to rise, manual inspection is becoming increasingly untenable. In response, we present MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams. MacroBase enables efficient, accurate, and modular analyses that highlight and aggregate important and unusual behavior, acting as a search engine for fast data. MacroBase is able to deliver order-of-magnitude speedups over alternatives by optimizing the combination of explanation and classification tasks and by leveraging a new reservoir sampler and heavy-hitters sketch specialized for fast data streams. As a result, MacroBase delivers accurate results at speeds of up to 2M events per second per query on a single core. The system has delivered meaningful results in production, including at a telematics company monitoring hundreds of thousands of vehicles."}, "keywords": ["outliers", "Macrobase"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To the best of our knowledge, the problem of summarizing outliers has not been addressed. However, MacroBase, a data analytics engine that prioritizes end-user attention in high-volume fast data streams, could potentially be used to efficiently and accurately analyze outlier detection results. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently analyze high-volume fast data streams and prioritize end-user attention, MacroBase has been proposed as a data analytics engine that delivers accurate results at speeds of up to 2M events per second per query on a single core #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently summarize and explain the anomaly detection results, a system that prioritizes end-user attention in high-volume fast data streams is required, as demonstrated by MacroBase #REFR. This system delivers accurate results at high speeds and has been successfully implemented in production, making it a promising solution for handling outliers.", "given_citation_intent": "background", "given_keywords": ["outliers", "Macrobase"]}]}
{"citing_id": "2303.16173v1", "cited_id": "1909.01326", "section_title": "Limitations, Challenges, And Future Directions", "citation": "Since language models often encode biases and stereotypes derived from training corpora #REFR , they may have difficulty producing relevant individuals who are not prototypical (i.e., they do not have a particular stereotype).", "text_before_citation": ["Therefore, future work should investigate more diverse annotator pools or matching annotators to targeted groups, as well as examining how annotator's familiarity with essentialist beliefs and identities affect their judgements.", "Furthermore, prior work in countering hatespeech has show that effective strategies can vary widely depending on the target group #OTHEREFR .", "In our work, we consider results aggregated across all groups.", "However, community-specific investigations are an important future step towards developing effective counter-statements.", "Accuracy of generated exceptions The selection of specific individuals for direct exceptions presents an ongoing challenge, based on the high number of DIR-IND marked incorrect."], "text_after_citation": ["We illustrate incorrect individuals and subgroups in the bottom two examples of Table 1 .", "Additionally, as mentioned in \u00a74, many stereotypes are subjective (e.g., \"women are vain\").", "Therefore, individuals who are counterexamples to the stereotype may be judged differently by different people (e.g., our system proposes that \"taylor swift, sarah palin, and scarlett johansson\" are not vain).", "Producing accurate and relevant direct exceptions to a stereotype is important for understanding the role of such examples to counter essentialist beliefs.", "Our results and discussion highlight the complexity of countering essentialist beliefs."], "citing_paper_content": {"title": "Towards Countering Essentialism Through Social Bias Reasoning", "abstract": "Essentialist beliefs (i.e., believing that members of the same group are fundamentally alike) play a central role in social stereotypes and can lead to harm when left unchallenged. In our work, we conduct exploratory studies into the task of countering essentialist beliefs (e.g., \"liberals are stupid\"). Drawing on prior work from psychology and NLP, we construct five types of counterstatements and conduct human studies on the effectiveness of these different strategies. Our studies also investigate the role in choosing a counterstatement of the level of explicitness with which an essentialist belief is conveyed. We find that statements that broaden the scope of a stereotype (e.g., to other groups, as in \"conservatives can also be stupid\") are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in this area (e.g., improving factuality, studying community-specific variation) and we emphasize the importance of work at the intersection of NLP and psychology."}, "cited_paper_content": {"title": "The Woman Worked As A Babysitter: On Biases In Language Generation", "abstract": "We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions of different demographic groups. In this work, we introduce the notion of the regard towards a demographic, use the varying levels of regard towards different demographics as a defining metric for bias in NLG, and analyze the extent to which sentiment scores are a relevant proxy metric for regard. To this end, we collect strategically-generated text from language models and manually annotate the text with both sentiment and regard scores. Additionally, we build an automatic regard classifier through transfer learning, so that we can analyze biases in unseen text. Together, these methods reveal the extent of the biased nature of language model generations. Our analysis provides a study of biases in NLG, bias metrics and correlated human judgments, and empirical evidence on the usefulness of our annotated dataset."}, "keywords": ["stereotypes", "language models"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Furthermore, prior work in countering hatespeech has shown that effective strategies can vary widely depending on the target group (OtherRef). In our work, we consider results aggregated across all groups. However, community-specific investigations are an important future step towards developing effective counter-statements. Accuracy of generated exceptions The selection of specific individuals for direct exceptions presents an ongoing challenge, based on the high number of DIR-IND marked incorrect. Future work should investigate more diverse annotator pools or matching annotators to targeted groups, as well as examining how annotator's familiarity with essentialist beliefs and identities affect their judgments (OtherRef).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the extent of biases in natural language generation, recent work has introduced the notion of \"regard\" towards different demographics as a defining metric for bias, and has analyzed the extent to which sentiment scores are a relevant proxy metric for regard #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Prior work has shown that language models can perpetuate stereotypes, and recent studies have investigated the extent of biases in natural language generation by analyzing text generated from prompts that contain mentions of different demographic groups #REFR.", "given_citation_intent": "background", "given_keywords": ["stereotypes", "language models"]}]}
{"citing_id": "2303.13004v1", "cited_id": "1901.05761", "section_title": "Synthesized 1D Data Regression", "citation": "Notably, while ACNP is arguably more powerful than CNP and indeed reports better results as expected for GP data #REFR , it performs worse with sine and oscillator data, probably due to the number of functions being too limited. Also, CCNP does not improve CNP consistently either.", "text_before_citation": ["For sine waves, we vary the values of amplitude U[\u22121, 1] Results.", "Table 1 presents the quantitative results of various baseline CNPs with and without adversarial training.", "The baseline models include CNP #OTHEREFR , ACNP #OTHEREFR , and CCNP #OTHEREFR , where the first two are commonly adopted as per most CNPs literature.", "We consider CCNP as a comparison to the previous NCE setting, where NCE is also applied but on the context set as a regularization term to CNPs.", "We find that adversarial training consistently results in performance improvements to baselines in most cases."], "text_after_citation": ["Further, we observe that adversarial training can be adapted to CCNP, showing potential in improving generative CNPs by leveraging composite contrastive objectives."], "citing_paper_content": {"title": "Adversarially Contrastive Estimation Of Conditional Neural Processes", "abstract": "Conditional Neural Processes (CNPs) formulate distributions over functions and generate function observations with exact conditional likelihoods. CNPs, however, have limited expressivity for high-dimensional observations, since their predictive distribution is factorized into a product of unconstrained (typically) Gaussian outputs. Previously, this could be handled using latent variables or autoregressive likelihood, but at the expense of intractable training and quadratically increased complexity. Instead, we propose calibrating CNPs with an adversarial training scheme besides regular maximum likelihood estimates. Specifically, we train an energy-based model (EBM) with noise contrastive estimation, which enforces EBM to identify true observations from the generations of CNP. In this way, CNP must generate predictions closer to the ground-truth to fool EBM, instead of merely optimizing with respect to the fixed-form likelihood. From generative function reconstruction to downstream regression and classification tasks, we demonstrate that our method fits mainstream CNP members, showing effectiveness when unconstrained Gaussian likelihood is defined, requiring minimal computation overhead while preserving foundation properties of CNPs."}, "cited_paper_content": {"title": "Attentive Neural Processes", "abstract": "Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled."}, "keywords": ["CNP", "better results"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We consider CCNP as a comparison to the previous NCE setting, where NCE is also applied but on the context set as a regularization term to CNPs. We find that adversarial training consistently results in performance improvements to baselines in most cases, as shown in Table 1. This is in line with recent work on Attentive Neural Processes by Garnelo et al. (2018a;b), which also addressed the issue of underfitting by incorporating attention into NPs. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We find that incorporating attention into Neural Processes (#REFR) greatly improves the accuracy of predictions, as demonstrated in Table 1, resulting in noticeably faster training and expanding the range of functions that can be modeled.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We find that adversarial training consistently results in better results to most baseline CNPs, including CNP #OTHEREFR and ACNP #OTHEREFR, which are commonly adopted as per most CNPs literature, and CCNP #OTHEREFR, which is used as a comparison to the previous NCE setting, where NCE is also applied but on the context set as a regularization term to CNPs (#REFR).", "given_citation_intent": "result", "given_keywords": ["CNP", "better results"]}]}
{"citing_id": "2304.04918v1", "cited_id": "1910.01108", "section_title": "Smart Reply For Customer Support", "citation": "Customized tokenization was first applied to message pairs and canned replies, and then DistilBERT #REFR was used to vectorize the queries and documents for ranking.", "text_before_citation": ["The Smart Reply task is to take the most recent support agent message and the most recent customer message in a customer support conversation, and choose the best reply from a set of canned reply templates.", "An example Smart Reply could be \"This link has step-by-step instructions for how to activate Microsoft 365.\" An efficient CPU-based classifier is applied at runtime to classify which specific product an incoming support message interaction is for, and then our learning-to-rank model is used to select the best reply from the canned replies for that product.", "Smart Reply is able to support customer support interactions across 22 Microsoft products such as Office 365, Teams, Surface, and Remote Assistance. Table 1 describes statistics for the Smart Reply task.", "Training and test data were based on an 80%:20% split ratio."], "text_after_citation": ["One difference from traditional retrieval systems which always retrieve the top-k documents is that we do not want to overwhelm support agents with replies when there are no good canned replies for a customer message.", "To achieve this, we added a \"Silent\" class in the product classifier and an \"Empty\" canned reply in each candidate reply set. We used data augmentation to generate synthetic conversations.", "For instance, appending a message pair that returns the \"Silent\" class or \"Empty\" reply to non-empty questions enriched non-empty triplets, and only using agent or customer messages further enlarged the data size.", "Table 2 : SR offline and online metric gains (%) Table 2 shows the 11.7% offline top-one accuracy gain of sRank compared to our previous DSSM-based #OTHEREFR system that took transformer embeddings as inputs.", "We exposed Smart Reply to insider agents for initial feedback then to 50% global agents during A/B testing."], "citing_paper_content": {"title": "Explicit And Implicit Semantic Ranking Framework", "abstract": "The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-torank framework, Self-training Semantic Cross-attention Ranking (). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and has enabled 38.7% time reduction in composing messages in telemetry recorded since its general release in January 2021. In the ACI task, selects relevant historical physician templates that serve as guidance for a text summarization model to generate higher quality medical notes. It achieves 35.5% top-one accuracy gain, along with 46% relative ROUGE-L gain in generated medical notes. CCS CONCEPTS \u2022 Information systems \u2192 Learning to rank; \u2022 Computing methodologies \u2192 Learning to rank."}, "cited_paper_content": {"title": "Distilbert, A Distilled Version Of Bert: Smaller, Faster, Cheaper And Lighter", "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."}, "keywords": ["canned replies", "DistilBERT"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To pre-train a smaller general-purpose language representation model, the authors of this paper proposed a method called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and can reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train a smaller general-purpose language representation model, the authors propose a method called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and has been shown to be 60% faster while retaining 97% of its language understanding capabilities. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To select the best reply from the canned replies for each product, the authors used a learning-to-rank model that was trained with a linear pairwise loss and achieved high efficiency and quality gains, which is similar to the method proposed in DistilBERT #REFR.", "given_citation_intent": "method", "given_keywords": ["canned replies", "DistilBERT"]}]}
{"citing_id": "2304.00610v1", "cited_id": "1601.01487", "section_title": "Introduction", "citation": "With R redefined to have a sparse complementa string is in R unless exponentially compressible-the complement of the #REFR .", "text_before_citation": ["#OTHEREFR We show: if it is possible to efficiently rule out length t proofs of some unprovable sentence \u03c6, it is also possible to efficiently rule out a slightly shorter proof of inconsistency, which could be used in a length t proof of \u03c6 by contradiction.", "This implies a powerful generalization-if it is hard to rule out length t proofs of inconsistency, it is hard to rule of length t proofs of any unprovable sentence.", "This in turn implies that facts about unprovability and noncomputability, which are well understood, can be imported into complexity theory.", "This has wide ramifications-diverse types of unprovable sentences translate into assertions that open questions in complexity theory have the expected answers.", "For instance, unprovable sentences of the form x\u2208R are dense, so hard families of tautologies encoding \"no length t proof shows x\u2208R\" are also dense."], "text_after_citation": ["Pudl\u00e1k #OTHEREFR shows the initial conjecture was incorrect-a theory T can efficiently prove that T lacks a length t proof of '0=1'.", "The 1989 reformulation refers to the lack of efficient proofs in a weaker theory. See also Theorem 59 of Pudl\u00e1k #OTHEREFR .", "3 See also Kraj\u00ed\u010dek #OTHEREFR Section 21.3.", "language { x, 1 t | no length t proof exists of x\u2208R} is neither in P nor NPcomplete, but is NP-intermediate.", "The hardness of ruling out length t proofs of any unprovable sentence implies a deep linkage between noncomputability and complexity."], "citing_paper_content": {"title": "Ruling Out Short Proofs Of Unprovable Sentences Is Hard", "abstract": "If no optimal propositional proof system exists, we (and independently Pudl\u00e1k) prove that ruling out length t proofs of any unprovable sentence is hard. This mapping from unprovable to hard-to-prove sentences powerfully translates facts about noncomputability into complexity theory. For instance, because proving string x is Kolmogorov random (x\u2208R) is typically impossible, it is typically hard to prove \"no length t proof shows x\u2208R\", or tautologies encoding this. Therefore, a proof system with one family of hard tautologies has these densely in an enumeration of families. The assumption also implies that a natural language is NP-intermediate: with R redefined to have a sparse complement, the complement of the language { x, 1 t | no length t proof exists of x\u2208R} is also sparse. Efficiently ruling out length t proofs of x\u2208R might violate the constraint on using the fact of x\u2208R's unprovability. We conjecture: any computable predicate on R that might be used in if-then statements (or case-based proofs) does no better than branching at random, because R appears random by any effective test. This constraint could also inhibit the usefulness in circuits and propositional proofs of NOT gates and cancellation-needed to encode if-then statements. If R defeats if-then logic, exhaustive search is necessary."}, "cited_paper_content": {"title": "Incompleteness In The Finite Domain", "abstract": "Motivated by the problem of finding finite versions of classical incompleteness theorems, we present some conjectures that go beyond ${\\bf NP\\neq co NP}$. These conjectures formally connect computational complexity with the difficulty of proving some sentences, which means that high computational complexity of a problem associated with a sentence implies that the sentence is not provable in a weak theory, or requires a long proof. Another reason for putting forward these conjectures is that some results in proof complexity seem to be special cases of such general statements and we want to formalize and fully understand these statements. In this paper we review some conjectures that we have presented earlier, introduce new conjectures, systematize them and prove new connections between them and some other statements studied before."}, "keywords": ["sparse complementa string"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors review some conjectures that connect computational complexity with the difficulty of proving certain sentences, and present new conjectures that formalize and expand upon these statements. They also prove new connections between these conjectures and other previously studied statements. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The conjectures presented in the reference paper \"Incompleteness In The Finite Domain\" formally connect computational complexity with the difficulty of proving some sentences, which is relevant to the discussion of ruling out short proofs of unprovable sentences in this manuscript. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The conjectures presented in #REFR formally connect computational complexity with the difficulty of proving some sentences, implying that high computational complexity of a problem associated with a sentence requires a long proof or that the sentence is not provable in a weak theory, which could be useful in understanding the results in proof complexity, as well as the problem of finding finite versions of classical incompleteness theorems.", "given_citation_intent": "background", "given_keywords": ["sparse complementa string"]}]}
{"citing_id": "2304.09167v1", "cited_id": "1507.00473", "section_title": "Theorem 2.8 ([Hlw94, Hau95]). Fix A Hypothesis Class", "citation": "Corollary 2.9 recovers the known optimal risk upper bound for binary classification first proven by Hanneke #REFR .", "text_before_citation": ["Corollary 2.9.", "Fix a hypothesis class H \u2286 {0, 1} X with VC dimension d.", "There is a predictor f : X \u00d7 U \u2192 {0, 1} which, for any \u03b4 \u2208 (0, 1), and S \u223c P n sampled from any realizable distribution P , satisfies", "err P ( f (\u2022; S)) 9.64 d n + 1 n log 2 \u03b4 ,", "with probability at least 1 \u2212 \u03b4 over the randomness of S."], "text_after_citation": ["Recently, Larsen #OTHEREFR showed that an implementation of the natural bagging heuristic also achieves an optimal risk bound.", "Our proof of the optimal bound is remarkably simpler than both of their proofs.", "Furthermore, both Hanneke and Larsen state that the constant factors in their upper bounds are very large and explicitly ask whether these constants can be reduced.", "Our new analysis reduces the constant factors by a few orders of magnitude.", "In addition, our result can be seen as a partial answer to a question of Warmuth #OTHEREFR who asked whether the one-inclusion graph algorithm can achieve an optimal PAC risk bound."], "citing_paper_content": {"title": "Optimal Pac Bounds Without Uniform Convergence", "abstract": "In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon [Sim15] and Hanneke [Han16a] established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification. In this paper, we address this issue by providing optimal high probability risk bounds through a framework that surpasses the limitations of uniform convergence arguments. Our framework converts the leave-one-out error of permutation invariant predictors into high probability risk bounds. As an application, by adapting the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth [HLW94], we propose an algorithm that achieves an optimal PAC bound for binary classification. Specifically, our result shows that certain aggregations of one-inclusion graph algorithms are optimal, addressing a variant of a classic question posed by Warmuth [War04]. We further instantiate our framework in three settings where uniform convergence is provably suboptimal. For multiclass classification, we prove an optimal risk bound that scales with the one-inclusion hypergraph density of the class, addressing the suboptimality of the analysis of Daniely and Shalev-Shwartz [DS14]. For partial hypothesis classification, we determine the optimal sample complexity bound, resolving a question posed by Alon, Hanneke, Holzman, and Moran [AHHM22]. For realizable bounded regression with absolute loss, we derive an optimal risk bound that relies on a modified version of the scale-sensitive dimension, refining the results of Bartlett and Long [BL98]. Our rates surpass standard uniform convergence-based results due to the smaller complexity measure in our risk bound."}, "cited_paper_content": {"title": "The Optimal Sample Complexity Of Pac Learning", "abstract": "This work establishes a new upper bound on the number of samples sufficient for PAC learning in the realizable case. The bound matches known lower bounds up to numerical constant factors. This solves a long-standing open problem on the sample complexity of PAC learning. The technique and analysis build on a recent breakthrough by Hans Simon."}, "keywords": ["binary classification"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The technique and analysis used in this paper build on a recent breakthrough by Hans Simon to establish a new upper bound on the number of samples sufficient for PAC learning in the realizable case, which matches known lower bounds up to numerical constant factors, solving a long-standing open problem on the sample complexity of PAC learning. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The optimal sample complexity of PAC learning in the realizable case has been a long-standing open problem in statistical learning theory, which has been recently solved by establishing a new upper bound on the number of samples sufficient for PAC learning. This bound matches known lower bounds up to numerical constant factors, and the technique and analysis build on a recent breakthrough by Hans Simon, providing valuable background information for the results presented in this manuscript. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The optimal PAC bound for binary classification has been achieved by aggregating certain one-inclusion graph algorithms, as shown in the reference paper by Simon. #REFR", "given_citation_intent": "background", "given_keywords": ["binary classification"]}]}
{"citing_id": "2303.14595v2", "cited_id": "1611.07725", "section_title": "Baselines", "citation": "Incremental Classifier and Presentation Learning (iCaRL) #REFR performs classification using the nearest mean-of-exemplars, where the exemplars selected by herding algorithm in the feature space.", "text_before_citation": ["In our evaluation, we test our method by combing it with two popular experience replay methods, ER #OTHEREFR and DER++ #OTHEREFR .", "ER uses a memory buffer to store the training examples from past tasks and interleaves them with the current task data for training.", "In addition to this, DER++ records the output logits of the examples in the memory and performs logit distillation when doing experience replay.", "We combine the proposed BFP loss with ER and DER++ and denote them as ER w/ BFP and DER++ w/ BFP respectively.", "We also compare the proposed method with some other state-of-the-art CL baselines as listed in Table 1."], "text_after_citation": ["Functional Distance Regularization (FDR) #OTHEREFR regularize the output of the network to its past value.", "Different from DER/DER++, FDR applies the regularization on the output classification probability.", "Learning a Unified Classifier Incrementally via Rebalancing (LUCIR) #OTHEREFR augments experience replay with multiple modifications to preserve old knowledge and enforce separation class separation in continual learning.", "Bias Correction (BiC) #OTHEREFR augments the experience replay by learning a separate layer to correct the bias in the output logits.", "ER with Asymmetric Cross-Etronpy (ER-ACE) #OTHEREFR proposes to reduce representation drift by using separate cross-entropy loss for online and replayed training data."], "citing_paper_content": {"title": "Preserving Linear Separability In Continual Learning By Backward Feature Projection", "abstract": "Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting [16, 18, 26]. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space, in which linear separability is well preserved during continual learning and linear probing achieves high classification accuracy. The code can be found at https://github.com/rvl-lab-utoronto/BFP."}, "cited_paper_content": {"title": "Icarl: Incremental Classifier And Representation Learning", "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail."}, "keywords": ["Incremental Classifier"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve a better stability-plasticity trade-off in continual learning, the authors propose Backward Feature Projection (BFP), a method that allows the new features to change up to a learnable linear transformation of the old features while preserving the linear separability of the old classes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve a better stability-plasticity trade-off in continual learning, Backward Feature Projection (BFP) is proposed, which allows the new features to change up to a learnable linear transformation of the old features, preserving the linear separability of the old classes while accommodating new classes with new feature directions #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To tackle the challenge of catastrophic forgetting in continual learning, the authors propose Backward Feature Projection (BFP), which allows the new features to change up to a learnable linear transformation of the old features, preserving the linear separability of the old classes while accommodating new classes with new feature directions, and the proposed method is compared with some other state-of-the-art CL baselines as listed in Table 1, and combined with ER and DER++ to form ER w/ BFP and DER++ w/ BFP respectively, achieving a significant performance boost #REFR. The proposed BFP method is based on the Incremental Classifier And Representation Learning (iCaRL) strategy, which allows learning in a class-incremental way and learns strong classifiers and a data representation simultaneously.", "given_citation_intent": "method", "given_keywords": ["Incremental Classifier"]}]}
{"citing_id": "2303.16424v1", "cited_id": "1911.03038", "section_title": "Y N2\u00d7N1", "citation": "Additionally, our results demonstrate meaningful gains over Turbo Autoencoder (TurboAE) #REFR and state-of-the-art classical codes.", "text_before_citation": ["k M = k for some positive integer M .", "In other words, ProductAE boils down the complex problem of training autoencoders of large dimensions and lengths to less-complex sub-problems of training encoders and decoders for smaller dimensions and lengths.", "Our training results for a relatively short-length ProductAE of dimension k = 100 show significant performance gains compared to the polar code under successive cancellation (SC) decoding.", "More importantly, we demonstrate achieving similar gains for moderate-length ProductAEs of dimensions as large as k = 300 bits.", "This clearly establishes the generalization of our proposed architecture for training higher-dimension codes."], "text_after_citation": ["These achievements are attained by applying innovative ideas from deep learning and intuitions from coding theory to further improve the training performance of our proposed ProductAE architecture.", "The main contributions of the paper are summarized as follows.", "\u2022 We introduce a new class of neural error-correction codes, namely ProductAE, aimed at enabling the training of higher-dimension codes.", "Building upon ideas from classical product codes, ProductAE boils down the complex problem of training large autoencoders to less-complex sub-problems of training smaller encoding and decoding components.", "\u2022 We present several useful modifications to our proposed ProductAE architecture, which significantly contribute to achieving excellent training performances."], "citing_paper_content": {"title": "Productae: Toward Deep Learning Driven Error-Correction Codes Of Large Dimensions", "abstract": "While decades of theoretical research have led to the invention of several classes of error-correction codes, the design of such codes is an extremely challenging task, mostly driven by human ingenuity. Recent studies demonstrate that such designs can be effectively automated and accelerated via tools from machine learning (ML), thus enabling ML-driven classes of errorcorrection codes with promising performance gains compared to classical designs. A fundamental challenge, however, is that it is prohibitively complex, if not impossible, to design and train fully ML-driven encoder and decoder pairs for large code dimensions. In this paper, we propose Product Autoencoder (ProductAE)-a computationally-efficient family of deep learning driven (encoder, decoder) pairs-aimed at enabling the training of relatively large codes (both encoder and decoder) with a manageable training complexity. We build upon ideas from classical product codes and propose constructing large neural codes using smaller code components. ProductAE boils down the complex problem of training the encoder and decoder for a large code dimension k and blocklength n to less-complex sub-problems of training encoders and decoders for smaller dimensions and blocklengths. Our training results show successful training of ProductAEs of dimensions as large as k = 300 bits with meaningful performance gains compared to state-of-the-art classical and neural designs. Moreover, we demonstrate excellent robustness and adaptivity of ProductAEs to channel models different than the ones used for training."}, "cited_paper_content": {"title": "Turbo Autoencoder: Deep Learning Based Channel Codes For Point-To-Point Communication Channels", "abstract": "Designing codes that combat the noise in a communication medium has remained a significant area of research in information theory as well as wireless communications. Asymptotically optimal channel codes have been developed by mathematicians for communicating under canonical models after over 60 years of research. On the other hand, in many non-canonical channel settings, optimal codes do not exist and the codes designed for canonical models are adapted via heuristics to these channels and are thus not guaranteed to be optimal. In this work, we make significant progress on this problem by designing a fully end-to-end jointly trained neural encoder and decoder, namely, Turbo Autoencoder (TurboAE), with the following contributions: ($a$) under moderate block lengths, TurboAE approaches state-of-the-art performance under canonical channels; ($b$) moreover, TurboAE outperforms the state-of-the-art codes under non-canonical settings in terms of reliability. TurboAE shows that the development of channel coding design can be automated via deep learning, with near-optimal performance."}, "keywords": ["Turbo Autoencoder"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed Product Autoencoder (ProductAE) architecture for training higher-dimension codes is computationally efficient and enables the training of relatively large codes with a manageable complexity, as demonstrated in our results (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our training results for a relatively short-length ProductAE of dimension k = 100 show significant performance gains compared to the polar code under successive cancellation (SC) decoding. More importantly, we demonstrate achieving similar gains for moderate-length ProductAEs of dimensions as large as k = 300 bits, which outperforms the state-of-the-art codes under non-canonical settings in terms of reliability #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Product Autoencoder (ProductAE) architecture for training higher-dimension codes shows significant performance gains compared to the polar code under successive cancellation decoding, and achieves similar gains for moderate-length ProductAEs of dimensions as large as k = 300 bits, demonstrating the generalization of the architecture for training higher-dimension codes with near-optimal performance compared to state-of-the-art classical and neural designs. This result highlights the potential of deep learning-based channel codes, such as Turbo Autoencoder, in automating the development of channel coding design with near-optimal performance. #REFR", "given_citation_intent": "result", "given_keywords": ["Turbo Autoencoder"]}]}
{"citing_id": "2304.07519v1", "cited_id": "1505.04597", "section_title": "Effects Of Different Window Size W In Dsbe (Section Iii-C).", "citation": "Our experimental results on Pancreas dataset show that changing one base model to 3D U-Net #REFR results in similar results to those reported in Table III : 73.89\u00b14.42 vs. 74.03\u00b14.00.", "text_before_citation": ["That is, the pseudo-labels generated by its peer networks will be exactly the same as the predictions made by the base model, and each base model independently learns from its own predictions in every iteration, which is equivalent to Entropy Minimization.", "The only difference between the degraded ComWin model and regular Entropy Minimization, is that the former fits a categorical probability distribution while the latter learns from a continuous soft counterpart. This disparity is minor.", "As shown in Table III , switching to Entropy Minimization would incur a huge performance drop (from 74.03 to 44.55) .", "This demonstrates the significance of initializing each base model differently. Base model architectures.", "This framework is structure agnostic and does not limit the choices of architecture configurations."], "text_after_citation": ["Thorough exploration regarding diversity and the combination of base model structures would be an interesting direction for future studies."], "citing_paper_content": {"title": "Compete To Win: Enhancing Pseudo Labels For Barely-Supervised Medical Image Segmentation", "abstract": "This study investigates barely-supervised medical image segmentation where only few labeled data, i.e., singledigit cases are available. We observe the key limitation of the existing state-of-the-art semi-supervised solution cross pseudo supervision is the unsatisfactory precision of foreground classes, leading to a degenerated result under barely-supervised learning. In this paper, we propose a novel Compete-to-Win method (ComWin) to enhance the pseudo label quality. In contrast to directly using one model's predictions as pseudo labels, our key idea is that high-quality pseudo labels should be generated by comparing multiple confidence maps produced by different networks to select the most confident one (a compete-to-win strategy). To further refine pseudo labels at near-boundary areas, an enhanced version of ComWin, namely, ComWin + , is proposed by integrating a boundary-aware enhancement module. Experiments show that our method can achieve the best performance on three public medical image datasets for cardiac structure segmentation, pancreas segmentation and colon tumor segmentation, respectively. The source code is now available at https://github.com/Huiimin5/comwin."}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["3D U-Net results"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The architecture presented in this paper consists of a contracting path to capture context and a symmetric expanding path that enables precise localization, and has been shown to outperform the prior best method on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed ComWin method achieved state-of-the-art performance on three public medical image datasets for segmentation tasks, including neuronal structures in electron microscopic stacks, by enhancing the quality of pseudo labels through a compete-to-win strategy that compares multiple confidence maps produced by different networks to select the most confident one #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the experimental results presented in this study, the proposed ComWin method outperforms the existing state-of-the-art semi-supervised solution, achieving the best performance on three public medical image datasets for cardiac structure segmentation, pancreas segmentation, and colon tumor segmentation, respectively, with the help of 3D U-Net results #REFR.", "given_citation_intent": "result", "given_keywords": ["3D U-Net results"]}]}
{"citing_id": "2303.07815v1", "cited_id": "1810.00736", "section_title": "Method", "citation": "This is in line with other results in the literature #REFR and is related to the trade-off between model depth and GPU compute units available.", "text_before_citation": ["In all cases, we use the authors' provided code, but with a modification to accept randomly generated video sequences of two different durations.", "The results can be seen in Table 4 and show that our MobileVOS models are significantly smaller, while retaining a constant low latency across both the short and long-duration videos.", "This result is attributed to the smaller backbones and the constant memory costs due to a finite memory queue length.", "Our largest model with ResNet18 has more than 3\u02c6the FPS than RDE-VOS but with 8f ewer parameters.", "Finally, we observe that the ResNet models perform better on the server-grade GPUs."], "text_after_citation": ["In the following section, we show that we obtain a different outcome for mobile GPUs. Table 4 . Performance evaluation on the same sets of hardware.", "The FPS metrics were evaluated on two randomly generated short and long video sequences with shape 480\u02c6910.", "The short videos consist of 50 frames, while the long videos consist of 500.", "In all cases, we use the authors' provided code and indicates that the models are exceeding the GPU memory limit."], "citing_paper_content": {"title": "Mobilevos: Real-Time Video Object Segmentation Contrastive Learning Meets Knowledge Distillation", "abstract": "This paper tackles the problem of semi-supervised video object segmentation on resource-constrained devices, such as mobile phones. We formulate this problem as a distillation task, whereby we demonstrate that small spacetime-memory networks with finite memory can achieve competitive results with state of the art, but at a fraction of the computational cost (32 milliseconds per frame on a Samsung Galaxy S22). Specifically, we provide a theoretically grounded framework that unifies knowledge distillation with supervised contrastive representation learning. These models are able to jointly benefit from both pixel-wise contrastive learning and distillation from a pretrained teacher. We validate this loss by achieving competitive J &F to state of the art on both the standard DAVIS and YouTube benchmarks, despite running up to\u02c65 faster, and with\u02c632 fewer parameters."}, "cited_paper_content": {"title": "Benchmark Analysis Of Representative Deep Neural Network Architectures", "abstract": "This work presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online."}, "keywords": ["GPU compute units"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our MobileVOS models achieve competitive results with state of the art, but at a fraction of the computational cost, thanks to the smaller backbones and the constant memory costs due to a finite memory queue length. This result is similar to the findings in the reference paper #REFR, where the authors analyzed the performance indices of various deep neural networks proposed for image recognition and discussed the behavior of such indices and some combinations of them.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our MobileVOS models achieve competitive results with state of the art, but at a fraction of the computational cost, as demonstrated by the benchmark analysis of representative deep neural network architectures presented in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our observation that the ResNet models perform better on server-grade GPUs is consistent with the findings in the benchmark analysis of representative deep neural network architectures by the authors, which compared the performance of various DNNs on different GPU compute units. #REFR", "given_citation_intent": "result", "given_keywords": ["GPU compute units"]}]}
{"citing_id": "2303.02862v1", "cited_id": "1704.07809", "section_title": "3D Joint Annotation", "citation": "Since our annotations have been checked manually, we do not perform a bootstrapping procedure as #REFR .", "text_before_citation": ["Our dataset is of single hands and we apply 21 keypoints scheme #OTHEREFR to annotate each hand.", "Inspired by Interhand2.6M #OTHEREFR and FreiHand #OTHEREFR , we use multi-view RGB images for 3D annotation and apply a two-stage process consisting of machine annotation and human annotation.", "First, we use Mediapipe [47] to detect 2D hand keypoints on all the RGB images and triangulate 2D keypoints to obtain 3D keypoints with RANSAC method.", "Then we manually verify all the keypoints re-projected by 3D keypoints and select the unqualified views for human annotation."], "text_after_citation": ["Due to the cost of human annotation, manual annotation is only applied to fixed pose and random short sequences and machine annotation is then applied to random long sequences.", "For fast motion sequences, we can not get accurate 3D joint annotations due to the severe motion blur in the captured images.", "To quantitatively evaluate our method, we manually annotate the 2D joints on the event sequences."], "citing_paper_content": {"title": "Evhandpose: Event-Based 3D Hand Pose Estimation With Sparse Supervision", "abstract": "Event camera shows great potential in 3D hand pose estimation, especially addressing the challenges of fast motion and high dynamic range in a low-power way. However, due to the asynchronous differential imaging mechanism, it is challenging to design event representation to encode hand motion information especially when the hands are not moving (causing motion ambiguity), and it is infeasible to fully annotate the temporally dense event stream. In this paper, we propose EvHandPose with novel hand flow representations in Event-to-Pose module for accurate hand pose estimation and alleviating the motion ambiguity issue. To solve the problem under sparse annotation, we design contrast maximization and edge constraints in Pose-to-IWE (Image with Warped Events) module and formulate EvHandPose in a self-supervision framework. We further build EvRealHands, the first large-scale real-world event-based hand pose dataset on several challenging scenes to bridge the domain gap due to relying on synthetic data and facilitate future research. Experiments on EvRealHands demonstrate that EvHandPose outperforms previous event-based method under all evaluation scenes with 15 \u223c 20 mm lower MPJPE and achieves accurate and stable hand pose estimation in fast motion and strong light scenes compared with RGB-based methods. Furthermore, EvHandPose demonstrates 3D hand pose estimation at 120 fps or higher."}, "cited_paper_content": {"title": "Hand Keypoint Detection In Single Images Using Multiview Bootstrapping", "abstract": "We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions."}, "keywords": ["annotations"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To obtain 3D keypoints from multi-view RGB images, the authors used a two-stage process consisting of machine annotation and human annotation, inspired by Interhand2.6M and FreiHand. The machine annotation step involved using an initial keypoint detector to produce noisy labels in multiple views of the hand, which were then triangulated in 3D using multiview geometry or marked as outliers. The resulting reprojected triangulations were used as new labeled training data to improve the detector, a process known as multiview bootstrapping #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain 3D keypoints from multi-view RGB images, the authors used a two-stage process consisting of machine annotation and human annotation, as described in detail in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain 3D annotations for single hands, the authors used multi-view RGB images and a two-stage process consisting of machine annotation and human annotation, as described in detail in #REFR.", "given_citation_intent": "method", "given_keywords": ["annotations"]}]}
{"citing_id": "2303.06696v1", "cited_id": "1904.00071", "section_title": "V. Analysis & Results", "citation": "The shortcomings of standard RC algorithm on C-V2X has been explored in #REFR , which is highly suggested for readers interested in congestion control approaches in C-V2X.", "text_before_citation": ["Mean ITT for the tested scenarios is presented in figure 4 , which heavily correlates with the CBR at corresponding densities.", "At very low traffic (1veh/s), 35% CBR (figure 3) causes RC to remain dormant, resulting in VUEs transmitting BSMs with 100ms ITT (figure 4).", "But a slight increase of traffic flow (5veh/s) vastly increases the CBR to 90%.", "This change is not reflected comparably at ITT, which barely increases to 110ms.", "This low sensitivity of RC algorithm at low traffic flow appears from the algorithm originally being optimized for the older DSRC technology, which follows a different abstraction of physical layer bandwidth than C-V2X."], "text_after_citation": ["Further increase in density (10 veh/s) causes gradual increase in ITT.", "With higher ITT, larger volume of resources are available within a unit time-frame due to less frequent transmission by the VUEs.", "As a result, CBR comparably decreases and the now-accessible resources are utilized by the service application, making SCT at 10veh/s better than 5veh/s (figure 2).", "At high CBR, SPS resource allocation procedure is more aggressive in resource shortlisting and selection.", "When a VUE transmitting in a high density traffic scenario increases its SPS RSRP threshold while shortlisting a resource for its own transmission, one implication of this occurrence is that the VUE can likely select a resource with low RSRP for its upcoming transmission."], "citing_paper_content": {"title": "On Batching Acknowledgements In C-V2X Services", "abstract": "Cellular Vehicle-to-Everything (C-V2X) is a frontier in the evolution of distributed communication introduced in 3GPP release 14 to advanced use cases. While research efforts continue to optimize the accessible bandwidth for transportation ecosystem, a bottom up analysis from the application layer perspective is necessary prior to deployment, as it can expose potential issues that can emerge in a dynamic road environment. This emphasizes on assessing the network using applicationoriented metrics to evaluate its capacity of providing advanced vehicular services with stringent latency and throughput requirements. C-V2X enables advanced applications like autonomous driving and on-the-go transaction services where consecutive exchange of messages is required. For such services, the network level metrics fails to capture the edge case service quality as they express an average measure of performance. In this paper, we present an application-oriented analysis of a transaction service built on C-V2X protocol. We analyze different design choices that affects quality of service both from network-oriented and user-centric metrics and we highlight the issues regarding packet dissemination from infrastructures for vehicle-to-infrastructure (V2I) based service applications. We also present our study on the impact of batching in disseminating acknowledgement packets (ACK) and its consequence on both the service reliability and network congestion. Our results show that time-sensitive and mission-sensitive vehicular applications should aim for a balance between achieving the mission utility in shortest duration possible, while keeping minimal impact on the system-wide stability."}, "cited_paper_content": {"title": "Analysis Of Distributed Congestion Control In Cellular Vehicle-To-Everything Networks", "abstract": "Cellular Vehicle-to-everything (C-V2X) communication has been proposed in the 3rd Generation Partnership Project release 14 standard to address the latency and reliability requirements of cooperative safety applications. Such applications can involve highly congested vehicular scenarios where the network experiences high data loads. Thus, a sophisticated congestion control solution is vital in order to maintain the network performance required for safety-related applications. With the aid of our high-fidelity link-level network simulator, we investigate the feasibility of implementing the distributed congestion control algorithm specified in SAE J2945/1 standard on top of the C-V2X stack. We describe our implementation and evaluate the performance of transmission rate and range control mechanisms using relevant metrics. Additionally, we identify areas for potential design enhancements and further investigation."}, "keywords": ["congestion control approaches"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "This low sensitivity of RC algorithm at low traffic flow appears from the algorithm originally being optimized for the older DSRC technology, which follows a different abstraction of physical layer bandwidth than C-V2X. Further investigation on congestion control solutions for C-V2X is necessary to maintain the network performance required for safety-related applications #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The RC algorithm in C-V2X was originally optimized for the older DSRC technology, which follows a different abstraction of physical layer bandwidth than C-V2X, as described in the analysis of distributed congestion control in cellular vehicle-to-everything networks #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The RC algorithm in C-V2X is optimized for the older DSRC technology, which follows a different abstraction of physical layer bandwidth than C-V2X, highlighting the need for sophisticated #REFR congestion control approaches in highly congested vehicular scenarios.", "given_citation_intent": "background", "given_keywords": ["congestion control approaches"]}]}
{"citing_id": "2303.15230v1", "cited_id": "1706.03762", "section_title": "Cross-Modal Traction", "citation": "Specifically, the Cross-Modal Traction module is composed of a stack of N blocks, and in each block, we first consider a scaled dot product attention mechanism #REFR with the prompt representation attending to all patch tokens.", "text_before_citation": ["Given the same semantic concept, the static and monotonous prompt representation naturally fails to be commonly optimal for all input images that come from a plentiful distribution.", "This issue becomes more serious in the additional state and object branches, as the visual content of the same primitive changes considerably when paired with different primitives.", "Therefore, we further develop a Cross-Modal Traction module for Troika.", "The module adaptively shifts the prompt representation to accommodate the content diversity and diminish the cross-modal discrepancies.", "In this process, relevant patch features serve as the guidance to avoid noise from semantic-agnostic sub-regions interfering with the traction."], "text_after_citation": ["Given the input prompt representation t that comes from an arbitrary branch, we first acquire the patch tokens X p \u2208 R N p \u00d7d after projecting them with the linear layer g proj .", "Then, the query, key and value can be derived as", "EQUATION", "where W q , W K , W V \u2208 R d\u00d7d attn are the parameter matrices, and d attn is the dimension of the single-head attention.", "The dot product attention gives relevance weights from t to each patch token, which are used to aggregate the value-projected patch tokens as"], "citing_paper_content": {"title": "Troika: Multi-Path Cross-Modal Traction For Compositional Zero-Shot Learning", "abstract": "Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is our implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt representation towards the current visual content. We conduct extensive experiments on three popular benchmarks, where our method significantly outperforms existing methods in both closed-world and open-world settings."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["prompt representation", "attention mechanism"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of prompt representation failing to be optimal for all input images, we further develop a Cross-Modal Traction module for Troika that adaptively shifts the prompt representation to accommodate content diversity and diminish cross-modal discrepancies, using relevant patch features as guidance to avoid noise from semantic-agnostic sub-regions interfering with the traction #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of static and monotonous prompt representation in compositional zero-shot learning, the authors propose a Cross-Modal Traction module in Troika that adaptively shifts the prompt representation to accommodate content diversity and diminish cross-modal discrepancies, guided by relevant patch features to avoid noise from semantic-agnostic sub-regions interfering with the traction (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of static and monotonous prompt representation, the authors propose a Cross-Modal Traction module that adaptively shifts the prompt representation to accommodate content diversity and diminish cross-modal discrepancies, which is inspired by the attention mechanism introduced in the Transformer model #REFR.", "given_citation_intent": "method", "given_keywords": ["prompt representation", "attention mechanism"]}]}
{"citing_id": "2303.07868v1", "cited_id": "1703.06870", "section_title": "Related Work", "citation": "To date, most of the topperforming IS methods still follow the Mask R-CNN metaarchitecture #REFR .", "text_before_citation": ["Instance Segmentation."], "text_after_citation": ["These proposal-based approaches typically employ an object detector to localize each instance in bounding boxes.", "Then the instance-wise features are cropped and extracted from FPN features based on the detected bounding boxes by using RoI pooling/align #OTHEREFR .", "Finally, a compact segmentation head is deployed to obtain the desired object masks.", "Mask Scoring R-CNN #OTHEREFR aligns the mask quality and score by using a branch to explicitly learn the quality of predicted masks.", "BMask R-CNN #OTHEREFR leverages boundary details to improve mask localization ability."], "citing_paper_content": {"title": "Dynamask: Dynamic Mask Selection For Instance Segmentation", "abstract": "The representative instance segmentation methods mostly segment different object instances with a mask of the fixed resolution, e.g., 28 \u00d7 28 grid. However, a lowresolution mask loses rich details, while a high-resolution mask incurs quadratic computation overhead. It is a challenging task to predict the optimal binary mask for each instance. In this paper, we propose to dynamically select suitable masks for different object proposals. First, a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation is developed to gradually increase the mask grid resolution, ensuring high-quality segmentation of objects. Specifically, an efficient region-level top-down path (r-FPN) is introduced to incorporate complementary contextual and detailed information from different stages of image-level FPN (i-FPN). Then, to alleviate the increase of computation and memory costs caused by using large masks, we develop a Mask Switch Module (MSM) with negligible computational cost to select the most suitable mask resolution for each instance, achieving high efficiency while maintaining high segmentation accuracy. Without bells and whistles, the proposed method, namely DynaMask, brings consistent and noticeable performance improvements over other state-ofthe-arts at a moderate computation overhead. The source code: https://github.com/lslrh/DynaMask."}, "cited_paper_content": {"title": "Mask R-Cnn", "abstract": "We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron ."}, "keywords": ["Mask R-CNN metaarchitecture"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve high-quality segmentation of objects with high efficiency, the proposed DynaMask method dynamically selects suitable masks for different object proposals using a Mask Switch Module (MSM) and a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To dynamically select suitable masks for different object proposals, the proposed method, namely DynaMask, utilizes a Mask Switch Module (MSM) with negligible computational cost, achieving high efficiency while maintaining high segmentation accuracy (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To incorporate the Mask R-CNN metaarchitecture, the authors utilized a dual-level Feature Pyramid Network and a Mask Switch Module to dynamically select suitable masks for different object proposals, achieving high efficiency while maintaining high segmentation accuracy (#REFR).", "given_citation_intent": "method", "given_keywords": ["Mask R-CNN metaarchitecture"]}]}
{"citing_id": "2304.06019v1", "cited_id": "1803.02077", "section_title": "Learning Objectives", "citation": "The Contextual Loss (CX) #REFR is a viable choice as it treats features of images as a set and measures the similarity between images, ignoring the spatial positions of the features.", "text_before_citation": ["The training of AlignFormer requires an objective function that does not forcefully match each spatial position as the problem lacks exact spatially aligned supervision."], "text_after_citation": ["This property enables us to compare images that are spatially deformed.", "Given two images x and y, CX loss aims to minimize the summed distance of all matched feature pairs, formulated as", "EQUATION", "where \u03d5(x) j and \u03d5(y) i are the j-th point of \u03d5(x) and ith point of \u03d5(y), respectively.", "\u03d5(x) denotes feature maps of x extracted from the VGG network \u03d5, and D is some distance measure."], "citing_paper_content": {"title": "Generating Aligned Pseudo-Supervision From Non-Aligned Data For Image Restoration In Under-Display Camera", "abstract": "Due to the difficulty in collecting large-scale and perfectly aligned paired training data for Under-Display Camera (UDC) image restoration, previous methods resort to monitor-based image systems or simulation-based methods, sacrificing the realness of the data and introducing domain gaps. In this work, we revisit the classic stereo setup for training data collection-capturing two images of the same scene with one UDC and one standard camera. The key idea is to \"copy\" details from a high-quality reference image and \"paste\" them on the UDC image. While being able to generate real training pairs, this setting is susceptible to spatial misalignment due to perspective and depth of field changes. The problem is further compounded by the large domain discrepancy between the UDC and normal images, which is unique to UDC restoration. In this paper, we mitigate the non-trivial domain discrepancy and spatial misalignment through a novel Transformer-based framework that generates well-aligned yet high-quality target data for the corresponding UDC input. This is made possible through two carefully designed components, namely, the Domain Alignment Module (DAM) and Geometric Alignment Module (GAM), which encourage robust and accurate discovery of correspondence between the UDC and normal views. Extensive experiments show that high-quality and well-aligned pseudo UDC training pairs are beneficial for training a robust restoration network. Code and the dataset are available at https://github.com/ jnjaby/AlignFormer."}, "cited_paper_content": {"title": "The Contextual Loss For Image Transformation With Non-Aligned Data", "abstract": "Feed-forward CNNs trained for image transformation problems rely on loss functions that measure the similarity between the generated image and a target image. Most of the common loss functions assume that these images are spatially aligned and compare pixels at corresponding locations. However, for many tasks, aligned training pairs of images will not be available. We present an alternative loss function that does not require alignment, thus providing an effective and simple solution for a new space of problems. Our loss is based on both context and semantics -- it compares regions with similar semantic meaning, while considering the context of the entire image. Hence, for example, when transferring the style of one face to another, it will translate eyes-to-eyes and mouth-to-mouth. Our code can be found at https://www.github.com/roimehrez/contextualLoss"}, "keywords": ["Contextual Loss"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The training of AlignFormer requires an objective function that does not forcefully match each spatial position as the problem lacks exact spatially aligned supervision. An alternative loss function that does not require alignment, based on both context and semantics, has been proposed in #REFR, which could be a potential solution for this problem.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of lacking exact spatially aligned supervision, previous methods have used loss functions that assume spatial alignment, but this assumption is not always valid. Therefore, alternative loss functions that do not require alignment, such as the one proposed in the reference paper \"#REFR\", have been developed to provide an effective solution for a new space of problems.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of lacking exact spatially aligned supervision, recent works have proposed alternative loss functions such as the Contextual Loss #REFR, which compares regions with similar semantic meaning while considering the context of the entire image.", "given_citation_intent": "background", "given_keywords": ["Contextual Loss"]}]}
{"citing_id": "2303.14531v1", "cited_id": "1905.10887", "section_title": "Cifar", "citation": "While this is not the focus of this work, our finding suggests that synthetic samples can be helpful for accuracy if used properly, challenging previous beliefs #REFR .", "text_before_citation": ["On CIFAR-10 far-OOD detection, SIO does not outperform vanilla OE, which incorporates a large set of external OOD samples for training.", "We suspect that this is because far-OOD detection on CIFAR-10 is dominated by low-level statistics #OTHEREFR , and using synthetic ID samples may slightly push the model away from the real low-level statistics of the ID data, causing a shrinked difference between ID and OOD samples.", "Nonetheless, SIO improves far-OOD detection in all other cases and effectively closes the gap between non-OE methods and OE.", "Notably, LogitNorm + SIO yields a 97.09% AUROC without any OOD training data, which is on par with the 97.16% AUROC achieved by OE.", "Lastly, we observe that SIO can also benefit ID classification accuracy."], "text_after_citation": ["On the other hand, however, in our later experiments where we vary the hyperparameters, we find that SIO consistently boosts OOD detection performance even if it does not improve ID accuracy.", "More discussion on this can be found in Section 4.3.", "On CIFAR-100, we observe similar results to CIFAR-10, with the general observation that SIO can benefit OOD detection performance.", "With SIO, the average near-OOD / far-OOD AUROC is lifted from 76.63% / 80.29% to 77.20% / 83.65%, and the best numbers are improved from 80.22% / 87.54% to 80.34% / 89.37%."], "citing_paper_content": {"title": "Sio: Synthetic In-Distribution Data Benefits Out-Of-Distribution Detection", "abstract": "Building up reliable Out-of-Distribution (OOD) detectors is challenging, often requiring the use of OOD data during training. In this work, we develop a data-driven approach which is distinct and complementary to existing works: Instead of using external OOD data, we fully exploit the internal in-distribution (ID) training set by utilizing generative models to produce additional synthetic ID images. The classifier is then trained using a novel objective that computes weighted loss on real and synthetic ID samples together. Our training framework, which is termed SIO, serves as a \"plug-and-play\" technique that is designed to be compatible with existing and future OOD detection algorithms, including the ones that leverage available OOD training data. Our experiments on CIFAR-10, CIFAR-100, and ImageNet variants demonstrate that SIO consistently improves the performance of nearly all stateof-the-art (SOTA) OOD detection algorithms. For instance, on the challenging CIFAR-10 v.s. CIFAR-100 detection problem, SIO improves the average OOD detection AUROC of 18 existing methods from 86.25% to 89.04% and achieves a new SOTA of 92.94% according to the OpenOOD benchmark. Code is available at https://github.com/ zjysteven/SIO."}, "cited_paper_content": {"title": "Classification Accuracy Score For Conditional Generative Models", "abstract": "Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes\u2014variational autoencoders, autoregressive models, and generative adversarial networks (GANs)\u2014to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9% and 41.6%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric."}, "keywords": ["synthetic samples"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We find that traditional GAN metrics such as Inception Score (IS) and FID neither predict Classification Accuracy Score (CAS) nor are useful when evaluating non-GAN models #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors found that traditional GAN metrics such as Inception Score (IS) and FID were neither predictive of Classification Accuracy Score (CAS) nor useful when evaluating non-GAN models, as reported in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of SIO utilize generative models to produce additional synthetic ID images, which improves the performance of nearly all state-of-the-art OOD detection algorithms, including LogitNorm + SIO that achieves a 97.09% AUROC without any OOD training data, on par with OE, as shown in #REFR. The use of synthetic samples from generative models is a novel approach that effectively closes the gap between non-OE methods and OE.", "given_citation_intent": "result", "given_keywords": ["synthetic samples"]}]}
{"citing_id": "2304.00086v2", "cited_id": "1903.10075", "section_title": "When Ml Is Used In Economics?", "citation": "Also, article #REFR point that the methods developed in the ML have been particularly successful in big data settings, where we observe information on a large number of units, many pieces of information on each unit, or both.", "text_before_citation": ["Lastly, large datasets allow for more flexible relationships than simple linear models can capture.", "ML techniques are handy in those cases due to their ability to model intricate and nonlinear relationships potentially offering new insights.", "Similarly, article #OTHEREFR argue that ML not only provides new tools but also solves a different problem.", "They assert ML's success is largely due to its ability to discover the complex structure that was not specified in advance.", "They suggest applying ML to economics requires finding relevant tasks, for instance, where the focus is on increasing prediction accuracy or uncovering generalizable patterns from complex datasets."], "text_after_citation": ["The authors suggest that for using ML tools for economics research and analysis, researchers should clearly articulate their goals and why certain properties of ML algorithms may or may not be important."], "citing_paper_content": {"title": "Machine Learning For Economics Research: When What And How? *", "abstract": "This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used to process nontraditional and unstructured data, capture strong nonlinearity, and improve prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggests that ML is becoming an essential addition to the econometrician's toolbox."}, "cited_paper_content": {"title": "Machine Learning Methods Economists Should Know About", "abstract": "We discuss the relevance of the recent Machine Learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the machine learning literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, as well as matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics, methods that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, problems that include causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models."}, "keywords": ["ML"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Similarly, #OTHEREFR argue that ML not only provides new tools but also solves a different problem. They assert ML's success is largely due to its ability to discover the complex structure that was not specified in advance. They suggest applying ML to economics requires finding relevant tasks, for instance, where the focus is on increasing prediction accuracy or uncovering generalizable patterns from complex datasets. The authors of the reference paper \"Machine Learning Methods Economists Should Know About\" discuss some specific methods from the machine learning literature that are important for empirical researchers in economics, including supervised learning methods for regression and classification, unsupervised learning methods, as well as matrix completion methods. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Article #REFR highlights the importance of supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods from the machine learning literature for empirical researchers in economics, particularly when dealing with large datasets and intricate and nonlinear relationships.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Article #OTHEREFR suggests that machine learning (ML) is particularly useful for modeling intricate and nonlinear relationships in large datasets, potentially offering new insights, and discovering complex structures that were not specified in advance, which is in line with the findings of the reference paper.", "given_citation_intent": "method", "given_keywords": ["ML"]}]}
{"citing_id": "2304.12830v1", "cited_id": "1903.07163", "section_title": "V. Evaluation", "citation": "If any iteration fails to converge in 512 integration steps, then the spin output s is such that Ts = 0 (where T is the transform matrix defined by #REFR ).", "text_before_citation": ["For the scenarios simulated in this paper, dt = 0.005 was found to be sufficient.", "The initial values x i are i.i.d N (0, 0.001) and e i are initialized using a folded N (0, 0.001) distribution.", "We set p = 0.98, \u03b2 = 1, a = 2 and \u03b3 = 1000/(256 \u2022 0.01).", "These parameters were empirically selected, based on trial-and-error experiments, such that the system can achieve a steady state and attain good performance.", "Note that performance can be further improved by optimally selecting these parameters, and we plan to address this in our future work."], "text_after_citation": ["2) Evaluation Metrics: Our evaluation setup simulates an uplink N t \u00d7 N r MIMO system which has N t users (with one transmit antenna each) and N r receive antennas at the base station (N r > N t ).", "We assume a slow-fading channel, and channel instances are assumed to follow the Rayleigh fading model.", "The BER is computed as the mean BER of all users.", "We compare our methods against MMSE-SIC with optimal ordering #OTHEREFR and the MMSE detector in both large and massive MIMO scenarios.", "Spectral efficiency computations are based on convolutional coding with code-rates #OTHEREFR 3 , 1 2 , 2 3 , and an oracle Adaptive Modulation and Coding (AMC) module that selects the best modulation and code-rate based on SNR."], "citing_paper_content": {"title": "Uplink Mimo Detection Using Ising Machines: A Multi-Stage Ising Approach", "abstract": "Multiple-Input-Multiple-Output (MIMO) signal detection is central to every state-of-the-art communication system, and enhancements in error performance and computation complexity of MIMO detection would significantly enhance data rate and latency experienced by the users. Theoretically, the optimal MIMO detector is the maximum-likelihood (ML) MIMO detector; however, due to its extremely high complexity, it is not feasible for large real-world communication systems. Over the past few years, algorithms based on physics-inspired Ising solvers, like Coherent Ising machines and Quantum Annealers, have shown significant performance improvements for the MIMO detection problem. However, the current state-of-the-art is limited to low-order modulations or systems with few users. In this paper, we propose an adaptive multi-stage Ising machine-based MIMO detector that extends the performance gains of physics-inspired computation to Large and Massive MIMO systems with a large number of users and very high modulation schemes (up to 256-QAM). We enhance our previously proposed delta Ising formulation and develop a heuristic that adaptively optimizes the performance and complexity of our proposed method. We perform extensive micro-benchmarking to optimize several free parameters of the system and evaluate our methods' BER and spectral efficiency for Large and Massive MIMO systems (up to 32 users and 256 QAM modulation)."}, "cited_paper_content": {"title": "Oim: Oscillator-Based Ising Machines For Solving Combinatorial Optimisation Problems", "abstract": "We present a new way to make Ising machines, i.e., using networks of coupled self-sustaining nonlinear oscillators. Our scheme is theoretically rooted in a novel result that establishes that the phase dynamics of coupled oscillator systems, under the influence of subharmonic injection locking, are governed by a Lyapunov function that is closely related to the Ising Hamiltonian of the coupling graph. As a result, the dynamics of such oscillator networks evolve naturally to local minima of the Lyapunov function. Two simple additional steps (i.e., adding noise, and turning subharmonic locking on and off smoothly) enable the network to find excellent solutions of Ising problems. We demonstrate our method on Ising versions of the MAX-CUT and graph colouring problems, showing that it improves on previously published results on several problems in the G benchmark set. Our scheme, which is amenable to realisation using many kinds of oscillators from different physical domains, is particularly well suited for CMOS IC implementation, offering significant practical advantages over previous techniques for making Ising machines. We present working hardware prototypes using CMOS electronic oscillators."}, "keywords": ["transform matrix", "iteration"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The Ising machine-based approach proposed in this paper has shown significant performance improvements for solving combinatorial optimization problems, including Ising versions of the MAX-CUT and graph coloring problems, and is particularly well-suited for CMOS IC implementation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The phase dynamics of coupled oscillator systems, under the influence of subharmonic injection locking, are governed by a Lyapunov function that is closely related to the Ising Hamiltonian of the coupling graph, as shown in the work by #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of physics-inspired Ising solvers, such as Coherent Ising machines, has shown significant improvements in solving combinatorial optimization problems, as demonstrated in the reference paper #REFR, which proposes a novel approach using oscillator-based Ising machines that can be implemented using CMOS IC, and has achieved excellent results on Ising versions of the MAX-CUT and graph coloring problems.", "given_citation_intent": "background", "given_keywords": ["transform matrix", "iteration"]}]}
{"citing_id": "2303.04456v1", "cited_id": "1806.01260", "section_title": "Implementation Details", "citation": "Same augmentations are performed on the training data as #REFR , namely 50% horizontal flips, random brightness, contrast, saturation, and hue jitter.", "text_before_citation": ["Particularly, the top two levels are not used in the depth encoder.", "For the motion network, the pose decoder is adopted from #OTHEREFR .", "The object motion decoder 4 uses 9 and 2 RMUs in level 4 and the remained levels, respectively.", "RMUs are not shared across different levels in order to maximize filter diversity for different scales.", "Training Details. The whole system is implemented in TensorFlow [1] ."], "text_after_citation": ["Following #OTHEREFR , the length of each image sequence is fixed to 3 frames. The central frame is treated as the target view.", "The depth and motion networks are jointly trained using Adam #OTHEREFR with a batch size varying from 16 to 40 on multiple GPUs.", "To address the stationary pixels and the oc-clusion problem, the auto-masking and the per-pixel minimum reprojection loss #OTHEREFR are adopted.", "Depth map and motion field are regularized by an edge-aware smoothness loss #OTHEREFR while the proposed outlier-aware regularization loss is further imposed on the object motion field.", "The selfsupervision #OTHEREFR is also adopted but no cropping is applied. Some parts of RM-Depth require pre-training #OTHEREFR ."], "citing_paper_content": {"title": "Rm-Depth: Unsupervised Learning Of Recurrent Monocular Depth In Dynamic Scenes *", "abstract": "Unsupervised methods have showed promising results on monocular depth estimation. However, the training data must be captured in scenes without moving objects. To push the envelope of accuracy, recent methods tend to increase their model parameters. In this paper, an unsupervised learning framework is proposed to jointly predict monocular depth and complete 3D motion including the motions of moving objects and camera. (1) Recurrent modulation units are used to adaptively and iteratively fuse encoder and decoder features. This not only improves the singleimage depth inference but also does not overspend model parameters. (2) Instead of using a single set of filters for upsampling, multiple sets of filters are devised for the residual upsampling. This facilitates the learning of edgepreserving filters and leads to the improved performance. (3) A warping-based network is used to estimate a motion field of moving objects without using semantic priors. This breaks down the requirement of scene rigidity and allows to use general videos for the unsupervised learning. The motion field is further regularized by an outlier-aware training loss. Despite the depth model just uses a single image in test time and 2.97M parameters, it achieves state-of-the-art results on the KITTI and Cityscapes benchmarks. * This research work is not for commercial use unless a prior arrangement has been made with the author. 1 The words, ego-motion, camera motion and pose, are interchangeably used throughout the paper."}, "cited_paper_content": {"title": "Digging Into Self-Supervised Monocular Depth Estimation", "abstract": "Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark."}, "keywords": ["training data"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed unsupervised learning framework for monocular depth estimation and 3D motion prediction, as described in this paper, achieves state-of-the-art results on the KITTI and Cityscapes benchmarks with only a single image and 2.97M parameters, as shown in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of monocular depth estimation, the authors propose a set of improvements, including a minimum reprojection loss, a full-resolution multi-scale sampling method, and an auto-masking loss, which are demonstrated to be effective in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the challenge of acquiring per-pixel ground-truth depth data at scale, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation on scenes with moving objects, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["training data"]}]}
{"citing_id": "2304.03932v1", "cited_id": "1903.10384", "section_title": "3D Gans Variants", "citation": "Using MeshGAN, the first intrinsic GAN architecture operating directly on 3D meshes, we can generate high-fidelity 3D faces with rich identities, and expressions #REFR .", "text_before_citation": ["It samples images at various scales using a patchbased discriminator to produce radiance fields.", "Another strategy is to use an effective and efficient tri-plane-based 3D GAN framework #OTHEREFR .", "It uses dual discrimination to promote consistency from several viewpoints while the generator is conditioned on poses to faithfully model attribute distributions dependent on the pose in the real-world datasets.", "\u2022 Generating texture models or 3DMMs: In #OTHEREFR , the authors introduce a novel 3D Morphable Model (3DMM) in the form of a GAN Texture Model to provide excellent facial shape and texture reconstructions in arbitrary recording conditions from 2D images.", "They also show the results to be both photorealistic and identity preserving in both qualitative and quantitative experiments."], "text_after_citation": ["\u2022 Feedback learning: By considering the generator network as an encoder and decoder, the spatial output from multiple discriminators can be used to provide feedback to the generator so it can improve on its previous generations using Adaptive Spatial Transform #OTHEREFR .", "\u2022 Meshes: In industrial design, gaming, computer graphics and other digital art, automatically generating shapes based on meshes is necessary.", "Most of the current research is involved with voxel and point cloud generation, alienating itself from the design and graphics communities.", "MeshGAN, as mentioned above, is the first intrinsic GAN architecture operating directly on 3D meshes and can generate high-fidelity 3D faces with rich identities and expressions.", "To automatically generate shapes based on meshes, we use the signed distance function representation to generate detail-preserving threedimensional surface meshes #OTHEREFR ."], "citing_paper_content": {"title": "3D Gans And Latent Space: A Comprehensive Survey", "abstract": "Generative Adversarial Networks (GANs) have emerged as a significant player in generative modeling by mapping lower-dimensional random noise to higher-dimensional spaces. These networks have been used to generate high-resolution images and 3D objects. The efficient modeling of 3D objects and human faces is crucial in the development process of 3D graphical environments such as games or simulations. 3D GANs are a new type of generative model used for 3D reconstruction, point cloud reconstruction, and 3D semantic scene completion. The choice of distribution for noise is critical as it represents the latent space. Understanding a GAN's latent space is essential for fine-tuning the generated samples, as demonstrated by the morphing of semantically meaningful parts of images. In this work, we explore the latent space and 3D GANs, examine several GAN variants and training methods to gain insights into improving 3D GAN training, and suggest potential future directions for further research."}, "cited_paper_content": {"title": "Meshgan: Non-Linear 3D Morphable Models Of Faces", "abstract": "Generative Adversarial Networks (GANs) are currently the method of choice for generating visual data. Certain GAN architectures and training methods have demonstrated exceptional performance in generating realistic synthetic images (in particular, of human faces). However, for 3D object, GANs still fall short of the success they have had with images. One of the reasons is due to the fact that so far GANs have been applied as 3D convolutional architectures to discrete volumetric representations of 3D objects. In this paper, we propose the first intrinsic GANs architecture operating directly on 3D meshes (named as MeshGAN). Both quantitative and qualitative results are provided to show that MeshGAN can be used to generate high-fidelity 3D face with rich identities and expressions."}, "keywords": ["first intrinsic GAN", "MeshGAN"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In #REFR, the authors propose the first intrinsic GAN architecture operating directly on 3D meshes, which can be used to generate high-fidelity 3D faces with rich identities and expressions, overcoming the limitations of GANs applied as 3D convolutional architectures to discrete volumetric representations of 3D objects.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In #REFR, the authors propose the first intrinsic GANs architecture operating directly on 3D meshes, which can be used to generate high-fidelity 3D face with rich identities and expressions, providing a promising method for efficient modeling of 3D objects and human faces in the development process of 3D graphical environments such as games or simulations.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In #REFR, the authors propose the first intrinsic GAN architecture operating directly on 3D meshes, named MeshGAN, which can be used to generate high-fidelity 3D faces with rich identities and expressions, addressing the limitations of GANs applied as 3D convolutional architectures to discrete volumetric representations of 3D objects.", "given_citation_intent": "method", "given_keywords": ["first intrinsic GAN", "MeshGAN"]}]}
{"citing_id": "2304.14674v1", "cited_id": "1807.01697", "section_title": "Experiments", "citation": "Besides, we comprehensively test the robustness of SAM with 18 types of data corruption at 5 severity levels by following #REFR .", "text_before_citation": ["In this study, two widely-used surgical instrument segmentation datasets, i.e., EndoVis17 #OTHEREFR and EndoVis18 #OTHEREFR , have been adopted in our experiments. Our evaluation involves three categories.", "Firstly, we have provided both quantitative and qualitative assessments on the promptable segmentation performance of SAM, with bounding boxes and single points, for binary and instrument-wise segmentation."], "text_after_citation": ["Moreover, we also examine SAM on its automatic mask generation in unprompted settings for surgical scene segmentation."], "citing_paper_content": {"title": "Sam Meets Robotic Surgery: An Empirical Study In Robustness Perspective", "abstract": "Segment Anything Model (SAM) is a foundation model for semantic segmentation and shows excellent generalization capability with the prompts. In this empirical study, we investigate the robustness and zero-shot generalizability of the SAM in the domain of robotic surgery in various settings of (i) prompted vs. unprompted; (ii) bounding box vs. points-based prompt; (iii) generalization under corruptions and perturbations with five severity levels; and (iv) state-of-the-art supervised model vs. SAM. We conduct all the observations with two well-known robotic instrument segmentation datasets of MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict the parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as different classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, it is unable to identify instruments in some complex surgical scenarios of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. Therefore, we can argue that SAM is not ready for downstream surgical tasks without further domain-specific fine-tuning."}, "cited_paper_content": {"title": "Benchmarking Neural Network Robustness To Common Corruptions And Surface Variations", "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize."}, "keywords": ["robustness"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate the robustness of our model, we benchmarked its performance against common corruptions and perturbations using the ImageNet-P dataset proposed by Hendrycks et al. in their paper \"Benchmarking Neural Network Robustness To Common Corruptions And Surface Variations\" #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the robustness of the SAM model, the authors benchmarked its performance against common corruptions and perturbations, as proposed in the reference paper by Hendrycks and Dietterich (2019) #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the robustness of the SAM model, the authors benchmarked its performance against common corruptions and perturbations, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["robustness"]}]}
{"citing_id": "2303.15027v2", "cited_id": "0908.3817", "section_title": "I.I.D. Datasets", "citation": "The ground-truth graph is a large network with 70 nodes and 123 edges which is available in the bnlearn #REFR ) repository.", "text_before_citation": ["(1989) ) designed to provide an alarm message for patients, and has an associated synthetic dataset.", "In particular, it implements a cautionary alarm message for patient monitoring.", "The ground-truth graph is a medium-sized network with 37 nodes and 46 edges. This dataset was used by #OTHEREFR , and to evaluate their approaches. The ground-truth network is available in this repository: https://www.bnlearn.com/bnrepository/.", "\u2022 HEPAR2: It is a probabilistic causal model for the diagnosis of liver disorders #OTHEREFR ).", "This causal Bayesian network tries to capture the causal links among different risk factors, diseases, symptoms, and test results."], "text_after_citation": [], "citing_paper_content": {"title": "A Survey On Causal Discovery Methods For Temporal And Non-Temporal Data", "abstract": "Causal Discovery (CD) is the process of identifying the cause-effect relationships among the variables of a system from data. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (i.i.d.) data and time series data. For this purpose, we first introduce the common terminologies in causal discovery, and then provide a comprehensive discussion of the algorithms designed to identify the causal edges in different settings. We further discuss some of the benchmark datasets available for evaluating the performance of the causal discovery methods, available tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also test some common causal discovery algorithms on different benchmark datasets, and compare their performances. Finally, we conclude by presenting the common challenges involved in causal discovery, and also, discuss the applications of causal discovery in multiple areas of interest."}, "cited_paper_content": {"title": "Learning Bayesian Networks With The Bnlearn R Package", "abstract": "bnlearn is an R package (R Development Core Team 2010) which includes several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables. Both constraint-based and score-based algorithms are implemented, and can use the functionality provided by the snow package (Tierney et al. 2008) to improve their performance via parallel computing. Several network scores and conditional independence algorithms are available for both the learning algorithms and independent use. Advanced plotting options are provided by the Rgraphviz package (Gentry et al. 2010)."}, "keywords": ["ground-truth graph", "bnlearn"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors utilized the bnlearn R package for learning the structure of Bayesian networks with both discrete and continuous variables, and implemented both constraint-based and score-based algorithms (R Development Core Team 2010). #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn the structure of Bayesian networks with either discrete or continuous variables, the authors used the bnlearn R package, which includes several algorithms for this purpose, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The ground-truth graph for a medium-sized network with 37 nodes and 46 edges is available in the bnlearn R package (#REFR), which implements several algorithms for learning the structure of Bayesian networks with either discrete or continuous variables, and can use the functionality provided by the snow package to improve their performance via parallel computing.", "given_citation_intent": "method", "given_keywords": ["ground-truth graph", "bnlearn"]}]}
{"citing_id": "2304.07522v1", "cited_id": "1412.0035", "section_title": "Id Descriptor Information & Inversion", "citation": "In a seminal work, in 2015, Mahendran and Vedaldi #REFR set out to analyse the visual information contained in both shallow (e.g.", "text_before_citation": [], "text_after_citation": ["HOG) and deep feature representations, to investigate the question: given an encoding of an image, to which extent is it possible to reconstruct the image itself.", "They propose an optimisation method to invert representations using gradient descent.", "Among their findings are that networks retain rich information even at deep levels and that a progressively more invariant and abstract notion of the image content is formed in the network.", "As face identity descriptors usually are made up of the final layer of a deep network, it would therefore be the most invariant and abstract representation.", "Few works so far have investigated the inversion of a face descriptor back to a face image. Genova et al."], "citing_paper_content": {"title": "Id2Image: Leakage Of Non-Id Information Into Face Descriptors And Inversion From Descriptors To Images", "abstract": "Embedding a face image to a descriptor vector using a deep CNN is a widely used technique in face recognition. Via several possible training strategies, such embeddings are supposed to capture only identity information. Information about the environment (such as background and lighting) or changeable aspects of the face (such as pose, expression, presence of glasses, hat etc.) should be discarded since they are not useful for recognition. In this paper, we present a surprising result that this is not the case. We show that non-ID attributes, as well as landmark positions and the image histogram can be recovered from the ID embedding of state-of-the-art face embedding networks (VGGFace2 and ArcFace). In fact, these non-ID attributes can be predicted from ID embeddings with similar accuracy to a prediction from the original image. Going further, we present an optimisation strategy that uses a generative model (specifically StyleGAN2 for faces) to recover images from an ID embedding. We show photorealistic inversion from ID embedding to face image in which not only is the ID realistically reconstructed but the pose, lighting and background/apparel to some extent as well."}, "cited_paper_content": {"title": "Understanding Deep Image Representations By Inverting Them", "abstract": "Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance."}, "keywords": ["visual information"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors show that representations of images can be inverted to reconstruct the original image, and that this method can be applied to CNNs, including state-of-the-art networks such as VGGFace2 and ArcFace (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Image representations are a crucial component of almost any image understanding system, and recent state-of-the-art CNN image representations retain photographically accurate information about the image, as shown by the inversion technique proposed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Image representations are a crucial component of almost any image understanding system, and our understanding of them remains limited. Inverting representations can provide a direct analysis of the visual information contained in them, and the reference paper \"#REFR\" presents a general framework to achieve this.", "given_citation_intent": "background", "given_keywords": ["visual information"]}]}
{"citing_id": "2304.11668v1", "cited_id": "2004.00288", "section_title": "Experiment Results", "citation": "For the identification task, CoReFace achieves competitive performance which is only 0.02% lower compared to the highest one Curricular-Face #REFR .", "text_before_citation": ["When there is a higher FAR bound (10 \u22124 ) or the evaluation dataset is larger, CoReFace still outperforms the competitors.", "Results on MegaFace.", "Finally, we demonstrate the efficacy of our method on the MegaFace Challenge.", "The gallery set of MegaFace contains 1M images of 690K subjects, and the probe set is FaceScrub, which contains 100K photos of 530 unique individuals.", "We follow #OTHEREFR to remove the face images with wrong labels and evaluate our method on the refined dataset. Table 3 shows the performance of different methods."], "text_after_citation": ["For the verification task, CoReFace outperforms all the other approaches with a clear margin.", "The Broad-Face #OTHEREFR also shows competitive performance by building a dynamic queue to gain extra training on the classification layer.", "Without complex structure reformation, CoReFace adds an image-image regularization to improve the feature distribution and boost the performance of open-set face recognition."], "citing_paper_content": {"title": "Coreface: Sample-Guided Contrastive Regularization For Deep Face Recognition", "abstract": "The discriminability of feature representation is the key to open-set face recognition. Previous methods rely on the learnable weights of the classification layer that represent the identities. However, the evaluation process learns no identity representation and drops the classifier from training. This inconsistency could confuse the feature encoder in understanding the evaluation goal and hinder the effect of identity-based methods. To alleviate the above problem, we propose a novel approach namely Contrastive Regularization for Face recognition (CoReFace) to apply image-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to regularize the training with the image-image relationship directly, which is consistent with the evaluation process. To integrate contrastive learning into face recognition, we augment embeddings instead of images to avoid the image quality degradation. Then, we propose a novel contrastive loss for the representation distribution by incorporating an adaptive margin and a supervised contrastive mask to generate steady loss values and avoid the collision with the classification supervision signal. Finally, we discover and solve the semantically repetitive signal problem in contrastive learning by exploring new pair coupling protocols. Extensive experiments demonstrate the efficacy and efficiency of our CoReFace which is highly competitive with the state-of-the-art approaches."}, "cited_paper_content": {"title": "Curricularface: Adaptive Curriculum Learning Loss For Deep Face Recognition", "abstract": "As an emerging topic in face recognition, designing margin-based loss functions can increase the feature margin between different classes for enhanced discriminability. More recently, the idea of mining-based strategies is adopted to emphasize the misclassified samples, achieving promising results. However, during the entire training process, the prior methods either do not explicitly emphasize the sample based on its importance that renders the hard samples not fully exploited; or explicitly emphasize the effects of semi-hard/hard samples even at the early training stage that may lead to convergence issue. In this work, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Specifically, our CurricularFace adaptively adjusts the relative importance of easy and hard samples during different training stages. In each stage, different samples are assigned with different importance according to their corresponding difficultness. Extensive experimental results on popular benchmarks demonstrate the superiority of our CurricularFace over the state-of-the-art competitors."}, "keywords": ["CoReFace"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We compare our CoReFace method with state-of-the-art competitors on the refined MegaFace dataset, and our method outperforms the others, as shown in Table 3. Our approach employs sample-guided contrastive learning to regularize the training with the image-image relationship directly, which is consistent with the evaluation process. Specifically, we propose a novel Adaptive Curriculum Learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Table 3 shows that our proposed CoReFace outperforms the state-of-the-art competitors on the MegaFace Challenge, demonstrating its superior performance in face recognition. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed CoReFace method achieves highly competitive results compared to the state-of-the-art approaches on the MegaFace Challenge dataset (#REFR).", "given_citation_intent": "result", "given_keywords": ["CoReFace"]}]}
{"citing_id": "2303.13900v1", "cited_id": "1802.05642", "section_title": "C. Perceptual Loss", "citation": "Our perceptual loss(L perc in algorithm1) is defined as a l 1 loss in feature space, see equation #REFR .", "text_before_citation": ["To ensure the perceptual fidelity of the SR, we propose to use a perceptual loss function (L perc in algorithm1) to constrain the quality of SR results in feature space.", "The perceptual loss was first introduced in #OTHEREFR , where a pretrained VGG-19 network #OTHEREFR was used to extract features.", "However, our implementation differs in that we do not use the pretrained VGG-19 network, but an actively updating ResNet10 network.", "More details are explained in the \"Feature Matching\" section below."], "text_after_citation": ["Here I i,j,k and\u00ce i,j,k represent HR and SR intensity value at the i, j, k-th voxel in all three image dimensions(W, H, D), F(\u2022) represent the feature network.", "L perc = 1 W \u00d7 H \u00d7 D W,H,D i,j,k=1 |F(I i,j,k ) \u2212 F(\u00ce i,j,k )| (1)", "As shown in #OTHEREFR , perceptual loss functions generally outperform MSE loss functions, because they better preserve the detailed texture of the image."], "citing_paper_content": {"title": "A Three-Player Gan For Super-Resolution In Magnetic Resonance Imaging", "abstract": "Learning based single image super resolution (SISR) task is well investigated in 2D images. However, SISR for 3D Magnetics Resonance Images (MRI) is more challenging compared to 2D, mainly due to the increased number of neural network parameters, the larger memory requirement and the limited amount of available training data. Current SISR methods for 3D volumetric images are based on Generative Adversarial Networks (GANs), especially Wasserstein GANs due to their training stability. Other common architectures in the 2D domain, e.g. transformer models, require large amounts of training data and are therefore not suitable for the limited 3D data. However, Wasserstein GANs can be problematic because they may not converge to a global optimum and thus produce blurry results. Here, we propose a new method for 3D SR based on the GAN framework. Specifically, we use instance noise to balance the GAN training. Furthermore, we use a relativistic GAN loss function and an updating feature extractor during the training process. We show that our method produces highly accurate results. We also show that we need very few training samples. In particular, we need less than 30 samples instead of thousands of training samples that are typically required in previous studies. Finally, we show improved out-of-sample results produced by our model."}, "cited_paper_content": {"title": "The Mechanics Of N-Player Differentiable Games", "abstract": "The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -- whilst at the same time being applicable to -- and having guarantees in -- much more general games."}, "keywords": ["perceptual loss(L perc", "l 1 loss"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To ensure the perceptual fidelity of the SR, we propose to use a perceptual loss function (L perc in algorithm1) to constrain the quality of SR results in feature space. The perceptual loss was first introduced in The Mechanics Of N-Player Differentiable Games, where a decomposition of the second-order dynamics into two components was proposed. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the behavior of gradient-based methods in games, especially in multi-objective architectures like GANs, the authors propose to use Symplectic Gradient Adjustment (SGA) algorithm, which has been shown to be competitive with other algorithms for finding stable fixed points in GANs and applicable to much more general games, as described in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of perceptual loss (L perc) to constrain the quality of SR results in feature space has been proposed in previous studies #REFR.", "given_citation_intent": "background", "given_keywords": ["perceptual loss(L perc", "l 1 loss"]}]}
{"citing_id": "2303.10343v1", "cited_id": "1902.04103", "section_title": "Pascal Voc \u2192 Clipart1K", "citation": "We also see sub-optimal results for Union #REFR due to errors in the approximation of unweighted union, similar to object detection experiments.", "text_before_citation": ["a small \u03bb value on target domain image without any pseudo labels.", "This strategy is similar to our noise mixing during the warmup, but is used throughout the training.", "Note that this approach performs worse than our AT basedline.", "This is because although noise mixing could be helpful in general, as shown by both AFAN #OTHEREFR and our following ablation studies, heavily relying on it in the adaptation phase of Mean Teacher can lead to bias towards the source domain due to the fact that \"mixed-in\" target information is only limited to a tiny amount to act as a domain-aware augmentation.", "Indeed, we believe for cross-domain mean teacher, the pseudo labels are much stronger target signals and should be taken advantage of appropriately."], "text_after_citation": ["PASCAL VOC \u2192 Watercolor2k Next, we are interested in answering the question whether or not the encouraging gains observed in PASCAL VOC \u2192 Clipart1k can be reproduced on a different dataset.", "To do this, we use Wa-tercolor2k and evaluate the performance of PASCAL VOC \u2192 Watercolor2k adaptation.", "Note that after experimenting with Clipart1k, we narrowed down our set of hyperparameters to ones that work best for both Adaptive Teacher and our method for fair competition.", "For Watercolor2k, to test our method's robustness, we directly perform grid search on this small set of hyper-parameters without any further tuning or manual supervision.", "Nonetheless, even without exhaustive tuning, our results in Table 5 show that we can still outperform AT (mAP=57.7) with a +1.5 improvement and archive mAP=59.3."], "citing_paper_content": {"title": "Lossmix: Simplify And Generalize Mixup For Object Detection And Beyond", "abstract": "The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation, which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Building on this framework, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix consistently outperforms currently popular mixing strategies. Furthermore, we design a two-stage domain mixing method that leverages LossMix to surpass Adaptive Teacher (CVPR 2022) and set a new state of the art for unsupervised domain adaptation. * Work done during a residency with Mineral."}, "cited_paper_content": {"title": "Bag Of Freebies For Training Object Detection Neural Networks", "abstract": "Training heuristics greatly improve various image classification model accuracies~\\cite{he2018bag}. Object detection models, however, have more complex neural network structures and optimization targets. The training strategies and pipelines dramatically vary among different models. In this works, we explore training tweaks that apply to various models including Faster R-CNN and YOLOv3. These tweaks do not change the model architectures, therefore, the inference costs remain the same. Our empirical results demonstrate that, however, these freebies can improve up to 5% absolute precision compared to state-of-the-art baselines."}, "keywords": ["detection experiments"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As shown in previous works~#REFR, noise mixing can be helpful in general, but heavily relying on it in the adaptation phase of Mean Teacher can lead to bias towards the source domain.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our empirical results demonstrate that the freebies proposed in Bag Of Freebies For Training Object Detection Neural Networks #REFR can improve up to 5% absolute precision compared to state-of-the-art baselines.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our detection experiments demonstrate that LossMix consistently outperforms currently popular mixing strategies, including the Bag Of Freebies For Training Object Detection Neural Networks proposed by He et al. (#REFR).", "given_citation_intent": "result", "given_keywords": ["detection experiments"]}]}
{"citing_id": "2303.07475v1", "cited_id": "1906.07413", "section_title": "Lemma 16. Under Assumption 3, For Any", "citation": "To complete the proof, we need to show that any optimal solution to (28) satisfies the dropped constraint \u03c8 * (Q) \u2264 0 and is therefore also a solution to the original convex program #REFR .", "text_before_citation": ["n i=1 g \u22121 (q k,i ) = 1 for all k \u2208 [K].", "We now consider the relaxed convex program Q \u2208 arg min", "Q\u2208R Kn 1 2 Q CXX CQ F (Q) (28) subject to \u2212 q k,i < 0 for all i \u2208 [n] and k \u2208 [K], and 1 \u2212 n i=1 g \u22121 q k,i \u2264 0 for all k \u2208 [K]", "that is obtained by dropping the constraint \u03c8 * (Q) \u2264 0 and relaxing the equality constraint 1\u2212", "n i=1 g \u22121 q k,i = 0 for all k \u2208 [K] to an inequality constraint, 1 \u2212 n i=1 g \u22121 q k,i \u2264 0."], "text_after_citation": ["(Note that by Lemma 15, the equality constraint 1 \u2212 n i=1 g \u22121 q k,i = 0 for all k \u2208 [K], also ensures that Q \u2208 dom \u03c8 * .) It is necessary and sufficient for any optimal solutionQ to the relaxed convex program #OTHEREFR to satisfy its KKT conditions, listed below:", "EQUATION", "EQUATION", "EQUATION", "First, we claim that any optimal solutionQ needs to satisfy 1 \u2212"], "citing_paper_content": {"title": "General Loss Functions Lead To (Approximate) Interpolation In High Dimensions", "abstract": "We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primaldual analysis of [29], allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the effect of certain loss functions designed for out-of-distribution problems on the closed-form solution."}, "cited_paper_content": {"title": "Learning Imbalanced Datasets With Label-Distribution-Aware Margin Loss", "abstract": "Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains."}, "keywords": ["original convex program", "dropped constraint \u03c8"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose a label-distribution-aware margin (LDAM) loss to improve the performance of deep learning algorithms on imbalanced datasets, which can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. This method has been tested on several benchmark vision tasks, including the real-world imbalanced dataset iNaturalist 2018, and has shown significant improvement over existing techniques. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a label-distribution-aware margin (LDAM) loss to improve the performance of deep learning algorithms on imbalanced datasets, which can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' proposed relaxed convex program, obtained by dropping the constraint \u03c8 * (Q) \u2264 0 and relaxing the equality constraint 1\u2212 n i=1 g \u22121 q k,i = 0 for all k \u2208 [K] to an inequality constraint, 1 \u2212 n i=1 g \u22121 q k,i \u2264 0, is a novel approach to address class-imbalance in deep learning, and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling (#REFR).", "given_citation_intent": "background", "given_keywords": ["original convex program", "dropped constraint \u03c8"]}]}
{"citing_id": "2303.04341v1", "cited_id": "1911.11227", "section_title": "Experimental Protocol", "citation": "We first demonstrate the ability of our NVF to reconstruct non-watertight meshes by category-specific reconstruction in Sec. #REFR Table 1 . Quantitative evaluation on ShapeNet Cars.", "text_before_citation": ["Tasks.", "We evaluate the effectiveness of our framework on four tasks: 1) category-specific, 2) category-agnostic, 3) category-unseen and 4) cross-domain reconstruction."], "text_after_citation": ["We train and evaluate our method on the raw data of the ShapeNet \"Car\" category.", "Our method achieves better performance than the state-ofthe-art UDF-based methods.", "we compare our NVF with existing methods on categoryagnostic and category-unseen reconstruction in Sec. 4.3.", "We also test cross-domain reconstruction by reconstructing real scanned data without training or fine-tuning in Sec. 4.4. Implementations.", "We employ PointTransformer #OTHEREFR as our feature encoder and set k = 16 for the nearest points."], "citing_paper_content": {"title": "Neural Vector Fields: Implicit Representation By Explicit Learning", "abstract": "Deep neural networks (DNNs) are widely applied for nowadays 3D surface reconstruction tasks and such methods can be further divided into two categories, which respectively warp templates explicitly by moving vertices or represent 3D surfaces implicitly as signed or unsigned distance functions. Taking advantage of both advanced explicit learning process and powerful representation ability of implicit functions, we propose a novel 3D representation method, Neural Vector Fields (NVF). It not only adopts the explicit learning process to manipulate meshes directly, but also leverages the implicit representation of unsigned distance functions (UDFs) to break the barriers in resolution and topology. Specifically, our method first predicts the displacements from queries towards the surface and models the shapes as Vector Fields. Rather than relying on network differentiation to obtain direction fields as most existing UDF-based methods, the produced vector fields encode the distance and direction fields both and mitigate the ambiguity at \"ridge\" points, such that the calculation of direction fields is straightforward and differentiation-free. The differentiation-free characteristic enables us to further learn a shape codebook via Vector Quantization, which encodes the cross-object priors, accelerates the training procedure, and boosts model generalization on cross-category reconstruction. The extensive experiments on surface reconstruction benchmarks indicate that our method outperforms those state-of-the-art methods in different evaluation scenarios including watertight vs nonwatertight shapes, category-specific vs category-agnostic reconstruction, category-unseen reconstruction, and crossdomain reconstruction. Our code will be publicly released."}, "cited_paper_content": {"title": "Shape Reconstruction By Learning Differentiable Surface Representations", "abstract": "Generative models that produce point clouds have emerged as a powerful tool to represent 3D surfaces, and the best current ones rely on learning an ensemble of parametric representations. Unfortunately, they offer no control over the deformations of the surface patches that form the ensemble and thus fail to prevent them from either overlapping or collapsing into single points or lines. As a consequence, computing shape properties such as surface normals and curvatures becomes difficult and unreliable. ::: In this paper, we show that we can exploit the inherent differentiability of deep networks to leverage differential surface properties during training so as to prevent patch collapse and strongly reduce patch overlap. Furthermore, this lets us reliably compute quantities such as surface normals and curvatures. We will demonstrate on several tasks that this yields more accurate surface reconstructions than the state-of-the-art methods in terms of normals estimation and amount of collapsed and overlapped patches."}, "keywords": ["ShapeNet Cars"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve accurate surface reconstructions, recent works have explored differentiable surface representations. One such approach is to leverage differential surface properties during training to prevent patch collapse and reduce patch overlap #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prevent patch collapse and reduce patch overlap, the authors propose a method that leverages differential surface properties during training, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To prevent patch collapse and reduce patch overlap in 3D surface reconstruction, the authors propose a method that leverages differential surface properties during training, as demonstrated on several tasks including category-specific and category-agnostic reconstruction, and outperforms state-of-the-art methods in terms of normals estimation and amount of collapsed and overlapped patches (#REFR).", "given_citation_intent": "method", "given_keywords": ["ShapeNet Cars"]}]}
{"citing_id": "2304.12329v1", "cited_id": "1607.04606", "section_title": "Static Pre-Trained Models", "citation": "FastText #REFR conceives each word as a group of n-grams instead of a single string. It is trained to vectorize n-grams.", "text_before_citation": ["It employs a local context window, as a continuous bagof-word (order-agnostic) or a continuous skip-gram (order-aware).", "The latter can link words that behave similarly in a sentence, but fails to utilize the statistics of a corpus.", "GloVe #OTHEREFR combines matrix factorization, i.e., the global cooccurrence counts, with a local context window, i.e., word analogy.", "It is trained on large corpora, such as Wikipedia, to provide pre-trained vectors for general use.", "Since it operates on a global dictionary, it identifies words with a specific writing and fails to detect slight modifications."], "text_after_citation": ["It then represents each word as the sum of its underlying n-grams."], "citing_paper_content": {"title": "Pre-Trained Embeddings For Entity Resolution: An Experimental Analysis [Experiment, Analysis & Benchmark", "abstract": "Many recent works on Entity Resolution (ER) leverage Deep Learning techniques involving language models to improve effectiveness. This is applied to both main steps of ER, i.e., blocking and matching. Several pre-trained embeddings have been tested, with the most popular ones being fastText and variants of the BERT model. However, there is no detailed analysis of their pros and cons. To cover this gap, we perform a thorough experimental analysis of 12 popular language models over 17 established benchmark datasets. First, we assess their vectorization overhead for converting all input entities into dense embeddings vectors. Second, we investigate their blocking performance, performing a detailed scalability analysis, and comparing them with the state-of-the-art deep learning-based blocking method. Third, we conclude with their relative performance for both supervised and unsupervised matching. Our experimental results provide novel insights into the strengths and weaknesses of the main language models, facilitating researchers and practitioners to select the most suitable ones in practice."}, "cited_paper_content": {"title": "Enriching Word Vectors With Subword Information", "abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram, words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks."}, "keywords": ["FastText", "word"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To provide pre-trained vectors for general use, GloVe combines matrix factorization with a local context window, and is trained on large corpora such as Wikipedia #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitation of assigning a distinct vector to each word, recent studies have proposed new approaches to enrich word vectors with subword information, such as representing each word as a bag of character n-grams and associating a vector representation to each character n-gram, as proposed in the reference paper Enriching Word Vectors With Subword Information (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "FastText is one of the most popular pre-trained embeddings used in natural language processing tasks, but it ignores the morphology of words, which limits its effectiveness for languages with large vocabularies and many rare words. To address this limitation, the authors of the manuscript propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams, and achieve state-of-the-art performance on word similarity and analogy tasks in nine different languages. #REFR", "given_citation_intent": "background", "given_keywords": ["FastText", "word"]}]}
{"citing_id": "2304.12272v1", "cited_id": "1910.10683", "section_title": "Arxiv:2304.12272V1 [Cs.Cl] 24 Apr 2023", "citation": "Like all models derived from T5 models, #REFR , we pose AMR parsing as a text-totext problem and train models to transfer a text to a linearized AMR graph with the task prefix amr generation.", "text_before_citation": ["2 AMR Parsing with FLAN-T5 Models Flan-T5 models, (Chung et al., 2022) , are obtained by instruction fine-tuning T5-LM adapted models, , on a collection of 1.8K instruction annotated tasks.", "1 They are prefix language models 2 and achieve strong few-shot performance even compared to much larger models, such as PaLM 62B."], "text_after_citation": ["FLAN-T5 model size variants, all of which use 32,128 unique vocabulary, is shown in Table 1 ."], "citing_paper_content": {"title": "Amr Parsing With Instruction Fine-Tuned Pre-Trained Language Models", "abstract": "Instruction fine-tuned language models on a collection of instruction annotated datasets (FLAN) have shown highly effective to improve model performance and generalization to unseen tasks. However, a majority of standard parsing tasks including abstract meaning representation (AMR), universal dependency (UD), semantic role labeling (SRL) has been excluded from the FLAN collections for both model training and evaluations. In this paper, we take one of such instruction fine-tuned pre-trained language models, i.e. FLAN-T5, and fine-tune them for AMR parsing. Our extensive experiments on various AMR parsing tasks including AMR2.0, AMR3.0 and BioAMR indicate that FLAN-T5 fine-tuned models out-perform previous state-of-the-art models across all tasks. In addition, full finetuning followed by the parameter efficient finetuning, LoRA, further improves the model performances, setting new state-of-the-arts in Smatch on AMR2.0 (86.4), AMR3.0 (84.9) and BioAMR (82.3)."}, "cited_paper_content": {"title": "Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."}, "keywords": ["train models", "text-totext problem"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Flan-T5 models, which are obtained by instruction fine-tuning T5-LM adapted models on a collection of instruction annotated tasks, achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. The authors of this paper have explored the landscape of transfer learning techniques for natural language processing by introducing a unified framework that converts every language problem into a text-to-text format, and achieved state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To explore the landscape of transfer learning techniques for NLP, the authors of this manuscript cite the unified text-to-text transformer framework proposed in \"Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer\" (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To fine-tune FLAN-T5 models for AMR parsing, Chung et al. (#REFR) instruction fine-tuned T5-LM adapted models on a collection of instruction annotated tasks, which are prefix language models that achieve strong few-shot performance even compared to much larger models.", "given_citation_intent": "method", "given_keywords": ["train models", "text-totext problem"]}]}
{"citing_id": "2304.13539v1", "cited_id": "1706.03762", "section_title": "1) Multi-Linear Attention:", "citation": "For the machine translation task, the baseline is a Transformer-big model on WMT 2014 English-German dataset #REFR . This dataset has around 4.5 million sentence pairs. The results are summarized in Table IX .", "text_before_citation": ["For comparison, each of the attention layers was replaced with Multi-linear attention in the Encoder of the Transformer. The results are summarized in Table VIII .", "Notice that tensorized transformers with Multi-linear attention achieve better performance with fewer parameters than the vanilla Transformer.", "2) Tensorized Embedding Layers: The proposed TTembedding layer was tested on two language modeling tasks (PTB and WikiText-103) and a machine translation task (WMT 2014 English-German).", "As shown in Table VII , Transformer-XL+TT stands for the transformers with TTembedding layers.", "Compared to the Transformer with Multilinear attention, Transformer-XL+TT can not achieve that high compression ratio."], "text_after_citation": ["Notice that the embedding layers can be compressed significantly at the cost of a small drop in the BLEU scores."], "citing_paper_content": {"title": "Tensor Decomposition For Model Reduction In Neural Networks: A Review", "abstract": "Modern neural networks have revolutionized the fields of computer vision (CV) and Natural Language Processing (NLP). They are widely used for solving complex CV tasks and NLP tasks such as image classification, image generation, and machine translation. Most state-of-the-art neural networks are over-parameterized and require a high computational cost. One straightforward solution is to replace the layers of the networks with their low-rank tensor approximations using different tensor decomposition methods. This paper reviews six tensor decomposition methods and illustrates their ability to compress model parameters of convolutional neural networks (CNNs), recurrent neural networks (RNNs) and Transformers. The accuracy of some compressed models can be higher than the original versions. Evaluations indicate that tensor decompositions can achieve significant reductions in model size, run-time and energy consumption, and are well suited for implementing neural networks on edge devices."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["Transformer-big model", "machine translation task"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For comparison, each of the attention layers was replaced with Multi-linear attention in the Encoder of the Transformer. The results are summarized in Table VIII. Notice that tensorized transformers with Multi-linear attention achieve better performance with fewer parameters than the vanilla Transformer. 2) Tensorized Embedding Layers: The proposed TTembedding layer was tested on two language modeling tasks (PTB and WikiText-103) and a machine translation task (WMT 2014 English-German). As shown in Table VII, Transformer-XL+TT stands for the transformers with TTembedding layers. Compared to the Transformer with Multilinear attention, Transformer-XL+TT can not achieve that high compression ratio. This is consistent with the findings in the reference paper #REFR, where the authors proposed a new simple network architecture, the Transformer, based solely on attention mechanisms, which achieved superior performance in machine translation tasks while being more parallelizable and requiring significantly less time to train.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Multi-linear attention in the Encoder of the Transformer, as described in the reference paper #REFR, achieved better performance with fewer parameters than the vanilla Transformer, as shown in Table VIII.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript implemented tensor decomposition methods to compress the Transformer-big model and achieved better performance with fewer parameters on the machine translation task, as demonstrated in Table VIII and Table VII. #REFR was used as the basis for the tensorized transformers with Multi-linear attention and TTembedding layers.", "given_citation_intent": "method", "given_keywords": ["Transformer-big model", "machine translation task"]}]}
{"citing_id": "2304.10707v1", "cited_id": "1903.12370", "section_title": "Proposed Method", "citation": "These results not only confirm the previous findings about longrun stability for persistent training #REFR , but also reveal a new phenomenon that the learned energies from persistent training appear to be locally meaningful, but globally misaligned.", "text_before_citation": ["(ii) The long-run sample (MALA starting from real data) also resemble the training data.", "(iii) The post-training sample (MALA starting from standard Gaussian noises) shows two local modes at \u22122 and 2, but with proportions different from 3:1 as in the training data.", "(iv) The learned energy function exhibits two local modes about \u22122 and 2, but is globally misaligned.", "The estimated energy at the local mode \u22122 is substantially higher than at the other mode 2, and hence is also higher than (for example) at x = 1, an OOD point near no real data.", "More troubling is that the estimated energies near \u22122 and 2 may reverse the direction of relative magnitudes from different training runs, as shown in Appendix E.1."], "text_after_citation": ["Hence application of such learned energies to OOD detection is problematic.", "Motivated by the 1D example, we provide some new theoretical understanding of persistent training of EBMs with MCMC sampling which enables only local mixing for mul-timodal distributions with modes separated by low-density (i.e., high-energy) barriers.", "Our discussion is heuristic, but highlights the main ideas which can be exploited to develop formal analysis.", "In the limit of the persistent training process (assumed to exist), let\u03b8 be a limit value of the network parameter \u03b8, andq be a limit distribution for the synthetic data (which can be represented by the empirical distribution of the replay buffer).", "Then we expect that (\u03b8,q) satisfy the following stationarity conditions:"], "citing_paper_content": {"title": "Persistently Trained, Diffusion-Assisted Energy-Based Models", "abstract": "Maximum likelihood (ML) learning for energybased models (EBMs) is challenging, partly due to non-convergence of Markov chain Monte Carlo. Several variations of ML learning have been proposed, but existing methods all fail to achieve both post-training image generation and proper density estimation. We propose to introduce diffusion data and learn a joint EBM, called diffusion assisted-EBMs, through persistent training (i.e., using persistent contrastive divergence) with an enhanced sampling algorithm to properly sample from complex, multimodal distributions. We present results from a 2D illustrative experiment and image experiments and demonstrate that, for the first time for image data, persistently trained EBMs can simultaneously achieve long-run stability, post-training image generation, and superior out-of-distribution detection."}, "cited_paper_content": {"title": "On The Anatomy Of Mcmc-Based Maximum Likelihood Learning Of Energy-Based Models", "abstract": "This study investigates the effects of Markov Chain Monte Carlo (MCMC) sampling in unsupervised Maximum Likelihood (ML) learning. Our attention is restricted to the family of unnormalized probability densities for which the negative log density (or energy function) is a ConvNet. In general, we find that many of the techniques used to stabilize training in previous studies can have the opposite effect. Stable ML learning with a ConvNet potential can be achieved with only a few hyper-parameters and no regularization. Using this minimal framework, we identify a variety of ML learning outcomes that depend on the implementation of MCMC sampling. ::: On one hand, we show that it is easy to train an energy-based model which can sample realistic images with short-run Langevin. ML can be effective and stable even when MCMC samples have much higher energy than true steady-state samples throughout training. Based on this insight, we introduce an ML method with purely noise-initialized MCMC, high-quality short-run synthesis, and the same budget as ML with informative MCMC initialization such as CD or PCD. Unlike previous models, our model can obtain realistic high-diversity samples from a noise signal after training with no auxiliary networks. ::: On the other hand, ConvNet potentials learned with highly non-convergent MCMC do not have a valid steady-state and cannot be considered approximate unnormalized densities of the training data because long-run MCMC samples differ greatly from observed images. We show that it is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples. With correct tuning of Langevin noise, we train the first ConvNet potentials for which long-run and steady-state MCMC samples are realistic images."}, "keywords": ["learned energies"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors' findings suggest that \"it is much harder to train a ConvNet potential to learn a steady-state over realistic images\" and that \"long-run MCMC samples of all previous models lose the realism of short-run samples\" (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript compare their results to those of previous models and show that their persistently trained EBMs can achieve long-run stability, post-training image generation, and superior out-of-distribution detection, which was not possible in previous methods. #REFR was cited as a reference for the challenges of non-convergence of Markov chain Monte Carlo in maximum likelihood learning for energy-based models.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The estimated #REFR learned energies near -2 and 2 may reverse the direction of relative magnitudes from different training runs, as shown in Appendix E.1, which is consistent with the findings in the reference paper on the effects of Markov Chain Monte Carlo sampling in unsupervised Maximum Likelihood learning.", "given_citation_intent": "result", "given_keywords": ["learned energies"]}]}
{"citing_id": "2303.01234v1", "cited_id": "1910.01108", "section_title": "Victim Models", "citation": "The BERT classifier for AG's News is structured by the Distil-RoBERTa-base #REFR connected with two fully connected layers, and it is trained for 10 epochs with a learning rate of 0.0001.", "text_before_citation": ["We apply our attack algorithm to two popular and well-performed types of victim models. The details of the models can be found below.", "BERT-based Classifiers To do convincing experiments, we choose three well-performed and popular BERT-based models, which we call BERT-C models, pre-trained by Huggingface 3 .", "Due to the different sizes of the datasets, the structures of BERT-based classifiers are adjusted accordingly."], "text_after_citation": ["For the Emotion dataset, its BERT-C adopts another version of BERT, Distil-BERT-base-uncased #OTHEREFR , and the training hyper-parameters remain the same as BERT-C for AG's News.", "Since the SST2 dataset is relatively small compared with the other two models, the corresponding BERT classifier utilizes a small-size version of BERT, BERT-base-uncased #OTHEREFR .", "The test accuracy of these BERT-based classifiers before they are under attack are listed in Table 1 which are publicly accessible 4 5 6 .", "TextCNN-based models The other type of victim model is TextCNN #OTHEREFR , structured with a 100-dimension embedding layer followed by a 128-unit long short-term memory layer.", "This classifier is trained 10 epochs by ADAM optimizer with parameters: learning rate lr = 0.005, the two coefficients used for computing running averages of gradient and its square are set to be 0.9 and 0.999 (\u03b2 1 = 0.9, \u03b2 2 = 0.999), the denominator to improve numerical stability \u03c3 = 10 \u22125 ."], "citing_paper_content": {"title": "Fraud'S Bargain Attack: Generating Adversarial Text Samples Via Word Manipulation Process", "abstract": "Recent studies on adversarial examples expose vulnerabilities of natural language processing (NLP) models. Existing techniques for generating adversarial examples are typically driven by deterministic heuristic rules that are agnostic to the optimal adversarial examples, a strategy that often results in attack failures. To this end, this research proposes Fraud's Bargain Attack (FBA) which utilizes a novel randomization mechanism to enlarge the search space and enables high-quality adversarial examples to be generated with high probabilities. FBA applies the Metropolis-Hasting sampler, a member of Markov Chain Monte Carlo samplers, to enhance the selection of adversarial examples from all candidates proposed by a customized stochastic process that we call Word Manipulation Process (WMP). WMP perturbs one word at a time via insertion, removal or substitution in a contextual-aware manner. Extensive experiments demonstrate that FBA outperforms the state-of-the-art methods in terms of both attack success rate and imperceptibility."}, "cited_paper_content": {"title": "Distilbert, A Distilled Version Of Bert: Smaller, Faster, Cheaper And Lighter", "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."}, "keywords": ["BERT classifier"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To adjust the structures of BERT-based classifiers, the authors pre-trained a smaller general-purpose language representation model called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and reduced the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pre-train a smaller general-purpose language representation model, the authors of this study propose a method called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and can reduce the size of a BERT model by 40% while retaining 97% of its language understanding capabilities and being 60% faster. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To do convincing experiments, we choose three well-performed and popular #REFRBERT classifier models, pre-trained by Huggingface, as they are widely used in natural language processing.", "given_citation_intent": "method", "given_keywords": ["BERT classifier"]}]}
{"citing_id": "2303.07002v1", "cited_id": "1206.0834", "section_title": "Implementation", "citation": "Also the columns of the bifiltration (with s fixed) have been studied earlier in #REFR .", "text_before_citation": ["By excision and the fact that all pairs are \"good pairs\" #OTHEREFR , we have isomorphisms #OTHEREFR", "H n (L s , L s \\ B o r (q)) \u223c = H n (L s \u2229 B r (q), L s \u2229 \u2202B r (q)) \u223c =Hn(Ls \u2229 B r (q)/(L s \u2229 \u2202B r (q)))", "withH denoting reduced homology.", "These isomorphisms imply that the relative localized persistence module corresponds point-wise to the space considered in Figure 2 (right) .", "Moreover, the excision isomorphism commutes with the inclusion maps L s \u2286 L s for s \u2264 s which implies that the rows of the localized relative persistence module (with fixed r) are isomorphic to the module (H n (L s \u2229 B r (q), L s \u2229 \u2202B r (q))) s\u22650 which was studied in #OTHEREFR , #OTHEREFR , #OTHEREFR ."], "text_after_citation": ["Hence, the localized relative bifiltration encodes both types of modules of local persistent homology studied in previous work (see Appendix D for a summary of basic notions).", "We also remark that the computation of the relative localized persistence module can easily be reduced to the case of absolute homology via the well-known coning construction [30, p.125] , yielding isomorphisms H n (X, A) \u223c =Hn(X \u222a \u03c9 * A) for pairs of topological spaces (X, A) with \u03c9 denoting a new vertex.", "These isomorphisms are functorial, yielding an isomorphism between the relative persistence module of a pair of bifiltrations and the absolute persistence module of a bifiltration (using reduced homology).", "Moreover, if the pair (X, A) is finite simplicial, so is X \u222a \u03c9 * A.", "Nerves of pairs."], "citing_paper_content": {"title": "The Localized Union-Of-Balls Bifiltration", "abstract": "We propose an extension of the classical union-of-balls filtration of persistent homology: fixing a point q, we focus our attention to a ball centered at q whose radius is controlled by a second scale parameter. We discuss an absolute variant, where the union is just restricted to the q-ball, and a relative variant where the homology of the q-ball relative to its boundary is considered. Interestingly, these natural constructions lead to bifiltered simplicial complexes which are not k-critical for any finite k. Nevertheless, we demonstrate that these bifiltrations can be computed exactly and efficiently, and we provide a prototypical implementation using the CGAL library. We also argue that some of the recent algorithmic advances for 2-parameter persistence (which usually assume k-criticality for some finite k) carry over to the \u221e-critical case."}, "cited_paper_content": {"title": "Approximating Local Homology From Samples", "abstract": "Recently, multi-scale notions of local homology (a variant of persistent homology) have been used to study the local structure of spaces around a given point from a point cloud sample. Current reconstruction guarantees rely on constructing embedded complexes which become difficult in high dimensions. We show that the persistence diagrams used for estimating local homology, can be approximated using families of Vietoris-Rips complexes, whose simple constructions are robust in any dimension. To the best of our knowledge, our results, for the first time, make applications based on local homology, such as stratification learning, feasible in high dimensions."}, "keywords": ["bifiltration", "columns"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Moreover, the excision isomorphism commutes with the inclusion maps L s \u2286 L s for s \u2264 s which implies that the rows of the localized relative persistence module (with fixed r) are isomorphic to the module (H n (L s \u2229 B r (q), L s \u2229 \u2202B r (q))) s\u22650 which was studied in the paper on approximating local homology from samples by Zomorodian and Carlsson. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' approach of using families of Vietoris-Rips complexes to approximate persistence diagrams for estimating local homology is a significant advancement, as it makes applications based on local homology feasible in high dimensions #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' proposed extension of the classical union-of-balls filtration to a bifiltration, as well as their efficient computation method, is a significant contribution to the field of persistent homology. This approach could potentially be applied to other bifiltrations with non-k-critical columns. #REFR", "given_citation_intent": "background", "given_keywords": ["bifiltration", "columns"]}]}
{"citing_id": "2303.11879v1", "cited_id": "2001.04830", "section_title": "I. Introduction", "citation": "S EQUENTIAL recommendation systems are designed to capture the dynamic preferences of users based on their historical behaviors, with the goal of predicting the next item that they will be interested in #REFR .", "text_before_citation": [], "text_after_citation": ["The primary supervision signal utilized for learning the parameters of these models is typically derived from the sequential interactions of users with items.", "However, given the sparsity of user behavior data, sequential recommendation methods that rely solely on such data are susceptible to the problem of data sparsity, resulting in suboptimal performance.", "In practice, there exists a significant amount of multimodal information associated with items (e.g., images and text descriptions), which has been employed to alleviate the data sparsity problem in building conventional recommendation systems #OTHEREFR - #OTHEREFR .", "For example, #OTHEREFR , #OTHEREFR leverage item multimodal content as a regularization factor and integrate it with collaborative filtering frameworks.", "Recent studies #OTHEREFR - #OTHEREFR utilize graph neural networks to uncover the hidden links between different modalities and establish an in-depth understanding of users' preferences."], "citing_paper_content": {"title": "Multimodal Pre-Training Framework For Sequential Recommendation Via Contrastive Learning", "abstract": "Sequential recommendation systems utilize the sequential interactions of users with items as their main supervision signals in learning users' preferences. However, existing methods usually generate unsatisfactory results due to the sparsity of user behavior data. To address this issue, we propose a novel pre-training framework, named Multimodal Sequence Mixup for Sequential Recommendation (MSM4SR), which leverages both users' sequential behaviors and items' multimodal content (i.e., text and images) for effectively recommendation. Specifically, MSM4SR tokenizes each item image into multiple textual keywords and uses the pre-trained BERT model to obtain initial textual and visual features of items, for eliminating the discrepancy between the text and image modalities. A novel backbone network, i.e., Multimodal Mixup Sequence Encoder (M 2 SE), is proposed to bridge the gap between the item multimodal content and the user behavior, using a complementary sequence mixup strategy. In addition, two contrastive learning tasks are developed to assist M 2 SE in learning generalized multimodal representations of the user behavior sequence. Extensive experiments on real-world datasets demonstrate that MSM4SR outperforms state-of-the-art recommendation methods. Moreover, we further verify the effectiveness of MSM4SR on other challenging tasks including cold-start and cross-domain recommendation."}, "cited_paper_content": {"title": "Sequential Recommender Systems: Challenges, Progress And Prospects", "abstract": "The emerging topic of sequential recommender systems has attracted increasing attention in recent years.Different from the conventional recommender systems including collaborative filtering and content-based filtering, SRSs try to understand and model the sequential user behaviors, the interactions between users and items, and the evolution of users preferences and item popularity over time. SRSs involve the above aspects for more precise characterization of user contexts, intent and goals, and item consumption trend, leading to more accurate, customized and dynamic recommendations.In this paper, we provide a systematic review on SRSs.We first present the characteristics of SRSs, and then summarize and categorize the key challenges in this research area, followed by the corresponding research progress consisting of the most recent and representative developments on this topic.Finally, we discuss the important research directions in this vibrant area."}, "keywords": ["EQUENTIAL recommendation systems"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of sparsity in user behavior data, the authors propose a novel pre-training framework that leverages both users' sequential behaviors and items' multimodal content for effective recommendation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sequential recommendation systems have attracted increasing attention in recent years due to their ability to utilize the sequential interactions of users with items as their main supervision signals in learning users' preferences, and the emerging topic of sequential recommender systems has been systematically reviewed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sequential recommendation systems have attracted increasing attention in recent years due to their ability to understand and model the sequential user behaviors and interactions between users and items, leading to more accurate and dynamic recommendations (#REFR).", "given_citation_intent": "background", "given_keywords": ["EQUENTIAL recommendation systems"]}]}
{"citing_id": "2303.13794v1", "cited_id": "1712.07629", "section_title": "Matching Key-Points Crop (Mkpc)", "citation": "Before applying MKPC, we already have two images I 1 and I 2 , and matched key-points on both images with any model (or any combinations of models), which is defined as X #REFR 1 and X 1 2 , respectively.", "text_before_citation": ["The MKPC algorithm crops critical regions by clustering the matching key-points between two images outputted by arbitrary image matching models. The workflow is shown is Figure 2 ."], "text_after_citation": ["X 1 i denotes the i th image in the first stage of the two-stage pipeline which will be proposed in the next subsection.", "It consists of the following three steps: (i) Clustering the matching key-points(X 1 1 and X #OTHEREFR 2 ) of two images(I 1 and I 2 ) with DBSCAN.", "(ii) Generate a bounding box by filtering and gathering those clusters (iii) Crop the area covered by the Bounding box.", "The Algorithm 1 describes the MKPC algorithm flow in detail.", "With the input of two images (I 1 , I 2 ) and corresponding matched key-points in stage-one (X 1 1 and X 1 2 ), the MKPC outputs the respective cropped critical regions."], "citing_paper_content": {"title": "Efficient And Accurate Co-Visible Region Localization With Matching Key-Points Crop (Mkpc): A Two-Stage Pipeline For Enhancing Image Matching Performance", "abstract": "Image matching is a classic and fundamental task in computer vision. In this paper, under the hypothesis that the areas outside the co-visible regions carry little information, we propose a matching key-points crop (MKPC) algorithm. The MKPC locates, proposes and crops the critical regions, which are the co-visible areas with great efficiency and accuracy. Furthermore, building upon MKPC, we propose a general two-stage pipeline for image matching, which is compatible to any image matching models or combinations. We experimented with plugging SuperPoint + SuperGlue into the two-stage pipeline, whose results show that our method enhances the performance for outdoor pose estimations. What's more, in a fair comparative condition, our method outperforms the SOTA on Image Matching Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of image matching currently. * denotes contributing equally to this work. Preprint. Under review."}, "cited_paper_content": {"title": "Superpoint: Self-Supervised Interest Point Detection And Description", "abstract": "This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB."}, "keywords": ["two images", "key-points"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The MKPC algorithm crops critical regions by clustering the matching key-points between two images outputted by arbitrary image matching models, and has been shown to enhance the performance for outdoor pose estimations and outperform the SOTA on Image Matching Challenge 2022 Benchmark #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To locate and crop critical regions, the MKPC algorithm clusters matching key-points between two images, and when combined with SuperPoint + SuperGlue, it outperforms the SOTA on Image Matching Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of image matching currently #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To locate and cluster key-points between two images, the authors propose the MKPC algorithm, which efficiently crops critical regions and enhances the performance of image matching methods such as SuperPoint + SuperGlue, as demonstrated by their results on the Image Matching Challenge 2022 Benchmark #REFR.", "given_citation_intent": "method", "given_keywords": ["two images", "key-points"]}]}
{"citing_id": "2304.05741v1", "cited_id": "1412.6980", "section_title": "B. Training", "citation": "During the training phase, all our models were optimized with the Adam algorithm #REFR and a learning rate of lr = 0.001, for a maximum of 100 epoch with an early stopping mechanism activated when the validation loss stops improving after a duration of 5 epochs.", "text_before_citation": [], "text_after_citation": ["Additionally, every dropout is performed with r Dropout = 0.5, the batch normalization uses = 0.001 and \u03b3 = 0.99, and we use a batch size of 256 in every module apart from the fixation prediction performed with high-level features.", "\u2022 Fixation Prediction from High-Level Features During training, we estimate the best weight and bias parameters that minimize the loss between the predicted output\u0177 and the ground truth label y, with the cross entropy function computed for every fixation time step t for every sequence s of each mini-batch b:", "EQUATION", "where S corresponds to the batch size, T corresponds to the sequence length which is set to 6 (in addition to the initial fixation point at t = 0) and H \u00d7 W is the output size which is set to 160.", "We set with F = 5 filters, a kernel size of K = 4 and a stride of S = 2, and, during training, we varied the batch size between 32, 64, 128 and 256 and we conducted an ablation study over theses additional hyper-parameters and settings: -Fovea size: In this work we utilized the same real-time foveation system as in #OTHEREFR , and assessed the model performance when varying the fovea size, which defines the radius of the region with highest visual acuity with the values of 50, 75 and 100 pixels."], "citing_paper_content": {"title": "Learning To Search For And Detect Objects In Foveal Images Using Deep Learning", "abstract": "The human visual system processes images with varied degrees of resolution, with the fovea, a small portion of the retina, capturing the highest acuity region, which gradually declines toward the field of view's periphery. However, the majority of existing object localization methods rely on images acquired by image sensors with space-invariant resolution, ignoring biological attention mechanisms. As a region of interest pooling, this study employs a fixation prediction model that emulates human objective-guided attention of searching for a given class in an image. The foveated pictures at each fixation point are then classified to determine whether the target is present or absent in the scene. Throughout this twostage pipeline method, we investigate the varying results obtained by utilizing high-level or panoptic features and provide a groundtruth label function for fixation sequences that is smoother, considering in a better way the spatial structure of the problem. Finally, we present a novel dual task model capable of performing fixation prediction and detection simultaneously, allowing knowledge transfer between the two tasks. We conclude that, due to the complementary nature of both tasks, the training process benefited from the sharing of knowledge, resulting in an improvement in performance when compared to the previous approach's baseline scores."}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["learning rate"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To implement the proposed fixation prediction model, the authors utilized a two-stage pipeline method that involves classifying foveated pictures at each fixation point to determine the presence or absence of the target in the scene, and the results demonstrate its effectiveness in object detection. The method is based on adaptive estimates of lower-order moments and is computationally efficient, making it well-suited for problems that are large in terms of data and/or parameters (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the stochastic objective functions, the authors utilized the Adam algorithm, which is computationally efficient and well-suited for large-scale problems with noisy and/or sparse gradients. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the performance of the proposed dual task model, the authors implemented the Adam algorithm with carefully tuned #REFR learning rate hyperparameters.", "given_citation_intent": "method", "given_keywords": ["learning rate"]}]}
{"citing_id": "2303.14218v1", "cited_id": "1911.07559", "section_title": "Related Work", "citation": "FFANet #REFR introduces feature attention (FA) blocks that leverage both channel and pixel attention to improve haze removal.", "text_before_citation": ["They focus on designing hand-crafted priors such as the dark channel prior #OTHEREFR and color attenuation prior #OTHEREFR .", "However, these priors may not be powerful enough to characterize complex scenes in practice.", "Early learning-based methods #OTHEREFR use deep neural networks to predict the transmission map and atmospheric light in the physics model to obtain a latent clear image.", "However, inaccuracies in the estimations may accumulate, hindering the reliable inference of the haze-free image.", "With the advent of large haze datasets #OTHEREFR , data-driven methods #OTHEREFR have been developed rapidly."], "text_after_citation": ["DeHamer #OTHEREFR combines CNN and Transformer for image dehazing, which can aggregate long-term attention in Transformer and local attention in CNN features.", "Note that these methods do not consider the physics of the hazing process. Further, Dong et al.", "propose a feature dehazing unit (FDU) #OTHEREFR derived based on the physics model.", "To the best of our knowledge, this work is the only one that considers the physics model in the feature space, avoiding the cumulative errors that occur in the raw space.", "However, FDU uses a shared structure to predict those unknown factors without considering their different physical characteristics."], "citing_paper_content": {"title": "Curricular Contrastive Regularization For Physics-Aware Single Image Dehazing", "abstract": "Considering the ill-posed nature, contrastive regularization has been developed for single image dehazing, introducing the information from negative images as a lower bound. However, the contrastive samples are nonconsensual, as the negatives are usually represented distantly from the clear (i.e., positive) image, leaving the solution space still under-constricted. Moreover, the interpretability of deep dehazing models is underexplored towards the physics of the hazing process. In this paper, we propose a novel curricular contrastive regularization targeted at a consensual contrastive space as opposed to a non-consensual one. Our negatives, which provide better lower-bound constraints, can be assembled from 1) the hazy image, and 2) corresponding restorations by other existing methods. Further, due to the different similarities between the embeddings of the clear image and negatives, the learning difficulty of the multiple components is intrinsically imbalanced. To tackle this issue, we customize a curriculum learning strategy to reweight the importance of different negatives. In addition, to improve the interpretability in the feature space, we build a physics-aware dual-branch unit according to the atmospheric scattering model. With the unit, as well as curricular contrastive regularization, we establish our dehazing network, named C 2 PNet. Extensive experiments demonstrate that our C 2 PNet significantly outperforms state-of-the-art methods, with extreme PSNR boosts of 3.94dB and 1.50dB, respectively, on SOTSindoor and SOTS-outdoor datasets. Code is available at https://github.com/YuZheng9/C2PNet."}, "cited_paper_content": {"title": "Ffa-Net: Feature Fusion Attention Network For Single Image Dehazing", "abstract": "In this paper, we propose an end-to-end feature fusion at-tention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components: ::: 1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attention-based different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers. ::: The experimental results demonstrate that our proposed FFA-Net surpasses previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23db to 36.39db on the SOTS indoor test dataset. ::: Code has been made available at GitHub."}, "keywords": ["haze removal"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Early learning-based methods use deep neural networks to predict the transmission map and atmospheric light in the physics model to obtain a latent clear image. However, inaccuracies in the estimations may accumulate, hindering the reliable inference of the haze-free image. With the advent of large haze datasets, data-driven methods have been developed rapidly. To this end, FFA-Net proposes an end-to-end feature fusion attention network to directly restore the haze-free image, surpassing previous state-of-the-art single image dehazing methods by a very large margin both quantitatively and qualitatively #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "With the advent of large haze datasets, data-driven methods have been developed rapidly, including the end-to-end feature fusion attention network proposed in #REFR for single image dehazing.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recently, data-driven methods have been rapidly developed for haze removal #REFR, surpassing the performance of traditional hand-crafted priors and early learning-based methods, which may not be powerful enough to characterize complex scenes in practice and hinder the reliable inference of the haze-free image due to the accumulation of inaccuracies in the estimations.", "given_citation_intent": "background", "given_keywords": ["haze removal"]}]}
{"citing_id": "2303.01465v1", "cited_id": "1704.04861", "section_title": "Mobilenet V1", "citation": "We have opted MobileNet #REFR as a feature extractor to develop an FPAD model for devices with limited computational resources.", "text_before_citation": [], "text_after_citation": ["It is advantageous over state-ofthe-art CNN architectures as it utilizes depth-wise separable convolution instead of standard convolution.", "A standard convolution operation takes input as X channels of size D x \u00d7 D x and produces D y \u00d7 D y \u00d7 Y feature maps by applying D k \u00d7 D k \u00d7 Y filters where the spatial height and width of the squared input image are denoted with D x .", "X denotes the number of input channels while D y is the spatial height and width.", "The number of output feature maps is denoted with Y .", "Equation 1describes the calculation of the output feature map for standard convolution operation #OTHEREFR having stride one with padding."], "citing_paper_content": {"title": "Mosfpad: An End-To-End Ensemble Of Mobilenet And Support Vector Classifier For Fingerprint Presentation Attack Detection", "abstract": "Automatic fingerprint recognition systems are the most extensively used systems for person authentication although they are vulnerable to Presentation attacks. Artificial artifacts created with the help of various materials are used to deceive these systems causing a threat to the security of fingerprint-based applications. This paper proposes a novel end-to-end model to detect fingerprint Presentation attacks. The proposed model incorporates MobileNet as a feature extractor and a Support Vector Classifier as a classifier to detect presentation attacks in cross-material and cross-sensor paradigms. The feature extractor's parameters are learned with the loss generated by the support vector classifier. The proposed model eliminates the need for intermediary data preparation procedures, unlike other static hybrid architectures. The performance of the proposed model has been validated on benchmark"}, "cited_paper_content": {"title": "Mobilenets: Efficient Convolutional Neural Networks For Mobile Vision Applications", "abstract": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."}, "keywords": ["feature extractor", "MobileNet"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed model incorporates MobileNet as a feature extractor and a Support Vector Classifier as a classifier to detect presentation attacks in cross-material and cross-sensor paradigms (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect fingerprint Presentation attacks, the proposed model incorporates MobileNet as a feature extractor and a Support Vector Classifier as a classifier, and the feature extractor's parameters are learned with the loss generated by the support vector classifier (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed model in this paper incorporates MobileNet as a feature extractor to detect fingerprint Presentation attacks, and the feature extractor's parameters are learned with the loss generated by the support vector classifier, as described in #REFR.", "given_citation_intent": "method", "given_keywords": ["feature extractor", "MobileNet"]}]}
{"citing_id": "2305.01905v1", "cited_id": "1801.07698", "section_title": "Iii. Experiments A. Training Details", "citation": "Following the setup of #REFR , we set the scale s to 64 and the margin m to 0.5 for arcface loss.", "text_before_citation": ["We train networks using the standard data augmentation (i.e., flipping, translation, cropping), and mask augmentation using the tools introduced in the ICCV2021-MFR/Insightface track #OTHEREFR .", "We train the CASIA-Webface and MS1MV3 datasets #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR by employing SGD with a minibatch size of 512.", "Momentum and weight decay are set to 0.9 and 5e-4, respectively.", "We set initial learning rate to 0.2, and employ the polynomial learning rate decay scheduler #OTHEREFR , #OTHEREFR with 2 epochs of warm restart.", "We finish the training at 25 epochs and 34 epochs for MS1MV3 and CASIA-Webface datasets, respectively."], "text_after_citation": [], "citing_paper_content": {"title": "Localization Using Multi-Focal Spatial Attention For Masked Face Recognition", "abstract": "Since the beginning of worldwide COVID-19 pandemic, facial masks have been recommended to limit the spread of the disease. However, these masks hide certain facial attributes. Hence, it has become difficult for existing face recognition systems to perform identity verification on masked faces. In this context, it is necessary to develop masked Face Recognition (MFR) for contactless biometric recognition systems. Thus, in this paper, we propose Complementary Attention Learning and Multi-Focal Spatial Attention that precisely removes masked region by training complementary spatial attention to focus on two distinct regions: masked regions and backgrounds. In our method, standard spatial attention and networks focus on unmasked regions, and extract maskinvariant features while minimizing the loss of the conventional Face Recognition (FR) performance. For conventional FR, we evaluate the performance on the IJB-C, Age-DB, CALFW, and CPLFW datasets. We evaluate the MFR performance on the ICCV2021-MFR/Insightface track, and demonstrate the improved performance on the both MFR and FR datasets. Additionally, we empirically verify that spatial attention of proposed method is more precisely activated in unmasked regions."}, "cited_paper_content": {"title": "Arcface: Additive Angular Margin Loss For Deep Face Recognition", "abstract": "One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available."}, "keywords": ["arcface loss"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We employ the Additive Angular Margin Loss (ArcFace) proposed in #REFR to obtain highly discriminative features for face recognition, achieving state-of-the-art performance on ten face recognition benchmarks, including a new large-scale image database with trillions of pairs and a large-scale video dataset.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain highly discriminative features for face recognition, the proposed Additive Angular Margin Loss (ArcFace) in #REFR incorporates margins in well-established loss functions and consistently outperforms the state of the art on ten face recognition benchmarks, including large-scale image and video datasets.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain highly discriminative features for face recognition, the proposed Complementary Attention Learning and Multi-Focal Spatial Attention method uses the Additive Angular Margin Loss (ArcFace) #REFR, which has been shown to outperform the state of the art in face recognition.", "given_citation_intent": "method", "given_keywords": ["arcface loss"]}]}
{"citing_id": "2304.10891v1", "cited_id": "1903.11027", "section_title": "E. Benchmark Of Transformer Models", "citation": "As shown in TableII, for 3D object detection task using Nuscenes dataset #REFR , both DETR3D and FUTR3D exhibit comparable performance due to their similar structures.", "text_before_citation": ["We benchmark major Transformer-based models on an NVIDIA GPU 3090 considering indicators such as input size, runtime, accuracy, and datasets."], "text_after_citation": ["BEVFormer outperforms DETR3D by generating BEV features and querying 3D objects from these features.", "PETR and CrossDTR transform 2D features into 3D features using CNN networks, accelerating the query process and yielding better performance than DETR3D.", "ResNet101's higher precision compared to ResNet50 can be attributed to its deformable convolution mechanism and increased convolution depth, although at the cost of slower runtime speed #OTHEREFR .", "On the other hand, Transformerbased road element detection research exhibits greater variation, with different models and evaluation criteria for tasks such as 2D lane (TuSimple), 3D lane (OpenLane), and local map (Nuscenes).", "Lane and local map Transformer queries are faster than object detection due to fewer key-point queries and smaller CNN backbones that utilize shallower layer features."], "citing_paper_content": {"title": "Transformer-Based Models And Hardware Acceleration Analysis In Autonomous Driving: A Survey", "abstract": "Transformer architectures have exhibited promising performance in various autonomous driving applications in recent years. On the other hand, its dedicated hardware acceleration on portable computational platforms has become the next critical step for practical deployment in real autonomous vehicles. This survey paper provides a comprehensive overview, benchmark, and analysis of Transformer-based models specifically tailored for autonomous driving tasks such as lane detection, segmentation, tracking, planning, and decision-making. We review different architectures for organizing Transformer inputs and outputs, such as encoder-decoder and encoder-only structures, and explore their respective advantages and disadvantages. Furthermore, we discuss Transformer-related operators and their hardware acceleration schemes in depth, taking into account key factors such as quantization and runtime. We specifically illustrate the operator level comparison between layers from convolutional neural network, Swin-Transformer, and Transformer with 4D encoder. The paper also highlights the challenges, trends, and current insights in Transformerbased models, addressing their hardware deployment and acceleration issues within the context of long-term autonomous driving applications."}, "cited_paper_content": {"title": "Nuscenes: A Multimodal Dataset For Autonomous Driving", "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first published dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online at this http URL."}, "keywords": ["3D object detection"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We benchmark major Transformer-based models on an NVIDIA GPU 3090 considering indicators such as input size, runtime, accuracy, and datasets. The evaluation is conducted on nuScenes, the first published dataset to carry the full autonomous vehicle sensor suite, including 6 cameras, 5 radars, and 1 lidar, all with full 360 degree field of view, and fully annotated with 3D bounding boxes for 23 classes and 8 attributes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compare our results with the state-of-the-art nuScenes dataset, which provides a full autonomous vehicle sensor suite, including lidar, radar, and cameras, and has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We compare the 3D object detection performance of our Transformer-based models with state-of-the-art methods on the nuScenes dataset #REFR.", "given_citation_intent": "result", "given_keywords": ["3D object detection"]}]}
{"citing_id": "2304.11445v1", "cited_id": "1807.09441", "section_title": "Interpretability Analysis", "citation": "Less discrepancy between different distributions can be observed at later stages, the same pattern authors of IBNnet #REFR have observed.", "text_before_citation": ["Let F denote a feature map, and \u00b5, \u03c3 2 denote its mean and variance values, respectively.", "Then symmetric KL divergence between two distributions S and S is computed as", "EQUATION", "We use the average divergence of all channels for each image and report the mean value for the whole set.", "From Figure 2(a) , it can be seen that there is less feature divergence for the modified network, suggesting that learned representations are more generalizable."], "text_after_citation": ["Next, we visualize mean covariance and variance matrices (Figure 2(b) ) and observe that the covariance matrix computed from intermediate outputs of the modified model has more bright spots.", "This might result from suppressing sensitive covariances previously present in the baseline.", "There are also fewer activations in the variance matrix for H&E, which means that the distribution shift is causing less divergence for all channels on average.", "The same pattern can be observed for TRI and PAS variance matrices (Figure 10 in Appendix C), however, for the latter, some of the brighter spots appear darker compared to the baseline.", "t-SNE visualization (Figure 9 in Appendix C) of learned representations from stain normalized and original image versions of the first and last encoder stages show that they are closer when we integrate our method into training scheme."], "citing_paper_content": {"title": "Improving Stain Invariance Of Cnns For Segmentation By Fusing Channel Attention And Domain-Adversarial Training", "abstract": "Variability in staining protocols, such as different slide preparation techniques, chemicals, and scanner configurations, can result in a diverse set of whole slide images (WSIs). This distribution shift can negatively impact the performance of deep learning models on unseen samples, presenting a significant challenge for developing new computational pathology applications. In this study, we propose a method for improving the generalizability of convolutional neural networks (CNNs) to stain changes in a single-source setting for semantic segmentation. Recent studies indicate that style features mainly exist as covariances in earlier network layers. We design a channel attention mechanism based on these findings that detects stain-specific features and modify the previously proposed stain-invariant training scheme. We reweigh the outputs of earlier layers and pass them to the stain-adversarial training branch. We evaluate our method on multi-center, multi-stain datasets and demonstrate its effectiveness through interpretability analysis. Our approach achieves substantial improvements over baselines and competitive performance compared to other methods, as measured by various evaluation metrics. We also show that combining our method with stain augmentation leads to mutually beneficial results and outperforms other techniques. Overall, our study makes significant contributions to the field of computational pathology."}, "cited_paper_content": {"title": "Two At Once: Enhancing Learning And Generalization Capacities Via Ibn-Net", "abstract": "Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as well as its generalization capacity on another domain (e.g. GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions. (1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information. (2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost. (3) When applying the trained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18%."}, "keywords": ["different distributions", "IBNnet"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "From Figure 2(a), it can be seen that there is less feature divergence for the modified network, suggesting that learned representations are more generalizable (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The modified network in our study shows less feature divergence, indicating that the learned representations are more generalizable (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The modified network shows less feature divergence, suggesting that learned representations are more generalizable to different distributions (#REFR).", "given_citation_intent": "result", "given_keywords": ["different distributions", "IBNnet"]}]}
{"citing_id": "2304.11319v1", "cited_id": "1703.10593", "section_title": "Implementation Details", "citation": "Horse \u2192 Zebra is provided in #REFR , which contains 1,067 and 1,334 training images for horse and zebra, respectively.", "text_before_citation": ["Datasets.", "SN-DCR is trained and evaluated on Horse \u2192 Zebra, Cat \u2192 Dog , Van Gogh \u2192 Photo and CityScapes datasets."], "text_after_citation": ["We use 120 horse images as the test images on Horse \u2192 Zebra.", "Cat \u2192 Dog is from #OTHEREFR , which consists of 5,153 and 4,739 training images for cat and dog, respectively.", "We used 500 images of cats as test images on Cat \u2192 Dog .", "Van Gogh \u2192 Photo is a dataset of 400 Van Gogh paintings and 6287 photos extracted from #OTHEREFR . We used 400 Van Gogh images as test images.", "Cityscapes contains street scenes from German cities, with 2,975 training images and 500 test images."], "citing_paper_content": {"title": "Spectral Normalized Dual Contrastive Regularization For Image-To-Image Translation", "abstract": "Existing image-to-image(I2I) translation methods achieve state-of-the-art performance by incorporating the patch-wise contrastive learning into Generative Adversarial Networks. However, patch-wise contrastive learning only focuses on the local content similarity but neglects the global structure constraint, which affects the quality of the generated images. In this paper, we propose a new unpaired I2I translation framework based on dual contrastive regularization and spectral normalization, namely SN-DCR. To maintain consistency of the global structure and texture, we design the dual contrastive regularization using different feature spaces respectively. In order to improve the global structure information of the generated images, we formulate a semantically contrastive loss to make the global semantic structure of the generated images similar to the real images from the target domain in the semantic feature space. We use Gram Matrices to extract the style of texture from images. Similarly, we design style contrastive loss to improve the global texture information of the generated images. Moreover, to enhance the stability of model, we employ the spectral normalized convolutional network in the design of our generator. We conduct the comprehensive experiments to evaluate the effectiveness of SN-DCR, and the results prove that our method achieves SOTA in multiple tasks."}, "cited_paper_content": {"title": "Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks", "abstract": "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach."}, "keywords": ["1,334 training images"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To maintain consistency of the global structure and texture, the authors propose a new unpaired I2I translation framework based on dual contrastive regularization and spectral normalization, namely SN-DCR #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn the mapping between an input image and an output image using a training set of aligned image pairs, paired training data is required, however, for many tasks, paired training data will not be available, and thus, unpaired image-to-image translation methods such as Cycle-Consistent Adversarial Networks are proposed to address this issue #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn a mapping from an input image to an output image using a training set of aligned image pairs, paired training data is required, which is not always available for many tasks. To address this issue, an approach for learning to translate an image from a source domain to a target domain in the absence of paired examples was presented in the reference paper #REFR, which could be useful for the SN-DCR method that was trained and evaluated on Horse \u2192 Zebra, Cat \u2192 Dog, Van Gogh \u2192 Photo, and CityScapes datasets, consisting of 1,334 training images.", "given_citation_intent": "background", "given_keywords": ["1,334 training images"]}]}
{"citing_id": "2305.02375v1", "cited_id": "1606.03556", "section_title": "Example Applications", "citation": "Motivated by #REFR , to understand whether visual question answering (VQA) models and humans focus on the same parts of an image to answer a question, a researcher may want to compare the saliency maps of a VQA model and human attention maps.", "text_before_citation": ["To ensure that a model is classifying the image based on the properties of the object, a developer can look at the number of mask pixels with high values within the bounding box of the object and compare that number to either the area of the object or the total number of mask pixels with high values.", "A developer could further query for the top-images where such ratios are highest or lowest.", "Concretely, a query over our toy example could count the number of pixels with values higher than 0.85 in the bounding box, which produces a count of two pixels, and compare against the area, which is 6 pixels, for a ratio of 0.33.", "The query could return all images with a ratio below some threshold or could return the images with the lowest ratios.", "Example 2: Comparing model saliency maps and human attention maps."], "text_after_citation": ["To achieve this, they can generate saliency maps for the VQA model and human attention maps for the same images, and then issue a query to retrieve images with the highest number of pixels with values above some threshold in the intersection of the two masks.", "The returned images are those for which the VQA model and humans focus on the same parts of the image to answer a question."], "citing_paper_content": {"title": "Masksearch: Querying Image Masks At Scale", "abstract": "Machine learning tasks over image databases often generate masks that annotate image content (e.g., saliency maps, segmentation maps) and enable a variety of applications (e.g., determine if a model is learning spurious correlations or if an image was maliciously modified to mislead a model). While queries that retrieve examples based on mask properties are valuable to practitioners, existing systems do not support such queries efficiently. In this paper, we formalize the problem and propose a system, MaskSearch, that focuses on accelerating queries over databases of image masks. MaskSearch leverages a novel indexing technique and an efficient filter-verification query execution framework. Experiments on realworld datasets with our prototype show that MaskSearch, using indexes approximately 5% the size of the data, accelerates individual queries by up to two orders of magnitude and consistently outperforms existing methods on various multi-query workloads that simulate dataset exploration and analysis processes."}, "cited_paper_content": {"title": "Human Attention In Visual Question Answering: Do Humans And Deep Networks Look At The Same Regions?", "abstract": "We conduct large-scale studies on \u2018human attention\u2019 in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans. Finally, we train VQA models with explicit attention supervision, and find that it improves VQA performance."}, "keywords": ["saliency maps", "human attention maps"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To compare attention maps generated by state-of-the-art VQA models against human attention, one could refer to the large-scale studies conducted by the authors in #REFR, which show that current attention models in VQA do not seem to be looking at the same regions as humans.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand human attention in Visual Question Answering (VQA), recent studies have designed and tested multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question, resulting in the VQA-HAT dataset (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the differences between model saliency maps and human attention maps, recent work has shown that current attention models in Visual Question Answering (VQA) do not seem to be looking at the same regions as humans, as demonstrated by large-scale studies on human attention in VQA using game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question, and the evaluation of attention maps generated by state-of-the-art VQA models against human attention both qualitatively and quantitatively, as reported in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["saliency maps", "human attention maps"]}]}
{"citing_id": "2305.01668v1", "cited_id": "1901.00850", "section_title": "Synthetic Data: Trance", "citation": "Existing methods such as CLEVR and CLEVR-Change use text which has ambiguity issues making the evaluation unreliable, while CLEVR-Ref+ #REFR employs bounding boxes that are specific but require the additional ability of detection.", "text_before_citation": ["This is the major reason that we choose CLEVR to extend.", "Another reason is that images can be synthesized using Blender #OTHEREFR with small costs. Therefore, it is practicable to create millions of samples.", "CLEVR provides a good foundation on attributes and values, which are fundamental items of the atomic transformation triplet (o, a, v), as we introduced in Sec. 3.", "However, the distance to defining atomic transformations well still exists unless we proceed with several modifications or designs.", "The first problem is how to represent an object in the answer."], "text_after_citation": ["Therefore, we design to provide additional information, which is the attributes of the initial objects, including the index, color, material, and other attribute values.", "In this way, an object can be referred to with its index.", "Note machines still need to perform their own recognition to align objects in images with given attributes.", "The second problem is available values in size and material are too few, therefore we add medium size and glass material.", "The last problem is the available values of position transformation are infinite in the space of R 2 , which is not computational friendly."], "citing_paper_content": {"title": "Visual Reasoning: From State To Transformation", "abstract": "Most existing visual reasoning tasks, such as CLEVR in VQA, ignore an important factor, i.e. transformation. They are solely defined to test how well machines understand concepts and relations within static settings, like one image. Such state driven visual reasoning has limitations in reflecting the ability to infer the dynamics between different states, which has shown to be equally important for human cognition in Piaget's theory. To tackle this problem, we propose a novel transformation driven visual reasoning (TVR) task. Given both the initial and final states, the target becomes to infer the corresponding intermediate transformation. Following this definition, a new synthetic dataset namely TRANCE is first constructed on the basis of CLEVR, including three levels of settings, i.e. Basic (single-step transformation), Event (multi-step transformation), and View (multi-step transformation with variant views). Next, we build another real dataset called TRANCO based on COIN, to cover the loss of transformation diversity on TRANCE. Inspired by human reasoning, we propose a three-staged reasoning framework called TranNet, including observing, analyzing, and concluding, to test how recent advanced techniques perform on TVR. Experimental results show that the state-of-the-art visual reasoning models perform well on Basic, but are still far from human-level intelligence on Event, View, and TRANCO. We believe the proposed new paradigm will boost the development of machine visual reasoning. More advanced methods and new problems need to be investigated in this direction. The resource of TVR is available at https://hongxin2019.github.io/TVR/."}, "cited_paper_content": {"title": "Clevr-Ref+: Diagnosing Visual Reasoning With Referring Expressions", "abstract": "Referring object detection and referring image segmentation are important tasks that require joint understanding of visual information and natural language. Yet there has been evidence that current benchmark datasets suffer from bias, and current state-of-the-art models cannot be easily evaluated on their intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators. In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we also propose IEP-Ref, a module network approach that significantly outperforms other models on our dataset. In particular, we present two interesting and important findings using IEP-Ref: (1) the module trained to transform feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step; (2) even if all training data has at least one object referred, IEP-Ref can correctly predict no-foreground when presented with false-premise referring expressions. To the best of our knowledge, this is the first direct and quantitative proof that neural modules behave in the way they are intended. We will release data and code for CLEVR-Ref+."}, "keywords": ["detection", "CLEVR-Ref+"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the representation problem of objects in the answer, the authors of this manuscript chose to extend CLEVR, a dataset that provides a good foundation on attributes and values #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the representation problem of an object in the answer, the authors of this manuscript choose to extend CLEVR dataset, which provides a good foundation on attributes and values, and introduce a new synthetic diagnostic dataset for referring expression comprehension called CLEVR-Ref+ #REFR, which allows control over dataset bias and enables intermediate reasoning ground truth without human annotators.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of bias in referring object detection and segmentation, the authors propose CLEVR-Ref+, a synthetic diagnostic dataset that allows control over dataset bias and enables intermediate reasoning ground truth without human annotators #REFR.", "given_citation_intent": "method", "given_keywords": ["detection", "CLEVR-Ref+"]}]}
{"citing_id": "2304.09402v1", "cited_id": "1910.10683", "section_title": "Implementation Details", "citation": "A pretrained T5 model #REFR is used to fill the blanks and generate an augmented sample.", "text_before_citation": ["As mentioned in Section 2.2, we augment the original prompt and generate both label-preserving and label-flipping prompts #OTHEREFR ) before applying the three-level Mixup.", "Both the input text and the templates are augmented separately.", "We generate label-preserving and label-flipping augmented text, but only label-preserving templates are generated.", "Following , we use a cloze pattern to combine both the input text (template) and the label into a single sequence, and then randomly mask a fixed percentage of the input tokens."], "text_after_citation": ["For model training, we adopt the experimental settings used in PET #OTHEREFR and conduct a grid search for the three-level Mixup, as indicated in Table 2 .", "To evaluate the effectiveness of our DA methods, we employ PET #OTHEREFR as the backbone and augment it with MIXPRO and other DA baselines.", "We utilize Albert-xxlarge-v2 #OTHEREFR as the PLM and measure the performance using the identical metrics in Table1, namely Acc., F1, F1 a , and EM.", "Since few-shot learning typically exhibits significant performance fluctuations (Dodge et al., 2020; #OTHEREFR , we use 5 independent seeds and report the average performance across these seeds for each model.", "All of our experiments were performed on a Linux platform equipped with NVIDIA A100 (40G)."], "citing_paper_content": {"title": "Mixpro: Simple Yet Effective Data Augmentation For Prompt-Based Learning", "abstract": "Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods (Schick and Sch\u00fctze, 2021c) using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MIXPRO, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MIXPRO outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation."}, "cited_paper_content": {"title": "Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."}, "keywords": ["augmented sample", "pretrained T5 model"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As mentioned in Section 2.2, we augment the original prompt and generate both label-preserving and label-flipping prompts #OTHEREFR ) before applying the three-level Mixup. Both the input text and the templates are augmented separately. We generate label-preserving and label-flipping augmented text, but only label-preserving templates are generated. Following the landscape of transfer learning techniques for NLP explored in \"Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer\", we use a cloze pattern to combine both the input text (template) and the label into a single sequence, and then randomly mask a fixed percentage of the input tokens.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To explore the landscape of transfer learning techniques for NLP, the authors of Mixpro proposed a unified framework that converts every language problem into a text-to-text format, achieving state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To augment the input text and templates, we propose a new method called MIXPRO, which applies token-level, sentence-level, and epoch-level Mixup strategies to both the vanilla input text and the templates #REFR.", "given_citation_intent": "method", "given_keywords": ["augmented sample", "pretrained T5 model"]}]}
{"citing_id": "2304.07504v1", "cited_id": "1507.02000", "section_title": "G.3 Proof Of Proposition 4.2", "citation": "For x \u2208 F k , by #REFR , if k \u2208 L i , then u i \u2208 F k+1 ; otherwise, u i \u2208 F k . This completes the proof.", "text_before_citation": ["Thus, if x \u2208 F 0 , prox \u03b3 r1 (x) \u2208 F 1 ; if x \u2208 F k for k \u2265 1, prox \u03b3 r1 (x) \u2208 F k .", "(ii) For i = 1, we define u i := prox \u03b3 ri (x) for simplicity. Then u i satisfies the following equation", "I + n\u03b3 c\u03b3 + 1 B i B i \u22121 x = x \u2212 B i D i B i x c\u03b3 + 1 .", "Then we have", "B i D i B i x = l\u2208Li d i,l b l b l x."], "text_after_citation": ["which implies our desired result."], "citing_paper_content": {"title": "Stochastic Distributed Optimization Under Average Second-Order Similarity: Algorithms And Analysis", "abstract": "We study finite-sum distributed optimization problems with n-clients under popular \u03b4-similarity condition and \u00b5-strong convexity. We propose two new algorithms: SVRS and AccSVRS motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient-sliding and variance reduction, which achieves superior communication complexity\u00d5(n+ \u221a n\u03b4/\u00b5) compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X [6], we also build a direct accelerated practical version named AccSVRS with totally smoothness-free\u00d5(n+n 3/4 \u03b4/\u00b5) communication complexity that improves upon existing algorithms on ill-conditioning cases. Furthermore, we show a nearly matched lower bound to verify the tightness of our AccSVRS method."}, "cited_paper_content": {"title": "An Optimal Randomized Incremental Gradient Method", "abstract": "In this paper, we consider a class of finite-sum convex optimization problems whose objective function is given by the summation of $m$ ($\\ge 1$) smooth components together with some other relatively simple terms. We first introduce a deterministic primal-dual gradient (PDG) method that can achieve the optimal black-box iteration complexity for solving these composite optimization problems using a primal-dual termination criterion. Our major contribution is to develop a randomized primal-dual gradient (RPDG) method, which needs to compute the gradient of only one randomly selected smooth component at each iteration, but can possibly achieve better complexity than PDG in terms of the total number of gradient evaluations. More specifically, we show that the total number of gradient evaluations performed by RPDG can be ${\\cal O} (\\sqrt{m})$ times smaller, both in expectation and with high probability, than those performed by deterministic optimal first-order methods under favorable situations. We also show that the complexity of the RPDG method is not improvable by developing a new lower complexity bound for a general class of randomized methods for solving large-scale finite-sum convex optimization problems. Moreover, through the development of PDG and RPDG, we introduce a novel game-theoretic interpretation for these optimal methods for convex optimization."}, "keywords": ["k \u2208 L", "k+1"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors introduce a randomized primal-dual gradient (RPDG) method for solving composite optimization problems, which computes the gradient of only one randomly selected smooth component at each iteration and achieves better complexity than deterministic optimal first-order methods. The RPDG method can perform ${\\cal O} (\\sqrt{m})$ times fewer gradient evaluations than deterministic methods, both in expectation and with high probability, under favorable situations. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed randomized primal-dual gradient (RPDG) method in this paper can achieve better complexity than deterministic optimal first-order methods, with the total number of gradient evaluations performed by RPDG being ${\\cal O} (\\sqrt{m})$ times smaller, both in expectation and with high probability, than those performed by deterministic optimal first-order methods under favorable situations. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The approach proposed in this manuscript is related to the finite-sum convex optimization problems discussed in #REFR, where the authors introduced a randomized primal-dual gradient method that computes the gradient of only one randomly selected smooth component at each iteration, achieving better complexity in terms of the total number of gradient evaluations.", "given_citation_intent": "background", "given_keywords": ["k \u2208 L", "k+1"]}]}
{"citing_id": "2304.14886v1", "cited_id": "1910.09328", "section_title": "A. Linear Systems With Gaussian Uncertainties", "citation": "Some notes regarding the synthesis: (a) It is possible to derive the Hessian as well (see #REFR ).", "text_before_citation": ["However, when optimizing for cases where p \u03d5 f 1, we want smaller steps so we do not overshoot.", "We introduce p \u03d5 f in the gradient descent update as a normalizing factor so that the learning rate is easier to obtain and can remain relatively constant.", "We also introduce a sign variable where v dir = 1 if we search for p \u03d5 s and v dir = \u22121, if we search for p \u03d5 f .", "The gradient descent process for some parameter \u03b3 of the system, is then:", "EQUATION"], "text_after_citation": ["A possible improvement to the gradient descent is by incorporating the second-degree derivative for more accuracy.", "(b) The lower the probability of failure is, the longer the computation (more nestings).", "Keep in mind that p \u03d5 f = 0 is not possible due to the assumption that the uncertainty is an unbounded Gaussian.", "Therefore, it is up to the designer to specify the stopping criteria at the requisite level of performance.", "(c) STL-based ESS can also attempt to find the initial trajectory."], "citing_paper_content": {"title": "Ensuring Reliable Robot Task Performance Through Probabilistic Rare-Event Verification And Synthesis", "abstract": "Providing guarantees on the safe operation of robots against edge cases is challenging as testing methods such as traditional Monte-Carlo require too many samples to provide reasonable statistics. Built upon recent advancements in rareevent sampling, we present a model-based method to verify if a robotic system satisfies a Signal Temporal Logic (STL) specification in the face of environment variations and sensor/actuator noises. Our method is efficient and applicable to both linear and nonlinear and even black-box systems with arbitrary, but known, uncertainty distributions. For linear systems with Gaussian uncertainties, we exploit a feature to find optimal parameters that minimize the probability of failure. We demonstrate illustrative examples on applying our approach to real-world autonomous robotic systems."}, "cited_paper_content": {"title": "Integrals Over Gaussians Under Linear Domain Constraints", "abstract": "Integrals of linearly constrained multivariate Gaussian densities are a frequent problem in machine learning and statistics, arising in tasks like generalized linear models and Bayesian optimization. Yet they are notoriously hard to compute, and to further complicate matters, the numerical values of such integrals may be very small. We present an efficient black-box algorithm that exploits geometry for the estimation of integrals over a small, truncated Gaussian volume, and to simulate therefrom. Our algorithm uses the Holmes-Diaconis-Ross (HDR) method combined with an analytic version of elliptical slice sampling (ESS). Adapted to the linear setting, ESS allows for efficient, rejection-free sampling, because intersections of ellipses and domain boundaries have closed-form solutions. The key idea of HDR is to decompose the integral into easier-to-compute conditional probabilities by using a sequence of nested domains. Remarkably, it allows for direct computation of the logarithm of the integral value and thus enables the computation of extremely small probability masses. We demonstrate the effectiveness of our tailored combination of HDR and ESS on high-dimensional integrals and on entropy search for Bayesian optimization."}, "keywords": ["synthesis", "Hessian"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently estimate integrals over a small, truncated Gaussian volume, the authors of this manuscript cite a black-box algorithm that combines the Holmes-Diaconis-Ross method with an analytic version of elliptical slice sampling, as presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently estimate integrals over a small, truncated Gaussian volume, the authors of this manuscript could use the black-box algorithm presented in the reference paper #REFR, which combines the Holmes-Diaconis-Ross method with an analytic version of elliptical slice sampling.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently estimate integrals over a small, truncated Gaussian volume, the authors of this manuscript could use the HDR method combined with an analytic version of elliptical slice sampling, as presented in the reference paper #REFR, which exploits geometry and allows for direct computation of the logarithm of the integral value, making it possible to compute extremely small probability masses and thus enabling the synthesis of reliable robotic systems.", "given_citation_intent": "background", "given_keywords": ["synthesis", "Hessian"]}]}
{"citing_id": "2303.14865v1", "cited_id": "1608.03983", "section_title": "B.2. Implementation Details", "citation": "We set the learning rate as 0.005, and decay it to 0 following the cosine strategy #REFR .", "text_before_citation": ["We set the maximum iterations number to 1,000, and determine the L2 regularization weights following DECLIP's hyperparameter sweeping strategy #OTHEREFR .", "We do not report the results on the ImageNet-1K dataset, due to the high computational cost of conducting hyperparameter sweeping on the dataset. Non-linear Probe Task.", "The downstream task head consists of a fully-connected layer with GELU activation and a fully-connected layer.", "The extracted FDT features of images and questions are concatenated and then fed to the downstream task head to predict the answers. The encoders and FDT are frozen during the training.", "The downstream head is optimized by the AdamW optimizer #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "Revisiting Multimodal Representation In Contrastive Learning: From Patch And Token Embeddings To Finite Discrete Tokens", "abstract": "Contrastive learning-based vision-language pretraining approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visualsemantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes. 1 * This work was done during a research internship at ByteDance. \u2020 Dimitris N. Metaxas has been supported by NSF IUCRC CARTA"}, "cited_paper_content": {"title": "Sgdr: Stochastic Gradient Descent With Warm Restarts", "abstract": "Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR"}, "keywords": ["learning rate"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We set the maximum iterations number to 1,000, and determine the L2 regularization weights following DECLIP's hyperparameter sweeping strategy #OTHEREFR. We do not report the results on the ImageNet-1K dataset, due to the high computational cost of conducting hyperparameter sweeping on the dataset. Non-linear Probe Task. The downstream task head consists of a fully-connected layer with GELU activation and a fully-connected layer. The extracted FDT features of images and questions are concatenated and then fed to the downstream task head to predict the answers. The encoders and FDT are frozen during the training. The downstream head is optimized by the AdamW optimizer #REFR. The authors' proposed warm restart technique for stochastic gradient descent can be used to improve the anytime performance when training deep neural networks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We optimized the downstream head using the AdamW optimizer #OTHEREFR and froze the encoders and FDT during training, following the method proposed in Sgdr: Stochastic Gradient Descent With Warm Restarts #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We optimize the downstream head using the AdamW optimizer with a determined learning rate #REFR, and freeze the encoders and FDT during training, following the method proposed in \"Sgdr: Stochastic Gradient Descent With Warm Restarts\".", "given_citation_intent": "method", "given_keywords": ["learning rate"]}]}
{"citing_id": "2304.05379v1", "cited_id": "2001.04057", "section_title": "Viii. Transmission Power Analysis", "citation": "To find the minimum power requirement for a 2-Group IC-NOMA transmission that makes R IN \u2212N OMA #REFR (n,f ) at least as good as R IC , consider (25) and (31).", "text_before_citation": ["Since, l IC \u2265 l m , we have P #OTHEREFR saving > 0", "3)CASE III (l n > l f > l m ) : The 3-Group NOMA transmission of the scheme saves power compared to conventional IC to achieve equal information rate as given by (27).", "Let P", "(n,f ) denote the power per 2-Group NOMA transmission in this case. Then, (16) is modified as", "R IN \u2212N OMA (2) (n,f ) = log 2 \uf8eb \uf8ed (1 + P (2) (n,f ) g f )(1 + \u03b1 1 P (2) (n,f ) g n ) (1 + \u03b1 1 P (2) (n,f ) g f ) \uf8f6 \uf8f8 (31)"], "text_after_citation": ["EQUATION", "From (32), we have \u03b6 (n,f ) > 0 =\u21d2 P IC > P", "(n,f ) , i.e., To achieve a rate at least as good as that of conventional IC, power per transmission in conventional IC system is greater when compared to power per a 2-Group NOMA transmission in this case . Then,", "EQUATION", "Let P IC n denote the power per IC transmission in this case, then (4) is modified as"], "citing_paper_content": {"title": "Design And Analysis Of Index Codes For 3-Group Noma In Vehicular Adhoc Networks", "abstract": "Index coding (IC) is a source coding technique employed to improve spectral utilisation, where the source node aims to satisfy users' demands by making minimum transmissions. Non-orthogonal multiple access (NOMA) is integral to the radio access technique used in 5G networks. Index-coded NOMA (IC-NOMA) transmission scheme in Vehicular Adhoc Networks (VANETs) involves applying NOMA principles on index-coded data to avoid network congestion and to improve spectral efficiency compared to conventional IC systems. In this work, a spectral efficient transmission scheme called 3-Group IC-NOMA is proposed, and an innovative index code design that fits with NOMA decoding principles to obtain improved spectral efficiency is developed. Through exhaustive analytical studies, we demonstrate that the proposed transmission scheme always supports higher rates than the conventional IC systems and requires less power to achieve an information rate at least as good as conventional IC systems."}, "cited_paper_content": {"title": "Efficient 3D Road Map Data Exchange For Intelligent Vehicles In Vehicular Fog Networks", "abstract": "Through connecting intelligent vehicles as well as the roadside infrastructure, the perception range of vehicles can be significantly extended, and hidden objects at blind spots can be efficiently detected and avoided. To realize this, accurate road map data must be downloaded in real time to these intelligent vehicles for navigation and localization purposes. Besides, the cloud must be updated with dynamic changes that happened in the road network. These involve the transmissions of high-definition 3D road map data for accurately representing the physical environments. In this work, we propose solutions under the fog computing architecture in a heterogeneous vehicular network to optimize data exchange among intelligent vehicles, the roadside infrastructure, as well as regional databases. Specifically, the efficiency of 3D road map data dissemination at roadside fog nodes is achieved by exploiting index coding techniques to reduce the overall data load, while opportunistic scheduling of heterogeneous transmissions can be done to judiciously manage network resources and minimize operating cost. In addition, 3D point cloud coding and hashing techniques are applied to expedite the updates of various dynamic changes in the network. We empirically evaluate the proposed solutions based on real-world mobility traces of vehicles and 3D LIght Detection And Ranging (LIDAR) data of city streets. The proposed system is also implemented in a multi-robotic testbed for practical evaluation."}, "keywords": ["2-Group IC-NOMA transmission", "R IC"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To efficiently disseminate 3D road map data in vehicular fog networks, the authors of this manuscript propose exploiting index coding techniques and opportunistic scheduling of heterogeneous transmissions at roadside fog nodes, as well as applying 3D point cloud coding and hashing techniques to expedite the updates of various dynamic changes in the network #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently download accurate road map data for navigation and localization purposes in vehicular fog networks, the proposed solutions under the fog computing architecture in a heterogeneous vehicular network optimize data exchange among intelligent vehicles, the roadside infrastructure, as well as regional databases by exploiting index coding techniques and opportunistic scheduling of heterogeneous transmissions, as well as applying 3D point cloud coding and hashing techniques to expedite the updates of various dynamic changes in the network #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve higher rates and improved spectral efficiency in 2-Group IC-NOMA transmission, an innovative index code design that fits with NOMA decoding principles is proposed in this work #REFR.", "given_citation_intent": "background", "given_keywords": ["2-Group IC-NOMA transmission", "R IC"]}]}
{"citing_id": "2303.09824v3", "cited_id": "1705.10528", "section_title": "B. Reinforcement Learning", "citation": "Constrained Policy Optimization (CPO) #REFR is a pioneering general-purpose policy exploit algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration.", "text_before_citation": ["#OTHEREFR train a DQN agent combined with DNN which outputs two discrete actions.", "The safety and agility of the ego vehicle can be balanced on-the-go, indicating that the RL agent can learn an adaptive behavior. Furthermore, Ronecker et al.", "#OTHEREFR propose a safer navigating method for IVs in highway scenarios by combining Deep Q-Networks from control theory.", "The proposed network is trained in simulation for central decision-making by proposing targets for a trajectory planner, which shows that the value-based RL can produce efficient and safe driving behavior in highway traffic scenarios.", "The security of end-to-end autonomous driving also raises significant apprehension."], "text_after_citation": ["Building on this, #OTHEREFR and #OTHEREFR present the Safety Gym benchmark suite and validate several constrained deep RL algorithms under constrained conditions. Li et al.", "#OTHEREFR introduce a risk awareness algorithm into DRL frameworks to learn a riskaware driving decision policy for lane-changing tasks with the minimum expected risk. Chow et al.", "#OTHEREFR propose safe policy optimization algorithms that employ a Lyapunov-based approach #OTHEREFR to address CMDP problems. Furthermore, Yang et al.", "#OTHEREFR construct a model-free safe RL algorithm that integrates policy and neural barrier certificate learning in a stepwise state constraint scenario. Mo et al.", "#OTHEREFR leverage Monte Carlo Tree Search to reduce unsafe behaviors on overtaking subtasks at highway scenarios. Fig. 7 ."], "citing_paper_content": {"title": "Motion Planning For Autonomous Driving: The State Of The Art And Future Perspectives", "abstract": "Thanks to the augmented convenience, safety advantages, and potential commercial value, Intelligent vehicles (IVs) have attracted wide attention throughout the world. Although a few autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their implementation is still restricted to small-scale validation due to various issues, among which precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verification scenarios of driving tasks are points of concern. Experimental platforms are reviewed to facilitate readers in selecting suitable training and validation methods. Finally, the current challenges and future directions are discussed. The sideby-side comparison presented in this survey not only helps to gain insights into the strengths and limitations of the reviewed methods but also assists with system-level design choices. Index Terms-Pipeline planning, end-to-end planning, imitation learning, reinforcement learning, parallel learning. I. INTRODUCTION I NTELLIGENT vehicles (IVs) have gained considerable attention from government, industry, academia, and the"}, "cited_paper_content": {"title": "Constrained Policy Optimization", "abstract": "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. ::: We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety."}, "keywords": ["Constrained Policy Optimization"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the constrained setting of reinforcement learning, it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. Recent advances in policy search algorithms have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. To address this, the authors propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Reinforcement learning algorithms have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. However, Constrained Policy Optimization (CPO) proposed by Amos et al. #REFR is the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration, allowing the training of neural network policies for high-dimensional control while making guarantees about policy behavior throughout training.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Reinforcement learning algorithms have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. However, Constrained Policy Optimization (CPO) is proposed as the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration, which allows training of neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. #REFR", "given_citation_intent": "background", "given_keywords": ["Constrained Policy Optimization"]}]}
{"citing_id": "2304.04664v1", "cited_id": "1803.08494", "section_title": "Computational Blocks", "citation": "Various forms of normalisation differ along which dimensions they calculate the reference statistics (see #REFR for a comparison).", "text_before_citation": ["Residual connections preserve high resolution information, facilitate the propagation of gradient information to early encoding layers, and thus avoid the vanishing gradient problem and enabled the training of very deep neural networks #OTHEREFR .", "As a result, residual blocks are now used in essentially all modern DL architectures.", "Besides data normalisation, discussed in Section 2.3, dedicated normalisation modules between (or after) the projection and the non-linearity within a processing block may be applied #OTHEREFR .", "Empirically, it has been found that normalisation leads to faster and more stable convergence (for a theoretical perspective see e.g. #OTHEREFR ).", "Normalisation re-scales all intermediate activations within a layer to have zero mean and unit variance."], "text_after_citation": ["Among the reviewed models, P22, K22, and E21, and H22 utilise normalisation over the channel and spatial dimensions, whereas R21 use normalisation over the batch dimension and W21 omit internal normalisation entirely.", "Immediately after the normalisation, an affine transformation with learned parameters is often applied to allow the activation statistics to be adapted to the data.", "Recent work in #OTHEREFR and #OTHEREFR suggests that learned affine parameters may achieve the beneficial effects of normalisation by themselves.", "In E21, for example, the parameters of the affine transformation are predicted by a secondary neural network based on the targeted forecast lead-time, which enables maximally effective, layer-specific lead-time encodings.", "The same principle has been used in previous work #OTHEREFR to condition on various kinds of auxiliary information, an approach that could hold promise in DLWP for integrating other relevant data sources."], "citing_paper_content": {"title": "Inductive Biases In Deep Learning Models For Weather Prediction", "abstract": "Deep learning has recently gained immense popularity in the Earth sciences as it enables us to formulate purely data-driven models of complex Earth system processes. Deep learning-based weather prediction (DLWP) models have made significant progress in the last few years, achieving forecast skills comparable to established numerical weather prediction (NWP) models with comparatively lesser computational costs. In order to train accurate, reliable, and tractable DLWP models with several millions of parameters, the model design needs to incorporate suitable inductive biases that encode structural assumptions about the data and modelled processes. When chosen appropriately, these biases enable faster learning and better generalisation to unseen data. Although inductive biases play a crucial role in successful DLWP models, they are often not stated explicitly and how they contribute to model performance remains unclear. Here, we review and analyse the inductive biases of six state-of-the-art DLWP models, involving a deeper look at five key design elements: input data, forecasting objective, loss components, layered design of the deep learning architectures, and optimisation methods. We show how the design choices made in each of the five design elements relate to structural assumptions. Given recent developments in the broader DL community, we anticipate that the future of DLWP will likely see a wider use of foundation models-large models pre-trained on big databases with self-supervised learning-combined with explicit physics-informed inductive biases that allow the models to provide competitive forecasts even at the more challenging subseasonal-to-seasonal scales."}, "cited_paper_content": {"title": "Group Normalization", "abstract": "Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries."}, "keywords": ["normalisation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Empirically, it has been found that normalisation leads to faster and more stable convergence (for a theoretical perspective see e.g. #OTHEREFR). Normalisation re-scales all intermediate activations within a layer to have zero mean and unit variance. Group Normalization (GN) is a simple alternative to Batch Normalization (BN) that divides the channels into groups and computes within each group the mean and variance for normalization, making its computation independent of batch sizes and stable in a wide range of batch sizes #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Batch Normalization (BN) is a widely used technique in deep learning, but it has limitations when it comes to smaller batch sizes; Group Normalization (GN) is a simple alternative that can be used instead #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Batch normalization is a widely used technique in deep learning, but it has limitations when it comes to normalizing along the batch dimension, which can be overcome by using Group Normalization (GN) that divides the channels into groups and computes within each group the mean and variance for normalization, making its computation independent of batch sizes and its accuracy stable in a wide range of batch sizes #REFR.", "given_citation_intent": "background", "given_keywords": ["normalisation"]}]}
{"citing_id": "2304.11130v1", "cited_id": "1308.4941", "section_title": "Introduction", "citation": "Figure 2 highlights the aim of our work at large which is inline with MITRE's vision on the standardization of CVE records, an extremely valuable task to the community #REFR .", "text_before_citation": ["\u2022 Experimental results.", "We approached the task as a ranking problem, using text similarity SBERT #OTHEREFR , optimized for sentences (vs.", "the document level) and the recently released ranking T5 #OTHEREFR model on the dataset.", "Our results were compared to the more general-built models BERT #OTHEREFR and RoBERTa #OTHEREFR . BM25 #OTHEREFR was used as the baseline model.", "Moreover, we used T5 as Seq2Seq generation #OTHEREFR model as well as a ranker."], "text_after_citation": ["The rest of this paper is organized as follows: Section 2 provides a summary of related work.", "Section 3 briefly summarizes the MITRE weaknesses types and how they are used.", "Section 4 explains the rationale behind annotating and maintaining the released cyber-security AI dataset. Section 5 introduces our methodology. Section 6 describes our experimentation and discusses the results.", "Finally, Section 7 concludes and points to possible directions for future work."], "citing_paper_content": {"title": "Automated Mapping Of Cve Vulnerability Records To Mitre Cwe Weaknesses", "abstract": "In recent years, a proliferation of cyber-security threats and diversity has been on the rise culminating in an increase in their reporting and analysis. To counter that, many non-profit organizations have emerged in this domain, such as MITRE and OSWAP, which have been actively tracking vulnerabilities, and publishing defense recommendations in standardized formats. As producing data in such formats manually is very time-consuming, there have been some proposals to automate the process. Unfortunately, a major obstacle to adopting supervised machine learning for this problem has been the lack of publicly available specialized datasets. Here, we aim to bridge this gap. In particular, we focus on mapping CVE records into MITRE CWE Weaknesses, and we release to the research community a manually annotated dataset of 4,012 records for this task. With a human-in-the-loop framework in mind, we approach the problem as a ranking task and aim to incorporate reinforced learning to make use of the human feedback in future work. Our experimental results using fine-tuned deep learning models, namely Sentence-BERT and rankT5, show sizable performance gains over BM25, BERT, and RoBERTa, which demonstrates the need for an architecture capable of good semantic understanding for this task."}, "cited_paper_content": {"title": "Automatic Labeling For Entity Extraction In Cyber Security", "abstract": "Timely analysis of cyber-security information necessitates automated information extraction from unstructured text. While state-of-the-art extraction methods produce extremely accurate results, they require ample training data, which is generally unavailable for specialized applications, such as detecting security related entities; moreover, manual annotation of corpora is very costly and often not a viable solution. In response, we develop a very precise method to automatically label text from several data sources by leveraging related, domainspecific, structured data and provide public access to a corpus annotated with cyber-security entities. Next, we implement a Maximum Entropy Model trained with the average perceptron on a portion of our corpus ( 750,000 words) and achieve near perfect precision, recall, and accuracy, with training times under 17 seconds."}, "keywords": ["CVE records"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We used a Maximum Entropy Model trained with the average perceptron on a portion of our corpus and achieved near perfect precision, recall, and accuracy, with training times under 17 seconds #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To automate information extraction from unstructured text, previous work has developed a precise method to automatically label text by leveraging related, domain-specific, structured data, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To map CVE records into MITRE CWE Weaknesses, specialized datasets are required, which are often unavailable. However, recent work has shown that automatic labeling of text can be achieved with high precision, recall, and accuracy, even with limited training data, by leveraging related, domain-specific, structured data #REFR.", "given_citation_intent": "background", "given_keywords": ["CVE records"]}]}
{"citing_id": "2304.08204v1", "cited_id": "1612.06890", "section_title": "Rq2. Effects On Geometric Primitives And Concepts: How Well Does Lbs Represent Geometric Primitives And Understand Their Conceptual Relationships?", "citation": "We validate whether LBS can reflect the local geometric information of each object in a synthetic photo dataset consisting of multiple objects, using the CLEVR dataset #REFR .", "text_before_citation": ["To evaluate whether LBS is suitable for representing geometric primitives and concepts, we perform a classification task on the Geoclidean dataset #OTHEREFR .", "The Geoclidean dataset consists of realized images from a concept of Euclidean geometry, e.g., black parallel lines on a white background.", "Geoclidean is divided into two categories: Geoclidean-Elements and Geoclidean-Constraints, visualized in Fig. 7 on Appendix B.", "By providing a limited training set consisting of only 10 images per concept, we evaluate whether LBS can effectively learn the high-level relationships between each primitive and generalize them across different examples by classifying the concept of the test image. RQ3.", "Local geometric information and spatial reasoning: How effectively does LBS reflect local geometric information and extend it to spatial reasoning tasks?"], "text_after_citation": ["We train our model with very limited descriptions, where the label for the entire scene is provided as the rightmost object or without any descriptions at all.", "We validate the effectiveness of our representation by evaluating its ability to successfully classify attributes that are not provided as labels, such as determining the color of the leftmost object.", "Additionally, we test its ability to perform simple spatial reasoning, such as shifting the rightmost object and inferring the attribute of the current rightmost object. RQ4.", "Domain transfer: Can the geometric concepts of LBS trained in a specific domain be extended to other domains?", "To investigate whether the learned representation within a specific image domain provides meaningful geometric information across other domains, we evaluate the model by shifting the distribution from the STL-10 [11] dataset to CLEVR and vice versa."], "citing_paper_content": {"title": "Learning Geometry-Aware Representations By Sketching", "abstract": "Understanding geometric concepts, such as distance and shape, is essential for understanding the real world and also for many vision tasks. To incorporate such information into a visual representation of a scene, we propose learning to represent the scene by sketching, inspired by human behavior. Our method, coined Learning by Sketching (LBS), learns to convert an image into a set of colored strokes that explicitly incorporate the geometric information of the scene in a single inference step without requiring a sketch dataset. A sketch is then generated from the strokes where CLIP-based perceptual loss maintains a semantic similarity between the sketch and the image. We show theoretically that sketching is equivariant with respect to arbitrary affine transformations and thus provably preserves geometric information. Experimental results show that LBS substantially improves the performance of object attribute classification on the unlabeled CLEVR dataset, domain transfer between CLEVR and STL-10 datasets, and for diverse downstream tasks, confirming that LBS provides rich geometric information."}, "cited_paper_content": {"title": "Clevr: A Diagnostic Dataset For Compositional Language And Elementary Visual Reasoning", "abstract": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short-comings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations."}, "keywords": ["local geometric information", "synthetic photo dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate whether LBS is suitable for representing geometric primitives and concepts, we perform a classification task on the Geoclidean dataset #OTHEREFR. The Geoclidean dataset consists of realized images from a concept of Euclidean geometry, e.g., black parallel lines on a white background. Geoclidean is divided into two categories: Geoclidean-Elements and Geoclidean-Constraints, visualized in Fig. 7 on Appendix B. By providing a limited training set consisting of only 10 images per concept, we evaluate whether LBS can effectively learn the high-level relationships between each primitive and generalize them across different examples by classifying the concept of the test image. RQ3. Local geometric information and spatial reasoning: How effectively does LBS reflect local geometric information and extend it to spatial reasoning tasks? To address this question, we compare our results with those obtained on the CLEVR dataset, which contains minimal biases and has detailed annotations describing the kind of reasoning each question requires #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To effectively learn the high-level relationships between each primitive and generalize them across different examples, the authors propose Learning by Sketching (LBS) #REFR, a method that converts an image into a set of colored strokes that explicitly incorporate the geometric information of the scene in a single inference step without requiring a sketch dataset.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To effectively learn high-level relationships between geometric primitives and generalize them across different examples, the authors utilized a synthetic photo dataset #REFR that tests a range of visual reasoning abilities and contains minimal biases, providing detailed annotations describing the kind of reasoning each question requires.", "given_citation_intent": "method", "given_keywords": ["local geometric information", "synthetic photo dataset"]}]}
{"citing_id": "2304.14736v1", "cited_id": "1604.01685", "section_title": "Experiments", "citation": "Semantic segmentation results with different sensor resolutions on the Cityscapes dataset #REFR . For all resolutions, the learned rectangular layout performs best.", "text_before_citation": ["MNIST classification To illustrate the principle of our end-to-end sensor layout optimization, we start with a toy example and optimize the layout for hand-written digit recognition on MNIST #OTHEREFR with a sensor size of only 4 \u00d7 4 instead of the original 28 \u00d7 28 pixels.", "The digits in MNIST are always centered, so the hypothesis is that an optimized layout puts smaller pixels in the middle in order to capture the higher information density there.", "We apply the curvilinear layout \u03c6 curv whose parameters we initialize with 0, i.e., Table 2 ."], "text_after_citation": [], "citing_paper_content": {"title": "Differentiable Sensor Layouts For End-To-End Learning Of Task-Specific Camera Parameters", "abstract": "The success of deep learning is frequently described as the ability to train all parameters of a network on a specific application in an end-to-end fashion. Yet, several design choices on the camera level, including the pixel layout of the sensor, are considered as pre-defined and fixed, and high resolution, regular pixel layouts are considered to be the most generic ones in computer vision and graphics, treating all regions of an image as equally important. While several works have considered non-uniform, e.g., hexagonal or foveated, pixel layouts in hardware and image processing, the layout has not been integrated into the end-to-end learning paradigm so far. In this work, we present the first truly end-to-end trained imaging pipeline that optimizes the size and distribution of pixels on the imaging sensor jointly with the parameters of a given neural network on a specific task. We derive an analytic, differentiable approach for the sensor layout parameterization that allows for taskspecific, local varying pixel resolutions. We present two pixel layout parameterization functions: rectangular and curvilinear grid shapes that retain a regular topology. We provide a drop-in module that approximates sensor simulation given existing high-resolution images to directly connect our method with existing deep learning models. We show that network predictions benefit from learnable pixel layouts for two different downstream tasks, classification and semantic segmentation."}, "cited_paper_content": {"title": "The Cityscapes Dataset For Semantic Urban Scene Understanding", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."}, "keywords": ["learned rectangular layout", "Semantic segmentation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To optimize the layout for hand-written digit recognition on MNIST with a sensor size of only 4 \u00d7 4, the authors applied the curvilinear layout \u03c6 curv from the reference paper, which presents the first truly end-to-end trained imaging pipeline that optimizes the size and distribution of pixels on the imaging sensor jointly with the parameters of a given neural network on a specific task. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For semantic urban scene understanding, large-scale datasets have been shown to be beneficial for object detection, especially in the context of deep learning #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For semantic segmentation, recent works have shown that network predictions can benefit from a learned rectangular layout #REFR that optimizes the size and distribution of pixels on the imaging sensor jointly with the parameters of a given neural network on a specific task.", "given_citation_intent": "background", "given_keywords": ["learned rectangular layout", "Semantic segmentation"]}]}
{"citing_id": "2303.18223v7", "cited_id": "2001.08361", "section_title": "Background For Llms", "citation": "As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity #REFR 34] .", "text_before_citation": ["Key Techniques for LLMs.", "It has been a long way that LLMs evolve into the current state: general and capable learners.", "In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs.", "Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.", "\u2022 Scaling."], "text_after_citation": ["As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively.", "Furthermore, since compute budget is usually limited, scaling laws can be employed to conduct a more compute-efficient allocation of the compute resources.", "For example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget [34] .", "While, it should be noted that data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity.", "\u2022 Training."], "citing_paper_content": {"title": "A Survey Of Large Language Models", "abstract": "Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers."}, "cited_paper_content": {"title": "Scaling Laws For Neural Language Models", "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence."}, "keywords": ["Transformer language models"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Scaling has been an important technique that potentially leads to the success of large language models (LLMs) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The scaling effect of language models has been widely studied, and it has been found that larger models are significantly more sample-efficient, which allows for optimally compute-efficient training by training very large models on a relatively modest amount of data and stopping significantly before convergence (Brown et al., 2020). #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The success of large language models (LLMs) heavily relies on the scaling technique, which increases the parameter scale to an even larger size, leading to an improved model capacity and special abilities such as in-context learning that are not present in small-scale language models like BERT. The recent study on scaling laws for neural language models by #REFR provides empirical evidence that the loss scales as a power-law with model size, dataset size, and the amount of compute used for training, which sheds light on the optimal allocation of a fixed compute budget for training LLMs.", "given_citation_intent": "background", "given_keywords": ["Transformer language models"]}]}
{"citing_id": "2304.00759v2", "cited_id": "1905.08114", "section_title": "B. Heterogeneous Models", "citation": "The basic ideas of data-free KD are to optimize noise inputs to minimize the distance to prior knowledge #REFR , and Chen et al.", "text_before_citation": ["In RHFL #OTHEREFR , a server calculates the weights of clients by the symmetric cross-entropy loss function, and clients distilled knowledge from the unlabeled dataset.", "FCCL #OTHEREFR computed a cross-correlation matrix also based on the unlabeled public dataset.", "2) Data-free knowledge distillation: However, the former methods using KD in FL acquire a public dataset.", "The server may not collect sufficient data due to data availability and privacy concerns.", "In contrast to the aforementioned methods, data-free KD is a novel approach to complete the knowledge distillation process without the training data."], "text_after_citation": ["#OTHEREFR train The process for FedIN is described as follows.", "First, the extractor and the classifier are updated by the averaged weights we and wc from the server.", "The second step of the clients is training their models from the local private dataset and completing the IN training for the feature inputs and outputs (s in , sout) from the server.", "Generative Adversarial Networks (GANs) #OTHEREFR to generate training data for the entire KD process, utilizing the knowledge distilled from the teacher model.", "To free the limitation from a public dataset, a few researches consider data-free KD in FL."], "citing_paper_content": {"title": "Fedin: Federated Intermediate Layers Learning For Model Heterogeneity", "abstract": "Federated learning (FL) facilitates edge devices to cooperatively train a global shared model while maintaining the training data locally and privately. However, a common but impractical assumption in FL is that the participating edge devices possess the same required resources and share identical global model architecture. In this study, we propose a novel FL method called Federated Intermediate Layers Learning (FedIN), supporting heterogeneous models without utilizing any public dataset. The training models in FedIN are divided into three parts, including an extractor, the intermediate layers, and a classifier. The model architectures of the extractor and classifier are the same in all devices to maintain the consistency of the intermediate layer features, while the architectures of the intermediate layers can vary for heterogeneous devices according to their resource capacities. To exploit the knowledge from features, we propose IN training, training the intermediate layers in line with the features from other clients. Additionally, we formulate and solve a convex optimization problem to mitigate the gradient divergence problem induced by the conflicts between the IN training and the local training. The experiment results show that FedIN achieves the best performance in the heterogeneous model environment compared with the state-of-the-art algorithms. Furthermore, our ablation study demonstrates the effectiveness of IN training and the solution to the convex optimization problem."}, "cited_paper_content": {"title": "Zero-Shot Knowledge Distillation In Deep Networks", "abstract": "Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method \"Zero-Shot Knowledge Distillation\" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets."}, "keywords": ["prior knowledge"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "However, in contrast to the aforementioned methods, data-free KD is a novel approach to complete the knowledge distillation process without the training data, as proposed in \"Zero-Shot Knowledge Distillation In Deep Networks\" #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Data-free knowledge distillation is a novel approach to complete the knowledge distillation process without the training data, as proposed in \"Zero-Shot Knowledge Distillation In Deep Networks\" #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Prior knowledge is required for most knowledge distillation methods, but accessing the dataset on which the teacher model has been trained may not always be feasible due to privacy or safety concerns; however, the proposed data-free method in Zero-Shot Knowledge Distillation In Deep Networks #REFR synthesizes data impressions from the teacher model and utilizes these as surrogates for the original training data samples to transfer its learning to the student via knowledge distillation.", "given_citation_intent": "background", "given_keywords": ["prior knowledge"]}]}
{"citing_id": "2303.14770v1", "cited_id": "1311.1249", "section_title": "Corpora Coverage", "citation": "We used the SDSL library #REFR to implement our corpus indexer. We index each corpus separately.", "text_before_citation": ["(2016) and use the FM-Index #OTHEREFR that utilises the text compressibility vi the Burrows-Wheeler transformation (BWT) #OTHEREFR of the text. The BWT is defined as, BWT", "[i] = [SA[i] \u2212 1 mod |T |].", "Searching for a sequence in BWT is done in reverse order and requires O(|u| log |\u03c3|).", "For more details on BWT and reverse searching, refer to #OTHEREFR .", "The CSA is at the core of Koala's index and search backbone."], "text_after_citation": ["Once a corpus is indexed, its constructed index sits on disk and could be queried through the Koala web interface (introduced shortly).", "Each query is launched into the indexed collection of corpora and returns the hit counts of the query in the corresponding corpus.", "Table 1 reports the time and memory usage for construction of indexes."], "citing_paper_content": {"title": "Koala: An Index For Quantifying Overlaps With Pre-Training Corpora", "abstract": "In very recent years more attention has been placed on probing the role of pre-training data in Large Language Models (LLMs) downstream behaviour. Despite the importance, there is no public tool that supports such analysis of pre-training corpora at large scale. To help research in this space, we launch Koala, a searchable index over large pretraining corpora using compressed suffix arrays with highly efficient compression rate and search support. In its first release we index the public proportion of OPT 175B pretraining data. Koala provides a framework to do forensic analysis on the current and future benchmarks as well as to assess the degree of memorization in the output from the LLMs. Koala is available for public use at https: //koala-index.erc.monash.edu/ * Corresponding author 1 We plan to extend our coverage of pre-training corpora."}, "cited_paper_content": {"title": "From Theory To Practice: Plug And Play With Succinct Data Structures", "abstract": "Engineering efficient implementations of compact and succinct structures is time-consuming and challenging, since there is no standard library of easy-to-use, highly optimized, and composable components. One consequence is that measuring the practical impact of new theoretical proposals is difficult, since older baseline implementations may not rely on the same basic components, and reimplementing from scratch can be time-consuming. In this paper we present a framework for experimentation with succinct data structures, providing a large set of configurable components, together with tests, benchmarks, and tools to analyze resource requirements. We demonstrate the functionality of the framework by recomposing two succinct solutions for top-k document retrieval which can operate on both character and integer alphabets."}, "keywords": ["corpus indexer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors utilized a framework for experimentation with succinct data structures, providing a large set of configurable components, together with tests, benchmarks, and tools to analyze resource requirements, as presented in the reference paper \"From Theory To Practice: Plug And Play With Succinct Data Structures\" #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement the compressed suffix arrays with highly efficient compression rate and search support, the authors followed the method presented in \"From Theory To Practice: Plug And Play With Succinct Data Structures\" #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To build Koala's highly efficient #OTHEREFR corpus indexer, the authors utilized a framework for experimentation with succinct data structures, providing a large set of configurable components, as presented in the paper \"From Theory To Practice: Plug And Play With Succinct Data Structures\" #REFR.", "given_citation_intent": "method", "given_keywords": ["corpus indexer"]}]}
{"citing_id": "2304.02313v1", "cited_id": "1609.08144", "section_title": "Feature Extraction", "citation": "For the text modality, we utilize WordPiece #REFR to generate a vocabulary, and randomly initialize a matrix E \u2208 R n E \u00d7d E for word embeddings, where n E denotes the length of the vocabulary, d E represents the feature dimension. E is optimized during training.", "text_before_citation": ["We extract features from multimodal information as well as MBTI personality information.", "For V, we use the same feature extraction method as HERO #OTHEREFR .", "That is, employ pre-training 2D ResNet-152 #OTHEREFR and 3D SlowFast #OTHEREFR to extract video feature expressions V 2D \u2208 R n V \u00d7d 2D and V 3D \u2208 R n V \u00d7d 3D .", "To acquire the final feature V \u2208 R n V \u00d7d V , we concatenate V 2D and V 3D and lower the dimension through a linear layer, where n V represents the length of image frame sequence, and d V represents the feature dimension."], "text_after_citation": ["In particular, features obtained from various information are expressed as follows: D \u2208 R n D \u00d7l D \u00d7d E for dialogue information, B \u2208 R n B \u00d7l B \u00d7d E for behavior description, and A \u2208 R n A \u00d7l A \u00d7d E for multiple choice option.", "Here, n D denotes the number of dialogue utterances, n B expresses the number of behavior descriptions, n A represents the number of multiple choice options, l D , l B and l A signify the sentence length, respectively.", "To represent P, we concatenate the name tags and the MBTI personalities of characters into phrases and use E to retrieve the personality feature P \u2208 R n P \u00d7l P \u00d7d E .", "Here, n P denotes the number of relevant characters in VC, and l P represents the phrases length.", "To enhance the personality feature, we concatenate the initial personality feature with the feature through the Self-Attention module to obtain final P C by"], "citing_paper_content": {"title": "Personality-Aware Human-Centric Multimodal Reasoning: A New Task", "abstract": "Multimodal reasoning, an area of artificial intelligence that aims at make inferences from multimodal signals such as vision, language and speech, has drawn more and more attention in recent years. People with different personalities may respond differently to the same situation. However, such individual personalities were ignored in the previous studies. In this work, we introduce a new Personality-aware Human-centric Multimodal Reasoning (Personality-aware HMR) task, and accordingly construct a new dataset based on The Big Bang Theory television shows, to predict the behavior of a specific person at a specific moment, given the multimodal information of its past and future moments. The Myers-Briggs Type Indicator (MBTI) was annotated and utilized in the task to represent individuals' personalities. We benchmark the task by proposing three baseline methods, two were adapted from the related tasks and one was newly proposed for our task. The experimental results demonstrate that personality can effectively improve the performance of human-centric multimodal reasoning. To further solve the lack of personality annotation in real-life scenes, we introduce an extended task called Personality-predicted HMR, and propose the corresponding methods, to predict the MBTI personality at first, and then use the predicted personality to help multimodal reasoning. The experimental results show that our method can accurately predict personality and achieves satisfactory multimodal reasoning performance without relying on personality annotations."}, "cited_paper_content": {"title": "Google'S Neural Machine Translation System: Bridging The Gap Between Human And Machine Translation", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."}, "keywords": ["feature dimension", "word embeddings"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To acquire the final feature V \u2208 R n V \u00d7d V , we concatenate V 2D and V 3D and lower the dimension through a linear layer, where n V represents the length of image frame sequence, and d V represents the feature dimension, following the feature extraction method proposed in HERO #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To acquire the final feature V, the authors concatenate V 2D and V 3D and lower the dimension through a linear layer, as proposed in HERO #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To acquire the final feature V \u2208 R n V \u00d7d V , we concatenate V 2D and V 3D and lower the dimension through a linear layer, where n V represents the length of image frame sequence, and d V represents the feature dimension, as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["feature dimension", "word embeddings"]}]}
{"citing_id": "2303.08594v2", "cited_id": "1604.01685", "section_title": "B. Additional Datasets B.1. Cityscapes", "citation": "Cityscapes #REFR is a high-resolution (1024\u00d72048 pixels) street-view dataset that contains 2975 training, 500 validation, and 1525 testing images.", "text_before_citation": [], "text_after_citation": ["We evaluate the performance of FastInst in terms of instance segmentation AP over eight semantic classes of the dataset. Training settings.", "We use a batch size of 16 and train the model for 90K iterations.", "We set the initial learning rate as 0.0001 and drop it by multiplying 0.1 at 0.9 and 0.95 fractions of the total number of training steps.", "During training, we randomly resize the image to a shorter edge from 800 to 1024 pixels with a step of 32 pixels, followed by a crop size of 512\u00d71024.", "During inference, we operate on the full image with a resolution of 1024\u00d72048. Results."], "citing_paper_content": {"title": "Fastinst: A Simple Query-Based Model For Real-Time Instance Segmentation", "abstract": "Recent attention in instance segmentation has focused on query-based models. Despite being non-maximum suppression (NMS)-free and end-to-end, the superiority of these models on high-accuracy real-time benchmarks has not been well demonstrated. In this paper, we show the strong potential of query-based models on efficient instance segmentation algorithm designs. We present FastInst, a simple, effective query-based framework for real-time instance segmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while yielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells and whistles. Specifically, FastInst follows the meta-architecture of recently introduced Mask2Former. Its key designs include instance activation-guided queries, dual-path update strategy, and ground truth mask-guided learning, which enable us to use lighter pixel decoders, fewer Transformer decoder layers, while achieving better performance. The experiments show that FastInst outperforms most state-ofthe-art real-time counterparts, including strong fully convolutional baselines, in both speed and accuracy. Code can be found at https://github.com/junjiehe96/ FastInst. Recently, with the success of DETR [4] in object detection, query-based single-stage instance segmentation methods [9, 10, 26, 44] have emerged. Instead of convolution, they exploit the versatile and powerful attention mechanism [40] combined with a sequence of learnable queries to infer the object class and segmentation mask. For example, Mask2Former [9] simplifies the workflow of instance segmentation by adding a pixel decoder and a maskedattention Transformer decoder on top of a backbone. Unlike previous methods [16, 43], Mask2Former does not require additional handcrafted components, such as training target assignment and NMS post-processing. While being simple, Mask2Former has its own issues: (1) it requires a large number of decoder layers to decode the object queries since its queries are learned static and need a lengthy process to refine; (2) It relies upon a heavy pixel decoder, e.g., multi-scale deformable attention Transformer (MSDefor-mAttn) [51], because its object segmentation mask straightforwardly depends on the output of the pixel decoder, which 1"}, "cited_paper_content": {"title": "The Cityscapes Dataset For Semantic Urban Scene Understanding", "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark."}, "keywords": ["Cityscapes"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, with the success of DETR [4] in object detection, query-based single-stage instance segmentation methods [9, 10, 26, 44] have emerged. Instead of convolution, they exploit the versatile and powerful attention mechanism [40] combined with a sequence of learnable queries to infer the object class and segmentation mask. For example, Mask2Former [9] simplifies the workflow of instance segmentation by adding a pixel decoder and a maskedattention Transformer decoder on top of a backbone. Unlike previous methods [16, 43], Mask2Former does not require additional handcrafted components, such as training target assignment and NMS post-processing. While being simple, Mask2Former has its own issues: (1) it requires a large number of decoder layers to decode the object queries since its queries are learned static and need a lengthy process to refine; (2) It relies upon a heavy pixel decoder, e.g., multi-scale deformable attention Transformer (MSDefor-mAttn) [51], because its object segmentation mask straightforwardly depends on the output of the pixel decoder, which 1 #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the lack of datasets that capture the complexity of real-world urban scenes for semantic urban scene understanding, the authors of Fastinst cite the Cityscapes dataset, which provides a benchmark suite and large-scale dataset for pixel-level and instance-level semantic labeling, and exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For semantic urban scene understanding, current datasets are not sufficient to capture the complexity of real-world urban scenes, but the recently introduced Cityscapes dataset provides a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling, which can be used as a valuable resource for future research. #REFR", "given_citation_intent": "background", "given_keywords": ["Cityscapes"]}]}
{"citing_id": "2303.12417v2", "cited_id": "1903.11027", "section_title": "Conclusion", "citation": "The zero-shot transfer results on various indoor and outdoor benchmarks validate the ability of CLIP #REFR for 3D open-world understanding.", "text_before_citation": ["In this paper, we present a novel contrastive languageimage-point cloud pretraining framework, CLIP 2 , which consists of a triplet proxy collection scheme and a crossmodal contrastive learning mechanism.", "Based on the observation that realistic scenarios contain a massive amount of open-world objects, we innovatively propose to collect triplet proxies from realistic scenes as pretraining data.", "We then conduct cross-modal contrastive alignment across language, image and point cloud feature space to learn transferable 3D representation."], "text_after_citation": [], "citing_paper_content": {"title": "Clip 2 : Contrastive Language-Image-Point Pretraining From Real-World Point Cloud Data", "abstract": "Indoor Scene Outdoor Scene Figure 1. Illustration of our open-world recognition results. Benefiting from our CLIP 2 , the 3D representation is aligned to the openworld language representation, which enables flexible zero-shot transfer. Best viewed in colors."}, "cited_paper_content": {"title": "Nuscenes: A Multimodal Dataset For Autonomous Driving", "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image-based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first published dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online at this http URL."}, "keywords": ["3D open-world understanding"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We collected triplet proxies from realistic scenes as pretraining data and conducted cross-modal contrastive alignment across language, image and point cloud feature space to learn transferable 3D representation using our CLIP 2 framework. This approach was inspired by the need to train and evaluate machine learning-based methods on datasets containing range sensor data along with images, as demonstrated in the #REFR paper.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our proposed CLIP 2 framework is evaluated on the nuScenes dataset, which contains a full autonomous vehicle sensor suite, including lidar, radar, and cameras, and has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our approach for 3D open-world understanding is aligned with the state-of-the-art, as demonstrated by the results on the nuScenes dataset #REFR.", "given_citation_intent": "result", "given_keywords": ["3D open-world understanding"]}]}
{"citing_id": "2304.09433v2", "cited_id": "2003.10555", "section_title": "Closedie", "citation": "We study the effectiveness of span-extractor models, which are commonly used in QA systems to extract the information that is relevant to a user-query from a provided document context #REFR .", "text_before_citation": [], "text_after_citation": ["Given the ground truth attributes, we evaluate the ability of these models to extract their values from the relevant paragraphs.", "We evaluate the DebertaV3 Large model fine-tuned on the Squad 2.0 dataset, which achieves 90.8 F1 on the Squad 2.0 dev set in Table 6 .", "We find our EVAPORATE function generation approach (Table 1 ) significantly outperforms this pre-trained QA model on ClosedIE in all settings, over text and HTML documents."], "citing_paper_content": {"title": "Language Models Enable Simple Systems For Generating Structured Views Of Heterogeneous Data Lakes", "abstract": "A long standing goal of the data management community is to develop general, automated systems that ingest semi-structured documents and output queryable tables without human effort or domain specific customization. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using large language models (LLMs). LLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions. We propose and evaluate EVAPORATE, a simple, prototype system powered by LLMs. We identify two fundamentally different strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended code synthesis implementation, EVAPORATE-CODE+, which achieves better quality than direct extraction. Our key insight is to generate many candidate functions and ensemble their extractions using weak supervision. EVAPORATE-CODE+ not only outperforms the state-of-the art systems, but does so using a sublinear pass over the documents with the LLM. This equates to a 110\u00d7 reduction in the number of tokens the LLM needs to process, averaged across 16 real-world evaluation settings of 10k documents each."}, "cited_paper_content": {"title": "Electra: Pre-Training Text Encoders As Discriminators Rather Than Generators", "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute."}, "keywords": ["span-extractor models"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose a more sample-efficient pre-training task called replaced token detection, which corrupts the input by replacing some tokens with plausible alternatives sampled from a small generator network, and trains a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. This approach substantially outperforms the ones learned by BERT given the same model size, data, and compute, and could potentially be used to improve the performance of systems like EVAPORATE. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the efficiency of pre-training language models, the authors propose a new method called replaced token detection, which replaces some tokens with plausible alternatives and trains a discriminative model to predict whether each token in the corrupted input was replaced by a generator sample or not, as described in Electra: Pre-Training Text Encoders As Discriminators Rather Than Generators. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the quality of the extraction while maintaining low cost, EVAPORATE-CODE+ generates many candidate functions and ensemble their extractions using weak supervision, which achieves better quality than direct extraction of values from documents with the LLM, as demonstrated in the evaluation, and is a promising approach for span-extractor models. #REFR", "given_citation_intent": "method", "given_keywords": ["span-extractor models"]}]}
{"citing_id": "2304.14509v1", "cited_id": "2003.08061", "section_title": "Xai In The Field Of Biometrics And Pad", "citation": "The authors in #REFR proposed a method that includes two sub-networks, a spatial network, and a temporal network.", "text_before_citation": ["#OTHEREFR proposed a novel deep learning framework for oneclass classification, called Deep SVDD (Support Vector Data Description), which learns a high-dimensional data representation using a deep neural network. Zee et al.", "#OTHEREFR presented in their work the interpretability potential of a Siamese CNN to assist humans in hard prediction tasks i.e.", "authors used (CAM) to primarily know where to look in the subject images to correctly understand the decisions made by black box CNN models.", "In #OTHEREFR , the authors used the depth map and the Remote Photoplethysmography (rPPG) signal as the auxiliary supervision to enhance the effectiveness of the face anti-spoofing method using a Residual Neural Network.", "In recent years, authors are trying to exploit the methodologies of explainability and interpretability of a ML models in the field of biometrics, in particular, face PAD by making attempts such as depth map #OTHEREFR [39], #OTHEREFR , producing saliency maps for CNN models #OTHEREFR , #OTHEREFR or some relevant works on estimating the patterns that define a spoofed sample #OTHEREFR , #OTHEREFR ."], "text_after_citation": ["The spatial network processes the input image to extract spatial features, while the temporal network processes a sequence of images to capture temporal features.", "In #OTHEREFR , the authors proposed a Central Difference Convolutional Network (CDCN) architecture that is specifically designed for this task.", "The CDCN is a deep neural network that accepts a face image as input and outputs a probability score indicating whether the input image is genuine or spoofed.", "The authors in #OTHEREFR proposed an approach consisting of two stages: the metatraining stage and the meta-testing stage.", "In the meta-training stage, a meta-learner is trained to learn a good initialization for the feature extractor network that is used to extract features from face images."], "citing_paper_content": {"title": "An Efficient Ensemble Explainable Ai (Xai) Approach For Morphed Face Detection", "abstract": "The extensive utilization of biometric authentication systems have emanated attackers / imposters to forge user identity based on morphed images. In this attack, a synthetic image is produced and merged with genuine. Next, the resultant image is user for authentication. Numerous deep neural convolutional architectures have been proposed in literature for face Morphing Attack Detection (MADs) to prevent such attacks and lessen the risks associated with them. Although, deep learning models achieved optimal results in terms of performance, it is difficult to understand and analyse these networks since they are black box/opaque in nature. As a consequence, incorrect judgments may be made. There is, however, a dearth of literature that explains decision-making methods of black box deep learning models for biometric Presentation Attack Detection (PADs) or MADs that can aid the biometric community to have trust in deep learning-based biometric systems for identification and authentication in various security applications such as border control, criminal database establishment etc. In this work, we present a novel visual explanation approach named Ensemble XAI integrating Saliency maps, Class Activation Maps (CAM) and Gradient-CAM (Grad-CAM) to provide a more comprehensive visual explanation for a deep learning prognostic model (EfficientNet-B1) that we have employed to predict whether the input presented to a biometric authentication system is morphed or genuine. The experimentations have been performed on three publicly available datasets namely Face Research Lab London Set, Wide Multi-Channel Presentation Attack (WMCA), and Makeup Induced Face Spoofing (MIFS). The experimental evaluations affirms that the resultant visual explanations highlight more fine-grained details of image features/areas focused by EfficientNet-B1 to reach decisions along with appropriate reasoning."}, "cited_paper_content": {"title": "Deep Spatial Gradient And Temporal Depth Learning For Face Anti-Spoofing", "abstract": "Face anti-spoofing is critical to the security of face recognition systems. Depth supervised learning has been proven as one of the most effective methods for face anti-spoofing. Despite the great success, most previous works still formulate the problem as a single-frame multi-task one by simply augmenting the loss with depth, while neglecting the detailed fine-grained information and the interplay between facial depths and moving patterns. In contrast, we design a new approach to detect presentation attacks from multiple frames based on two insights: 1) detailed discriminative clues (e.g., spatial gradient magnitude) between living and spoofing face may be discarded through stacked vanilla convolutions, and 2) the dynamics of 3D moving faces provide important clues in detecting the spoofing faces. The proposed method is able to capture discriminative details via Residual Spatial Gradient Block (RSGB) and encode spatio-temporal information from Spatio-Temporal Propagation Module (STPM) efficiently. Moreover, a novel Contrastive Depth Loss is presented for more accurate depth supervision. To assess the efficacy of our method, we also collect a Double-modal Anti-spoofing Dataset (DMAD) which provides actual depth for each sample. The experiments demonstrate that the proposed approach achieves state-of-the-art results on five benchmark datasets including OULU-NPU, SiW, CASIA-MFSD, Replay-Attack, and the new DMAD. Codes will be available at https://github.com/clks-wzz/FAS-SGTD."}, "keywords": ["spatial network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In recent years, authors are trying to exploit the methodologies of explainability and interpretability of a ML models in the field of biometrics, in particular, face PAD by making attempts such as depth map #REFR [39], #OTHEREFR , producing saliency maps for CNN models #OTHEREFR , #OTHEREFR or some relevant works on estimating the patterns that define a spoofed sample #OTHEREFR , #OTHEREFR . The proposed method in #REFR presents a new approach to detect presentation attacks from multiple frames based on detailed fine-grained information and the interplay between facial depths and moving patterns, and achieves state-of-the-art results on five benchmark datasets including OULU-NPU, SiW, CASIA-MFSD, Replay-Attack, and the new DMAD.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To enhance the effectiveness of face anti-spoofing, the authors in this work used a Residual Neural Network and auxiliary supervision in the form of depth map and Remote Photoplethysmography (rPPG) signal, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In their work, Zee et al. used a Residual Neural Network with depth map and Remote Photoplethysmography (rPPG) signal as auxiliary supervision to enhance the effectiveness of the face anti-spoofing method using a Residual Spatial Gradient Block (RSGB) and Spatio-Temporal Propagation Module (STPM) to capture discriminative details and encode spatio-temporal information efficiently, which could be further improved by incorporating the proposed Contrastive Depth Loss #REFR. The citation sentence reflects the citation intent as method and includes the specified keyword \"spatial network\".", "given_citation_intent": "method", "given_keywords": ["spatial network"]}]}
{"citing_id": "2303.11040v1", "cited_id": "1807.01697", "section_title": "Discussion And Conclusion", "citation": "By conducting large-scale experiments on diverse 3D object detection models under corruptions, we draw some important findings, as summarized below: 1) In general, the corruption robustness of 3D object detection models is largely correlated with their clean performance, similar to the observation in #REFR .", "text_before_citation": ["In this paper, we systematically design 27 types of common corruptions in 3D object detection to benchmark corruption robustness of existing 3D object detectors.", "We establish three corruption robustness benchmarks-KITTI-C, nuScenes-C, and Waymo-C by synthesizing the corruptions on public datasets."], "text_after_citation": ["2) Among all corruption types, motion-level ones degrade the model performance most, which pose a significant threat to autonomous driving.", "Weather-level corruptions are also influential to models trained on normal weather.", "3) Among all 3D detectors, LiDAR-camera fusion models have better corruption robustness, especially under those that apply distortions to only one modality.", "However, they are also exposed to corruptions from both sensors, leading to degraded performance in this case.", "Besides, there is a trade-off between robustness under image corruptions and point cloud corruptions of fusion models."], "citing_paper_content": {"title": "Benchmarking Robustness Of 3D Object Detection To Common Corruptions In Autonomous Driving", "abstract": "3D object detection is an important task in autonomous driving to perceive the surroundings. Despite the excellent performance, the existing 3D detectors lack the robustness to real-world corruptions caused by adverse weathers, sensor noises, etc., provoking concerns about the safety and reliability of autonomous driving systems. To comprehensively and rigorously benchmark the corruption robustness of 3D detectors, in this paper we design 27 types of common corruptions for both LiDAR and camera inputs considering real-world driving scenarios. By synthesizing these corruptions on public datasets, we establish three corruption robustness benchmarks-KITTI-C, nuScenes-C, and Waymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object detection models to evaluate their corruption robustness. Based on the evaluation results, we draw several important findings, including: 1) motion-level corruptions are the most threatening ones that lead to significant performance drop of all models; 2) LiDAR-camera fusion models demonstrate better robustness; 3) camera-only models are extremely vulnerable to image corruptions, showing the indispensability of LiDAR point clouds. We release the benchmarks and codes at https://github.com/kkkcx/ 3D_Corruptions_AD. We hope that our benchmarks and findings can provide insights for future research on developing robust 3D object detection models."}, "cited_paper_content": {"title": "Benchmarking Neural Network Robustness To Common Corruptions And Surface Variations", "abstract": "In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize."}, "keywords": ["3D object detection", "corruption robustness"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this paper, we systematically design 27 types of common corruptions in 3D object detection to benchmark corruption robustness of existing 3D object detectors. We establish three corruption robustness benchmarks-KITTI-C, nuScenes-C, and Waymo-C by synthesizing the corruptions on public datasets. Our benchmarking approach is inspired by the work of Hendrycks and Dietterich #REFR, who established rigorous benchmarks for image classifier robustness and proposed a new dataset called ImageNet-P to evaluate performance on common corruptions and perturbations.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our corruption robustness benchmarks-KITTI-C, nuScenes-C, and Waymo-C, are established by synthesizing 27 types of common corruptions on public datasets, and can be used to compare the performance of existing 3D object detectors with those in the benchmark paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our benchmarking results show that the corruption robustness of existing 3D object detectors in real-world scenarios is still limited, which is consistent with the findings in the reference paper #REFR on benchmarking neural network robustness to common corruptions and surface variations.", "given_citation_intent": "result", "given_keywords": ["3D object detection", "corruption robustness"]}]}
{"citing_id": "2304.01401v1", "cited_id": "1505.04597", "section_title": "I. Introduction", "citation": "One popular backbone of the deep learning model for segmentation is U-Net #REFR , which is a general CNN model with an encoder and decoder structure ( Fig. 1(a) ).", "text_before_citation": ["Medical image segmentation aims to use machine learning models (e.g., Convolutional Neural Networks or CNNs for short) to automatically segment the target regions (organs or lesions) from the input medical images with different modalities #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR ."], "text_after_citation": ["The encoder path decomposes the input image from local to global deep features where the spatial size is gradually reduced by using the max-pooling operation.", "As the layer (Sheng He and Yangming Ou are the corresponding authors.) S. He, R. Bao, P. Grant and Y.", "Ou are with the Boston Children's Hospital and Harvard Medical School, Harvard University, 300 Longwood Ave., Boston, MA, USA.", "E-mail: heshengxgd@gmail.com; rina.bao@childrens.harvard.edu, ellen.grant@childrens.harvard.edu, yangming.ou@childrens.harvard.edu goes to deep, it extracts the high-level contextual information by discarding the detailed information on each local pixel to remove noise and irrelevant information #OTHEREFR .", "To recover the lost detailed spatial information, the decoder path hierarchically fuses global features from the output of the encoder and local features from the intermediate output of the encoder #OTHEREFR for computing the final segmentation map. In summary, as shown in Fig."], "citing_paper_content": {"title": "U-Netmer: U-Net Meets Transformer For Medical Image Segmentation", "abstract": "The combination of the U-Net based deep learning models and Transformer is a new trend for medical image segmentation. U-Net can extract the detailed local semantic and texture information and Transformer can learn the long-rang dependencies among pixels in the input image. However, directly adapting the Transformer for segmentation has \"token-flatten\" problem (flattens the local patches into 1D tokens which losses the interaction among pixels within local patches) and \"scalesensitivity\" problem (uses a fixed scale to split the input image into local patches). Compared to directly combining U-Net and Transformer, we propose a new global-local fashion combination of U-Net and Transformer, named U-Netmer, to solve the two problems. The proposed U-Netmer splits an input image into local patches. The global-context information among local patches is learnt by the self-attention mechanism in Transformer and U-Net segments each local patch instead of flattening into tokens to solve the 'token-flatten\" problem. The U-Netmer can segment the input image with different patch sizes with the identical structure and the same parameter. Thus, the U-Netmer can be trained with different patch sizes to solve the \"scale-sensitivity\" problem. We conduct extensive experiments in 7 public datasets on 7 organs (brain, heart, breast, lung, polyp, pancreas and prostate) and 4 imaging modalities (MRI, CT, ultrasound, and endoscopy) to show that the proposed U-Netmer can be generally applied to improve accuracy of medical image segmentation. These experimental results show that U-Netmer provides stateof-the-art performance compared to baselines and other models. In addition, the discrepancy among the outputs of U-Netmer with different scales is linearly correlated to the segmentation accuracy which can be considered as a confidence score to rank test images by difficulty without ground-truth. The code will be available on GitHub."}, "cited_paper_content": {"title": "U-Net: Convolutional Networks For Biomedical Image Segmentation", "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."}, "keywords": ["segmentation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Medical image segmentation aims to use machine learning models (e.g., Convolutional Neural Networks or CNNs for short) to automatically segment the target regions (organs or lesions) from the input medical images with different modalities (other references). Recently, U-Net has been widely used in medical image segmentation due to its excellent performance in capturing local semantic and texture information. In particular, the U-Net model has been shown to outperform the prior best method on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Medical image segmentation using deep learning models requires a large number of annotated samples for successful training #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Medical image segmentation has been revolutionized by deep learning models such as U-Net #REFR, which have shown remarkable performance in extracting detailed local semantic and texture information for accurate segmentation.", "given_citation_intent": "background", "given_keywords": ["segmentation"]}]}
{"citing_id": "2303.08029v1", "cited_id": "1606.00915", "section_title": "Introduction", "citation": "For multiscale feature aggregation, Deeplab #REFR uses various dilation convolutions to capture contextual information at multiple scales.", "text_before_citation": ["We selected pixel features of the train(olive points) and road(green points) for tsne visualization.", "the pixel feature representation #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR .", "The other is to obtain contextual information to enhance the pixel representation #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR .", "This paper investigates the same direction as the latter, with the aim of how to obtain richer contextual/semantic information to improve the performance of segmentation.", "The methods obtaining contextual information are broadly divided into two types, multi-scale feature aggregation and relational contextual aggregation."], "text_after_citation": ["PSPNet #OTHEREFR introduces pyramid spatial pooling to aggregate contextual information.", "For relational contextual aggregation, ACFNet #OTHEREFR and OCRNet #OTHEREFR divide pixels in an image into multiple regions and then increase the pixel representation by weighting the aggregated region representation, the weights are determined by the relationship between the pixels and the regions.", "Although the above methods are effective, they ignore the potential contextual information between the input images.", "In other words,there is no consideration of using class-level features beyond image to enhance the pixel representations.", "In order to obtain class-level features beyond the input image, MCIBI #OTHEREFR proposes to use simulated annealing to find a semantic feature on each class."], "citing_paper_content": {"title": "Class-Level Multiple Distributions Representation Are Necessary For Semantic Segmentation", "abstract": "Existing approaches focus on using class-level features to improve semantic segmentation performance. How to characterize the relationships of intra-class pixels and inter-class pixels is the key to extract the discriminative representative class-level features. In this paper, we introduce for the first time to describe intra-class variations by multiple distributions. Then, multiple distributions representation learning(MDRL) is proposed to augment the pixel representations for semantic segmentation. Meanwhile, we design a class multiple distributions consistency strategy to construct discriminative multiple distribution representations of embedded pixels. Moreover, we put forward a multiple distribution semantic aggregation module to aggregate multiple distributions of the corresponding class to enhance pixel semantic information. Our approach can be seamlessly integrated into popular segmentation frameworks FCN/PSPNet/CCNet and achieve 5.61%/1.75%/0.75% mIoU improvements on ADE20K. Extensive experiments on the Cityscapes, ADE20K datasets have proved that our method can bring significant performance improvement."}, "cited_paper_content": {"title": "Deeplab: Semantic Image Segmentation With Deep Convolutional Nets, Atrous Convolution, And Fully Connected Crfs", "abstract": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First , we highlight convolution with upsampled filters, or \u2018atrous convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second , we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third , we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online."}, "keywords": ["multiscale feature aggregation", "Deeplab"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The methods obtaining contextual information are broadly divided into two types, multi-scale feature aggregation and relational contextual aggregation. To improve the performance of segmentation, the authors propose a multiple distribution semantic aggregation module that aggregates multiple distributions of the corresponding class, as well as atrous spatial pyramid pooling (ASPP) and fully connected Conditional Random Field (CRF) from #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain richer contextual/semantic information for semantic segmentation, methods are broadly divided into two types, multi-scale feature aggregation and relational contextual aggregation. A recent study proposed a method that combines both approaches, using atrous convolution and atrous spatial pyramid pooling to capture objects and image context at multiple scales, and a fully connected Conditional Random Field to improve localization performance #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve richer contextual/semantic information for segmentation, methods obtaining contextual information are broadly divided into two types, multi-scale feature aggregation and relational contextual aggregation. A popular method for multi-scale feature aggregation is Deeplab #REFR.", "given_citation_intent": "method", "given_keywords": ["multiscale feature aggregation", "Deeplab"]}]}
{"citing_id": "2304.13986v1", "cited_id": "1512.03385", "section_title": "Ablation Study", "citation": "Case (a) is our baseline which contains ResBlock #REFR with a similar number of parameters as OCT module.", "text_before_citation": ["In this part, we conduct ablation studies on Set11 dataset for our OCTUF whose iteration number is 10.", "Break-down Ablation.", "We first conduct a break-down ablation experiment in the case of CS ratio = 50% to investigate the effect of each component towards higher performance. The results are listed in Tab. 3."], "text_after_citation": ["When we successively apply our FFN and Dual-CA sub-modules respectively, namely Cases (b) and (c), the model achieves 0.71 dB and 2.91 dB improvements.", "And the model can greatly enhance 3.09 dB gains with little storage place when both sub-modules are used together.", "We also discuss the effect of the LayerNorm (LN) function in Case (d), which addresses that our OCTUF achieves better performance with the LayerNorm function.", "Note that without \"LayerNorm\" represents removing all LN from our OCTUF.", "What is more, we train our models with different learning rates as seen from Cases (e), (f), and (g). Table 4 ."], "citing_paper_content": {"title": "Optimization-Inspired Cross-Attention Transformer For Compressive Sensing", "abstract": "By integrating certain optimization solvers with deep neural networks, deep unfolding network (DUN) with good interpretability and high performance has attracted growing attention in compressive sensing (CS). However, existing DUNs often improve the visual quality at the price of a large number of parameters and have the problem of feature information loss during iteration. In this paper, we propose an Optimization-inspired Cross-attention Transformer (OCT) module as an iterative process, leading to a lightweight OCT-based Unfolding Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block. ISCA block introduces multi-channel inertia forces and increases the memory effect by a cross attention mechanism between adjacent iterations. And, PGCA block achieves an enhanced information interaction, which introduces the inertia force into the gradient descent step through a cross attention block. Extensive CS experiments manifest that our OCTUF achieves superior performance compared to state-of-the-art methods while training lower complexity. Codes are available at https : / / github. com / songjiechong / OCTUF."}, "cited_paper_content": {"title": "Deep Residual Learning For Image Recognition", "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."}, "keywords": ["ResBlock", "baseline"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this part, we conduct ablation studies on Set11 dataset for our OCTUF whose iteration number is 10. Break-down Ablation. We first conduct a break-down ablation experiment in the case of CS ratio = 50% to investigate the effect of each component towards higher performance. The results are listed in Tab. 3. As reported in the literature, deeper neural networks are more difficult to train. However, the authors of \"Deep Residual Learning For Image Recognition\" proposed a residual learning framework that eases the training of substantially deeper networks by reformulating the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To investigate the effect of each component towards higher performance, we conduct a break-down ablation experiment in the case of CS ratio = 50% on Set11 dataset for our OCTUF whose iteration number is 10 (Tab. 3) using the method proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To investigate the effect of each component towards higher performance, we conducted a break-down ablation experiment on Set11 dataset for our OCTUF whose iteration number is 10, and compared the results with the baseline ResBlock method #REFR.", "given_citation_intent": "method", "given_keywords": ["ResBlock", "baseline"]}]}
{"citing_id": "2304.09854v1", "cited_id": "1608.00272", "section_title": "Datasets And Metrics", "citation": "Ref-COCO #REFR 118k / 5k RIS mIoU Ref-COCO is a Reference Segmentation dataset based on the COCO.", "text_before_citation": ["#OTHEREFR 4,998 / 5,105 SS mIoU PASCAL Context dataset is an extension of PASCAL VOC containing 400+ classes (usually 59 most frequently).", "COCO #OTHEREFR 118k / 5k SS / IS / PS mIoU / mAP / PQ MS COCO dataset is a large-scale dataset with 80 thing categories and 91 stuff categories.", "ADE20k #OTHEREFR 20,210 / 2,000 SS / IS / PS mIoU / mAP / PQ ADE20k dataset is a large-scale dataset exhaustively annotated with pixel-level objects and object part labels.", "Cityscapes #OTHEREFR 2,975 / 500 SS / IS / PS mIoU / mAP / PQ Cityscapes dataset focuses on semantic understanding of urban street scenes, captured in 50 cities.", "Mapillary #OTHEREFR 18k / 2k SS / PS mIoU / PQ Mapillary dataset is a large-scale dataset with accurate high-resolution annotations."], "text_after_citation": ["VSPW #OTHEREFR 198k / 25k VSS mIoU VPSW is a larg-scale high-resolution dataset with long videos focusing on VSS.", "Youtube-VIS-2019 #OTHEREFR 95k / 14k VIS AP Extending from Youtube-VOS, Youtube-VIS is with exhaustive instance labels.", "VIP-Seg #OTHEREFR 67k / 8k VPS VPQ & STQ Extending from VSPW, VIP-Seg adds extra instance labels for VPS task.", "Cityscape-VPS #OTHEREFR 2,400 / 300 VPS VPQ Cityscapes-VPS dataset extracts from the val split of Cityscapes dataset, adding temporal annotations.", "KITTI-STEP #OTHEREFR 5,027 / 2,981 VPS STQ KITTI-STEP focuses on the long videos in the urban scenes."], "citing_paper_content": {"title": "Transformer-Based Visual Segmentation: A Survey", "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmenation-With-Transformer. We will also continually monitor developments in this rapidly evolving field."}, "cited_paper_content": {"title": "Modeling Context In Referring Expressions", "abstract": "Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer), shows the advantages of our methods for both referring expression generation and comprehension."}, "keywords": ["Reference Segmentation dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, referring expression models have been improved by incorporating better measures of visual context, such as visual comparison to other objects within an image, as shown in the reference paper \"Modeling Context In Referring Expressions\" (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To incorporate better measures of visual context into referring expression models, the authors could refer to the method proposed in \"Modeling Context In Referring Expressions\" (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To incorporate better measures of visual context into referring expression models, the authors of this manuscript could refer to the work of Mao et al. on modeling context in referring expressions using visual comparison to other objects within an image, as presented in their paper titled \"Modeling Context In Referring Expressions\" (#REFR). This method could be applied to improve segmentation results on the Reference Segmentation dataset.", "given_citation_intent": "method", "given_keywords": ["Reference Segmentation dataset"]}]}
{"citing_id": "2303.01911v1", "cited_id": "1910.10683", "section_title": "Related Work", "citation": "These results have since been confirmed for other monolingual LMs such as T5 #REFR and multilingual LMs such as XGLM , PALM (Chowdhery et al., 2022) , and ALEXATM (Soltan et al., 2022) .", "text_before_citation": ["Since the early attempts at using language models (LMs) as multi-task learners #OTHEREFR , MT has been a task of choice to gauge LMs' multilingual ability.", "Results for the zeroand few-shot ability of LMs were discussed for both GPT-2 and GPT-3 #OTHEREFR , which is especially intriguing as they were trained primarily on monolingual (English) data."], "text_after_citation": ["However, the focus has mainly been on global multi-task performance; often only a small part of the discussion is devoted to MT.", "Moreover, results are often only reported for a few well-resourced language pairs (e.g.", "English-French and English-German), and the scores reported (mostly BLEU), are hard to compare due to a non-systematic use of standardised evaluation protocols and metrics.", "3 There are however some in-depth analyses of MT performance of LLMs, each focusing on a specific LM's performance in a true multilingual setting with respect to prompt design and number of few-shots. For instance, #OTHEREFR", "(2022) reevaluate the MT performance of the multilingual PALM (Chowdhery et al., 2022) , focusing notably on the selection of few-shot examples."], "citing_paper_content": {"title": "Investigating The Translation Performance Of A Large Multilingual Language Model: The Case Of Bloom", "abstract": "The NLP community recently saw the release of a new large open-access multilingual language model, BLOOM (BigScience et al., 2022) covering 46 languages. We focus on BLOOM's multilingual ability by evaluating its machine translation performance across several datasets (WMT, Flores-101 and DiaBLa) and language pairs (high-and low-resourced). Our results show that 0-shot performance suffers from overgeneration and generating in the wrong language, but this is greatly improved in the few-shot setting, with very good results for a number of language pairs. We study several aspects including prompt design, model sizes, cross-lingual transfer and the use of discursive context."}, "cited_paper_content": {"title": "Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."}, "keywords": ["multilingual LMs", "monolingual LMs"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recent work has explored the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent work has explored the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent studies have shown that multilingual LMs, such as BLOOM, outperform monolingual LMs in zero-shot and few-shot settings for machine translation tasks (REFR).", "given_citation_intent": "result", "given_keywords": ["multilingual LMs", "monolingual LMs"]}]}
{"citing_id": "2304.01636v1", "cited_id": "1409.0575", "section_title": "Implementation Details.", "citation": "The weights of both the teacher and the student are initialized by training the networks on the ImageNet #REFR .", "text_before_citation": ["We perform experiments on ENet #OTHEREFR , PSPNet #OTHEREFR and BiseNet #OTHEREFR to evaluate the efficiency of the proposed method as they are three typical segmentation models of different structures.", "ENet is a classical Encoder-Decoder model, PSP-Net is a model without a decoder and BiseNet utilizes a compound structure."], "text_after_citation": ["We adopt the Stochastic Gradient Descent (SGD) update rule #OTHEREFR with the aforementioned loss function in Equation 3 to optimize the parameters of the network. Loss coefficient is set to 0.5.", "We adopt the 'poly' policy widely used in a lot of segmentation models to set the learning rate for each iteration, where the learning rate in an iteration equals to initial learning rate multiplied by (1 \u2212 ) .", "The power is set to 0.9 and the initial learning rate is set to 0.025 for student network and 0.007 for teacher network.", "All of the teacher networks are trained by 50 epochs.", "The student networks are trained by 150 epochs for TuSimple and 100 epochs for CULane."], "citing_paper_content": {"title": "Label-Guided Attention Distillation For Lane Segmentation", "abstract": "Contemporary segmentation methods are usually based on deep fully convolutional networks (FCNs). However, the layer-by-layer convolutions with a growing receptive field is not good at capturing longrange contexts such as lane markers in the scene. In this paper, we address this issue by designing a distillation method that exploits label structure when training segmentation network. The intuition is that the ground-truth lane annotations themselves exhibit internal structure. We broadcast the structure hints throughout a teacher network, i.e., we train a teacher network that consumes a lane label map as input and attempts to replicate it as output. Then, the attention maps of the teacher network are adopted as supervisors of the student segmentation network. The teacher network, with label structure information embedded, knows distinctly where the convolution layers should pay visual attention into. The proposed method is named as Label-guided Attention Distillation (LGAD). It turns out that the student network learns significantly better with LGAD than when learning alone. As the teacher network is deprecated after training, our method do not increase the inference time. Note that LGAD can be easily incorporated in any lane segmentation network. To validate the effectiveness of the proposed LGAD method, extensive experiments have been conducted on two popular lane detection benchmarks: TuSimple and CULane. The results show consistent improvement across a variety of convolutional neural network architectures. Specifically, we demonstrate the accuracy boost of LGAD on the lightweight model ENet. It turns out that the ENet-LGAD surpasses existing lane segmentation algorithms."}, "cited_paper_content": {"title": "Imagenet Large Scale Visual Recognition Challenge", "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements."}, "keywords": ["ImageNet"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate the efficiency of the proposed method, we perform experiments on three typical segmentation models of different structures, including ENet, PSPNet, and BiseNet #REFR, which have shown significant advances in object recognition and large-scale image classification.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of segmentation models, the proposed Label-guided Attention Distillation (LGAD) method exploits label structure information from the ground-truth annotations, which is inspired by the benchmark dataset created in the Imagenet Large Scale Visual Recognition Challenge #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the segmentation performance of ENet, PSPNet, and BiseNet, the proposed Label-guided Attention Distillation (LGAD) method leverages the label structure information from ImageNet #REFR to guide the attention maps of the teacher network, which significantly enhances the student network's learning ability.", "given_citation_intent": "method", "given_keywords": ["ImageNet"]}]}
{"citing_id": "2303.05566v1", "cited_id": "1710.07009", "section_title": "A. Nonlinear Filtering For Discrete-Time Systems", "citation": "The above derivation converts the problem into a fully observed controlled Markov process (\u03a0, u) via an enlargement of the state space, where control policies and even optimal control policies can be synthesized accordingly for the (hypothetically) fully observed \u03a0 #REFR .", "text_before_citation": ["1 {F (\u03c0t,yt,ut)\u2208D} \u2022 n(dy t ), D \u2208 B(P(X )).", "We also use \u03a0 u to emphasize the marginal behavior of the process (\u03a0, u).", "Given the observations and the adaptively generated control signal, the optimal estimation of the conditional probability of satisfying any \u03c9-regular formula \u03a8 is given by", "EQUATION", "Note that it is difficult to obtain the full knowledge of Y , our goal is to generate control policies such that the optimal estimation P \u00b50,u [X \u03a8 | Y ] possesses certain confidence of satisfying the probabilistic requirement given any realization of observation."], "text_after_citation": ["The policy fulfilling the goal mentioned above is thereby decidable.", "The construction of the optimal filter process (or the function F in (27)) can be decomposed into a two-step differential equation 8 .", "The approximation of such a solution already suffers from the curse of dimensionality.", "Using formal abstractions to enlarge the partially observed processes into the filter processes with full observations, based on which control policies can be determined and utilized back to the partially observed cases, seems tedious and impractical.", "Besides the theoretical formal guarantee of a confidence of a satisfaction probability (i.e., a probabilistic requirement of the probabilistic specification), the abstraction essentially solves the continuous probability law of a continuous conditional expectation (or a random measure) upon some process with discrete labels using discrete inclusions."], "citing_paper_content": {"title": "Robustly Complete Finite-State Abstractions For Control Synthesis Of Stochastic Systems", "abstract": "The essential step of abstraction-based control synthesis for nonlinear systems to satisfy a given specification is to obtain a finite-state abstraction of the original systems. The complexity of the abstraction is usually the dominating factor that determines the efficiency of the algorithm. For the control synthesis of discrete-time nonlinear stochastic systems modelled by nonlinear stochastic difference equations, recent literature has demonstrated the soundness of abstractions in preserving robust probabilistic satisfaction of \u03c9-regular lineartime properties. However, unnecessary transitions exist within the abstractions, which are difficult to quantify, and the completeness of abstraction-based control synthesis in the stochastic setting remains an open theoretical question. In this paper, we address this fundamental question from the topological view of metrizable space of probability measures, and propose constructive finite-state abstractions for control synthesis of probabilistic linear temporal specifications. Such abstractions are both sound and approximately complete. That is, given a concrete discrete-time stochastic system and an arbitrarily small L 1-perturbation of this system, there exists a family of finite-state controlled Markov chains that both abstracts the concrete system and is abstracted by the slightly perturbed system. In other words, given an arbitrarily small prescribed precision, an abstraction always exists to decide whether a control strategy exists for the concrete system to satisfy the probabilistic specification."}, "cited_paper_content": {"title": "Asymptotic Optimality Of Finite Model Approximations For Partially Observed Markov Decision Processes With Discounted Cost", "abstract": "We consider finite model approximations of discrete-time partially observed Markov decision processes (POMDPs) under the discounted cost criterion. After converting the original partially observed stochastic control problem to a fully observed one on the belief space, the finite models are obtained through the uniform quantization of the state and action spaces of the belief space Markov decision process (MDP). Under mild assumptions on the components of the original model, it is established that the policies obtained from these finite models are nearly optimal for the belief space MDP, and so, for the original partially observed problem. The assumptions essentially require that the belief space MDP satisfies a mild weak continuity condition. We provide an example and introduce explicit approximation procedures for the quantization of the set of probability measures on the state space of POMDP (i.e., belief space)."}, "keywords": ["state space"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To obtain nearly optimal policies for partially observed Markov decision processes, the authors in #REFR propose finite model approximations through uniform quantization of the state and action spaces of the belief space Markov decision process, which are established to be nearly optimal for the original problem under mild assumptions on the components of the original model.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Finite model approximations have been shown to be nearly optimal for partially observed Markov decision processes under the discounted cost criterion, as established in the reference paper by #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Finite model approximations have been used to solve partially observed Markov decision processes (POMDPs) under the discounted cost criterion, where the state space is quantized, and the policies obtained from these finite models are nearly optimal for the belief space MDP, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["state space"]}]}
{"citing_id": "2303.03922v1", "cited_id": "1907.11692", "section_title": "Training Details.", "citation": "Table 6 , we compare KGTransformer to recently proposed QA methods and report their accuracy on inhouse valid and test sets. All baselines are using RoBERTa-Large #REFR as language encoder.", "text_before_citation": ["We apply RoBERTa-large #OTHEREFR as language encoder with the first 1024 dimensional hidden state in the sequence as .", "We transform into a 768-dimensional one through a transformation matrix.", "We regard P and a triple in G containing words in W as related during the construction of .", "We tune the model with batch size set to 128 and Adam #OTHEREFR whose initial rate set as 0.00001.", "In the first 4 tuning epochs, the RoBERTa-large is frozen."], "text_after_citation": [], "citing_paper_content": {"title": "Structure Pretraining And Prompt Tuning For Knowledge Graph Transfer", "abstract": "Knowledge graphs (KG) are essential background knowledge providers in many tasks. When designing models for KG-related tasks, one of the key tasks is to devise the Knowledge Representation and Fusion (KRF) module that learns the representation of elements from KGs and fuses them with task representations. While due to the difference of KGs and perspectives to be considered during fusion across tasks, duplicate and ad hoc KRF modules design are conducted among tasks. In this paper, we propose a novel knowledge graph pretraining model KGTransformer that could serve as a uniform KRF module in diverse KG-related tasks. We pretrain KGTransformer with three self-supervised tasks with sampled sub-graphs as input. For utilization, we propose a general prompttuning mechanism regarding task data as a triple prompt to allow flexible interactions between task KGs and task data. We evaluate pretrained KGTransformer on three tasks, triple classification, zeroshot image classification, and question answering. KGTransformer consistently achieves better results than specifically designed task models. Through experiments, we justify that the pretrained KG-Transformer could be used off the shelf as a general and effective KRF module across KG-related tasks. The code and datasets are available at https://github.com/zjukg/KGTransformer. CCS CONCEPTS \u2022 Computing methodologies \u2192 Knowledge representation and reasoning."}, "cited_paper_content": {"title": "Roberta: A Robustly Optimized Bert Pretraining Approach", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."}, "keywords": ["language encoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We apply RoBERTa-large #REFR as language encoder with the first 1024 dimensional hidden state in the sequence as . We transform into a 768-dimensional one through a transformation matrix. We regard P and a triple in G containing words in W as related during the construction of . We tune the model with batch size set to 128 and Adam #REFR whose initial rate set as 0.00001. In the first 4 tuning epochs, the RoBERTa-large is frozen. (Method)", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To pretrain our KGTransformer model, we followed the approach proposed in Roberta: A Robustly Optimized Bert Pretraining Approach #REFR, which carefully measures the impact of many key hyperparameters and training data size.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We use RoBERTa-large as our language encoder and transform the first 1024 dimensional hidden state in the sequence into a 768-dimensional one through a transformation matrix. Our method is inspired by the robust optimization approach proposed in #REFR, which carefully measures the impact of hyperparameters and training data size on language model pretraining.", "given_citation_intent": "method", "given_keywords": ["language encoder"]}]}
{"citing_id": "2304.06403v1", "cited_id": "2003.14266", "section_title": "Related Work", "citation": "To alleviate the need for large annotated datasets, weakly supervised techniques for video segmentation involve using transcripts (ordered list of the actions occurring in the video), visual similarities, and audio information to generate pseudo-labels for training #REFR .", "text_before_citation": ["Encoder-Decoder Temporal Convolutional Networks (ED-TCNs) #OTHEREFR use a hierarchy of temporal convolutions to perform fine-grained action segmentation, but they can act solely on low-temporal resolution videos.", "Instead, Multi-Stage Temporal Convolutional Network (MS-TCN and its improved version MS-TCN++) can act on the full temporal resolution of the videos and achieves increased performance #OTHEREFR .", "Spatio-temporal convolutional layers #OTHEREFR have shown promising results in capturing temporal dependencies while being easier to train than previous methods.", "The main drawback of traditional supervised approaches to action segmentation is the requirement of a large amount of quality labelled data for training, which limits their applicability to large-scale domains outside of existing presegmented datasets #OTHEREFR .", "Weakly and semi-supervised approaches."], "text_after_citation": ["In #OTHEREFR , a Gaussian Mixture Models + Convolutional Neural Networks (GMM+CNN) is first initialized and used to infer the segments of a video given a transcription of it.", "The new segmentation is used to re-estimate and update the model parameters until convergence.", "In #OTHEREFR , a recurrent neural network is used to model a discriminative representation of subactions, and a coarse probabilistic model to allow for temporal alignment and inference over long sequences.", "Some approaches use machine learning models to infer the segments of the video #OTHEREFR .", "Other approaches, such as those based on frame-to-frame visual similarities #OTHEREFR , self-attentions mechanism #OTHEREFR or iterative soft boundary assignment #OTHEREFR , enforce consistency between the video and labels without the need for temporal supervision."], "citing_paper_content": {"title": "Leveraging Triplet Loss For Unsupervised Action Segmentation", "abstract": "In this paper, we propose a novel fully unsupervised framework that learns action representations suitable for the action segmentation task from the single input video itself, without requiring any training data. Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions and a novel triplet selection strategy that effectively models temporal and semantic priors to discover actions in the new representational space. Under these circumstances, we successfully recover temporal boundaries in the learned action representations with higher quality compared with existing unsupervised approaches. The proposed method is evaluated on two widely used benchmark datasets for the action segmentation task and it achieves competitive performance by applying a generic clustering algorithm on the learned representations. 1 * Work done during an internship at the IRI."}, "cited_paper_content": {"title": "Sct: Set Constrained Temporal Transformer For Set Supervised Action Segmentation", "abstract": "Temporal action segmentation is a topic of increasing interest, however, annotating each frame in a video is cumbersome and costly. Weakly supervised approaches therefore aim at learning temporal action segmentation from videos that are only weakly labeled. In this work, we assume that for each training video only the list of actions is given that occur in the video, but not when, how often, and in which order they occur. In order to address this task, we propose an approach that can be trained end-to-end on such data. The approach divides the video into smaller temporal regions and predicts for each region the action label and its length. In addition, the network estimates the action labels for each frame. By measuring how consistent the frame-wise predictions are with respect to the temporal regions and the annotated action labels, the network learns to divide a video into class-consistent regions. We evaluate our approach on three datasets where the approach achieves state-of-the-art results."}, "keywords": ["video segmentation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Weakly supervised approaches to action segmentation have been proposed to learn temporal action segmentation from videos that are only weakly labeled. #REFR proposed an approach that can be trained end-to-end on such data by dividing the video into smaller temporal regions and predicting for each region the action label and its length, achieving state-of-the-art results on three datasets.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Weakly supervised approaches have been proposed to learn temporal action segmentation from videos that are only weakly labeled, where only the list of actions is given that occur in the video, but not when, how often, and in which order they occur #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Sct: Set Constrained Temporal Transformer For Set Supervised Action Segmentation #REFR proposes an approach that can be trained end-to-end on weakly labeled videos by dividing the video into smaller temporal regions and predicting for each region the action label and its length, which is relevant for video segmentation.", "given_citation_intent": "method", "given_keywords": ["video segmentation"]}]}
{"citing_id": "2304.14557v1", "cited_id": "1712.08147", "section_title": "Related Work", "citation": "Interestingly, our reduction of k-cycles essentially mirrors the construction in the proof of Theorem 3.1 in #REFR .", "text_before_citation": ["Fine-Grained Complexity The study of fine-grained complexity aims to show (conditional) hardness of easy problems.", "Recent years have witnessed a bloom of development into this fascinating subject, resulting in many tight lower bounds which match exactly, or up to poly log factors, the running time of best-known algorithms #OTHEREFR 24, #OTHEREFR .", "Among many others, popular hardness assumptions include the Strong Exponential Time Hypothesis (SETH), Boolean Matrix Multiplication (BMM), and All-Pairs Shortest Paths (APSP).", "Our work can be seen as a particular instance under this framework, i.e.", "using Boolean or Min-Weight k-Clique Conjecture to show conditional lower bounds for BCQs."], "text_after_citation": ["Functional Aggregate Queries (FAQ) FAQ #OTHEREFR provides a Sum-of-Product framework to define the semantics of conjunctive queries over arbitrary semirings.", "The semiring point-ofview originated from the seminal paper #OTHEREFR .", "We show that the embedding from a k-clique into a hypergraph holds for arbitrary semirings, which enables one to transfer the hardness of k-clique to FAQ independent of the underlying semiring.", "To the best of our knowledge, this is the first semiring-oblivious reduction.", "Enumeration and Preprocessing #OTHEREFR characterized for which self-join-free conjunctive queries a linear or constant delay and linear preprocessing algorithm is possible."], "citing_paper_content": {"title": "The Fine-Grained Complexity Of Boolean Conjunctive Queries And Sum-Product Problems", "abstract": "We study the fine-grained complexity of evaluating Boolean Conjunctive Queries and their generalization to sum-of-product problems over an arbitrary semiring. For these problems, we present a general semiring-oblivious reduction from the k-clique problem to any query structure (hypergraph). Our reduction uses the notion of embedding a graph to a hypergraph, first introduced by Marx [18]. As a consequence of our reduction, we can show tight conditional lower bounds for many classes of hypergraphs, including cycles, Loomis-Whitney joins, some bipartite graphs, and chordal graphs. These lower bounds have a dependence on what we call the clique embedding power of a hypergraph H, which we believe is a quantity of independent interest. We show that the clique embedding power is always less than the submodular width of the hypergraph, and present a decidable algorithm for computing it. We conclude with many open problems for future research. 2012 ACM Subject Classification Database theory; Complexity theory and logic Keywords and phrases Fine-grained complexity, conjunctive queries, semiring-oblivious reduction Digital Object Identifier 10.4230/LIPIcs.CVIT.2016.23 1 Technically, the PANDA algorithm works for Boolean or full CQs. 2 Informally speaking, this requires the algorithm does not leverage the fast matrix multiplication."}, "cited_paper_content": {"title": "Tight Hardness For Shortest Cycles And Paths In Sparse Graphs", "abstract": "Fine-grained reductions have established equivalences between many core problems with $\\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs, such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement Paths, Second Shortest Paths, and so on. These problems also have $\\tilde{O}(mn)$-time algorithms on $m$-edge $n$-node weighted graphs, and such algorithms have wider applicability. Are these $mn$ bounds optimal when $m \\ll n^2$? Starting from the hypothesis that the minimum weight $(2\\ell+1)$-Clique problem in edge weighted graphs requires $n^{2\\ell+1-o(1)}$ time, we prove that for all sparsities of the form $m = \\Theta(n^{1+1/\\ell})$, there is no $O(n^2 + mn^{1-\\epsilon})$ time algorithm for $\\epsilon>0$ for \\emph{any} of the below problems: Minimum Weight $(2\\ell+1)$-Cycle in a directed weighted graph, Shortest Cycle in a directed weighted graph, APSP in a directed or undirected weighted graph, Radius (or Eccentricities) in a directed or undirected weighted graph, Wiener index of a directed or undirected weighted graph, Replacement Paths in a directed weighted graph, Second Shortest Path in a directed weighted graph, Betweenness Centrality of a given node in a directed weighted graph. That is, we prove hardness for a variety of sparse graph problems from the hardness of a dense graph problem. Our results also lead to new conditional lower bounds from several related hypothesis for unweighted sparse graph problems including $k$-cycle, shortest cycle, Radius, Wiener index and APSP."}, "keywords": ["k-cycles"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our work can be seen as a particular instance under this framework, i.e. using Boolean or Min-Weight k-Clique Conjecture to show conditional lower bounds for BCQs. For example, the authors of the reference paper #REFR have established tight hardness for Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement Paths, Second Shortest Paths, and other core problems on weighted graphs.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Fine-grained reductions have established equivalences between many core problems with $\\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs, such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement Paths, Second Shortest Paths, and so on, as shown in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our work can be seen as a particular instance under this framework, i.e. using Boolean or Min-Weight k-Clique Conjecture to show conditional lower bounds for BCQs, as shown in the reference paper Tight Hardness For Shortest Cycles And Paths In Sparse Graphs. #REFR", "given_citation_intent": "background", "given_keywords": ["k-cycles"]}]}
{"citing_id": "2303.04298v3", "cited_id": "1510.06750", "section_title": "Introduction", "citation": "More than a decade later, Fefferman and Kimmel #REFR proved a second black-box separation using a distributional in-place oracle, which is a non-standard type of oracles.", "text_before_citation": ["The first (but often false) thought is that phases and magnitudes are continuous, and a piece of quantum information may be able to store exponentially or infinitely more information than classical ones; which is always not true 1 .", "Since classical and quantum information present distinct and unique natures, the community studies their differences under different contexts and directions, including advice-aided quantum computation [ One way to understand their differences is by studying one-way communication complexity: i.e., Alice and Bob want to jointly compute a function with their private inputs, but only one-time quantum/classical communication from Alice to Bob is allowed.", "Among many works, Bar-Yossef, Jayram, and Kerenidis #OTHEREFR showed an exponential separation between quantum and classical one-way communication complexity, for the so-called hidden matching problem.", "The other approach is by looking at QMA v.s. QCMA.", "In 2007, Aaronson and Kuperberg #OTHEREFR showed a black-box separation with respect to a black-box quantum unitary and left the same separation with respect to a classical oracle as an open question."], "text_after_citation": ["Recently, Natarajan and Nirkhe #OTHEREFR moved a step closer to the goal by presenting a black-box separation with respect to a distributional oracle 2 .", "Therefore, we would like to further investigate the difference between quantum and classical proofs, i.e., the separation between QMA v.s. QCMA.", "In the work, we address the question by demonstrating a separation relative to classically accessible classical oracle.", "Definition 1.1 (QMA).", "A language L is said to be in QMA if there exists a quantum polynomialtime machine V together with a polynomial p(\u2022) such that,"], "citing_paper_content": {"title": "Classical Vs Quantum Advice And Proofs Under Classically-Accessible Oracle", "abstract": "It is a long-standing open question to construct a classical oracle relative to which BQP/qpoly = BQP/poly or QMA = QCMA. In this paper, we construct classically-accessible classical oracles relative to which BQP/qpoly = BQP/poly and QMA = QCMA. Here, classically-accessible classical oracles are oracles that can be accessed only classically even for quantum algorithms. Based on a similar technique, we also show an alternative proof for the separation of QMA and QCMA relative to a distributional quantumly-accessible classical oracle, which was recently shown by Natarajan and Nirkhe."}, "cited_paper_content": {"title": "Quantum Vs Classical Proofs And Subset Verification", "abstract": "We study the ability of efficient quantum verifiers to decide properties of exponentially large subsets given either a classical or quantum witness. We develop a general framework that can be used to prove that QCMA machines, with only classical witnesses, cannot verify certain properties of subsets given implicitly via an oracle. We use this framework to prove an oracle separation between QCMA and QMA using an ``in-place'' permutation oracle, making the first progress on this question since Aaronson and Kuperberg in 2007. We also use the framework to prove a particularly simple standard oracle separation between QCMA and AM."}, "keywords": ["oracles"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In 2007, Aaronson and Kuperberg showed a black-box separation with respect to a black-box quantum unitary and left the same separation with respect to a classical oracle as an open question #OTHEREFR, which is relevant to the study of QMA v.s. QCMA.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In 2007, Aaronson and Kuperberg showed a black-box separation with respect to a black-box quantum unitary and left the same separation with respect to a classical oracle as an open question, which highlights the differences between QMA and QCMA #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In 2007, Aaronson and Kuperberg showed a black-box separation with respect to a black-box quantum unitary and left the same separation with respect to a classical oracle as an open question, which is relevant to the study of oracles in the context of QMA v.s. QCMA #REFR.", "given_citation_intent": "background", "given_keywords": ["oracles"]}]}
{"citing_id": "2303.06484v2", "cited_id": "1805.09298", "section_title": "L Derivation Of Ce'S Lower Bound", "citation": "Combining the two lower bounds above, we can have that #REFR where the first two terms encourage larger inter-class hyperspherical uniformity, and the last term promotes smaller intra-class hyperspherical uniformity.", "text_before_citation": ["where \u03bb can be chosen such that both Q 1 (w) and Q 2 (w) become convex functions with respect to w.", "Taking advantage of the convexity, we can separately set the gradient of Q 1 (w) and Q 2 (w) with respect to w as 0 and compute their minima. Specifically, we end up with", "EQUATION", "EQUATION", "where l ic = exp( wc,wi ) j exp( wj ,xi ) denotes the softmax confidence."], "text_after_citation": ["L CE \u2265 Q 1 (w * Q1 ) + Q 2 (w * Q2 ) = n i=1 log C c=1 exp 1 \u03bbn n j=1 l jc x i , x j \u2212 n 2\u03bb C c=1 1 n n i=1 l ic x i 2 \u2212 1 2\u03bbn n i=1 j\u2208Ay i x i , x j"], "citing_paper_content": {"title": "", "abstract": "The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both crossentropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective-hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity. HUG not only provably converges to GNC, but also decouples GNC into two separate objectives. Unlike cross-entropy loss that couples intra-class compactness and inter-class separability, HUG enjoys more flexibility and serves as a good alternative loss function. Empirical results show that HUG works well in terms of generalization and robustness."}, "cited_paper_content": {"title": "Learning Towards Minimum Hyperspherical Energy", "abstract": "Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization."}, "keywords": ["larger inter-class hyperspherical"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors proposed a minimum hyperspherical energy (MHE) objective as a generic regularization for neural networks, which reduces the redundancy regularization problem to generic energy minimization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors draw inspiration from the Thomson problem in physics, seeking to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors draw inspiration from the Thomson problem in physics and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks, which can effectively reduce redundancy and improve generalization ability, as demonstrated by their experiments on several challenging tasks. #REFR", "given_citation_intent": "background", "given_keywords": ["larger inter-class hyperspherical"]}]}
{"citing_id": "2305.01095v1", "cited_id": "1412.6980", "section_title": "B. Determination Of Lstm Predictor", "citation": "The Adam optimizer is utilized to adjust the learning rate #REFR . The learning rate is set to 0.0001.", "text_before_citation": ["To determine the optimal configuration of the LSTM predictor, its performance on the validation dataset and training time is assessed.", "The flow chart of the proposed network layers is shown in Fig.2 . It consists of eight layers each has 200 neurons.", "A sequence input layer inputs sequence data to a network.", "A fully connected layer multiplies the input by a weight matrix and then adds a bias vector.The ReLU layer performs a threshold operation on each element of the input, where any value less than zero is set to zero.An LSTM layer learns long-term dependencies between time steps in time series and sequence data.The layer performs additive interactions, which can help improve gradient flow over long sequences during training and the regression layer computes the halfmean-squared-error loss for regression tasks."], "text_after_citation": ["The training process is stopped when the validation accuracy does not show improvement over five consecutive iterations/epochs to prevent overfitting.", "The performance of the vehicle acceleration prediction is evaluated in this section based on root mean square error (RMSE) as an indicator of the model prediction accuracy which is formulated as the following equation 1and 2 #OTHEREFR :", "\uf0e5 \uf03d \uf03d N 1 t t y 1 N y (1) N t y t y 1 t 2 RMSE \uf0e5 \uf03d \uf03d \uf0d9 \uf0f7 \uf0f7 \uf0f8 \uf0f6 \uf0e7 \uf0e7 \uf0e8 \uf0e6 \uf02d N (2)", "where y is the mean of the measured acceleration, t y is the measured acceleration, t y \uf0d9 is the predicted acceleration, and N is the number of elements in output. Fig.", "3 shows the RMSE values during the LSTM training process. As depicted in Fig."], "citing_paper_content": {"title": "Lstm-Based Preceding Vehicle Behaviour Prediction During Aggressive Lane Change For Acc Application", "abstract": "The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle.However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM)based ACC system that can learn from past driving experiences and adapt and predict new situations in realtime.The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The results show that the LSTM-based system is 19.25 % more accurate than the ANN model and 5.9 % more accurate than the MPC model in terms of predicting future values of subject vehicle acceleration. The simulation is done in Matlab/Simulink environment. I."}, "cited_paper_content": {"title": "Adam: A Method For Stochastic Optimization", "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."}, "keywords": ["learning rate", "Adam optimizer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To learn from past driving experiences and adapt and predict new situations in real-time, the proposed Long Short-Term Memory (LSTM)-based Adaptive Cruise Control (ACC) system is evaluated on a simulated driving environment. The results show that the LSTM-based system is more accurate than the feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model in terms of predicting future values of subject vehicle acceleration #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn from past driving experiences and adapt and predict new situations in real-time, the proposed LSTM-based ACC system utilizes the Adam algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To optimize the LSTM predictor, the authors implemented the Adam optimizer with a learning rate of 0.001, as proposed in the reference paper #REFR, which has been shown to be computationally efficient and well-suited for problems with large data and/or parameters.", "given_citation_intent": "method", "given_keywords": ["learning rate", "Adam optimizer"]}]}
{"citing_id": "2304.07460v1", "cited_id": "1712.07557", "section_title": "Convergence-Optimized Power Control Under Dp Guarantee", "citation": "Then, we substitute the objective function and the DP constraint in P1 with the convergence upper bound #REFR w.r.t.", "text_before_citation": ["Under Assumption 1, given a local update \u2206 t i \u2208 R d and rand k sparsification with random projection matrix A t , we have", "EQUATION", "Proof: Taking the expectation on the rand k , we have:", "E A t \u2206 t i 2 2 = d j=1 k d [\u2206 t i ] 2 j (35a) = k d \u03b8 t,\u03c4 i \u2212 \u03b8 t 2 2 (35b) = k d \u03b7 2 \u03c4 s=1 g t,s\u22121 i 2 2 (35c) \u2264 k d \u03b7 2 \u03c4 2 C 2 1 (35d)", "where (35c) holds due to #OTHEREFR , (35d) follows from Assumption 1 and Cauchy-Schwarz inequality."], "text_after_citation": ["{\u03b2 t } t\u2208[T \u22121] in Theorem 4 and the client-level DP result #OTHEREFR in Theorem 3, respectively. Therefore, we can approximate Problem P1 as follows:", "EQUATION", "Note that P2 can be readily solved as shown in the following result:", "Theorem 5. The optimal solution to Problem P2 is given by:", "EQUATION"], "citing_paper_content": {"title": "Communication And Energy Efficient Wireless Federated Learning With Intrinsic Privacy", "abstract": "Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless channel status without any artificial noise addition. We provide a privacy analysis for PFELS and prove the convergence of PFELS under general non-convex and non-IID settings. Experimental results show that compared with prior work, PFELS can improve the accuracy with the same DP guarantee and save communication and energy costs simultaneously."}, "cited_paper_content": {"title": "Differentially Private Federated Learning: A Client Level Perspective", "abstract": "Federated learning is a recent advance in privacy protection. In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. However, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. We tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. Empirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance."}, "keywords": ["DP constraint"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Taking the expectation on the rand k, we have: E(A_t\u2206_t_i^2)/2 = d\u2211_[j=1]^k [\u2206_t_i]_j^2/2 = kd(\u03b8_t,\u03c4_i - \u03b8_t)^2/2 = kd\u03b7^2\u2211_[s=1]^\u03c4 g_t,s-1^2i/2 \u2264 kd\u03b7^2\u03c4^2C^2_1/2, where the inequality holds due to #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the privacy issue in federated learning, differential privacy mechanisms are leveraged, and a recent study proposes an algorithm for client-sided differential privacy preserving federated optimization, which can maintain client-level differential privacy at only a minor cost in model performance #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee (#REFR), and the proposed algorithm for client sided differential privacy preserving federated optimization can be used to satisfy the DP constraint in wireless federated learning scenarios.", "given_citation_intent": "method", "given_keywords": ["DP constraint"]}]}
{"citing_id": "2305.01072v1", "cited_id": "1907.10121", "section_title": "A. Shortest Path Problem", "citation": "Using an optimized implementation of Dijkstra's algorithm (e.g., the one provided by scipy #REFR ), the search for a shortest path is also very fast.", "text_before_citation": ["As noted above, this curve is safe because each of its line segments is contained in at least a safe box.", "This shortest path step determines whether or not our path planning problem is feasible.", "If there is no path in the augmented line graph between the vertices associated with p init and p term , i.e., the distance between them is \u221e, then the path planning problem (4) is infeasible.", "Conversely, if there is a path between these two vertices, the original path planning problem is feasible, since a feasible trajectory can be constructed as in \u00a7II-C.", "The problem of identifying the safe boxes that contain the initial and terminal points is known as stabbing problem and, given the precomputations done to construct the line graph, this takes negligible time #OTHEREFR ."], "text_after_citation": [], "citing_paper_content": {"title": "Fast Path Planning Through Large Collections Of Safe Boxes", "abstract": "We present a fast algorithm for the design of smooth paths (or trajectories) that are constrained to lie in a collection of axis-aligned boxes. We consider the case where the number of these safe boxes is large, and basic preprocessing of them (such as finding their intersections) can be done offline. At runtime we quickly generate a smooth path between given initial and terminal positions. Our algorithm designs trajectories that are guaranteed to be safe at all times, and it detects infeasibility whenever such a trajectory does not exist. Our algorithm is based on two subproblems that we can solve very efficiently: finding a shortest path in a weighted graph, and solving (multiple) convex optimal control problems. We demonstrate the proposed path planner on large-scale numerical examples, and we provide an efficient open-source software implementation, fastpathplanning."}, "cited_paper_content": {"title": "Scipy 1.0: Fundamental Algorithms For Scientific Computing In Python", "abstract": "SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments. This Perspective describes the development and capabilities of SciPy 1.0, an open source scientific computing library for the Python programming language."}, "keywords": ["shortest path", "optimized implementation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The problem of identifying the safe boxes that contain the initial and terminal points is known as stabbing problem and, given the precomputations done to construct the line graph, this takes negligible time (Scipy 1.0).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct the line graph, the authors precomputed the safe boxes that contain the initial and terminal points, a problem known as the stabbing problem, which takes negligible time #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To find the shortest path in a weighted graph, we can efficiently solve a subproblem, as described in #REFR. Additionally, the proposed path planner provides an optimized implementation, making it suitable for large-scale numerical examples.", "given_citation_intent": "method", "given_keywords": ["shortest path", "optimized implementation"]}]}
{"citing_id": "2303.12936v1", "cited_id": "1905.06316", "section_title": "Comparing Bert And Distilbert", "citation": "The fairly comparable scores of ELMo and the traditional baselines in the null context supports the observation of #REFR that is, when it comes to contextual embeddings, there is only a small improvement in learning semantics over traditional ML methods.", "text_before_citation": ["If the models were also pretrained from scratch on the same corpus, it would be ensured that they utilize the same knowledge to learn the context.", "And this would enable a fairer comparison.", "Recently, it was shown that ELMo and BERT make no significant difference in semantic analysis #OTHEREFR .", "Here it is observed that although they are close-by in the null context, DistilBERT is more robust than ELMo in the cross-context in text classification.", "The findings of this study are in line with prior work."], "text_after_citation": ["DistilBERT is on par with or exceeding ELMo on a binary text classification task #OTHEREFR .", "DistilBERT, as a transformerbased model, is better in capturing long-term dependencies in an input sequence #OTHEREFR .", "DistilBERT is lighter than ELMo and has a shorter training time #OTHEREFR .", "Here it should be noted that the experimental settings of the previous work and In this study, ELMo and DistilBERT are compared on their fine-tuning performance on two binary text classification tasks.", "The main focus was to see how much can these models be benefited in a practical way without any modification to the pretraining outputs."], "citing_paper_content": {"title": "", "abstract": "I am grateful to my family for their unconditional love and patience. I am grateful to Arzucan\u00d6zg\u00fcr, for being such an inspiring figure by her selfless devotion to research the most righteous way with the passion to contribute to the community. I am grateful to Ali H\u00fcrriyetoglu, for being such a role model, who could somehow always find a way to turn the mist of research questions into a structured path to create practical solutions by combining creativity and technique. I cannot thank enough my dear friends who put up with my whims throughout this journey. I thank fellows from TabiLAB for inspiring me with their brilliance, invaluable insights and recommendations. I thank Ko\u00e7 University EMW research team for their generosity in sharing the data which was created with blood, sweat and tears. I feel lucky that I got to meet fellows in EMW project engineering team who invested their precious time and energy to support me in this study from the very beginning. Lastly, I owe the deepest gratitude to our professors and staff members in our department who taught us how to form such a great community and made it feel like the dearest home from the day one."}, "cited_paper_content": {"title": "What Do You Learn From Context? Probing For Sentence Structure In Contextualized Word Representations", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline."}, "keywords": ["contextual embeddings"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, it was shown that ELMo and BERT make no significant difference in semantic analysis #OTHEREFR. Here it is observed that although they are close-by in the null context, DistilBERT is more robust than ELMo in the cross-context in text classification, which is in line with the findings of Peters et al.'s study on probing sentence structure in contextualized word representations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to recent findings, while ELMo and BERT produce strong representations for syntactic phenomena, they only offer comparably small improvements on semantic tasks over a non-contextual baseline #REFR, which is in line with the observations made in this study.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The study compares the performance of contextual embeddings in syntactic and semantic tasks, and finds that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline #REFR.", "given_citation_intent": "result", "given_keywords": ["contextual embeddings"]}]}
{"citing_id": "2304.00193v1", "cited_id": "1902.00279", "section_title": "I. Introduction", "citation": "A distancedbased formation control algorithm for a team of quadrotors transporting a heavy object is presented in #REFR , which measures and resists the acceleration due to disturbances and rope tension using incremental nonlinear dynamic inversion control.", "text_before_citation": ["Although the precise attitude and position control of the payload can be realized, the dynamic information of the payload is required for real-time feedback control, which is hard to obtain in engineering practice.", "In contrast, in formation-based design, only the state information of the aerial vehicles is needed.", "When the vehicle group reaches its destination, the payload is also supposed to reach the target area.", "The validity and feasibility of such approach has been established via simulation #OTHEREFR and experiment #OTHEREFR , but the cable forces on the quadrotors are ignored.", "To implement formation-based robust collaborative transportation, several control algorithms have been developed."], "text_after_citation": ["In #OTHEREFR and #OTHEREFR , a passivity-based formation control strategy is proposed with adaptive compensation terms to eliminate the wind disturbance and the cable tension.", "The energy passivity property of the quadrotors-payload system is established in #OTHEREFR , where an adaptive damping term is used to dissipate the energy injected by the sudden perturbations.", "The studies mentioned above are all designed based on the rigid formation.", "As a matter of fact, maintaining a fixed formation for payload transportation is not necessary and it is better to employ a flexible formation, which can adapt the vehicles to the complex and uncertain environment and tasks #OTHEREFR .", "Force control-based approaches have been explored for collaborative payload transportation with flexible formation, e.g., force amplification #OTHEREFR and contact force regulation #OTHEREFR ."], "citing_paper_content": {"title": "Force-Coordination Control For Aerial Collaborative Transportation Based On Lumped Disturbance Separation And Estimation", "abstract": "This article studies the collaborative transportation of a cable-suspended pipe by two quadrotors. A force-coordination control scheme is proposed, where a force-consensus term is introduced to average the load distribution between the quadrotors. Since thrust uncertainty and cable force are coupled together in the acceleration channel, disturbance observer can only obtain the lumped disturbance estimate. Under the quasi-static condition, a disturbance separation strategy is developed to remove the thrust uncertainty estimate for precise cable force estimation. The stability of the overall system is analyzed using Lyapunov theory. Both numerical simulations and indoor experiments using heterogeneous quadrotors validate the effectiveness of thrust uncertainty separation and force-consensus algorithm."}, "cited_paper_content": {"title": "Flexible Collaborative Transportation By A Team Of Rotorcraft", "abstract": "We propose a combined method for the collaborative transportation of a suspended payload by a team of rotorcraft. A recent distance-based formation-motion control algorithm based on assigning distance disagreements among robots generates the acceleration signals to be tracked by the vehicles. In particular, the proposed method does not need global positions nor tracking prescribed trajectories for the motion of the members of the team. The acceleration signals are followed accurately by an Incremental Nonlinear Dynamic Inversion controller designed for rotorcraft that measures and resists the tensions from the payload. Our approach allows us to analyze the involved accelerations and forces in the system so that we can calculate the worst case conditions explicitly to guarantee a nominal performance, provided that the payload starts at rest in the 2D centroid of the formation, and it is not under significant disturbances. For example, we can calculate the maximum safe deformation of the team with respect to its desired shape. We demonstrate our method with a team of four rotorcraft carrying a suspended object two times heavier than the maximum payload for an individual. Last but not least, our proposed algorithm is available for the community in the open-source autopilot Paparazzi."}, "keywords": ["quadrotors"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To implement formation-based robust collaborative transportation, several control algorithms have been developed, including the method proposed by the authors in #REFR, which allows for the collaborative transportation of a suspended payload by a team of rotorcraft without the need for global positions or tracking prescribed trajectories.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement formation-based robust collaborative transportation, several control algorithms have been developed, including the method proposed in \"Flexible Collaborative Transportation By A Team Of Rotorcraft\" #REFR, which does not require global positions or tracking prescribed trajectories for the motion of the members of the team.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement formation-based robust collaborative transportation of a cable-suspended payload by a team of #KEYWORD, the authors proposed a combined method that assigns distance disagreements among robots to generate acceleration signals, which are accurately followed by an Incremental Nonlinear Dynamic Inversion controller designed for rotorcraft that measures and resists the tensions from the payload #REFR.", "given_citation_intent": "method", "given_keywords": ["quadrotors"]}]}
{"citing_id": "2305.01506v1", "cited_id": "1905.00414", "section_title": "Vi. How Does The Pre-Training Escalate Target Task Performance?", "citation": "To analyze the representations in neural networks consisting of different weights, we employed Central Kernel Alignment (CKA) #REFR as an index of measuring similarity between two representations from different neural networks.", "text_before_citation": ["Throughout experimental analyses in section IV and V-B, we discovered pre-training methods concretely contribute to the escalated target task performance in every image recognition task and problem settings.", "Based on the aforementioned discoveries, we aim to scrutinize an underlying reason behind the effectiveness of pre-training by analyzing learned knowledge at the neural networks."], "text_after_citation": ["Suppose we provide image samples into two neural networks (N 1 , N 2 ) trained from different pre-trained weights.", "Then, we can extract a pair of representation vectors from any layers of N 1 and N 2 denoted as (R 1 , R 2 ).", "Given these representation vectors R 1 and R 2 , CKA effectively measures the similarity between layers in the same neural networks with different weights and across entirely different architectures.", "The CKA yields a similarity metric lying between 0 to 1, where 0 implies less similarity and 1 illustrates high similarity.", "Due to its convenience and effectiveness in measuring similarity between two representations, we utilized it."], "citing_paper_content": {"title": "Discovering The Effectiveness Of Pre-Training In A Large-Scale Car-Sharing Platform", "abstract": "Recent progress of deep learning has empowered various intelligent transportation applications, especially in carsharing platforms. While the traditional operations of the carsharing service highly relied on human engagements in fleet management, modern car-sharing platforms let users upload car images before and after their use to inspect the cars without a physical visit. To automate the aforementioned inspection task, prior approaches utilized deep neural networks. They commonly employed pre-training, a de-facto technique to establish an effective model under the limited number of labeled datasets. As candidate practitioners who deal with car images would presumably get suffered from the lack of a labeled dataset, we analyzed a sophisticated analogy into the effectiveness of pretraining is important. However, prior studies primarily shed a little spotlight on the effectiveness of pre-training. Motivated by the aforementioned lack of analysis, our study proposes a series of analyses to unveil the effectiveness of various pre-training methods in image recognition tasks at the car-sharing platform. We set two real-world image recognition tasks in the car-sharing platform in a live service, established them under the many-shot and few-shot problem settings, and scrutinized which pre-training method accomplishes the most effective performance in which setting. Furthermore, we analyzed how does the pre-training and fine-tuning convey different knowledge to the neural networks for a precise understanding."}, "cited_paper_content": {"title": "Similarity Of Neural Network Representations Revisited", "abstract": "Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations."}, "keywords": ["neural networks", "Central Kernel Alignment"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Throughout experimental analyses in section IV and V-B, we discovered pre-training methods concretely contribute to the escalated target task performance in every image recognition task and problem settings. Based on the aforementioned discoveries, we aim to scrutinize an underlying reason behind the effectiveness of pre-training by analyzing learned knowledge at the neural networks. This analysis is motivated by recent work that has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To analyze the effectiveness of pre-training methods, we utilized canonical correlation analysis (CCA) and centered kernel alignment (CKA) to compare neural network representations, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To analyze the learned knowledge at the neural networks, we utilized Central Kernel Alignment (CKA) as a similarity index to measure the relationship between representational similarity matrices, which has been shown to reliably identify correspondences between representations in networks trained from different initializations of #REFR.", "given_citation_intent": "method", "given_keywords": ["neural networks", "Central Kernel Alignment"]}]}
{"citing_id": "2304.05963v1", "cited_id": "1804.05650", "section_title": "Introduction", "citation": "It has been shown, both in practice and in theory #REFR , to provide an advantage over fixed parameter strategies.", "text_before_citation": ["It is surprising that an evolutionary algorithm on bit vectors similar to CMA-ES is missing.", "One may object to this observation that PBIL is a natural candidate.", "However, its parameters are all continuous and, in the form of a probability vector, do not belong to the search space in the same way as, in CMA-ES, the mean of the search distribution does.", "Furthermore, it is expected that such a speculative evolutionary algorithm will be able to control the mutation rate, just as CMA-ES provides an optimal control of correlated mutations.", "Adaptive parameter control in evolutionary algorithms has been the subject of sustained efforts for decades #OTHEREFR ."], "text_after_citation": ["However, update rules are often introduced as heuristics and do not derive from first principles.", "A famous update rule is the so called one-fifth success rule [25] which adjusts the mutation rate so as to keep the success rate of mutations equal or close to one fifth (a mutation is successful if it produces an offspring of increased fitness).", "If the success rate is larger (smaller) than one fifth then the mutation rate should be increased (decreased).", "The one-fifth constant can be derived from theoretical considerations about the (1 + 1) evolution strategy applied to the sphere problem in Euclidean spaces.", "It has been proved that the (1 + 1) evolutionary algorithm equipped with a similar success-based update rule achieves the same performance on LeadingOnes as the (1+1) EA with an optimal fitness-dependent mutation rate #OTHEREFR ."], "citing_paper_content": {"title": "An Information-Theoretic Evolutionary Algorithm *", "abstract": "We propose a novel evolutionary algorithm on bit vectors which derives from the principles of information theory. The informationtheoretic evolutionary algorithm (it-EA) iteratively updates a search distribution with two parameters, the center, that is the bit vector at which standard bit mutation is applied, and the mutation rate. The mutation rate is updated by means of information-geometric optimization and the center is updated by means of a maximum likelihood principle. Standard elitist and non elitist updates of the center are also considered. Experiments illustrate the dynamics of the mutation rate and the influence of hyperparameters. In an empirical runtime analysis, on OneMax and LeadingOnes, the elitist and non elitist it-EAs obtain promising results."}, "cited_paper_content": {"title": "Theory Of Parameter Control For Discrete Black-Box Optimization: Provable Performance Gains Through Dynamic Parameter Choices", "abstract": "Parameter control is aimed at realizing performance gains through a dynamic choice of the parameters which determine the behavior of the underlying optimization algorithm. In the context of evolutionary algorithms, this research line has for a long time been dominated by empirical approaches. With the significant advances in running-time analysis achieved in the last ten years, the parameter control question has become accessible to theoretical investigations. A number of running-time results for a broad range of different parameter control mechanisms have been obtained in recent years. This chapter surveys these results, and puts them into context by proposing an updated classification scheme for parameter control."}, "keywords": ["fixed parameter strategies"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Adaptive parameter control in evolutionary algorithms has been the subject of sustained efforts for decades, and recent theoretical investigations have shown significant advances in running-time analysis for a broad range of different parameter control mechanisms #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adaptive parameter control in evolutionary algorithms has been the subject of sustained efforts for decades, with significant advances in running-time analysis achieved in the last ten years #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adaptive parameter control in evolutionary algorithms has been the subject of sustained efforts for decades, with significant advances in running-time analysis achieved in the last ten years. A number of running-time results for a broad range of different fixed parameter strategies have been obtained in recent years #REFR.", "given_citation_intent": "background", "given_keywords": ["fixed parameter strategies"]}]}
{"citing_id": "2303.02667v1", "cited_id": "1607.00376", "section_title": "Discussion And Conclusion", "citation": "From a gender point of view, our results converge with previous research that reported that male scholars have a greater propensity to self-cite than female counterparts #REFR .", "text_before_citation": ["These 'mechanical' factors affecting self-citations suggest that researchers from different cohorts cannot be held to the same standards in terms of self-citations.", "Third, at the individual researcher level, our results show that direct self-citations have, on average, little effect on the h-index, but can have a considerable impact for researchers that have high rates of self-citations.", "This suggests that, while self-citations are, in most cases, not affecting the reliability of citation analysis for assessing research impact, some researchers with unusually high levels of self-citations do.", "Fourth, it provides evidence that self-citations are correlated with external citations.", "That is, researchers who have high levels of self-citations are also highly cited by their colleagues. Women are also self-citing their work less than men."], "text_after_citation": ["These results are congruent with studies that show that women are relatively less prone to engage in selfpromoting behavior at work #OTHEREFR , and that women tend to face relatively greater social penalties for self-promotion than men #OTHEREFR .", "In line with those, our results suggest that women require a greater threshold of relevancy in order to self-cite.", "This \"higher bar\" results in lower self-citation rates for women vis-\u00e0-vis men.", "Finally, or results show that self-citations are more highly related to the citing paper than external citations, which suggest that, in most cases, self-citations are a normal feature of knowledge accumulation.", "However, such finding can be nuanced by the inverse relationship observed between text similarity and the extent of researchers' self-refencing practices: the higher researchers' percentage of self-references, the lower the similarity between citing and cited articles."], "citing_paper_content": {"title": "Title: Are Self-Citations A Normal Feature Of Knowledge Accumulation?", "abstract": "Science is a cumulative activity, which can manifest itself through the act of citing. Citations are also central to research evaluation, thus creating incentives for researchers to cite their own work. Using a dataset containing more than 63 million articles and 51 million disambiguated authors, this paper examines the relative importance of self-citations and selfreferences in the scholarly communication landscape, their relationship with the age and gender of authors, as well as their effects on various research evaluation indicators. Results show that self-citations and self-references evolve in different directions throughout researchers' careers, and that men and older researchers are more likely to self-cite. Although self-citations have, on average, a small to moderate effect on author's citation rates, they highly inflate citations for a subset of researchers. Comparison of the abstracts of cited and citing papers to assess the relatedness of different types of citations shows that self-citations are more similar to each other than other types of citations, and therefore more relevant. However, researchers that selfreference more tend to include less relevant citations. The paper concludes with a discussion of the role of self-citations in scholarly communication. One-Sentence Summary: This study provides evidence of career and gender effects in selfcitations, and of a higher similarity of citing and cited papers in the case of self-citations than external citations. Main Text: Citation analysis has been used in research evaluation for almost five decades (1-3). What started as a tool to help researchers and librarians find relevant literature more efficiently (4-6) slowly became, after the creation Science Citation Index in 1963, a means to assess research and various levels, from individuals to institutions and countries (7). In this context, citation analysis has come under scrutiny from researchers across all disciplines. Several authors criticized bibliometrics and citation analysis for their limitations (8). Those limitations can be divided into those that relate to the coverage of the database (9-11), accuracy of citation data (12), adverse effects (13-16), overabundance of indicators (17), and citations being questionable indicators of research impact (18-19). Literature on citation analysis has highlighted the diversity of roles of citations in scholarly papers. Based on papers published in high-energy physics, the classic study by Moravcsik and Murugesan (20) provides a typology of functions of citations, based on four non-exclusive dichotomies. Citations can be conceptual, related to theories or concepts contained in the cited"}, "cited_paper_content": {"title": "Men Set Their Own Cites High: Gender And Self-Citation Across Fields And Over Time", "abstract": "How common is self-citation in scholarly publication, and does the practice vary by gender? Using novel methods and a data set of 1.5 million research papers in the scholarly database JSTOR published between 1779 and 2011, the authors find that nearly 10 percent of references are self-citations by a paper\u2019s authors. The findings also show that between 1779 and 2011, men cited their own papers 56 percent more than did women. In the last two decades of data, men self-cited 70 percent more than women. Women are also more than 10 percentage points more likely than men to not cite their own previous work at all. While these patterns could result from differences in the number of papers that men and women authors have published rather than gender-specific patterns of self-citation behavior, this gender gap in self-citation rates has remained stable over the last 50 years, despite increased representation of women in academia. The authors break down self-citation patterns by academic field and number of authors ..."}, "keywords": ["self-cite"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Women are also self-citing their work less than men, as shown in a study analyzing 1.5 million research papers published between 1779 and 2011, which found that men cited their own papers 56 percent more than women did, and in the last two decades, men self-cited 70 percent more than women did (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the study by Men set their own cites high: gender and self-citation across fields and over time, men cited their own papers 56% more than women between 1779 and 2011, and 70% more in the last two decades of data, which is consistent with the finding that women are also self-citing their work less than men. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to a study by King et al., men self-cited their work 56% more than women between 1779 and 2011, and this gender gap in self-citation rates has remained stable over the last 50 years, despite increased representation of women in academia. #REFR", "given_citation_intent": "result", "given_keywords": ["self-cite"]}]}
{"citing_id": "2304.02916v1", "cited_id": "1904.09751", "section_title": "Guiding Text", "citation": "In order to avoid the \"unreliable tail\" of the distribution we use Nucleus Sampling (topp) #REFR .", "text_before_citation": ["PaSST has recently achieved state-of-the-art performance in audio classification tasks #OTHEREFR .", "Using a pre-trained PaSST model, we infer AudioSet class labels from the input audio.", "Each word in the label is embedded in the input space using trainable embeddings and concatenated with the extracted patches.", "Similarly to #OTHEREFR , in order to make our system more robust to PaSST's prediction errors, we sample each label from the output distribution."], "text_after_citation": ["During inference, we select the most probable output label instead of sampling.", "Since we add word-level information to our model, we want the input label to be semantically similar to the ground truth caption, functioning as a guiding text.", "We observe that PaSST tends to output labels that capture the general, high-level information in the audio and not labels that are more infrequent and specific.", "Such labels are more likely to be semantically similar or even be present verbatim in the ground truth captions.", "For example, an audio clip with the caption \"A short distance away, a group of people engage in indistinguishable chatter.\" is classified as Speech when in fact the AudioSet class Chatter would have higher semantic accuracy."], "citing_paper_content": {"title": "Efficient Audio Captioning Transformer With Patchout And Text Guidance", "abstract": "Automated audio captioning is multi-modal translation task that aim to generate textual descriptions for a given audio clip. In this paper we propose a full Transformer architecture that utilizes Patchout as proposed in [1], significantly reducing the computational complexity and avoiding overfitting. The caption generation is partly conditioned on textual AudioSet tags extracted by a pre-trained classification model which is fine-tuned to maximize the semantic similarity between AudioSet labels and ground truth captions. To mitigate the data scarcity problem of Automated Audio Captioning we introduce transfer learning from an upstream audio-related task and an enlarged in-domain dataset. Moreover, we propose a method to apply Mixup augmentation for AAC. Ablation studies are carried out to investigate how Patchout and text guidance contribute to the final performance. The results show that the proposed techniques improve the performance of our system and while reducing the computational complexity. Our proposed method received the Judges Award at the Task6A of DCASE Challenge 2022."}, "cited_paper_content": {"title": "The Curious Case Of Neural Text Degeneration", "abstract": "Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration \u2014 output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is the best decoding strategy for generating long-form text that is both high-quality \u2014 as measured by human evaluation \u2014 and as diverse as human-written text."}, "keywords": ["Nucleus Sampling"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the problem of text degeneration in neural language modeling, Nucleus Sampling was proposed as a more effective decoding strategy, which avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To avoid text degeneration in open-ended text generation, Nucleus Sampling is proposed as a simple but effective method that draws considerably higher quality text out of neural language models by truncating the unreliable tail of the probability distribution, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate high-quality and diverse captions for audio clips, we utilize Nucleus Sampling, a method proposed in the reference paper #REFR, which avoids text degeneration by sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass.", "given_citation_intent": "method", "given_keywords": ["Nucleus Sampling"]}]}
{"citing_id": "2304.13099v1", "cited_id": "1812.00676", "section_title": "", "citation": "These methods are based on the clever solution approximations that result in a time-stepping scheme requiring only a small number of previous solution states for the next state evaluation. With some exceptions ( e.g. #REFR ), these methods are also O(h p ).", "text_before_citation": ["The methods from the first class are sequential in nature and have algebraic convergence order that typically does not exceed 2, even for the multi-step methods #OTHEREFR , because of the intrinsic fractional-kernel singularity #OTHEREFR .", "In addition, at each time-step these methods need to query the entire solution history in order to evaluate \u2202 \u03b1 t or J \u03b1 , numerically.", "In the consequence of that, they are computationally costly and memory constrained.", "Nonetheless, the methods from this class are popular due to their simplicity, numerical stability #OTHEREFR and the ability to handle non-smooth initial data #OTHEREFR .", "The second class of numerical methods represented by the works #OTHEREFR , to name a few."], "text_after_citation": ["Spectral methods from #OTHEREFR deserve a separate mention.", "Although formally belonging to the second class, they make use of the exponentially convergent contour based propagator approximation, which permits to evaluate the transient component of the solution to linear problem without time-stepping.", "Authors of these works, however, do not apply it to (1.1), (1.4) directly.", "Instead they consider a special proxy problem \u2202 t u + I 1\u2212\u03b1 Au = g where I \u03b1 is a non-local operator equal to \u2202 \u03b1 t , if \u03b1 < 1, or to J \u03b1 , otherwise.", "It was shown in #OTHEREFR , that the existing methodology for parabolic problems #OTHEREFR can be transferred to the mild solution of such proxy problem with all important numerical features of the solution algorithms preserved, including uniform exponential convergence for t \u2208 [0, T ] and the capacity for multi-level parallelism."], "citing_paper_content": {"title": "Exponentially Convergent Numerical Method For Abstract Cauchy Problem With Fractional Derivative Of Caputo Type", "abstract": "We present an exponentially convergent numerical method to approximate the solution of the Cauchy problem for the inhomogeneous fractional differential equation with an unbounded operator coefficient and Caputo fractional derivative in time. The numerical method is based on the newly obtained solution formula that consolidates the mild solution representations of sub-parabolic, parabolic and sub-hyperbolic equations with sectorial operator coefficient A and non-zero initial data. The involved integral operators are approximated using the sinc-quadrature formulas that are tailored to the spectral parameters of A, fractional order \u03b1 and the smoothness of the first initial condition, as well as to the properties of the equation's right-hand side f (t). The resulting method possesses exponential convergence for positive sectorial A, any finite t, including t = 0, and the whole range \u03b1 \u2208 (0, 2). It is suitable for a practically important case, when no knowledge of f (t) is available outside the considered interval t \u2208 [0, T ]. The algorithm of the method is capable of multi-level parallelism. We provide numerical examples that confirm the theoretical error estimates."}, "cited_paper_content": {"title": "Efficient Multistep Methods For Tempered Fractional Calculus: Algorithms And Simulations", "abstract": "In this work, we extend the fractional linear multistep methods in [C. Lubich, SIAM J. Math. Anal., 17 (1986), pp.704--719] to the tempered fractional integral and derivative operators in the sense that the tempered fractional derivative operator is interpreted in terms of the Hadamard finite-part integral. We develop two fast methods, Fast Method I and Fast Method II, with linear complexity to calculate the discrete convolution for the approximation of the (tempered) fractional operator. Fast Method I is based on a local approximation for the contour integral that represents the convolution weight. Fast Method II is based on a globally uniform approximation of the trapezoidal rule for the integral on the real line. Both methods are efficient, but numerical experimentation reveals that Fast Method II outperforms Fast Method I in terms of accuracy, efficiency, and coding simplicity. The memory requirement and computational cost of Fast Method II are $O(Q)$ and $O(Qn_T)$, respectively, where $n_T$ is the number of the final time steps and $Q$ is the number of quadrature points used in the trapezoidal rule. The effectiveness of the fast methods is verified through a series of numerical examples for long-time integration, including a numerical study of a fractional reaction-diffusion model."}, "keywords": ["clever solution approximations"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The second class of numerical methods represented by the works #REFR, to name a few, offer more efficient alternatives to the sequential methods from the first class, which are computationally costly and memory constrained due to their need to query the entire solution history at each time-step.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript propose a new exponentially convergent numerical method for the Cauchy problem with fractional derivative of Caputo type, which is computationally efficient and suitable for long-time integration, and they compare their results with those obtained by the multistep methods proposed in #REFR, which are known for their numerical stability and ability to handle non-smooth initial data.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of this manuscript propose a clever solution approximation for the (tempered) fractional operator, which is computationally efficient and has linear complexity, and is similar to the Fast Method II developed in #REFR.", "given_citation_intent": "method", "given_keywords": ["clever solution approximations"]}]}
{"citing_id": "2303.01625v1", "cited_id": "1801.08967", "section_title": "Definition 3.4 (Qszk).", "citation": "In particular, Menda and Watrous #REFR showed that for oracle A, a problem L A in QSZK A if there exists a reduction from L A to QSD A . Theorem 3.6 ([44, Theorem 1]).", "text_before_citation": ["The completeness follows from the fact that the states's trace distance is negligibly close to one, and this implies there exists a measurement that perfectly distinguishes the states.", "For the soundness, since the states are negligibly close in trace distance, the prover does not succeed with non-negligibly advantage over random guessing.", "To show the protocol is zero-knowledge, the quantum simulator applies the verifier's quantum operation first.", "After receiving the reponse b \u2032 from the prover, it sets b \u2032 = b.", "The QSZK-completeness of QSD relativizes."], "text_after_citation": ["For alphabet \u03a3, \u0393, let L \u2286 \u0393 * be a language and A \u2286 \u03a3 * be an oracle.", "The language L A is contained in QSZK A if and only if there exists a polynomial-time uniform family of pairs of relativized quantum circuits (Q A 0 , Q A 1 ) with the following properties:", "\u2022 If x \u2208 L A , then (Q A 0 , Q A 1 ) \u2208 QSD A 1 .", "\u2022 If x / \u2208 L A , then (Q A 0 , Q A 1 ) \u2208 QSD A 0 .", "Ben-David and Kothari #OTHEREFR studied independently the so-called QSZK complexity of function f , denoted QSZK(f ), which is defined as the minimum number k made by a pair of query algorithms A, B given oracle access to x such that for every x such that (i) if f (x) = 1, then A x \u2212 B x tr \u2265 2/3, and (ii) if f (x) = 0, then A x \u2212 B x tr \u2264 1/3."], "citing_paper_content": {"title": "Certified Randomness From Quantum Supremacy", "abstract": "We propose an application for near-term quantum devices: namely, generating cryptographically certified random bits, to use (for example) in proof-of-stake cryptocurrencies. Our protocol repurposes the existing \"quantum supremacy\" experiments, based on random circuit sampling, that Google and USTC have successfully carried out starting in 2019. We show that, whenever the outputs of these experiments pass the now-standard Linear Cross-Entropy Benchmark (LXEB), under plausible hardness assumptions they necessarily contain \u2126(n) min-entropy, where n is the number of qubits. To achieve a net gain in randomness, we use a small random seed to produce pseudorandom challenge circuits. In response to the challenge circuits, the quantum computer generates output strings that, after verification, can then be fed into a randomness extractor to produce certified nearly-uniform bits-thereby \"bootstrapping\" from pseudorandomness to genuine randomness. We prove our protocol sound in two senses: (i) under a hardness assumption called Long List Quantum Supremacy Verification, which we justify in the random oracle model, and (ii) unconditionally in the random oracle model against an eavesdropper who could share arbitrary entanglement with the device. (Note that our protocol's output is unpredictable even to a computationally unbounded adversary who can see the random oracle.) Currently, the central drawback of our protocol is the exponential cost of verification, which in practice will limit its implementation to at most n \u223c 60 qubits, a regime where attacks are expensive but not impossible. Modulo that drawback, our protocol appears to be the only practical application of quantum computing that both requires a QC and is physically realizable today."}, "cited_paper_content": {"title": "Oracle Separations For Quantum Statistical Zero-Knowledge", "abstract": "This paper investigates the power of quantum statistical zero knowledge interactive proof systems in the relativized setting. We prove the existence of an oracle relative to which quantum statistical zero-knowledge does not contain UP intersect coUP, and we prove that quantum statistical zero knowledge does not contain UP relative to a random oracle with probability 1. Our proofs of these statements rely on a bound on output state discrimination for relativized quantum circuits based on the quantum adversary method of Ambainis, following a technique similar to one used by Ben-David and Kothari to prove limitations on a query complexity variant of quantum statistical zero-knowledge."}, "keywords": ["QSD", "oracle"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The QSZK-completeness of QSD relativizes, as shown by Ben-David and Kothari in their paper on \"Oracle Separations For Quantum Statistical Zero-Knowledge\" #REFR, where they prove the existence of an oracle relative to which quantum statistical zero-knowledge does not contain UP intersect coUP.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The QSZK-completeness of QSD relativizes, and it has been shown that quantum statistical zero knowledge does not contain UP relative to a random oracle with probability 1 #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The QSZK-completeness of QSD relativizes, and the authors prove that quantum statistical zero knowledge does not contain UP relative to a random #REFR oracle with probability 1.", "given_citation_intent": "background", "given_keywords": ["QSD", "oracle"]}]}
{"citing_id": "2303.13794v1", "cited_id": "1712.07629", "section_title": "Two-Stage Pipeline Of Image Matching", "citation": "The concatenated key-points of stage-one X #REFR 1 and X 1 2 are then fed .After the loop in 1, the green, yellow, and red cluster will be gathered, while the pink one and the blue one will be rejected.", "text_before_citation": ["As shown in Figure 4 , The two-stages pipeline support plugging any image matching models or recipe of models into both stages, such that the models set for stage 1 is", "M 1 = {m 1 1 , m 1 2 , \u2022 \u2022 \u2022 , m 1", "n1 }, and the models set for stage 2 is", "M 2 = {m 2 1 , m 2 2 , \u2022 \u2022 \u2022 , m 2 n2 }.", "Each of the models in stage-one produce a set of key-points for both images."], "text_after_citation": ["If the pink one is included, more areas without useful information will be included, making rejected the pink cluster makes sense.", "into MKPC algorithm.", "Based on the two images I 1 and I 2 and the key-points of stage-one X 1 1 and X #OTHEREFR 2 , The MKPC generates the cropped critical regions for both images (I 1 and I 2 ).", "With the cropped regions of both images, each of the models in stage-two outputs the matched key-points for stage-two (X 2 1 and X 2 2 ).", "Afterwards, the models in stage-two matches on the cropped area, producing the key-points for the second stage X 2 1 and X 2 2 . Concatenating the key-points of both stages, i.e."], "citing_paper_content": {"title": "Efficient And Accurate Co-Visible Region Localization With Matching Key-Points Crop (Mkpc): A Two-Stage Pipeline For Enhancing Image Matching Performance", "abstract": "Image matching is a classic and fundamental task in computer vision. In this paper, under the hypothesis that the areas outside the co-visible regions carry little information, we propose a matching key-points crop (MKPC) algorithm. The MKPC locates, proposes and crops the critical regions, which are the co-visible areas with great efficiency and accuracy. Furthermore, building upon MKPC, we propose a general two-stage pipeline for image matching, which is compatible to any image matching models or combinations. We experimented with plugging SuperPoint + SuperGlue into the two-stage pipeline, whose results show that our method enhances the performance for outdoor pose estimations. What's more, in a fair comparative condition, our method outperforms the SOTA on Image Matching Challenge 2022 Benchmark, which represents the hardest outdoor benchmark of image matching currently. * denotes contributing equally to this work. Preprint. Under review."}, "cited_paper_content": {"title": "Superpoint: Self-Supervised Interest Point Detection And Description", "abstract": "This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB."}, "keywords": ["concatenated key-points"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To detect and describe interest points suitable for multiple-view geometry problems, the authors propose a self-supervised framework that operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To locate and crop the critical regions efficiently and accurately, the authors proposed a matching key-points crop (MKPC) algorithm, which can be used in combination with SuperPoint + SuperGlue, as demonstrated in their experiments #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To locate and crop the critical regions with great efficiency and accuracy, the proposed matching key-points crop (MKPC) algorithm crops the concatenated key-points of the co-visible areas, as presented in Figure 4. #REFR", "given_citation_intent": "method", "given_keywords": ["concatenated key-points"]}]}
{"citing_id": "2304.05336v1", "cited_id": "1910.10683", "section_title": "Lemmatization", "citation": "Models based on the T5 #REFR model architecture have achieved state-of-the-art results in various natural language processing challenges and can be fine-tuned for specific tasks.", "text_before_citation": [], "text_after_citation": ["One of the applications of T5 can be lemmatization, the process of reducing a word or phrase to its basic form (lemma).", "In Slavic languages such as Polish, Czech and Russian, lemmatization is particularly important due to the complex inflection of these languages.", "We approached the lemmatization task as a textto-text problem.", "The input to the model is an inflected phrase or named entity, which can consist of several word forms.", "For example, it can consist of nouns in singular or plural form, or verbs in different tenses."], "citing_paper_content": {"title": "Exploring The Use Of Foundation Models For Named Entity Recognition And Lemmatization Tasks In Slavic Languages", "abstract": "This paper describes Adam Mickiewicz University's (AMU) solution for the 4th Shared Task on SlavNER. The task involves the identification, categorization, and lemmatization of named entities in Slavic languages. Our approach involved exploring the use of foundation models for these tasks. In particular, we used models based on the popular BERT and T5 model architectures. Additionally, we used external datasets to further improve the quality of our models. Our solution obtained promising results, achieving high metrics scores in both tasks. We describe our approach and the results of our experiments in detail, showing that the method is effective for NER and lemmatization in Slavic languages."}, "cited_paper_content": {"title": "Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."}, "keywords": ["various natural language"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our approach involved exploring the use of foundation models for named entity recognition and lemmatization tasks in Slavic languages, using models based on the popular BERT and T5 model architectures, as well as external datasets to further improve the quality of our models (Author et al., Year).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To explore the landscape of transfer learning techniques for natural language processing, the authors of this paper introduced a unified framework that converts every language problem into a text-to-text format, achieving state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more, which provides valuable background for our approach in this study. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Various natural language understanding tasks were explored in the reference paper, including summarization, question answering, and text classification, among others, using a unified text-to-text transformer framework. #REFR provides valuable background information on transfer learning techniques for natural language processing.", "given_citation_intent": "background", "given_keywords": ["various natural language"]}]}
{"citing_id": "2304.06831v1", "cited_id": "1704.01212", "section_title": "Dgnn Booster V2", "citation": "However, these designs suffer from high energy consumption and low computation resource utilization because of temporal data dependencies. #REFR Lack of parallelism between GNN and RNN.", "text_before_citation": ["Integrated DGNN GCRN-M2 #OTHEREFR , GC-LSTM #OTHEREFR LRGCN #OTHEREFR , RE-Net #OTHEREFR \u2022 Data dependencies between GNN and RNN in adjacent time steps.", "\u2022 Dependent GNN at different time steps.", "Weights-evolved DGNN EvolveGCN #OTHEREFR \u2022 Weights for GNN are evolved by RNN.", "\u2022 Independent GNN at different time steps.", "However, there still remain some challenges on DGNN hardware deployment. #OTHEREFR High energy consumption and low computation resource utilization. Previous works primarily focus on deploying DGNNs on GPUs."], "text_after_citation": ["Previous research focuses on treating GNN and RNN as separate parts, which limits parallelism.", "#OTHEREFR Lack of integrating GNN and RNN optimizations together into a single system.", "Previous research usually optimizes GNN and RNN individually, which limits achieving optimal hardware efficiency."], "citing_paper_content": {"title": "Dgnn-Booster: A Generic Fpga Accelerator Framework For Dynamic Graph Neural Network Inference", "abstract": "Dynamic Graph Neural Networks (DGNNs) are becoming increasingly popular due to their effectiveness in analyzing and predicting the evolution of complex interconnected graph-based systems. However, hardware deployment of DGNNs still remains a challenge. First, DGNNs do not fully utilize hardware resources because temporal data dependencies cause low hardware parallelism. Additionally, there is currently a lack of generic DGNN hardware accelerator frameworks, and existing GNN accelerator frameworks have limited ability to handle dynamic graphs with changing topologies and node features. To address the aforementioned challenges, in this paper, we propose DGNN-Booster, which is a novel Field-Programmable Gate Array (FPGA) accelerator framework for real-time DGNN inference using High-Level Synthesis (HLS). It includes two different FPGA accelerator designs with different dataflows that can support the most widely used DGNNs. We showcase the effectiveness of our designs by implementing and evaluating two representative DGNN models on ZCU102 board and measuring the end-to-end performance. The experiment results demonstrate that DGNN-Booster can achieve a speedup of up to 5.6\u00d7 compared to the CPU baseline (6226R), 8.4\u00d7 compared to the GPU baseline (A6000) and 2.1\u00d7 compared to the FPGA baseline without applying optimizations proposed in this paper. Moreover, DGNN-Booster can achieve over 100\u00d7 and over 1000\u00d7 runtime energy efficiency than the CPU and GPU baseline respectively. Our implementation code and on-board measurements are publicly"}, "cited_paper_content": {"title": "Neural Message Passing For Quantum Chemistry", "abstract": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels."}, "keywords": ["RNN"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "However, to address the challenges of low computation resource utilization and high energy consumption in deploying DGNNs, a promising approach is to use hardware accelerators. In this regard, the work by Gilmer et al. on Message Passing Neural Networks (MPNNs) #REFR provides a useful framework for accelerating DGNN inference on Field-Programmable Gate Arrays (FPGAs) using High-Level Synthesis (HLS).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of hardware deployment of DGNNs, including high energy consumption and low computation resource utilization, previous works have primarily focused on deploying DGNNs on GPUs (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Previous works primarily focus on deploying DGNNs on GPUs, but there is still a challenge in terms of dependent RNN and GNN at different time steps, low computation resource utilization, and high energy consumption #REFR.", "given_citation_intent": "background", "given_keywords": ["RNN"]}]}
{"citing_id": "2303.13520v1", "cited_id": "1310.2963", "section_title": "Shareability Network", "citation": "In this study, we extend the shareability network approach #REFR to quantify the potential ridesharing efficiency.", "text_before_citation": [], "text_after_citation": ["The shareability network transforms the spatiotemporal distribution of trips into a theoretic graph G = (V, E), where V is the set of trips and E the set of edges that indicates the shareability between trips.", "Following the original work, we assume that at most two trips can be shared.", "Two trips are shareable if serving them together with one vehicle can (1) generate total travel duration savings and (2) only incur an acceptable delay for the involved passengers.", "Each trip x \u2208 V can be represented as a vector (o", "x , d x , t o x , t d x )"], "citing_paper_content": {"title": "Quantifying The Uneven Efficiency Benefits Of Ridesharing Market Integration", "abstract": "Ridesharing is recognized as one of the key pathways to sustainable urban mobility. With the emergence of Transportation Network Companies (TNCs) such as Uber and Lyft, the ridesharing market has become increasingly fragmented in many cities around the world, leading to efficiency loss and increased traffic congestion. While an integrated ridesharing market (allowing sharing across TNCs) can improve the overall efficiency, how such benefits may vary across TNCs based on actual market characteristics is still not well understood. In this study, we extend a shareability network framework to quantify and explain the efficiency benefits of ridesharing market integration using available TNC trip records. Through a case study in Manhattan, New York City, the proposed framework is applied to analyze a real-world ridesharing market with 3 TNCs-Uber, Lyft, and Via. It is estimated that a perfectly integrated market in Manhattan would improve ridesharing efficiency by 13.3%, or 5% of daily TNC vehicle hours traveled. Further analysis reveals that (1) the efficiency improvement is negatively correlated with the overall demand density and inter-TNC spatiotemporal unevenness (measured by network modularity), (2) market integration would generate a larger efficiency improvement in a competitive market, and (3) the TNC with a higher intra-TNC demand concentration (measured by clustering coefficient) would benefit less from market integration. As the uneven benefits may deter TNCs from collaboration, we also illustrate how to quantify each TNC's marginal contribution based on the Shapley value, which can be used to ensure equitable profit allocation. These results can help market regulators and business alliances to evaluate and monitor market efficiency and dynamically adjust their strategies, incentives, and profit allocation schemes to promote market integration and collaboration."}, "cited_paper_content": {"title": "Quantifying The Benefits Of Vehicle Pooling With Shareability Networks", "abstract": "Taxi services are a vital part of urban transportation, and a considerable contributor to traffic congestion and air pollution causing substantial adverse effects on human health. Sharing taxi trips is a possible way of reducing the negative impact of taxi services on cities, but this comes at the expense of passenger discomfort quantifiable in terms of a longer travel time. Due to computational challenges, taxi sharing has traditionally been approached on small scales, such as within airport perimeters, or with dynamical ad hoc heuristics. However, a mathematical framework for the systematic understanding of the tradeoff between collective benefits of sharing and individual passenger discomfort is lacking. Here we introduce the notion of shareability network, which allows us to model the collective benefits of sharing as a function of passenger inconvenience, and to efficiently compute optimal sharing strategies on massive datasets. We apply this framework to a dataset of millions of taxi trips taken in New York City, showing that with increasing but still relatively low passenger discomfort, cumulative trip length can be cut by 40% or more. This benefit comes with reductions in service cost, emissions, and with split fares, hinting toward a wide passenger acceptance of such a shared service. Simulation of a realistic online system demonstrates the feasibility of a shareable taxi service in New York City. Shareability as a function of trip density saturates fast, suggesting effectiveness of the taxi sharing system also in cities with much sparser taxi fleets or when willingness to share is low."}, "keywords": ["potential ridesharing efficiency"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors extend a shareability network framework to quantify and explain the efficiency benefits of ridesharing market integration using available TNC trip records, and apply it to a real-world ridesharing market in Manhattan, New York City, showing that a perfectly integrated market would improve ridesharing efficiency by 13.3% #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently compute optimal sharing strategies on massive datasets, the authors introduced the notion of shareability network, which was applied to a dataset of millions of taxi trips taken in New York City, resulting in a 40% or more reduction in cumulative trip length with increasing but still relatively low passenger discomfort, as well as reductions in service cost and emissions, and split fares, indicating a wide passenger acceptance of such a shared service (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors extended the shareability network framework to quantify the potential ridesharing efficiency of an integrated market, and applied it to a real-world ridesharing market in Manhattan, New York City, showing that a perfectly integrated market would improve ridesharing efficiency by 13.3%, or 5% of daily TNC vehicle hours traveled, which is consistent with the findings in the reference paper on quantifying the benefits of vehicle pooling with shareability networks. #REFR", "given_citation_intent": "method", "given_keywords": ["potential ridesharing efficiency"]}]}
{"citing_id": "2304.01238v2", "cited_id": "1808.06226", "section_title": "Large Language Models", "citation": "We used the HuggingFace implementation of the Flan-T5 model (google/ Flan-t5-base) with the SentencePiece tokenizer #REFR .", "text_before_citation": ["The distance between the resulting embeddings is measured using the cosine similarity.", "Flan-T5.", "Flan-T5 (2022) is a family of models based on T5 (2019) #OTHEREFR , an encoder-decoder transformer architecture trained on multiple language tasks.", "The Flan-T5 models have undergone instruction-finetuning on over 1,800 language tasks, leading to a significant enhancement in their reasoning skills and promptability.", "However, it is worth noting that the Flan-T5 models were not trained to perform spam detection tasks."], "text_after_citation": ["Our experimentation included the small version of the Flan-T5 model (80M parameters), but it demonstrated limited generalization capabilities, which is why it was excluded from this study.", "The Flan-T5 model is a Seq2Seq model that is capable of generating textual outputs, as opposed to binary labels or probabilities.", "To leverage the capabilities of this model for spam detection, we fine-tuned it as a new task, introducing a dedicated prefix of \"classify as ham or spam:\" to every sample.", "As a result, the model was trained to correctly output either \"ham\" or \"spam\" based on the input text.", "To obtain numerical values for classification metrics, a postprocessing step was utilized to map the textual labels to 0 and 1."], "citing_paper_content": {"title": "Spam-T5: Benchmarking Large Language Models For Few-Shot Email Spam Detection", "abstract": "This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\u00efve Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly when there are a limited number of training samples available. Our code is publicly available at https://github.com/jpmorganchase/emailspamdetection."}, "cited_paper_content": {"title": "Sentencepiece: A Simple And Language Independent Subword Tokenizer And Detokenizer For Neural Text Processing", "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece."}, "keywords": ["SentencePiece tokenizer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly when there are a limited number of training samples available. The Flan-T5 models were not trained to perform spam detection tasks, but our approach builds on the use of subword units for Neural-based text processing, which was introduced in Sentencepiece #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To tokenize raw sentences for neural-based text processing, the authors utilized SentencePiece, a language-independent subword tokenizer and detokenizer, which can train subword models directly from raw sentences, making it a purely end-to-end and language independent system #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Flan-T5 was trained on multiple language tasks, but for subword tokenization, the authors of Spam-T5 utilized the SentencePiece tokenizer #REFR.", "given_citation_intent": "method", "given_keywords": ["SentencePiece tokenizer"]}]}
{"citing_id": "2303.09824v3", "cited_id": "1801.06503", "section_title": "A. Imitation Learning", "citation": "This is an active method based on the Follow-the-Leader algorithm #REFR , each validation iteration is an online learning example.", "text_before_citation": ["2) Direct Policy Learning: Direct Policy Learning (DPL), a training method based on BC, evaluates the current policy and then obtains more suitable training data for selfoptimization.", "Compared with BC, the main advantage of DPL leverages expert trajectories to instruct the agent how to recover from current errors #OTHEREFR .", "In this way, DPL alleviates the limitation of BC due to insufficient data.", "In this section, we summarize a series of DPL methods. Ross et al.", "#OTHEREFR construct a classical online IL method named Dataset Aggregation (DAgger) method."], "text_after_citation": ["The method modifies the main classifier or regressor on all state\u2212action pairs experienced by the agent.", "DAgger is a novel solution for sequential prediction problems, however, its learning efficiency might be suppressed by the far distance between policy space and learning space. In reply, He et al.", "#OTHEREFR propose a DAgger by coaching algorithm which employs a coach to demonstrate easy-to-learn policies for the learner and the demonstrated policies gradually converge to label.", "To better instruct the agent, the coach establishes a compromised policy which is not much worse than a ground truth control signal and much better than novice predicted action. As shown in Fig.", "3 , \u03c0 is the predicted command, \u03c0 * shows the expert trajectory, and \u03c0 presents the compromised trajectory."], "citing_paper_content": {"title": "Motion Planning For Autonomous Driving: The State Of The Art And Future Perspectives", "abstract": "Thanks to the augmented convenience, safety advantages, and potential commercial value, Intelligent vehicles (IVs) have attracted wide attention throughout the world. Although a few autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their implementation is still restricted to small-scale validation due to various issues, among which precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verification scenarios of driving tasks are points of concern. Experimental platforms are reviewed to facilitate readers in selecting suitable training and validation methods. Finally, the current challenges and future directions are discussed. The sideby-side comparison presented in this survey not only helps to gain insights into the strengths and limitations of the reviewed methods but also assists with system-level design choices. Index Terms-Pipeline planning, end-to-end planning, imitation learning, reinforcement learning, parallel learning. I. INTRODUCTION I NTELLIGENT vehicles (IVs) have gained considerable attention from government, industry, academia, and the"}, "cited_paper_content": {"title": "Global Overview Of Imitation Learning", "abstract": "Imitation Learning is a sequential task where the learner tries to mimic an expert's action in order to achieve the best performance. Several algorithms have been proposed recently for this task. In this project, we aim at proposing a wide review of these algorithms, presenting their main features and comparing them on their performance and their regret bounds."}, "keywords": ["online learning example", "Follow-the-Leader algorithm"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this section, we summarize a series of DPL methods. Ross et al. ( #REFR ) construct a classical online IL method named Dataset Aggregation (DAgger) method.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitation of insufficient data in BC, Direct Policy Learning (DPL) leverages expert trajectories to instruct the agent how to recover from current errors, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Ross et al. proposed a classical online learning example named Dataset Aggregation (DAgger) method, which is a Direct Policy Learning (DPL) algorithm that leverages expert trajectories to instruct the agent how to recover from current errors, thus alleviating the limitation of BC due to insufficient data #REFR.", "given_citation_intent": "method", "given_keywords": ["online learning example", "Follow-the-Leader algorithm"]}]}
{"citing_id": "2304.14749v1", "cited_id": "1810.03993", "section_title": "4.2.3", "citation": "Yet organisationfocused tools to provide information on points in the AI lifecycle (such as [40, #REFR ) are of limited help where information about interconnections between actors is needed.", "text_before_citation": ["Expanding the accountability horizon. The accountability horizon thus poses major problems for accountability.", "Interventions are needed to help expand the accountability horizon and better place actors to (i) know more about their own supply chains, and (ii) support others in knowing more about theirs."], "text_after_citation": ["Alternatively, tracking data flow between actors could help understand interconnections beyond the first few steps #OTHEREFR , as could legal and institutional mechanisms requiring information about arrangements.", "'Know your customer' requirements around customer on-boarding for AI services (common in financial services) could help providers understand customers' purposes and intentions [26] (though these may only give some visibility over one or two steps in the chain).", "Moreover, recent CJEU data protection jurisprudence regarding transparency rights confirms that data subjects have the right to know the identity of any recipients of their personal data [21], which may help understand data flows.", "However, where the data controller does not know the recipients' identity-which may be common due to the accountability horizon-data subjects can instead be told about the categories of recipients of the data [21] (significantly less useful information).", "A particular difficulty, however, is that accountability is contextual #OTHEREFR ."], "citing_paper_content": {"title": "Understanding Accountability In Algorithmic Supply Chains", "abstract": "Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by 'many hands'. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accountability (particularly regarding general purpose AI services). To highlight ways forward and areas warranting attention, we further discuss some implications raised by supply chains: challenges for allocating accountability stemming from distributed responsibility for systems between actors, limited visibility due to the accountability horizon, service models of use and liability, and cross-border supply chains and regulatory arbitrage. CCS CONCEPTS \u2022 Social and professional topics \u2192 Computing / technology policy; Socio-technical systems."}, "cited_paper_content": {"title": "Model Cards For Model Reporting", "abstract": "Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation."}, "keywords": ["AI lifecycle"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Expanding the accountability horizon. The accountability horizon thus poses major problems for accountability. Interventions are needed to help expand the accountability horizon and better place actors to (i) know more about their own supply chains, and (ii) support others in knowing more about theirs, as recommended by the use of model cards for transparent model reporting #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To expand the accountability horizon, it is recommended that released machine learning models be accompanied by documentation detailing their performance characteristics, such as benchmarked evaluation in a variety of conditions and relevant information about the context in which models are intended to be used #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the challenges of accountability in the AI lifecycle, it is recommended to provide benchmarked evaluation of machine learning models in a variety of conditions, as proposed by Mitchell et al. in their framework for model reporting called model cards #REFR.", "given_citation_intent": "background", "given_keywords": ["AI lifecycle"]}]}
{"citing_id": "2303.04683v2", "cited_id": "1204.3818", "section_title": "Transforming Problem P 1 Into Parametric Convex Optimization Problems", "citation": "With ( * , * , * ) denoting a globally optimal solution to Problem P 2 (and hence ( * , * ) denoting a globally optimal solution to Problem P 1 ), clearly setting ( , ) as ( * , * ) of #REFR and (18) satisfies( , ) = 0.", "text_before_citation": ["With Lemma 5.1 presented above, we now describe how to solve Problem P 2 using P 3 ( , ).", "Let [ # ( , ), # ( , )] denote a globally optimal solution to P 3 ( , ), where", "EQUATION", "1 ( ,", "EQUATION"], "text_after_citation": ["(23) Based on the above, solving Problem P 2 and hence P 1 can be transformed into solving (23) to obtain P 3 ( * , * ), and then setting", "[ * , * ] as [ # ( * , * ), # ( * , * )]", ", a globally optimal solution to P 3 ( * , * ), according to Lemma 5.1.", "Based on the above idea, we present Algorithm 1 next, where it will become clear that solving P 1 becomes solving a series of parametric convex optimization P 3 ( ( ) , ( ) ), with denoting the iteration index.", "(24) Readers may notice that our Lemma 5.1 provides just a necessary condition for a global optimum of Problem P 2 ."], "citing_paper_content": {"title": "Optimizing Utility-Energy Efficiency For The Metaverse Over Wireless Networks Under Physical Layer Security", "abstract": "The Metaverse, an emerging digital space, is expected to offer various services mirroring the real world. Wireless communications for mobile Metaverse users should be tailored to meet the following user characteristics: 1) emphasizing application-specific perceptual utility instead of simply the transmission rate, 2) concerned with energy efficiency due to the limited device battery and energy intensiveness of some applications, and 3) caring about security as the applications may involve sensitive personal data. To this end, this paper incorporates application-specific utility, energy efficiency, and physical-layer security (PLS) into the studied optimization in a wireless network for the Metaverse. Specifically, after introducing utility-energy efficiency (UEE) to represent each Metaverse user's application-specific objective under PLS, we formulate an optimization to maximize the network's weighted sum-UEE by deciding users' transmission powers and communication bandwidths. The formulated problem belongs to the sum-of-ratios optimization, for which prior studies have demonstrated its difficulty. Nevertheless, our proposed algorithm 1) obtains the global optimum for the weighted sum-UEE optimization, via a transform to parametric convex optimization problems, 2) applies to any utility function which is concave, increasing, and twice differentiable, and 3) achieves a linear time complexity in the number of users (the optimal complexity in the order sense). Simulations confirm the superiority of our algorithm over other approaches. We explain that our technique for solving the sum-of-ratios optimization is applicable to other optimization problems in wireless networks and mobile computing."}, "cited_paper_content": {"title": "Throughput Optimal Policies For Energy Harvesting Wireless Transmitters With Non-Ideal Circuit Power", "abstract": "Characterizing the fundamental tradeoffs for maximizing energy efficiency (EE) versus spectrum efficiency (SE) is a key problem in wireless communication. In this paper, we address this problem for a point-to-point additive white Gaussian noise (AWGN) channel with the transmitter powered solely via energy harvesting from the environment. In addition, we assume a practical on-off transmitter model with non-ideal circuit power, i.e., when the transmitter is on, its consumed power is the sum of the transmit power and a constant circuit power. Under this setup, we study the optimal transmit power allocation to maximize the average throughput over a finite horizon, subject to the time-varying energy constraint and the non-ideal circuit power consumption. First, we consider the off-line optimization under the assumption that the energy arrival time and amount are a priori known at the transmitter. Although this problem is non-convex due to the non-ideal circuit power, we show an efficient optimal solution that in general corresponds to a two-phase transmission: the first phase with an EE-maximizing on-off power allocation, and the second phase with a SE-maximizing power allocation that is non-decreasing over time, thus revealing an interesting result that both the EE and SE optimizations are unified in an energy harvesting communication system. We then extend the optimal off-line algorithm to the case with multiple parallel AWGN channels, based on the principle of nested optimization. Finally, inspired by the off-line optimal solution, we propose a new online algorithm under the practical setup with only the past and present energy state information (ESI) known at the transmitter."}, "keywords": ["globally optimal solution"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "With Lemma 5.1 presented above, we now describe how to solve Problem P 2 using P 3 ( , ). Let [ #REFR ] denote a globally optimal solution to P 3 ( , ), where EQUATION 1 ( , EQUATION.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of maximizing energy efficiency versus spectrum efficiency in wireless communication, the authors of this manuscript propose an optimization algorithm that achieves a globally optimal solution, building on the efficient optimal solution presented in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain a globally optimal solution for Problem P 2, the authors propose using the solution to P 3 ( , ) as described in Lemma 5.1, which provides a two-phase transmission strategy that maximizes both energy efficiency and spectrum efficiency, as shown in #REFR.", "given_citation_intent": "background", "given_keywords": ["globally optimal solution"]}]}
{"citing_id": "2303.05546v1", "cited_id": "1505.04474", "section_title": "Human-Object Interaction Detection", "citation": "The problem of detecting interactions between humans and objects was originally introduced in #REFR and has drawn immense attention in the computer vision community since then.", "text_before_citation": [], "text_after_citation": ["Most of the research efforts on this topic #OTHEREFR use a two-stage solution in which human/object locations are extracted along with their semantic labels by an off-the-shelf object detector first, and an interaction classification model is learnt on pairwise human-object features.", "Apart from human/object appearances, there exist models that make use of contextual features #OTHEREFR , spatial layouts #OTHEREFR and human pose estimations #OTHEREFR .", "Inspired by one-stage object detection efforts, researchers lately try to formulate end-to-end HOI detection approaches where human/instance localization and interaction classification are performed in parallel #OTHEREFR . These methods are analogous to CNN-based (e.g. YOLO #OTHEREFR ) and Transformer-based (e.g. DETR #OTHEREFR ) end-to-end object detectors.", "PPDM #OTHEREFR takes a step forward and drops the need for heuristically created \"anchors\", formulating HOI detection as a point matching problem between human and object locations.", "Regardless of being one-stage or two-stage, these methods rely on strong supervision which is costly to acquire."], "citing_paper_content": {"title": "Weakly-Supervised Hoi Detection From Interaction Labels Only And Language/Vision-Language Priors", "abstract": "Human-object interaction (HOI) detection aims to extract interacting human-object pairs and their interaction categories from a given natural image. Even though the labeling effort required for building HOI detection datasets is inherently more extensive than for many other computer vision tasks, weakly-supervised directions in this area have not been sufficiently explored due to the difficulty of learning human-object interactions with weak supervision, rooted in the combinatorial nature of interactions over the object and predicate space. In this paper, we tackle HOI detection with the weakest supervision setting in the literature, using only image-level interaction labels, with the help of a pretrained vision-language model (VLM) and a large language model (LLM). We first propose an approach to prune non-interacting human and object proposals to increase the quality of positive pairs within the bag, exploiting the grounding capability of the vision-language model. Second, we use a large language model to query which interactions are possible between a human and a given object category, in order to force the model not to put emphasis on unlikely interactions. Lastly, we use an auxiliary weaklysupervised preposition prediction task to make our model explicitly reason about space. Extensive experiments and ablations show that all of our contributions increase HOI detection performance."}, "cited_paper_content": {"title": "Visual Semantic Role Labeling", "abstract": "In this paper we introduce the problem of Visual Semantic Role Labeling: given an image we want to detect people doing actions and localize the objects of interaction. Classical approaches to action recognition either study the task of action classification at the image or video clip level or at best produce a bounding box around the person doing the action. We believe such an output is inadequate and a complete understanding can only come when we are able to associate objects in the scene to the different semantic roles of the action. To enable progress towards this goal, we annotate a dataset of 16K people instances in 10K images with actions they are doing and associate objects in the scene with different semantic roles for each action. Finally, we provide a set of baseline algorithms for this task and analyze error modes providing directions for future work."}, "keywords": ["objects", "computer vision community"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To increase the quality of positive pairs within the bag, the authors propose an approach to prune non-interacting human and object proposals, exploiting the grounding capability of the vision-language model #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To better understand the problem of human-object interaction detection, the authors of this manuscript cite the work on Visual Semantic Role Labeling, which introduces the task of detecting people doing actions and localizing the objects of interaction in an image, and proposes a dataset of annotated instances for this purpose. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' dataset annotation of objects in the scene with different semantic roles for each action is a valuable contribution to the computer vision community, as it enables a more complete understanding of the interactions between humans and #REFR.", "given_citation_intent": "background", "given_keywords": ["objects", "computer vision community"]}]}
{"citing_id": "2304.03980v1", "cited_id": "1911.11236", "section_title": "Methodology", "citation": "We considered RandLA-Net #REFR , one of the most famous point-based architectures as a reference, in order to frame the problem in a perspective different from images (i.e., using MLPs in place of convolutional setting).", "text_before_citation": [], "text_after_citation": ["RandLA-Net is an efficient point-based lightweight network composed of an MLP based encoder-decoder structure that achieves remarkably high efficiency in terms of memory and computation.", "In addition, we evaluate on Cylin-der3D #OTHEREFR voxel based architecture for comparison.", "Nonetheless, the framework can be applied on top of any architecture for point cloud semantic segmentation.", "SemanticKITTI #OTHEREFR has been chosen as a reference dataset, since it is one of the most popular benchmarks for LiDAR semantic segmentation in autonomous driving.", "Se-manticKITTI consists of 43, 552 densely annotated LiDAR scans, 19, 130 for training, and 4, 071 for validating (that we used for testing, as done by all competing works being the test labels not publicly available)."], "citing_paper_content": {"title": "Continual Learning For Lidar Semantic Segmentation: Class-Incremental And Coarse-To-Fine Strategies On Sparse Data", "abstract": "During the last few years, Continual Learning (CL) strategies for image classification and segmentation have been widely investigated designing innovative solutions to tackle catastrophic forgetting, like knowledge distillation and selfinpainting. However, the application of continual learning paradigms to point clouds is still unexplored and investigation is required, especially using architectures that capture the sparsity and uneven distribution of LiDAR data. The current paper analyzes the problem of class incremental learning applied to point cloud semantic segmentation, comparing approaches and state-of-the-art architectures. To the best of our knowledge, this is the first example of classincremental continual learning for LiDAR point cloud semantic segmentation. Different CL strategies were adapted to LiDAR point clouds and tested, tackling both classic finetuning scenarios and the Coarse-to-Fine learning paradigm. The framework has been evaluated through two different architectures on SemanticKITTI [2, 16], obtaining results in line with state-of-the-art CL strategies and standard offline learning."}, "cited_paper_content": {"title": "Randla-Net: Efficient Semantic Segmentation Of Large-Scale Point Clouds", "abstract": "We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200X faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI."}, "keywords": ["famous point-based architectures", "RandLA-Net"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To capture the sparsity and uneven distribution of LiDAR data, the authors propose a novel class-incremental and coarse-to-fine learning paradigm for point cloud semantic segmentation. The proposed framework is evaluated on SemanticKITTI and achieves results in line with state-of-the-art continual learning strategies and standard offline learning. The authors also compare their approach with existing methods, including RandLA-Net #REFR, which efficiently infers per-point semantics for large-scale point clouds using random point sampling and a novel local feature aggregation module.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of efficient semantic segmentation for large-scale 3D point clouds, the authors propose RandLA-Net, which uses random point sampling and a novel local feature aggregation module to achieve remarkable computation and memory efficiency while preserving geometric details, as demonstrated by their experiments on Semantic3D and SemanticKITTI datasets. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of efficient semantic segmentation for large-scale 3D point clouds, the authors propose RandLA-Net, an efficient and lightweight neural architecture that can process 1 million points in a single pass with up to 200X faster than existing approaches, making it a promising alternative to famous point-based architectures for LiDAR semantic segmentation. #REFR", "given_citation_intent": "method", "given_keywords": ["famous point-based architectures", "RandLA-Net"]}]}
{"citing_id": "2304.14474v1", "cited_id": "1411.2635", "section_title": "Examples", "citation": "We have the following result for the Gaussian RKHS, which parallels Maurer's result in #REFR Section 3.2] if we impose the same assumption that T is a projection of another RKHS function class onto a set of samples.", "text_before_citation": ["so our improvement consists in removing the logarithmic factor in front of the second term on the right-hand side.", "Our second example involves functions in a Reproducing Kernel Hilbert Space (RKHS).", "Due to space limitations, we can only give a brief sketch; the reader is invited to consult [3, Chaps. 2 and 4] for the background.", "Let X = B k 2 ( \u221a kR) for some R > 0.", "Let (H K , \u2022, \u2022 K ) be an RKHS associated with a Mercer kernel K : X \u00d7 X \u2192 R; then we consider F = I K (B K (\u033a)), where B K (\u033a) = {f \u2208 H K : f K \u2264 \u033a} is the zero-centered closed ball of radius \u033a in H K and I K is the embedding map from H K into the space C(X) of continuous real-valued functions on X equipped with the uniform norm \u2022 X ."], "text_after_citation": ["Proposition 3.", "Consider the Gaussian kernel K(x, y) = exp \u2212 1 2\u03c3 2 x \u2212 y 2 2 , where \u03c3 2 > 0 is the kernel bandwidth. Then, for any T", "EQUATION", "Remark 4.", "See the Remark following Theorem 2 for the motivation behind our choices of X and T ."], "citing_paper_content": {"title": "A Chain Rule For The Expected Suprema Of Bernoulli Processes *", "abstract": "We obtain an upper bound on the expected supremum of a Bernoulli process indexed by the image of an index set under a uniformly Lipschitz function class in terms of properties of the index set and the function class, extending an earlier result of Maurer for Gaussian processes. The proof makes essential use of recent results of Bednorz and Lata\u0142a on the boundedness of Bernoulli processes. * This research was suppoorted in part by DARPA under the Learning with Less Labels (LwLL) program."}, "cited_paper_content": {"title": "A Chain Rule For The Expected Suprema Of Gaussian Processes", "abstract": "The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models."}, "keywords": ["Gaussian RKHS"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For further details on the estimation of nonlinear transformations or the analysis of learning algorithms using composite classes, the reader is referred to the reference paper by #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' result on the upper bound of the expected supremum of a Bernoulli process indexed by an index set under a uniformly Lipschitz function class, in terms of properties of the index set and the function class, extends the earlier result of Maurer for Gaussian processes, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' result on the upper bound of the expected supremum of a Bernoulli process indexed by an index set under a uniformly Lipschitz function class can be compared with the earlier result of Maurer for Gaussian processes, as discussed in #REFR. The manuscript also briefly introduces functions in a Reproducing Kernel Hilbert Space (RKHS), which readers can find more details about in [3, Chaps. 2 and 4].", "given_citation_intent": "result", "given_keywords": ["Gaussian RKHS"]}]}
{"citing_id": "2304.00295v1", "cited_id": "1810.03292", "section_title": "Results And Discussion", "citation": "To visualize the effect of feature decomposition, we adopt deep neural network interpretability methods in #REFR .", "text_before_citation": ["Results on CelebA.", "To illustrate each model's ability for vision tasks, we choose smiling, wavy hair, and attractive to form three binary classification tasks.", "As shown in Figure 5, Fair-CDA achieves SOTA performance followed by two mixup methods.", "It is worth mentioning that the DP and EO gap of these methods on the smiling recognition task is smaller compared with other tasks, which is a relatively fair scenario, but Fair-CDA can still improve the fairness.", "Also, Fair-CDA is the only method that achieves considerable accuracy given high fairness requirements on both tasks."], "text_after_citation": ["We draw the saliency map on the wavy hair recognition task, as shown in Figure 4 .", "Sensitive features are those strongly related to gender, while non-sensitive features are those strongly related to wavy hair.", "It can be seen that the saliency maps of sensitive features focus more on the whole face, while those of non-sensitive features focus more on the hair of a man/woman.", "Additionally, we evaluate Fair-CDA on more sensitive features on CelebA dataset.", "We implement Fair-CDA on the same task as that in #OTHEREFR"], "citing_paper_content": {"title": "Fair-Cda: Continuous And Directional Augmentation For Group Fairness", "abstract": "In this work, we propose Fair-CDA, a fine-grained data augmentation strategy for imposing fairness constraints. We use a feature disentanglement method to extract the features highly related to the sensitive attributes. Then we show that group fairness can be achieved by regularizing the models on transition paths of sensitive features between groups. By adjusting the perturbation strength in the direction of the paths, our proposed augmentation is controllable and auditable. To alleviate the accuracy degradation caused by fairness constraints, we further introduce a calibrated model to impute labels for the augmented data. Our proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness. Experimental results show that Fair-CDA consistently outperforms state-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA and Movie-Lens. Especially, Fair-CDA obtains an 86.3% relative improvement for fairness while maintaining the accuracy on the Adult dataset. Moreover, we evaluate Fair-CDA in an online recommendation system to demonstrate the effectiveness of our method in terms of accuracy and fairness."}, "cited_paper_content": {"title": "Sanity Checks For Saliency Maps", "abstract": "Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings."}, "keywords": ["feature decomposition", "interpretability methods"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To evaluate the fairness of our proposed Fair-CDA method, we compared it with state-of-the-art methods on widely-used benchmarks, including CelebA. As shown in Figure 5, Fair-CDA achieves SOTA performance followed by two mixup methods, and it is the only method that achieves considerable accuracy given high fairness requirements on both tasks. Our proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness. #REFR proposes an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide, which is crucial for tasks that are sensitive to either data or model, such as finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To evaluate the fairness of the proposed augmentation strategy, the authors used a feature disentanglement method to extract the features highly related to the sensitive attributes, and regularized the models on transition paths of sensitive features between groups, achieving SOTA performance on widely-used benchmarks such as CelebA, as shown in Figure 5. It is worth mentioning that the proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness, which is consistent with the findings in the reference paper on the inadequacy of methods that fail the proposed tests for tasks that are sensitive to either data or model. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the interpretability of our proposed Fair-CDA method, we used feature decomposition techniques to extract the features highly related to the sensitive attributes, and we evaluated our method against the proposed tests in the reference paper #REFR, which showed that our method is suitable for tasks that require sensitivity to both data and model.", "given_citation_intent": "method", "given_keywords": ["feature decomposition", "interpretability methods"]}]}
{"citing_id": "2303.15113v1", "cited_id": "1905.12389", "section_title": "Introduction", "citation": "This pattern library extends the initial pattern catalog of 10-15 patterns originally identified by #REFR both quantitatively with additional patterns observed during the SMS and qualitatively, by offering the patterns in a machine-actionable rather than graphical representation.", "text_before_citation": ["While a number of papers about systems that learn and reason were collected as a basis for the analysis described in #OTHEREFR , these were not offered as a corpus of annotated papers to the community.", "We addressed both challenges by conducting a large-scale Systematic Mapping Study (SMS #OTHEREFR ) on SWeMLS #OTHEREFR , through which we (i) proposed a set of characteristics for describing SWeMLS and (ii) systematically collected, selected and extracted data from nearly 500 papers describing such systems.", "This led to the following artifacts which together are offered as one resource:", "the SWeMLS ontology that describes the main aspects of SWeMLS including their internal workflow in terms of boxology patterns as shown in Fig. 1 .", "The ontology schema (i.e., capturing important SWeMLS characteristics, e.g., StatisticalM odel) and relevant instances (e.g., DeepLearningM odel) were derived systematically during the scoping and analysis phases of the SMS, the SWeMLS-KG: a knowledge graph containing the machine-actionable description of almost 500 systems in terms of the SWeMLS ontology, and the SWeMLS Pattern Library containing the machine-actionable description of 45 SWeMLS patterns and their associated SHACL-based validation constraints."], "text_after_citation": ["This resource is timely considering the recent trend in the SW community (and beyond) to create systems that leverage both SW and ML components.", "To the best of our knowledge, it is also novel by (i) providing the first ontology (and associated pattern library) for describing SWeMLS in a machine-actionable way and (ii) a methodologically collected corpus of SWeMLS and their semantic description.", "The resource is of immediate benefit for (SW) researchers that aim to explore trends in the SWeMLS field by analysing the data in the SWeMLS-KG and as such promises to have an impact on the understanding of the status-quo in this emerging field.", "Furthermore, the resource provides a semantic framework for describing SWeMLS and their internal details, thus potentially strongly influencing this field in terms of being well-documented, data-driven, and transparent.", "We continue by discussing the impact of this resource (Sect."], "citing_paper_content": {"title": "Describing And Organizing Semantic Web And Machine Learning Systems In The Swemls-Kg", "abstract": "The overall AI trend of creating neuro-symbolic systems is reflected in the Semantic Web community with an increased interest in the development of systems that rely on both Semantic Web resources and Machine Learning components (SWeMLS, for short). However, understanding trends and best practices in this rapidly growing field is hampered by a lack of standardized descriptions of these systems and an annotated corpus of such systems. To address these gaps, we leverage the results of a large-scale systematic mapping study collecting information about 470 SWeMLS papers and formalize these into one resource containing: (i) the SWeMLS ontology, (ii) the SWeMLS pattern library containing machine-actionable descriptions of 45 frequently occurring SWeMLS workflows, and (iii) SWEMLS-KG, a knowledge graph including machine-actionable metadata of the papers in terms of the SWeMLS ontology. This resource provides the first framework for semantically describing and organizing SWeMLS thus making a key impact in (1) understanding the status quo of the field based on the published paper corpus and (2) enticing the uptake of machine-processable system documentation in the SWeMLS area."}, "cited_paper_content": {"title": "A Boxology Of Design Patterns For Hybrid Learning And Reasoning Systems", "abstract": "We propose a set of compositional design patterns to describe a large variety of systems that combine statistical techniques from machine learning with symbolic techniques from knowledge representation. As in other areas of computer science (knowledge engineering, software engineering, ontology engineering, process mining and others), such design patterns help to systematize the literature, clarify which combinations of techniques serve which purposes, and encourage re-use of software components. We have validated our set of compositional design patterns against a large body of recent literature."}, "keywords": ["machine-actionable rather", "additional patterns"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "While a number of papers about systems that learn and reason were collected as a basis for the analysis described in #OTHEREFR, these were not offered as a corpus of annotated papers to the community. We addressed both challenges by conducting a large-scale Systematic Mapping Study (SMS #OTHEREFR) on SWeMLS #OTHEREFR, through which we (i) proposed a set of characteristics for describing SWeMLS and (ii) systematically collected, selected and extracted data from nearly 500 papers describing such systems. This led to the following artifacts which together are offered as one resource: the SWeMLS ontology that describes the main aspects of SWeMLS including their internal workflow in terms of boxology patterns as shown in Fig. 1. The ontology schema (i.e., capturing important SWeMLS characteristics, e.g., StatisticalModel) and relevant instances (e.g., DeepLearningModel) were derived systematically during the scoping and analysis phases of the SMS, the SWeMLS-KG: a knowledge graph containing the machine-actionable description of almost 500 systems in terms of the SWeMLS ontology, and the SWeMLS Pattern Library containing the machine-actionable description of 45 SWeMLS patterns and their associated SHACL-based validation constraints. Our proposed set of compositional design patterns for describing hybrid learning and reasoning systems, as presented in #REFR, helped us to systematize the literature, clarify which combinations of techniques serve which purposes, and encourage re-use of software components.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "A set of compositional design patterns has been proposed to describe systems that combine statistical techniques from machine learning with symbolic techniques from knowledge representation, which can help to systematize the literature and encourage re-use of software components (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To make SWeMLS machine-actionable, the authors proposed a set of compositional design patterns to describe a large variety of systems that combine statistical techniques from machine learning with symbolic techniques from knowledge representation, which were validated against a large body of recent literature #REFR, and additionally provided a pattern library containing 45 frequently occurring SWeMLS workflows.", "given_citation_intent": "background", "given_keywords": ["machine-actionable rather", "additional patterns"]}]}
{"citing_id": "2304.11465v1", "cited_id": "1706.03762", "section_title": "A. Pointr-C: 3D Shape Completion Network", "citation": "The features are used as tokens to a transformer #REFR which captures the long-range relations among them and predicts the centers for the missing point cloud.", "text_before_citation": ["Given the current set of observations v o \u2208 V, we predict the complete volume using a learning-based predictor g, i.e., V = g(v o ).", "To obtainV, we use PoinTr #OTHEREFR , a transformer-based architecture that uses 3D point clouds as the input and output. PoinTr works in multiple steps for shapes completion.", "First, a k-nearest neighbor (kNN) algorithm is applied to the partial point cloud to find the cluster centers that represent geometric relationships at a low resolution.", "A DGCNN #OTHEREFR adds the local features around the these center points."], "text_after_citation": ["Lastly, FoldingNet #OTHEREFR performs a coarse-to-fine transformation over the predicted centers to predict the missing point cloud.", "This model was trained on the ShapeNet #OTHEREFR dataset and outperforms the previous methods on a range of objects.", "However, PoinTr was trained with implicit knowledge of the center of the object.", "Moving the partially observed point cloud to its center results in incorrect prediction from PoinTr.", "To improve the predictions, we fine-tune PoinTr using the curriculum framework, which dictates training the network over easy to hard tasks by increasing the difficulty in steps during learning #OTHEREFR ."], "citing_paper_content": {"title": "Pred-Nbv: Prediction-Guided Next-Best-View Planning For 3D Object Reconstruction", "abstract": "Prediction-based active perception has shown the potential to improve the navigation efficiency and safety of the robot by anticipating the uncertainty in the unknown environment. The existing works for 3D shape prediction make an implicit assumption about the partial observations and therefore cannot be used for real-world planning and do not consider the control effort for next-best-view planning. We present Pred-NBV, a realistic object shape reconstruction method consisting of PoinTr-C, an enhanced 3D prediction model trained on the ShapeNet dataset, and an information and control effort-based next-best-view method to address these issues. Pred-NBV shows an improvement of 25.46% in object coverage over the traditional methods in the AirSim simulator, and performs better shape completion than PoinTr, the stateof-the-art shape completion model, even on real data obtained from a Velodyne 3D LiDAR mounted on DJI M600 Pro."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["missing point cloud", "transformer"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To obtain V, we use PoinTr #REFR, a transformer-based architecture that uses 3D point clouds as the input and output, and works in multiple steps for shapes completion.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain the complete volume from the partial point cloud, the authors used PoinTr #REFR, a transformer-based architecture that applies a k-nearest neighbor algorithm to find the cluster centers and a DGCNN to add local features, achieving superior shape completion results.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain the missing point cloud, the authors use PoinTr #REFR, a transformer-based architecture that predicts the complete volume using 3D point clouds as input and output.", "given_citation_intent": "method", "given_keywords": ["missing point cloud", "transformer"]}]}
{"citing_id": "2305.00426v1", "cited_id": "1912.12055", "section_title": "Setup", "citation": "We used CQT transform as a spectrogram function for all experiments. We used nnAudio library #REFR for spectrogram calculation.", "text_before_citation": ["Split for training, validation and testing was datasetwise, which means that data from the training set for MAPS and the training set for GuitarSet were present in the training set for SynthesizedInstruments.", "We are using only randomly chosen fixed-size sequences of each composition for training, so it should not make this model overfit to traits specific to datasets distributions.", "Achieved recordings contain the clean version of each instrument, which is often not desired in analyzing noisy real-world data recorded by modern microphones.", "Each recording generated by the software synthesizer was later sampled with a 16 kHz sample rate and transformed into the CQT spectrogram.", "Data processing Each experiment was focused on training on a specific dataset."], "text_after_citation": ["To avoid recalculating CQT transform each time, we saved data on disk once it was calculated and loaded it in the subsequent experiments to not perform it again.", "Experimental protocol Datasets were split into training, validation, and testing sets.", "We checked the model using the validation set after every ten learning epochs.", "After training, all datasets were tested using corresponding testing sets for all available datasets.", "In the discussion of results, only datasets containing real-world recordings were considered (MAPS and GuitarSet)."], "citing_paper_content": {"title": "Transfer Of Knowledge Among Instruments In Automatic Music Transcription", "abstract": "Automatic music transcription (AMT) is one of the most challenging tasks in the music information retrieval domain. It is the process of converting an audio recording of music into a symbolic representation containing information about the notes, chords, and rhythm. Current research in this domain focuses on developing new models based on transformer architecture or using methods to perform semi-supervised training, which gives outstanding results, but the computational cost of training such models is enormous. This work shows how to employ easily generated synthesized audio data produced by software synthesizers to train a universal model. It is a good base for further transfer learning to quickly adapt transcription model for other instruments. Achieved results prove that using synthesized data for training may be a good base for pretraining general-purpose models, where the task of transcription is not focused on one instrument."}, "cited_paper_content": {"title": "Nnaudio: An On-The-Fly Gpu Audio To Spectrogram Conversion Toolbox Using 1D Convolution Neural Networks", "abstract": "Converting time domain waveforms to frequency domain spectrograms is typically considered to be a prepossessing step done before model training. This approach, however, has several drawbacks. First, it takes a lot of hard disk space to store different frequency domain representations. This is especially true during the model development and tuning process, when exploring various types of spectrograms for optimal performance. Second, if another dataset is used, one must process all the audio clips again before the network can be retrained. In this paper, we integrate the time domain to frequency domain conversion as part of the model structure, and propose a neural network based toolbox, nnAudio, which leverages 1D convolutional neural networks to perform time domain to frequency domain conversion during feed-forward. It allows on-the-fly spectrogram generation without the need to store any spectrograms on the disk. This approach also allows back-propagation on the waveforms-to-spectrograms transformation layer, which implies that this transformation process can be made trainable, and hence further optimized by gradient descent. nnAudio reduces the waveforms-to-spectrograms conversion time for 1,770 waveforms (from the MAPS dataset) from $10.64$ seconds with librosa to only $0.001$ seconds for Short-Time Fourier Transform (STFT), $18.3$ seconds to $0.015$ seconds for Mel spectrogram, $103.4$ seconds to $0.258$ for constant-Q transform (CQT), when using GPU on our DGX work station with CPU: Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz Tesla v100 32Gb GPUs. (Only 1 GPU is being used for all the experiments.) We also further optimize the existing CQT algorithm, so that the CQT spectrogram can be obtained without aliasing in a much faster computation time (from $0.258$ seconds to only $0.001$ seconds)."}, "keywords": ["CQT transform", "spectrogram function"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We employed a software synthesizer to generate audio data for training a universal model, and each recording was transformed into the CQT spectrogram. This approach allowed us to avoid storing different frequency domain representations and to perform on-the-fly spectrogram generation. The proposed neural network based toolbox, nnAudio, leverages 1D convolutional neural networks to perform time domain to frequency domain conversion during feed-forward, and reduces the waveforms-to-spectrograms conversion time significantly. The results presented in this paper demonstrate that using synthesized data for training may be a good base for pretraining general-purpose models, where the task of transcription is not focused on one instrument #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate CQT spectrograms without aliasing and in a much faster computation time, the authors of this work employed nnAudio, a neural network-based toolbox that integrates time domain to frequency domain conversion as part of the model structure, and proposed a method that allows on-the-fly spectrogram generation without the need to store any spectrograms on the disk, achieving a conversion time of only $0.001$ seconds for CQT spectrograms, as reported in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate the CQT spectrogram, the authors used a neural network-based toolbox, nnAudio, which performs time domain to frequency domain conversion during feed-forward, allowing on-the-fly spectrogram generation without the need to store any spectrograms on the disk, and also allows back-propagation on the waveforms-to-spectrograms transformation layer, making this transformation process trainable and hence further optimized by gradient descent. #REFR", "given_citation_intent": "method", "given_keywords": ["CQT transform", "spectrogram function"]}]}
{"citing_id": "2303.04450v1", "cited_id": "1705.00722", "section_title": "Numerical Results", "citation": "These results corroborate the previous observation in #REFR that, parametric filters offer a more robust alternative to the particle filter in case of parameter uncertainty.", "text_before_citation": ["It can be seen that overall the EKF approximation is poor and has the worst performance.", "UKF and ENKF improve upon this, where UKF is more robust. However PF outperforms all three of them, as expected.", "As for EFKF, for low and high values of \u03b1 the results are comparable to PF where EFKF performs better in some cases and PF better in the others. The same also applies to SKF.", "On the other hand, for mid range of \u03b1, EFKF is significantly better than all the other filters, including PF.", "The best value is highlighted in boldface, which occurs at \u03b1 = 0.7."], "text_after_citation": ["Interesting, unlike the previous case, we see that EFKF also provides better performance even when there is no parameter uncertainty.", "For this \"Match\" we have the values in the rightmost column.", "Firstly, we can see that the PF outperforms all competitors including SKF and MKF, except for EFKF.", "For the case of \u03b1 = 0.5 and \u03b1 = 0.7, EFKF gives significantly lower RMSE.", "The best is once again highlighted in boldface and occurs at \u03b1 = 0.7."], "citing_paper_content": {"title": "Nonlinear Kalman Filtering With Reparametrization Gradients", "abstract": "We introduce a novel nonlinear Kalman filter that utilizes reparametrization gradients. The widely used parametric approximation is based on a jointly Gaussian assumption of the state-space model, which is in turn equivalent to minimizing an approximation to the Kullback-Leibler divergence. It is possible to obtain better approximations using the alpha divergence, but the resulting problem is substantially more complex. In this paper, we introduce an alternate formulation based on an energy function, which can be optimized instead of the alpha divergence. The optimization can be carried out using reparametrization gradients, a technique that has recently been utilized in a number of deep learning models."}, "cited_paper_content": {"title": "Nonlinear Kalman Filtering With Divergence Minimization", "abstract": "We consider the nonlinear Kalman filtering problem using Kullback\u2013Leibler (KL) and $\\alpha$ -divergence measures as optimization criteria. Unlike linear Kalman filters, nonlinear Kalman filters do not have closed form Gaussian posteriors because of a lack of conjugacy due to the nonlinearity in the likelihood. In this paper, we propose novel algorithms to approximate this posterior by optimizing the forward and reverse forms of the KL divergence, as well as the $\\alpha$ -divergence that contains these two as limiting cases. Unlike previous approaches, our algorithms do not make approximations to the divergences being optimized, but use Monte Carlo techniques to derive unbiased algorithms for direct optimization. We assess performance on radar and sensor tracking, and options pricing, showing general improvement over the extended, unscented, and ensemble Kalman filters, as well as competitive performance with particle filtering."}, "keywords": ["particle filter"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For mid-range values of \u03b1, the proposed EFKF algorithm significantly outperforms all other filters, including PF, as highlighted in boldface in the results. This improvement is achieved by optimizing the forward and reverse forms of the KL divergence, as well as the $\\alpha$-divergence that contains these two as limiting cases, using novel algorithms proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed algorithms in this manuscript show general improvement over the extended, unscented, and ensemble Kalman filters, as well as competitive performance with particle filtering, as demonstrated in radar and sensor tracking, and options pricing, which is consistent with the results reported in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The particle filter outperforms the extended, unscented, and ensemble Kalman filters, as well as the EFKF and SKF filters for the specified mid range of \u03b1 (#REFR).", "given_citation_intent": "result", "given_keywords": ["particle filter"]}]}
{"citing_id": "2303.13272v1", "cited_id": "1908.07919", "section_title": "Multi-Scale Network", "citation": "To address the issue, we introduce the multi-scale network, which was firstly proposed in computer vision tasks #REFR .", "text_before_citation": ["If only a certain part of an IPT is considered, it tends to misjudge one IPT as another.", "For example, a portamento note will be misjudged as a normal pluck note if its inflection point in the spectrogram is out of the receptive field of the model.", "As solutions, the receptive field is usually enlarged by directly stacking multiple convolution layers or use large-size convolution kernels.", "But these methods both result in an excess of parameters.", "Simply increasing the receptive field will also lead to a loss in details for the subtle change of short IPTs."], "text_after_citation": ["As shown in Fig.2(a) , our proposed model is composed of three horizontal branches for different scales in the time axis.", "The resolution of the feature in the branches from top to bottom is from high to low.", "The middle branch with the medium resolution is used as a transition for the fusing between high-resolution features and long-range features.", "By downsampling/upsampling the feature to different scales, longrange features can be fused with high-resolution features repeatedly.", "To convert the spectral information into the channel domain, we first process the CQT input (1, F, T) into a sequence with the shape of (88, T, 1) by reshaping and batch normalization."], "citing_paper_content": {"title": "Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network And Self-Attention Mechanism", "abstract": "Instrument playing technique (IPT) is a key element of musical presentation. However, most of the existing works for IPT detection only concern monophonic music signals, yet little has been done to detect IPTs in polyphonic instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we formulate it as a framelevel multi-label classification problem and apply it to Guzheng, a Chinese plucked string instrument. We create a new dataset, Guzheng Tech99, containing Guzheng recordings and onset, offset, pitch, IPT annotations of each note. Because different IPTs vary a lot in their lengths, we propose a new method to solve this problem using multi-scale network and self-attention. The multi-scale network extracts features from different scales, and the self-attention mechanism applied to the feature maps at the coarsest scale further enhances the long-range feature extraction. Our approach outperforms existing works by a large margin, indicating its effectiveness in IPT detection."}, "cited_paper_content": {"title": "Deep High-Resolution Representation Learning For Visual Recognition", "abstract": "High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions \\emph{in series} (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams \\emph{in parallel}; (ii) Repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at~{\\url{this https URL}}."}, "keywords": ["multi-scale network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of loss in details for the subtle change of short IPTs while enlarging the receptive field, a new method has been proposed that uses multi-scale network and self-attention, as described in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of misjudging one IPT as another due to the inflection point being out of the receptive field of the model, a new method using multi-scale network and self-attention has been proposed, which extracts features from different scales and enhances long-range feature extraction through self-attention mechanism applied to the feature maps at the coarsest scale #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of loss in details for the subtle change of short IPTs while enlarging the receptive field, a multi-scale network is proposed in this paper, which extracts features from different scales and applies self-attention mechanism to the feature maps at the coarsest scale to further enhance the long-range feature extraction #REFR.", "given_citation_intent": "method", "given_keywords": ["multi-scale network"]}]}
{"citing_id": "2304.03718v1", "cited_id": "1905.10083", "section_title": "Introduction", "citation": "Edge-AI optimizes the response time and accelerates AI computation when data is generated and processed for inference on-device #REFR .", "text_before_citation": ["In the past, AI approaches including machine learning, computer vision, and robotics along with data mining and image processing techniques are utilized to address the SHM challenges #OTHEREFR .", "Especially deep learning approaches are broadly utilized in the past for detecting and segmenting bridge components #OTHEREFR .", "Results obtained through these computational approaches have shown immense potential to support health monitoring tasks such as crack detection, segmentation of bridge components, UAV inspection of bridges, and damage detection for bridge structure #OTHEREFR .", "However, real-time inferences in SHM, especially crack detection is challenging based on several factors like a wide range of various complex backgrounds and crack-like features.", "Edge-AI allows a process for artificial intelligence inference and computation on-device rather than on cloud servers or network connections."], "text_after_citation": ["Zhou et al 2019 #OTHEREFR suggests that this ability of edge-AI platforms has advantages as follows- In this research study, drawing from artificial intelligence (AI), neural network process acceleration, and edge computing literature, the study aims to investigate edge-AI integration in structural health monitoring tasks.", "The objective is to develop a lightweight neural network model and to optimize neural network response time performance in real time.", "To accomplish this, we are utilizing Kneron KL520 platform which includes a neural processing unit (NPU), an AI system-on-chip(SoC) hardware.", "To address the inherent challenges for real-time inference an edge-AI framework for SHM domain is proposed.", "This paper introduces a novel framework to generate lightweight optimized models on the edge within the SHM domain. Figure 1 describes the edge-AI-SHM framework."], "citing_paper_content": {"title": "Integrating Edge-Ai In Structural Health Monitoring Domain *", "abstract": "Structural health monitoring (SHM) tasks like damage detection are crucial for decision-making regarding maintenance and deterioration. For example, crack detection in SHM is crucial for bridge maintenance as crack progression can lead to structural instability. However, most AI/ML models in the literature have low latency and late inference time issues while performing in real-time environments. This study aims to explore the integration of edge-AI in SHM domain for real-time bridge inspections. Based on edge-AI literature, its capabilities will be valuable integration for a real-time decision support system in SHM tasks such that real-time inferences can be performed on physical sites. This study will utilize commercial edge-AI platforms, such as Google Coral Dev Board or Kneron KL520, to develop and analyze the effectiveness of edge-AI devices. Thus, this study proposes an edge AI framework for the structural health monitoring domain. An edge-AIcompatible deep learning model is developed to validate the framework to perform real-time crack classification. The effectiveness of this model will be evaluated based on its accuracy, the confusion matrix generated, and the inference time observed in a real-time setting."}, "cited_paper_content": {"title": "Edge Intelligence: Paving The Last Mile Of Artificial Intelligence With Edge Computing", "abstract": "With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI."}, "keywords": ["Edge-AI"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Edge-AI allows for real-time artificial intelligence inference and computation on-device, which is valuable for structural health monitoring tasks such as crack detection #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To fully unleash the potential of edge big data, there is an urgent need to push the AI frontiers to the network edge, which has been recognized as a promising solution #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Edge-AI has been recognized as a promising solution to push the AI frontiers to the network edge, and a comprehensive survey of recent research efforts on Edge Intelligence has been conducted to elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI #REFR.", "given_citation_intent": "background", "given_keywords": ["Edge-AI"]}]}
{"citing_id": "2305.01778v1", "cited_id": "1508.07909", "section_title": "Setup", "citation": "We learn a joint vocabulary for glosses and texts via byte pair encoding (BPE) #REFR .", "text_before_citation": ["Datasets We work on three SLT datasets: PHOENIX-2014T, CSL-Daily, and DGS3-T.", "PHOENIX-2014T and DGS3-T focus on German Sign Language, CSL-Daily on Chinese Sign Language.", "All three datasets provide triplet samples, each consisting of a sign language video, a sentence-level gloss annotation and their corresponding text translation. Detailed statistics are listed in Table 1 .", "We employ MuST-C English-German (En-De, 229K samples) and English-Chinese (En-Zh, 185K samples) #OTHEREFR as the augmented MT data for PHOENIX-2014T/DGS3-T and CSL-Daily, respectively."], "text_after_citation": ["We employ 1K BPE operations when MT data is not used, and increase it to 8K/8K/10K for PHOENIX-2014T/DGS3-T/CSL-Daily otherwise.", "Model Settings We experiment with Transformer and start our analysis with a Baseline system optimized on Sign2Text alone with the following configurations: encoder and decoder layers of N S enc = 2, N P enc = 0 and N dec = 2 respectively, model dimension of d = 512, feed-forward dimension of d f f = 2048, attention head of h = 8, and no CTC regularization.", "We adopt the SMKD model (Hao et al., 2021) 1 to extract sign embeddings, and pretrain the model on each benchmark separately on the Sign2Gloss task considering the large difference of sign videos across benchmarks.", "More details about datasets and model settings are given in Appendix A.1.", "Evaluation We report results mainly on the SLT task."], "citing_paper_content": {"title": "Sltunet: A Simple Unified Model For Sign Language Translation", "abstract": "Despite recent successes with neural models for sign language translation (SLT), translation quality still lags behind spoken languages because of the data scarcity and modality gap between sign video and text. To address both problems, we investigate strategies for cross-modality representation sharing for SLT. We propose SLTUNET, a simple unified neural model designed to support multiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and sign-to-text translation. Jointly modeling different tasks endows SLTUNET with the capability to explore the cross-task relatedness that could help narrow the modality gap. In addition, this allows us to leverage the knowledge from external resources, such as abundant parallel data used for spoken-language machine translation (MT). We show in experiments that SLTUNET achieves competitive and even state-of-theart performance on PHOENIX-2014T and CSL-Daily when augmented with MT data and equipped with a set of optimization techniques. We further use the DGS Corpus for end-to-end SLT for the first time. It covers broader domains with a significantly larger vocabulary, which is more challenging and which we consider to allow for a more realistic assessment of the current state of SLT than the former two. Still, SLTUNET obtains improved results on the DGS Corpus. Code is available at https://github.com/bzhangGo/sltunet."}, "cited_paper_content": {"title": "Neural Machine Translation Of Rare Words With Subword Units", "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English!German and English!Russian by up to 1.1 and 1.3 BLEU, respectively."}, "keywords": ["joint vocabulary"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the open-vocabulary problem in sign language translation, the authors propose a simple and effective approach of encoding rare and unknown words as sequences of subword units, which has been shown to improve translation quality in neural machine translation models #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the open-vocabulary problem in sign language translation, the authors propose a simple and effective approach of encoding rare and unknown words as sequences of subword units, which has been shown to improve translation quality in neural machine translation models according to the work by Sennrich et al. (2016). #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the open-vocabulary problem in sign language translation, the authors propose a joint vocabulary approach using subword units, which has been shown to improve translation quality in machine translation tasks by up to 1.1 and 1.3 BLEU for English-German and English-Russian, respectively #REFR.", "given_citation_intent": "method", "given_keywords": ["joint vocabulary"]}]}
{"citing_id": "2304.02175v1", "cited_id": "1903.09890", "section_title": "I. Introduction", "citation": "This method takes as input surface elevation data including buildings and other structures and uses the fluid-inspired method presented in #REFR to generate layers of air corridors.", "text_before_citation": ["Air networks have two uses.", "First, they provide [closelyspaced] smooth paths that offer appropriate clearance from buildings, terrain, other structures, and neighboring UAS.", "The consideration environmental maps and safe separation distances significantly simplifies UAS flight planning and dynamic rerouting.", "Second, they define safely separated candidate paths that can assure collision avoidance so long as each UAS tracks its planned trajectory to within expected error bounds.", "The authors previously presented a method to automatically generate a dense air network for any urban environment that wraps structures #OTHEREFR ."], "text_after_citation": ["Each layer is a fixedaltitude plane with air corridors oriented in a fixed nominal direction that wrap around buildings and other structures.", "This paper presents several advancements to the authors' previous work #OTHEREFR which increases its scalabity and usability.", "More specifically, this paper offers the following distinct and novel contributions:", "1) The acquisition method for surface elevation data has been fully automated for US cities.", "In our previous work #OTHEREFR , three dimensional models for buildings were manually created using Google Maps data which is not scalable."], "citing_paper_content": {"title": "Can A Laplace Pde Define Air Corridors Through Low-Altitude Airspace?", "abstract": "Urban Uncrewed Aircraft System (UAS) flight will require new regulations that assure safety and accommodate unprecedented traffic density levels. Multi-UAS coordination is essential to both objectives. This paper models UAS coordination as an ideal fluid flow with a stream field governed by the Laplace partial differential equation. Streamlines spatially define closely-spaced deconflicted routes through the airspace and define air corridors that safely wrap buildings and other structures so UAS can avoid collision even when flying among low-altitude vertical obstacles and near mountainous terrain. We divide a city into zones, with each zone having its own sub-network, to allow for modularity and assure computation time for route generation is linear as a function of total area. We demonstrate the strength of our proposed approach by computing air corridors through low altitude airspace of select cities with tall buildings. For US cities, we use open LiDAR elevation data to determine surface elevation maps. We select non-US cities with existing high-fidelity three-dimensional landscape models."}, "cited_paper_content": {"title": "Physics-Based Freely Scalable Continuum Deformation For Uas Traffic Coordination", "abstract": "This paper develops a novel physics-inspired traffic coordination approach and applies it to Unmanned Aircraft System (UAS) traffic management. We extend available physics-inspired approaches previously applied to 1-D traffic flow on highways and urban streets to support models of traffic coordination in higher dimension airspace for cases where no predefined paths exist. The paper considers airspace as a finite control volume while UAS coordination, treated as continuum deformation, is controlled at the airspace boundaries. By partitioning airspace into planned and unplanned spaces, the paper models nominal coordination in the planned airspace as the solution of a partial differential equation with spatiotemporal parameters. This paper also improves resilience to vehicle failures with a resilient boundary control algorithm to update the geometry of the planned space when UAS problems threaten safe coordination in existing navigable airspace channels. To support UAS coordination at the microscopic level, we propose clustering vehicles based on vehicle performance limits. UAS clusters, with each UAS treated as a particle of a virtual rigid body, use leader-follower containment to acquire the macroscopic desired trajectory."}, "keywords": ["air corridors"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Air networks have two uses. First, they provide [closelyspaced] smooth paths that offer appropriate clearance from buildings, terrain, other structures, and neighboring UAS. The consideration environmental maps and safe separation distances significantly simplifies UAS flight planning and dynamic rerouting. Second, they define safely separated candidate paths that can assure collision avoidance so long as each UAS tracks its planned trajectory to within expected error bounds. The authors previously presented a method to automatically generate a dense air network for any urban environment that wraps structures (OTHERREF). This method extends available physics-inspired approaches previously applied to 1-D traffic flow on highways and urban streets to support models of traffic coordination in higher dimension airspace for cases where no predefined paths exist.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate a dense air network for urban environments that wraps structures and provides appropriate clearance from buildings and terrain, the authors proposed a physics-inspired traffic coordination approach that treats UAS coordination as continuum deformation and models nominal coordination in the planned airspace as the solution of a partial differential equation with spatiotemporal parameters #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate closely-spaced air corridors that ensure safe separation from buildings and terrain, the authors propose a physics-inspired traffic coordination approach that models nominal coordination in planned airspace as the solution of a partial differential equation with spatiotemporal parameters #REFR.", "given_citation_intent": "method", "given_keywords": ["air corridors"]}]}
{"citing_id": "2305.02968v1", "cited_id": "1802.09477", "section_title": "Representations Of Mtm", "citation": "In the Walk task, we note it actually improves over the asymptotic performance of the base TD3 #REFR algorithm within 10% of training budget.", "text_before_citation": ["We additionally test state-action representations of MTM by using the latent representation of the state and action encoded jointly with MTM.", "We allow end to end finetuning of the representations during training.", "We compare training TD3 on raw states to training TD3 with (a) state representations from the MTM model, and (b) state-action representations from the MTM model with the offline RL loss (i.e. TD3 objective).", "Figure 7 depicts the learning curves for the aforementioned experiment.", "In all cases we see significant improvement in training efficiency by using MTM representations -both with state and state-action representations."], "text_after_citation": ["Additionally, we find that the state-action representation from MTM can provide significant benefits, as in the case of the Walk task.", "Here, finetuning state-action representation from MTM leads to better asymptotic performance compared to state-only representation or learning from scratch.", "We provide additional plots of MTM frozen representations in Appendix E.3"], "citing_paper_content": {"title": "Masked Trajectory Models For Prediction, Representation, And Control", "abstract": "We introduce Masked Trajectory Models (MTM) as a generic abstraction for sequential decision making. MTM takes a trajectory, such as a stateaction sequence, and aims to reconstruct the trajectory conditioned on random subsets of the same trajectory. By training with a highly randomized masking pattern, MTM learns versatile networks that can take on different roles or capabilities, by simply choosing appropriate masks at inference time. For example, the same MTM network can be used as a forward dynamics model, inverse dynamics model, or even an offline RL agent. Through extensive experiments in several continuous control tasks, we show that the same MTM network-i.e. same weights-can match or outperform specialized networks trained for the aforementioned capabilities. Additionally, we find that state representations learned by MTM can significantly accelerate the learning speed of traditional RL algorithms. Finally, in offline RL benchmarks, we find that MTM is competitive with specialized offline RL algorithms, despite MTM being a generic self-supervised learning method without any explicit RL components."}, "cited_paper_content": {"title": "Addressing Function Approximation Error In Actor-Critic Methods", "abstract": "In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and critic. Our algorithm takes the minimum value between a pair of critics to restrict overestimation and delays policy updates to reduce per-update error. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested."}, "keywords": ["Walk task", "training budget"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We additionally test state-action representations of MTM by using the latent representation of the state and action encoded jointly with MTM. We allow end to end finetuning of the representations during training. We compare training TD3 on raw states to training TD3 with (a) state representations from the MTM model, and (b) state-action representations from the MTM model with the offline RL loss (i.e. TD3 objective). Figure 7 depicts the learning curves for the aforementioned experiment. In all cases we see significant improvement in training efficiency by using MTM representations -both with state and state-action representations. This is consistent with the findings in the reference paper #REFR, which proposes novel mechanisms to minimize function approximation errors in actor-critic methods and outperforms the state of the art in OpenAI gym tasks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of function approximation errors in actor-critic methods, #REFR proposed novel mechanisms to minimize its effects on both the actor and critic, which were evaluated on the suite of OpenAI gym tasks and outperformed the state of the art in every environment tested.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of function approximation errors in actor-critic methods, the authors proposed a novel algorithm that restricts overestimation and reduces per-update error, which outperformed the state of the art in every environment tested in the OpenAI gym tasks #REFR.", "given_citation_intent": "background", "given_keywords": ["Walk task", "training budget"]}]}
{"citing_id": "2304.06645v1", "cited_id": "1903.05186", "section_title": "Ahmad", "citation": "These issues are addressed by the authors of #REFR , who introduced an arithmetic and geometric mean (AGM) robustness measure for STL.", "text_before_citation": ["Ahmad, Roberto Tron, and Calin Belta ({ahmadgh,tron,cbelta}@bu.edu) are with the Division of System Engineering, Boston University, Boston, MA 02215, USA.", "#OTHEREFR Cristian Vasile (cvr519@lehigh.edu) is with Mechanical Engineering and Mechanics Department at Lehigh University, Bethlehem, PA, 18015, USA robustly satisfy the specifications #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR , #OTHEREFR .", "The work in #OTHEREFR considers planning for syntactically cosafe LTL using RRT * , in addition to the task specifications, other spatial requirements are expressed using fragment-STL where its robustness is used as the optimality criterion for RRT * .", "In #OTHEREFR , the authors synthesize controllers for time-critical systems for which they quantify a temporal robustness measure that needs to be optimized.", "The traditional robustness metric is not differentiable and it is mostly determined by one value of the signal, i.e., it \"masks\" most of the signal."], "text_after_citation": ["TWTL has several advantages over STL, MTL, and other concrete-time TLs.", "First, its syntax and semantics can express serial tasks in an efficient and explicit way.", "This is important in many applications, especially in robotics #OTHEREFR . Second, TWTL formulae can be efficiently translated into automata.", "The complexity of the translation algorithm is independent of the formula time bounds #OTHEREFR .", "This makes this logic suitable for automata-based synthesis and planning problems (see #OTHEREFR for a planning application)."], "citing_paper_content": {"title": "Robustness Measures And Monitors For Time Window Temporal Logic", "abstract": "Temporal logics (TLs) have been widely used to formalize interpretable tasks for cyber-physical systems. Time Window Temporal Logic (TWTL) has been recently proposed as a specification language for dynamical systems. In particular, it can easily express robotic tasks, and it allows for efficient, automata-based verification and synthesis of control policies for such systems. In this paper, we define two quantitative semantics for this logic, and two corresponding monitoring algorithms, which allow for real-time quantification of satisfaction of formulas by trajectories of discrete-time systems. We demonstrate the new semantics and their runtime monitors on numerical examples."}, "cited_paper_content": {"title": "Arithmetic-Geometric Mean Robustness For Control From Signal Temporal Logic Specifications", "abstract": "We present a new average-based robustness for Signal Temporal Logic (STL) and a framework for optimal control of a dynamical system under STL constraints. By averaging the scores of different specifications or subformulae at different time points, our definition highlights the frequency of satisfaction as well as how robustly each specification is satisfied. Its usefulness in control synthesis problems is illustrated through case studies."}, "keywords": ["robustness measure"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In #REFR, the authors present a new average-based robustness for Signal Temporal Logic (STL) and a framework for optimal control of a dynamical system under STL constraints. This work could be relevant for the robust synthesis of control policies for cyber-physical systems, such as the time-critical systems studied in this manuscript.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' new average-based robustness for Signal Temporal Logic (STL) and the corresponding optimal control framework presented in #REFR provide a useful approach for control synthesis problems, highlighting the frequency of satisfaction and robustness of each specification, which can be applied to the time-critical systems considered in this manuscript.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' definition of average-based robustness for Signal Temporal Logic (STL) and its usefulness in control synthesis problems, as presented in #REFR, can provide insights into the quantification of temporal robustness measures for time-critical systems. #REFR", "given_citation_intent": "background", "given_keywords": ["robustness measure"]}]}
{"citing_id": "2303.11844v1", "cited_id": "1810.02733", "section_title": "Estimation From Independent Samples", "citation": "We follow a strategy similar to that used for the sample complexity of EOT #REFR . In Thm.", "text_before_citation": ["Let \u03c4, \u03bb > 0, define d = 2 d/2 and assume that c \u2208 C 1+d /2 (X \u00d7 X).", "Let \u00b5 \u03bb,\u03c4 be the empirical barycenter and \u00b5 * \u03bb,\u03c4 the population barycenter.", "Then there is C > 0 independent of (\u03bd k ) k such that", "E[H(\u03bc \u03bb,\u03c4 |\u00b5 * \u03bb,\u03c4 )] \u2264 C\u03c4 \u22121 (1 + \u03bb \u2212d /2 )n \u22121/2 .", "Proof."], "text_after_citation": ["4.1, one can replace the homogeneous Sobolev norm\u1e22 \u2212p by the (larger) inhomogeneous norm H \u2212p .", "For p = 1 + d /2, it is known that H p is a Reproducible Kernel Hilbert space norm, and by standard empirical process theory results #OTHEREFR one has", "E \u03bd k \u2212 \u03bd k H \u2212p \u2264 Cn \u22121/2 ."], "citing_paper_content": {"title": "Doubly Regularized Entropic Wasserstein Barycenters", "abstract": "We study a general formulation of regularized Wasserstein barycenters that enjoys favorable regularity, approximation, stability and (grid-free) optimization properties. This barycenter is defined as the unique probability measure that minimizes the sum of entropic optimal transport (EOT) costs with respect to a family of given probability measures, plus an entropy term. We denote it (\u03bb, \u03c4)-barycenter, where \u03bb is the inner regularization strength and \u03c4 the outer one. This formulation recovers several previously proposed EOT barycenters for various choices of \u03bb, \u03c4 \u2265 0 and generalizes them. First, in spite of-and in fact owing to-being doubly regularized, we show that our formulation is debiased for \u03c4 = \u03bb/2: the suboptimality in the (unregularized) Wasserstein barycenter objective is, for smooth densities, of the order of the strength \u03bb 2 of entropic regularization, instead of max{\u03bb, \u03c4 } in general. We discuss this phenomenon for isotropic Gaussians where all (\u03bb, \u03c4)-barycenters have closed form. Second, we show that for \u03bb, \u03c4 > 0, the barycenter has a smooth density and is strongly stable under perturbation of the marginals. In particular, it can be estimated efficiently: given n samples from each of the probability measures, it converges in relative entropy to the population barycenter at a rate n \u22121/2. And finally, this formulation lends itself naturally to a grid-free optimization algorithm: we propose a simple noisy particle gradient descent which, in the mean-field limit, converges globally at an exponential rate to the barycenter."}, "cited_paper_content": {"title": "Sample Complexity Of Sinkhorn Divergences", "abstract": "Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on \\emph{Sinkhorn divergences} (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength $\\varepsilon$, between OT ($\\varepsilon=0$) and MMD ($\\varepsilon=\\infty$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively $O(n^3\\log n)$, $O(n^2)$ and $n^2$ operations given a sample size $n$), much less is known in terms of their \\emph{sample complexity}, namely the gap between these quantities, when evaluated using finite samples \\emph{vs.} their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, $1/n^{1/d}$ for OT in dimension $d$ and $1/\\sqrt{n}$ for MMD, that for SDs has only been studied empirically. In this paper, we \\emph{(i)} derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer $\\varepsilon$, \\emph{(ii)} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and \\emph{(iii)} provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in $1/\\sqrt{n}$ (as in MMD), with a constant that depends however on $\\varepsilon$, making the bridge between OT and MMD complete."}, "keywords": ["sample complexity"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors derived a sample complexity bound for Sinkhorn divergences, which was obtained by reformulating SDs as a maximization problem in a RKHS, and showed that it scales as $1/\\sqrt{n}$, similar to MMD, with a constant that depends on the regularization strength $\\varepsilon$ #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To obtain the empirical barycenter, the authors used a maximization problem in a RKHS, as proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate the empirical barycenter, we can use the sample complexity bound for Sinkhorn divergences provided in #REFR, which scales as $1/\\sqrt{n}$ and depends on the regularizer strength $\\varepsilon$.", "given_citation_intent": "method", "given_keywords": ["sample complexity"]}]}
{"citing_id": "2303.11774v1", "cited_id": "1803.05350", "section_title": "Background And Related Work", "citation": "As recently #REFR showed, Rademacher random projections are asymptotically dimensionoptimal with exact constant ; this result improves upon a previous suboptimal bound of Kane and Nelson [26] .", "text_before_citation": ["Yet the quantitative analysis of the property #OTHEREFR has remained a difficult challenge, resulting in complex proofs simplified many times #OTHEREFR , crude statistical bounds (for example, sparse variants have an exponential gap with respect to the sharp no-go results #OTHEREFR ), and a lack of finitedimensional insights (bounds are input-oblivious which widens the gap between theory predictions and empirical performance #OTHEREFR ).", "This work addresses the aforementioned gap by revisiting the most promising construction of Rademacher random projections, which uses the following matrix", "EQUATION", "More specifically, this paper solves the following problem:", "Give a precise, non-asymptotic, non-oblivious analysis of random projections #OTHEREFR ."], "text_after_citation": ["The statistical performance of Rademacher projections is superior to the sparse ones, as demonstrated empirically in 1.", "Furthermore, the theoretical bounds for Rademacher random projections are much better than those available for sparse analogues #OTHEREFR .", "The best, prior to this paper, analysis of (2) is given by Achlioptas in #OTHEREFR .", "It is worth noting that Rademacher projections are also superior to their Gaussian counterparts; indeed, we know that they are dominated by the gaussian-based projections #OTHEREFR .", "The relation of statistical performance and input structure has not been understood in-depth yet; as for conceptually similar research, we note that recent results show that for sparse data one can improve the sparsity of random projections, gaining in computing time #OTHEREFR ."], "citing_paper_content": {"title": "Exact Non-Oblivious Performance Of Rademacher Random Embeddings", "abstract": "This paper revisits the performance of Rademacher random projections, establishing novel statistical guarantees that are numerically sharp and non-oblivious with respect to the input data. More specifically, the central result is the Schur-concavity property of Rademacher random projections with respect to the inputs. This offers a novel geometric perspective on the performance of random projections, while improving quantitatively on bounds from previous works. As a corollary of this broader result, we obtained the improved performance on data which is sparse or is distributed with small spread. This non-oblivious analysis is a novelty compared to techniques from previous work, and bridges the frequently observed gap between theory and practise. The main result uses an algebraic framework for proving Schur-concavity properties, which is a contribution of independent interest and an elegant alternative to derivative-based criteria."}, "cited_paper_content": {"title": "Optimal Bounds For Johnson-Lindenstrauss Transformations", "abstract": "In 1984, Johnson and Lindenstrauss proved that any finite set of data in a high-dimensional space can be projected to a lower-dimensional space while preserving the pairwise Euclidean distance between points up to a bounded relative error. If the desired dimension of the image is too small, however, Kane, Meka, and Nelson (2011) and Jayram and Woodruff (2013) independently proved that such a projection does not exist. In this paper, we provide a precise asymptotic threshold for the dimension of the image, above which, there exists a projection preserving the Euclidean distance, but, below which, there does not exist such a projection."}, "keywords": ["Rademacher random projections"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "This work addresses the aforementioned gap by revisiting the most promising construction of Rademacher random projections, which uses the following matrix EQUATION More specifically, this paper solves the following problem: Give a precise, non-asymptotic, non-oblivious analysis of random projections (REFR). The authors need to cite the reference paper: \"Optimal Bounds For Johnson-Lindenstrauss Transformations\" as it provides a relevant background on the topic of projecting data to a lower-dimensional space while preserving the pairwise Euclidean distance between points up to a bounded relative error.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This work addresses the aforementioned gap by revisiting the most promising construction of Rademacher random projections and comparing the results with the optimal bounds for Johnson-Lindenstrauss transformations provided in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This work addresses the aforementioned gap by revisiting the most promising construction of Rademacher random projections and comparing the results with the optimal bounds for Johnson-Lindenstrauss transformations #REFR.", "given_citation_intent": "result", "given_keywords": ["Rademacher random projections"]}]}
{"citing_id": "2303.06872v3", "cited_id": "1706.03762", "section_title": "B. Camera-Lidar Fusion For Relocalization With Multi-Head Self-Attention", "citation": "Like the Transformer encoder in #REFR , a normalization layer is applied before MHSA, and a residual connection is attached after MHSA.", "text_before_citation": ["W p f T Att1 , . . . , f T Attj , . . . , f T Att N h T ,", "where f Attj is the output of the j-th scaled dot-product attention, N h is the number of the attention heads, and", "a T 1 , . . . , a T n", "is the concatenation of {a T i } n i=1 .", "In this operation, each attention is scaled by a scaling factor N h so that its output has the same dimension as the input."], "text_after_citation": ["We employ batch normalization (BN, #OTHEREFR ) instead of layer normalization (LN, #OTHEREFR ) different from #OTHEREFR .", "It was demonstrated in #OTHEREFR that LN is more effective than BN for recurrent networks.", "However, we find out from experiments that BN is more effective than LN in this work.", "Also, we do not use the positional encoding, another input of the Transformer encoder, because the order of elements in the fusion feature is not important in this task, unlike a sequence.", "This MHSA block with identical architecture repeats N l times as in #OTHEREFR ."], "citing_paper_content": {"title": "Fusionloc: Camera-2D Lidar Fusion Using Multi-Head Self-Attention For End-To-End Serving Robot Relocalization", "abstract": "As technology advances in autonomous mobile robots, mobile service robots have been actively used more and more for various purposes. Especially, serving robots have been not surprising products anymore since the COVID-19 pandemic. One of the practical problems in operating serving a robot is that it often fails to estimate its pose on a map that it moves around. Whenever the failure happens, servers should bring the serving robot to its initial location and reboot it manually. In this paper, we focus on end-to-end relocalization of serving robots to address the problem. It is to predict robot pose directly from only the onboard sensor data using neural networks. In particular, we propose a deep neural network architecture for the relocalization based on camera-2D LiDAR sensor fusion. We call the proposed method FusionLoc. In the proposed method, the multi-head selfattention complements different types of information captured by the two sensors to regress the robot pose. Our experiments on a dataset collected by a commercial serving robot demonstrate that FusionLoc can provide better performances than previous endto-end relocalization methods taking only a single image or a 2D LiDAR point cloud as well as a straightforward fusion method concatenating their features."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["normalization layer", "Transformer encoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The multi-head self-attention mechanism used in our proposed FusionLoc method is based on the Transformer architecture proposed in the reference paper #REFR, which has been shown to be superior in quality and more parallelizable than traditional recurrent or convolutional neural networks.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed FusionLoc method utilizes multi-head self-attention to complement different types of information captured by camera-2D LiDAR sensors for end-to-end relocalization of serving robots (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement the multi-head self-attention mechanism, the proposed FusionLoc method concatenates the features from the camera and 2D LiDAR sensors and scales each attention output by a factor of N_h, as described in the Transformer encoder of the #REFR.", "given_citation_intent": "method", "given_keywords": ["normalization layer", "Transformer encoder"]}]}
{"citing_id": "2303.10894v1", "cited_id": "1912.05074", "section_title": "", "citation": "UNet++ #REFR uses nested and dense skip connections to reduce the semantic gap between the feature maps of encoder and decoder, as shown in Fig. 1 (c) .", "text_before_citation": ["sions.", "There are three general challenges in accurate segmentation: Firstly, U-shape structures #OTHEREFR , #OTHEREFR have received considerable attention due to their abilities of utilizing multi-level information to reconstruct high-resolution feature maps.", "In UNet #OTHEREFR , the up-sampled feature maps are concatenated with feature maps skipped from the encoder and convolutions and non-linearities are added between up-sampling steps, as shown in Fig. 1 (a) .", "Subsequent UNet-based methods design diverse feature enhancement modules via attention mechanism #OTHEREFR , #OTHEREFR , gate mechanism #OTHEREFR , #OTHEREFR , transformer technique #OTHEREFR , #OTHEREFR , as shown in Fig. 1 (b) ."], "text_after_citation": ["Generally speaking, different level features in encoder have different characteristics.", "High-level ones have more semantic information which helps localize the objects, while low-level ones have more detailed information which can capture the subtle boundaries of objects.", "The decoder leverages the levelspecific and cross-level characteristics to generate the final high-resolution prediction.", "Nevertheless, the aforementioned methods directly use an element-wise addition or concatenation to fuse any two level features from the encoder and transmit them to the decoder.", "These simple operations do not pay more attention to differential information between different levels."], "citing_paper_content": {"title": "M 2 Snet: Multi-Scale In Multi-Scale Subtraction Network For Medical Image Segmentation", "abstract": "Accurate medical image segmentation is critical for early medical diagnosis. Most existing methods are based on U-shape structure and use element-wise addition or concatenation to fuse different level features progressively in decoder. However, both the two operations easily generate plenty of redundant information, which will weaken the complementarity between different level features, resulting in inaccurate localization and blurred edges of lesions. To address this challenge, we propose a general multi-scale in multi-scale subtraction network (M 2 SNet) to finish diverse segmentation from medical image. Specifically, we first design a basic subtraction unit (SU) to produce the difference features between adjacent levels in encoder. Next, we expand the single-scale SU to the intralayer multi-scale SU, which can provide the decoder with both pixel-level and structure-level difference information. Then, we pyramidally equip the multi-scale SUs at different levels with varying receptive fields, thereby achieving the inter-layer multi-scale feature aggregation and obtaining rich multi-scale difference information. In addition, we build a training-free network \"LossNet\" to comprehensively supervise the task-aware features from bottom layer to top layer, which drives our multi-scale subtraction network to capture the detailed and structural cues simultaneously. Without bells and whistles, our method performs favorably against most state-of-the-art methods under different evaluation metrics on eleven datasets of four different medical image segmentation tasks of diverse image modalities, including color colonoscopy imaging, ultrasound imaging, computed tomography (CT), and optical coherence tomography (OCT). The source code can be available at https: //github.com/Xiaoqi-Zhao-DLUT/MSNet."}, "cited_paper_content": {"title": "Unet++: Redesigning Skip Connections To Exploit Multiscale Features In Image Segmentation", "abstract": "The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects -- an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at this https URL."}, "keywords": ["encoder", "dense skip connections"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "U-Net and fully convolutional networks (FCN) are state-of-the-art models for medical image segmentation, but suffer from limitations in terms of unknown optimal depth and restrictive fusion schemes for skip connections #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of the skip connections in U-Net, UNet++ proposes a new neural architecture for semantic and instance segmentation by redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitations of the skip connections in U-Net, UNet++ proposes a new neural architecture that redesigns the skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme, which partially shares an encoder and co-learns simultaneously using deep supervision #REFR.", "given_citation_intent": "method", "given_keywords": ["encoder", "dense skip connections"]}]}
{"citing_id": "2304.01592v1", "cited_id": "2001.00106", "section_title": "A. Pac-Based Safety Guarantees", "citation": "The approach taken in #REFR to place PAC guarantees applies this concept by attempting to estimate the VC-dimension of the classification algorithm.", "text_before_citation": ["There are a number of papers that address the specific topic using PAC-based guarantees to generalize error bounds within CPSs.", "Notable investigations in this area include #OTHEREFR , #OTHEREFR , #OTHEREFR and #OTHEREFR .", "Similar to the objective of this study, the PAC-based guarantees in the aforementioned works correlate the size of the training data to the failure rate with a particular level of confidence.", "The error bounds for learning described in #OTHEREFR and #OTHEREFR use a generalized term correlated to the size of the hypothesis space to describe the target concept sample complexity.", "This is further generalized in #OTHEREFR as a bound that is dependent on the VC-dimension of the model being used."], "text_after_citation": ["In contrast to this, #OTHEREFR proposes the formulation of PACbased error bounds through the formulation of the problem as an optimization problem with the objective of minimizing the constraint violation probability.", "One of the main contributions of #OTHEREFR is that stochastic perturbations within the input layer, with an underlying probability distribution, are factored into the derived error bounds.", "Because the problem investigated in this study can be framed similarly, a similar approach to #OTHEREFR is utilized when deriving the error bounds.", "However, because this study attempts to approximate safety constraints using conformal prediction #OTHEREFR the guarantee placed on the constraints being accurate are incorporated into the PAC-based generalized error bounds for the entire system.", "To the best of our knowledge, aside from this paper, there are no existing studies on combining multiple types of guarantees when bounding the failure rate of an entire system."], "citing_paper_content": {"title": "Pac-Based Formal Verification For Out-Of-Distribution Data Detection", "abstract": "Cyber-physical systems (CPS) like autonomous vehicles, that utilize learning components, are often sensitive to noise and out-of-distribution (OOD) instances encountered during runtime. As such, safety critical tasks depend upon OOD detection subsystems in order to restore the CPS to a known state or interrupt execution to prevent safety from being compromised. However, it is difficult to guarantee the performance of OOD detectors as it is difficult to characterize the OOD aspect of an instance, especially in high-dimensional unstructured data. To distinguish between OOD data and data known to the learning component through the training process, an emerging technique is to incorporate variational autoencoders (VAE) within systems and apply classification or anomaly detection techniques on their latent spaces. The rationale for doing so is the reduction of the data domain size through the encoding process, which benefits real-time systems through decreased processing requirements, facilitates feature analysis for unstructured data and allows more explainable techniques to be implemented. This study places probably approximately correct (PAC) based guarantees on OOD detection using the encoding process within VAEs to quantify image features and apply conformal constraints over them. This is used to bound the detection error on unfamiliar instances, , with user-defined confidence, 1 \u2212 \u03b4. The approach used in this study is to empirically establish these bounds by sampling the latent probability distribution and evaluating the error with respect to the constraint violations that are encountered. The guarantee is then verified using data generated from CARLA, an open-source driving simulator."}, "cited_paper_content": {"title": "Pac Confidence Sets For Deep Neural Networks Via Calibrated Prediction", "abstract": "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees---i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, and on a dynamics model the half-cheetah reinforcement learning problem."}, "keywords": ["classification algorithm", "PAC guarantees"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Similar to the objective of this study, the PAC-based guarantees in the aforementioned works correlate the size of the training data to the failure rate with a particular level of confidence. The error bounds for learning described in #REFR use a generalized term correlated to the size of the hypothesis space to describe the target concept sample complexity.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct confidence sets for deep neural networks with PAC guarantees, the authors propose an algorithm combining calibrated prediction and generalization bounds from learning theory, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct confidence sets for deep neural networks with PAC guarantees, the authors propose a novel algorithm that combines calibrated prediction and generalization bounds from learning theory, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["classification algorithm", "PAC guarantees"]}]}
{"citing_id": "2304.11130v1", "cited_id": "1908.10084", "section_title": "Sbert", "citation": "The model is based on sentence-BERT (SBERT) similarity measures #REFR , which specifically targets the STS task.", "text_before_citation": [], "text_after_citation": ["As demonstrated in previous research #OTHEREFR , document-level models suffer from the loss of details, which affects accuracy.", "SBERT, however, has been optimized to treat text at the sentence-level, and not document-level, yielding better results #OTHEREFR than BERT.", "CWE input is segmented into sentences and the model computes the cosine similarity between two sentence embeddings.", "It was observed that CVE records, in the released dataset, have on average 3.69 sentences.", "Alternatively, the 25 CWE inputs is a collate of the name, the description, and the extended description with an average of 8.2 sentences."], "citing_paper_content": {"title": "Automated Mapping Of Cve Vulnerability Records To Mitre Cwe Weaknesses", "abstract": "In recent years, a proliferation of cyber-security threats and diversity has been on the rise culminating in an increase in their reporting and analysis. To counter that, many non-profit organizations have emerged in this domain, such as MITRE and OSWAP, which have been actively tracking vulnerabilities, and publishing defense recommendations in standardized formats. As producing data in such formats manually is very time-consuming, there have been some proposals to automate the process. Unfortunately, a major obstacle to adopting supervised machine learning for this problem has been the lack of publicly available specialized datasets. Here, we aim to bridge this gap. In particular, we focus on mapping CVE records into MITRE CWE Weaknesses, and we release to the research community a manually annotated dataset of 4,012 records for this task. With a human-in-the-loop framework in mind, we approach the problem as a ranking task and aim to incorporate reinforced learning to make use of the human feedback in future work. Our experimental results using fine-tuned deep learning models, namely Sentence-BERT and rankT5, show sizable performance gains over BM25, BERT, and RoBERTa, which demonstrates the need for an architecture capable of good semantic understanding for this task."}, "cited_paper_content": {"title": "Sentence-Bert: Sentence Embeddings Using Siamese Bert-Networks", "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. ::: In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. ::: We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."}, "keywords": ["sentence-BERT (SBERT) similarity"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our experimental results using fine-tuned deep learning models, namely Sentence-BERT and rankT5, show sizable performance gains over BM25, BERT, and RoBERTa, which demonstrates the need for an architecture capable of good semantic understanding for this task. To achieve this, we utilized Sentence-BERT (SBERT) as a modification of the pretrained BERT network that uses siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve semantic understanding in mapping CVE records into MITRE CWE Weaknesses, the authors fine-tuned deep learning models, including Sentence-BERT, which showed sizable performance gains over other methods (#REFR).", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve semantic understanding in mapping CVE records to MITRE CWE Weaknesses, the authors fine-tuned deep learning models, including Sentence-BERT (SBERT), achieving sizable performance gains over other methods such as BM25, BERT, and RoBERTa. #REFR was used as a modification of the pretrained BERT network that reduced the effort for finding the most similar pair from 65 hours to about 5 seconds while maintaining accuracy.", "given_citation_intent": "method", "given_keywords": ["sentence-BERT (SBERT) similarity"]}]}
{"citing_id": "2304.06551v1", "cited_id": "1602.05629", "section_title": "Introduction", "citation": "Federated learning (FL), recently developed and proposed by Google as an emerging distributed machine learning technology, will provide further new technology to support the intelligence of drones #REFR .", "text_before_citation": ["However, traditional machine learning techniques require uploading all data to a cloud-based server for training and processing, which represents a considerable challenge for drone swarms #OTHEREFR .", "In a first consideration, the data generated by drones may be sensitive, and could be intercepted while uploading the data to the cloud, leading to a privacy breach.", "Secondly, drones' large numbers of data can result in impractical delays when uploading, thus creating a time lag for swarms of drones that prevents them from conducting real-time monitoring.", "Finally, drones can consume a great amount of energy when training models, meaning there may be related challenges to doing so in terms of energy constraints #OTHEREFR .", "Distributed machine learning techniques represent a new solution to address these issues and challenges, whereby drones train machine learning models without sharing raw data."], "text_after_citation": ["The concept of federated learning is allowing each drone to train its learning model based on its data.", "The parameters of each drone's trained model are then sent to a parameter server to update the model for a new round of training, without sending the raw data to the cloud.", "This training model allows for reasonable data security, latency and energy consumption.", "However, the highly mobile nature of drones means conventional FL is not well-suited, given their complex working environment.", "If the parameter server does not work properly, it will impact the training effectiveness of the whole UAV network #OTHEREFR ."], "citing_paper_content": {"title": "Decentralized Federated Learning Methods For Reducing Communication Cost And Energy Consumption In Uav Networks", "abstract": "Unmanned aerial vehicles (UAV) or drones play many roles in a modern smart city such as the delivery of goods, mapping real-time road traffic and monitoring pollution. The ability of drones to perform these functions often requires the support of machine learning technology. However, traditional machine learning models for drones encounter data privacy problems, communication costs and energy limitations. Federated Learning, an emerging distributed machine learning approach, is an excellent solution to address these issues. Federated learning (FL) allows drones to train local models without transmitting raw data. However, existing FL requires a central server to aggregate the trained model parameters of the UAV. A failure of the central server can significantly impact the overall training. In this paper, we propose two aggregation methods: Commutative FL and Alternate FL, based on the existing architecture of decentralised Federated Learning for UAV Networks (DFL-UN) by adding a unique aggregation method of decentralised FL. Those two methods can effectively control energy consumption and communication cost by controlling the number of local training epochs, local communication, and global communication. The simulation results of the proposed training methods are also presented to verify the feasibility and efficiency of the architecture compared with two benchmark methods (e.g. standard machine learning training and standard single aggregation server training). The simulation results show that the proposed methods outperform the benchmark methods in terms of operational stability, energy consumption and communication cost."}, "cited_paper_content": {"title": "Communication-Efficient Learning Of Deep Networks From Decentralized Data", "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."}, "keywords": ["Federated learning"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Distributed machine learning techniques represent a new solution to address these issues and challenges, whereby drones train machine learning models without sharing raw data. Recent work on Federated Learning has shown that it is possible to learn models without uploading raw data to a central server, and instead, learn a shared model by aggregating locally-computed updates #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Distributed machine learning techniques, such as Federated Learning, have emerged as a solution to address the challenges of traditional machine learning techniques for drones, where models can be trained without sharing raw data #REFR, which is particularly important given the sensitivity of the data generated by drones and the impractical delays when uploading large amounts of data to the cloud.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Distributed machine learning techniques, such as Federated Learning, have emerged as a solution to address the challenges of traditional machine learning techniques for drones, allowing them to train machine learning models without sharing raw data #REFR.", "given_citation_intent": "background", "given_keywords": ["Federated learning"]}]}
{"citing_id": "2303.07585v1", "cited_id": "1905.10650", "section_title": "Iv. Related Work", "citation": "A similar result was obtained in #REFR , where they argued that a reasonable amount of attention heads could be removed during test time without significant performance loss.", "text_before_citation": ["There has been substantial recent research on examining the attention mechanism.", "Layer-based attention distribution analysis for 128-token-long inputs was conducted in #OTHEREFR to measure the syntactic ability of attention heads.", "One of the findings of #OTHEREFR is that the self-attention heads within the same layer have the similar attention distribution."], "text_after_citation": ["According to #OTHEREFR , BERT's initial layers are crucial for capturing word-order information.", "In contrast, middle layers are essential for syntactic information #OTHEREFR and the final layer representations are prominent for taskspecific adaptation #OTHEREFR .", "However, the relationship between attention weights and model outputs is ambiguous.", "For example, #OTHEREFR finds that the attention values have weak correlation with feature importance measures using gradient or feature erasure methods.", "They also demonstrate that different sets of attention values learned using adversarial training can result in the same prediction, therefore attention values should not be utilised as an explanation of the model's predictions."], "citing_paper_content": {"title": "Input-Length-Shortening And Text Generation Via Attention Values", "abstract": "Identifying words that impact a task's performance more than others is a challenge in natural language processing. Transformers models have recently addressed this issue by incorporating an attention mechanism that assigns greater attention (i.e., relevance) scores to some words than others. Because of the attention mechanism's high computational cost, transformer models usually have an input-length limitation caused by hardware constraints. This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model. In this paper, we examined BERT's attention assignment mechanism, focusing on two questions: (1) How can attention be employed to reduce input length? (2) How can attention be used as a control mechanism for conditional text generation?We investigated these questions in the context of a text classification task. We discovered that BERT's early layers assign more critical attention scores for text classification tasks compared to later layers. We demonstrated that the first layer's attention sums could be used to filter tokens in a given sequence, considerably decreasing the input length while maintaining good test accuracy. We also applied filtering, which uses a computeefficient semantic similarities algorithm, and discovered that retaining approximately 6% of the original sequence is sufficient to obtain 86.5% accuracy. Finally, we showed that we could generate data in a stable manner and indistinguishable from the original one by only using a small percentage (10%) of the tokens with high attention scores according to BERT's first layer."}, "cited_paper_content": {"title": "Are Sixteen Heads Really Better Than One?", "abstract": "Multi-headed attention is a driving force behind recent state-of-the-art NLP models. By applying multiple attention mechanisms in parallel, it can express sophisticated functions beyond the simple weighted average. However we observe that, in practice, a large proportion of attention heads can be removed at test time without significantly impacting performance, and that some layers can even be reduced to a single head. Further analysis on machine translation models reveals that the self-attention layers can be significantly pruned, while the encoder-decoder layers are more dependent on multi-headedness."}, "keywords": ["attention heads"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "There has been substantial recent research on examining the attention mechanism. Layer-based attention distribution analysis for 128-token-long inputs was conducted in #OTHEREFR to measure the syntactic ability of attention heads. One of the findings of #OTHEREFR is that the self-attention heads within the same layer have the similar attention distribution. This finding is consistent with the observation in #REFR that a large proportion of attention heads can be removed at test time without significantly impacting performance.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of #REFR found that a large proportion of attention heads can be removed at test time without significantly impacting performance, which is consistent with the finding that self-attention heads within the same layer have similar attention distribution, as reported in the previous study.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The findings of #OTHEREFR suggest that a large proportion of attention heads can be removed at test time without significantly impacting performance, which is consistent with the observation that the self-attention layers can be significantly pruned while the encoder-decoder layers are more dependent on multi-headedness, as reported in the reference paper #REFR on attention heads.", "given_citation_intent": "result", "given_keywords": ["attention heads"]}]}
{"citing_id": "2304.00910v1", "cited_id": "1905.05833", "section_title": "B. Multiview-Activated Scvp Network Architecture", "citation": "We follow the setting in NBVNet #REFR to construct the shape size of our input and output.", "text_before_citation": ["The function of multiview-activated (MA-)SCVP network is a classic multilabel classification [71] function: #OTHEREFR This function takes a 32 \u00d732 \u00d732 occupancy grid and a vector of 32 bits as input, and predicts a vector of 32 bits so that the V * cover can be obtained."], "text_after_citation": ["Since convolution is easier to operate on data with equal dimensions, we extract a 32 \u00d7 32 \u00d7 32 cubic bounding box from our OctoMap M.", "We adopt the dynamic resolution of M so that the shape size of 32 \u00d7 32 \u00d7 32 is sufficient to generalize to different object sizes.", "In the real world, we assume o size is not greater than 15 cm, which can be predicted by solving the minimum bounding sphere in the point clouds to obtain the M.", "The view state vector V state is the same size as our candidate view space.", "A bit in our network output is bound to a certain candidate view because the SCOP is solved in such a fixed candidate view space."], "citing_paper_content": {"title": "One-Shot View Planning For Fast And Complete Unknown Object Reconstruction", "abstract": "Fig. 1: Comparison of surface details, views, and paths to reconstruct an untrained object: reconstructed 3D models (red point clouds), major missing surface details (gray voxels in enlarged areas), local paths (cyan), global paths (purple), views (red-green-blue), and the same initial view (black circle). (a) Results of an iterative learning-based NBV network. (b) Results of the one-shot SCVP network. (c) Results of our novel combined pipeline that selects four NBVs before activating the SCVP network. (d) Results of our novel combined pipeline that selects one NBV before activating the proposed MA-SCVP network. To ensure that sufficient surface details can be illuminated (black arrow), 4-NBV+SCVP requires more views and paths than MA-SCVP in this example (14 views with 4 local paths vs. 13 views with 1 local path)."}, "cited_paper_content": {"title": "Supervised Learning Of The Next-Best-View For 3D Object Reconstruction", "abstract": "Motivated by the advances in 3D sensing technology and the spreading of low-cost robotic platforms, 3D object reconstruction has become a common task in many areas. Nevertheless, the selection of the optimal sensor pose that maximizes the reconstructed surface is a problem that remains open. It is known in the literature as the next-best-view planning problem. In this paper, we propose a novel next-best-view planning scheme based on supervised deep learning. The scheme contains an algorithm for automatic generation of datasets and an original three-dimensional convolutional neural network (3D-CNN) used to learn the next-best-view. Unlike previous work where the problem is addressed as a search, the trained 3D-CNN directly predicts the sensor pose. We present a comparison of the proposed network against a similar net, and we present several experiments of the reconstruction of unknown objects validating the effectiveness of the proposed scheme."}, "keywords": ["NBVNet"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed multiview-activated (MA-)SCVP network in this study is based on a classic multilabel classification function that takes an occupancy grid and a vector of bits as input and predicts a vector of bits for obtaining the V * cover, as described in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn the next-best-view for 3D object reconstruction, the authors proposed a novel next-best-view planning scheme based on supervised deep learning, which includes an algorithm for automatic generation of datasets and a 3D-CNN that directly predicts the sensor pose, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the next-best-view planning problem, the authors propose a novel next-best-view planning scheme based on supervised deep learning, which contains an algorithm for automatic generation of datasets and an original 3D-CNN used to learn the next-best-view, as described in NBVNet. #REFR", "given_citation_intent": "method", "given_keywords": ["NBVNet"]}]}
{"citing_id": "2304.07918v1", "cited_id": "1906.01618", "section_title": "Experiments 4.1 Datasets", "citation": "We use the images rendered by #REFR and follow its split to separate the training and testing sets.", "text_before_citation": ["To evaluate the proposed NeRF-LEBM framework and the learning algorithms, we conduct experiments on three datasets.", "The Carla dataset is rendered by #OTHEREFR using the Carla Driving Simulator #OTHEREFR .", "It contains 10k cars of different shapes, colors and textures.", "Each car has one 2D image rendered from one random camera pose.", "Another dataset is the ShapeNet #OTHEREFR Car dataset, which contains 2.1k different cars for training and 700 cars for testing."], "text_after_citation": ["Each car in the training set has 250 views and we only use 50 views of them for training. Each car in the testing set has 251 views. each image is associated with its camera pose information."], "citing_paper_content": {"title": "Likelihood-Based Generative Radiance Field With Latent Space Energy-Based Model For 3D-Aware Disentangled Image Representation", "abstract": "1 We propose the NeRF-LEBM, a likelihoodbased top-down 3D-aware 2D image generative model that incorporates 3D representation via Neural Radiance Fields (NeRF) and 2D imaging process via differentiable volume rendering. The model represents an image as a rendering process from 3D object to 2D image and is conditioned on some latent variables that account for object characteristics and are assumed to follow informative trainable energy-based prior models. We propose two likelihood-based learning frameworks to train the NeRF-LEBM: (i) maximum likelihood estimation with Markov chain Monte Carlo-based inference and (ii) variational inference with the reparameterization trick. We study our models in the scenarios with both known and unknown camera poses. Experiments on several benchmark datasets demonstrate that the NeRF-LEBM can infer 3D object structures from 2D images, generate 2D images with novel views and objects, learn from incomplete 2D images, and learn from 2D images with known or unknown camera poses."}, "cited_paper_content": {"title": "Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations", "abstract": "Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model."}, "keywords": ["images"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To infer 3D object structures from 2D images, the proposed NeRF-LEBM framework utilizes Scene Representation Networks (SRNs) as a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To infer 3D object structures from 2D images, the proposed NeRF-LEBM framework incorporates 3D representation via Neural Radiance Fields (NeRF) and 2D imaging process via differentiable volume rendering, as well as informative trainable energy-based prior models for latent variables #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To infer 3D object structures from 2D #KEYWORD(images), the proposed NeRF-LEBM framework utilizes Neural Radiance Fields (NeRF) and differentiable volume rendering, as well as informative trainable energy-based prior models for latent variables #REFR.", "given_citation_intent": "method", "given_keywords": ["images"]}]}
{"citing_id": "2303.10771v2", "cited_id": "1803.02602", "section_title": "Randomized Linear Algebra", "citation": "In practice, the matrix Q can be obtained via (sparse) Cholesky factorization of R U , but it can also be any rectangular matrix such that Q T Q = R U , as pointed out in #REFR Remark 2.7] .", "text_before_citation": ["It is for example the case for the partial subsampled randomized Hadamard transform (P-SRHT) described in #OTHEREFR .", "Sufficient conditions on k are also available but they are very conservative.", "However, numerical experiments from #OTHEREFR showed performances similar to the Gaussian embedding for a given dimension k.", "A good practice is to use composed embeddings, for example \u0398 = \u0398 2 \u0398 1 with \u0398 1 a moderate sized P-SRHT embedding and \u0398 2 a small sized Gaussian embedding.", "Remark 4."], "text_after_citation": ["Such matrix may be obtained by Cholesky factorizations of small matrices, which are easy to compute. This is especially important for large scale problems."], "citing_paper_content": {"title": "Dictionary-Based Model Reduction For State Estimation", "abstract": "We consider the problem of state estimation from m linear measurements, where the state u to recover is an element of the manifold M of solutions of a parameter-dependent equation. The state is estimated using a prior knowledge on M coming from model order reduction. Variational approaches based on linear approximation of M, such as PBDW, yields a recovery error limited by the Kolmogorov m-width of M. To overcome this issue, piecewise-affine approximations of M have also be considered, that consist in using a library of linear spaces among which one is selected by minimizing some distance to M. In this paper, we propose a state estimation method relying on dictionary-based model reduction, where a space is selected from a library generated by a dictionary of snapshots, using a distance to the manifold. The selection is performed among a set of candidate spaces obtained from the path of a 1-regularized least-squares problem. Then, in the framework of parameter-dependent operator equations (or PDEs) with affine parameterizations, we provide an efficient offline-online decomposition based on randomized linear algebra, that ensures efficient and stable computations while preserving theoretical guarantees."}, "cited_paper_content": {"title": "Randomized Linear Algebra For Model Reduction. Part I: Galerkin Methods And Error Estimation", "abstract": "We propose a probabilistic way for reducing the cost of classical projection-based model order reduction methods for parameter-dependent linear equations. A reduced order model is here approximated from its random sketch, which is a set of low-dimensional random projections of the reduced approximation space and the spaces of associated residuals. This approach exploits the fact that the residuals associated with approximations in low-dimensional spaces are also contained in low-dimensional spaces. We provide conditions on the dimension of the random sketch for the resulting reduced order model to be quasi-optimal with high probability. Our approach can be used for reducing both complexity and memory requirements. The provided algorithms are well suited for any modern computational environment. Major operations, except solving linear systems of equations, are embarrassingly parallel. Our version of proper orthogonal decomposition can be computed on multiple workstations with a communication cost independent of the dimension of the full order model. The reduced order model can even be constructed in a so-called streaming environment, i.e., under extreme memory constraints. In addition, we provide an efficient way for estimating the error of the reduced order model, which is not only more efficient than the classical approach but is also less sensitive to round-off errors. Finally, the methodology is validated on benchmark problems."}, "keywords": ["Cholesky factorization"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors propose a probabilistic approach for reducing the cost of classical projection-based model order reduction methods for parameter-dependent linear equations by approximating a reduced order model from its random sketch, as described in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Randomized linear algebra has been proposed as a way to reduce the cost of classical projection-based model order reduction methods for parameter-dependent linear equations, as described in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The reduced order model can be constructed efficiently using randomized linear algebra techniques, as proposed in #REFR, which can significantly reduce the computational cost and memory requirements, while preserving theoretical guarantees.", "given_citation_intent": "background", "given_keywords": ["Cholesky factorization"]}]}
{"citing_id": "2303.16102v1", "cited_id": "1803.08494", "section_title": "A. Network Training", "citation": "To generalize the network Group Norm #REFR with group size 32 is used after each linear layer.", "text_before_citation": ["During each epoch we generate 160 point clouds for each component. Thus each training epoch consists of 232000 point clouds.", "The network is trained with a batch size of 14, 7 on each GPU, using the Adam optimizer #OTHEREFR , with an initial learning rate of 0.0001.", "We use a step scheduler with a step size of 20 and the gamma parameter set to 0.7.", "The loss is calculated with cross entropy using a 0.2/0.8 split for segmentation and keypoint loss.", "For the keypoint loss only points belonging to the object is used."], "text_after_citation": ["Group Norm is used as opposed to Batch Norm as a result of the small batch size.", "Dropout is used for the object features, as the network should not overfit to a specific part of the object, and is used after the to concurrent linear layers.", "The dropout is set to 40 %, used after the last two linear layers of the object feature and the first two of the combined feature.", "Additionally, up to 0.75 % Gaussian noise is applied to the object and scene point clouds, and 10 % position shift is applied to the object point cloud.", "The network was trained on a PC environment with two NVIDIA GeForce RTX 2080 GPUs."], "citing_paper_content": {"title": "Gp3D: Generalized Pose Estimation In 3D Point Clouds: A Case Study On Bin Picking", "abstract": "In this paper, we present GP3D, a novel network for generalized pose estimation in 3D point clouds. The method generalizes to new objects by using both the scene point cloud and the object point cloud with keypoint indexes as input. The network is trained to match the object keypoints to scene points. To address the pose estimation of novel objects we also present a new approach for training pose estimation. The typical solution is a single model trained for pose estimation of a specific object in any scenario. This has several drawbacks: training a model for each object is time-consuming, energy consuming, and by excluding the scenario information the task becomes more difficult. In this paper, we present the opposite solution; a scenariospecific pose estimation method for novel objects that do not require retraining. The network is trained on 1500 objects and is able to learn a generalized solution. We demonstrate that the network is able to correctly predict novel objects, and demonstrate the ability of the network to perform outside of the trained class. We believe that the demonstrated method is a valuable solution for many real-world scenarios. Code and trained network will be made available after publication. Index Terms-pose estimation, point cloud, deep learning Pose Estimation Scene Point Cloud (Colored for visualization) Object Point Cloud with Keypoint Indecies"}, "cited_paper_content": {"title": "Group Normalization", "abstract": "Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries."}, "keywords": ["network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of inaccurate batch statistics estimation in deep learning, Group Normalization (GN) was proposed as a simple alternative to Batch Normalization (BN) #REFR. GN's computation is independent of batch sizes, making it a suitable replacement for BN in a variety of tasks, including object detection and segmentation, as demonstrated in COCO and Kinetics datasets.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of inaccurate batch statistics estimation, the authors propose using Group Normalization (GN) as a simple alternative to Batch Normalization (BN), which divides the channels into groups and computes within each group the mean and variance for normalization, making its computation independent of batch sizes and stable in a wide range of batch sizes #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of inaccurate batch statistics estimation, the authors propose to use Group Normalization (GN) which divides the channels into groups and computes within each group the mean and variance for normalization, making its computation independent of batch sizes and stable in a wide range of batch sizes, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["network"]}]}
{"citing_id": "2304.05097v1", "cited_id": "1801.03924", "section_title": "Metrics", "citation": "LPIPS #REFR calculates the cosine distances between the network features of the two images layer by layer and averages them to estimate the perceived distance of the generated image from the ground truth image.", "text_before_citation": ["PSNR is numerically related to the mean squared error (MSE) between the ground truth and the reconstructed image, it is used to measure the image reconstruction quality.", "SSIM measures the structural similarity between patches of the input images.", "As a result, it is more robust to changes in the global illumination than PSNR."], "text_after_citation": ["CSIM #OTHEREFR To evaluate the effectiveness of identity preservation, we compute the cosine similarity using embedded vectors created by the pre-trained face recognition model.", "AUCON is used to calculate the ratio of the same facial action unit values between the generated images and the driving images."], "citing_paper_content": {"title": "One-Shot High-Fidelity Talking-Head Synthesis With Deformable Neural Radiance Field", "abstract": "Figure 1. Representative results of our method. The first three columns exhibit the source, driving, generated images, respectively. The rest columns show the exploration of the generated images to different yaw angles."}, "cited_paper_content": {"title": "The Unreasonable Effectiveness Of Deep Features As A Perceptual Metric", "abstract": "While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on the ImageNet classification task has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called\"perceptual losses\"? What elements are critical for their success? To answer these questions, we introduce a new Full Reference Image Quality Assessment (FR-IQA) dataset of perceptual human judgments, orders of magnitude larger than previous datasets. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by huge margins. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations."}, "keywords": ["generated image"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recently, it has been found that deep features of the VGG network trained on the ImageNet classification task are remarkably useful as a training loss for image synthesis, and outperform all previous metrics by huge margins, including PSNR and SSIM, which are widely used perceptual metrics today. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the limitations of traditional perceptual metrics, recent studies have shown that deep features extracted from various architectures, including ImageNet-trained VGG features, can be used as a perceptual loss for image synthesis, as demonstrated in the work by #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate high-fidelity talking-heads, recent works have explored the use of deep learning-based perceptual metrics to improve image synthesis. Specifically, the effectiveness of deep features as a perceptual metric has been shown to outperform all previous metrics by huge margins, even when using different deep architectures and levels of supervision, as demonstrated in the Full Reference Image Quality Assessment (FR-IQA) dataset #REFR.", "given_citation_intent": "method", "given_keywords": ["generated image"]}]}
{"citing_id": "2303.14552v1", "cited_id": "1512.00567", "section_title": "Fr\u00e9chet Inception Distance", "citation": "The FID passes images from both distributions through the pre-trained inception network #REFR obtaining embeddings for every image, e.g. 2048 dimensional embedding vectors for Inception V3.", "text_before_citation": ["The Fr\u00e9chet Inception Distance (FID) #OTHEREFR is a method for comparing similarity of two image distributions to each other.", "It is commonly used between real images and generated images by a GAN as a measure of GAN performance."], "text_after_citation": ["For both distribution, the estimated mean embedding vector and the covariance matrix is calculated. The final distance is defined as following:", "EQUATION", "3)", "where", "(m 1 , C 1 )"], "citing_paper_content": {"title": "Spatial Latent Representations In Generative Adversarial Networks For Image Generation Master'S Thesis In Computer Science", "abstract": "Generative Adversarial Networks (GANs) are currently state-of-the-art methods in image generation tasks. They generate new images by transforming a latent space into an image data distribution. In the vast majority of GAN architectures, the latent space is defined as a set of vectors of given dimensionality. Such representations are not easily interpretable and do not capture spatial information of image content directly. In this work, we define a family of spatial latent spaces for StyleGAN2, capable of capturing more details and representing images that are out-of-sample in terms of the number and arrangement of object parts, such as an image of multiple faces or a face with more than two eyes. We propose a method for encoding images into our spaces, together with an attribute model capable of performing attribute editing in these spaces. We show that our spaces are effective for image manipulation purposes and encode semantic information well. Our approach can be used on pre-trained generator models, and attribute edition can be done using pre-generated direction vectors making the barrier to entry for experimentation and use extremely low. We propose a regularization method for optimizing latent representations, which equalizes distributions of parts of latent spaces, making representations much closer to generated ones. We use it for encoding images into spatial spaces to obtain significant improvement in quality while keeping semantics and ability to use our attribute model for edition purposes. In total, using our methods gives encoding quality boost even as high as 30% in terms of LPIPS score comparing to standard methods, while keeping semantics. Additionally, we propose a StyleGAN2 training procedure on our spatial latent spaces, together with a custom spatial latent representation distribution to make spatially closer elements in the representation more dependent on each other than farther elements. Such approach improves the FID score by 29% on SpaceNet, and is able to generate consistent images of arbitrary sizes on spatially homogeneous datasets, like satellite imagery."}, "cited_paper_content": {"title": "Rethinking The Inception Architecture For Computer Vision", "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set."}, "keywords": ["pre-trained inception network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The Fr\u00e9chet Inception Distance (FID) is commonly used to measure GAN performance by comparing similarity of two image distributions. To achieve state-of-the-art results, recent works have explored ways to scale up convolutional networks while maintaining computational efficiency and low parameter count #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The Fr\u00e9chet Inception Distance (FID) is commonly used to measure GAN performance by comparing similarity of two image distributions to each other. In their paper, \"Rethinking The Inception Architecture For Computer Vision,\" the authors explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization, achieving substantial gains over the state of the art with a computational cost of 5 billion multiply-adds per inference and using less than 25 million parameters. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To compute the Fr\u00e9chet Inception Distance (FID) between real and generated images, a pre-trained inception network is commonly used, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["pre-trained inception network"]}]}
{"citing_id": "2303.11630v1", "cited_id": "1903.06874", "section_title": "Related Work", "citation": "Curve GCN #REFR regarded the initial contour as a graph and used a graph convolutional network to predict vertex-wise offsets.", "text_before_citation": ["For instance, Polygon RNN [9, 1] employed a CNN-RNN architecture to sequentially trace object boundaries in a given image patch.", "Two-stage Deep Snake #OTHEREFR created initial octagon contours using a detector and then iteratively deformed them through a circular convolution network.", "Based on that, DANCE #OTHEREFR utilized a segmentwise matching scheme and attentive contour deformation to facilitate learning.", "PolyTransform #OTHEREFR generated masks for each object using an off-the-shelf mask-based segmentation pipeline and converted the resulting mask contours into a set of vertices.", "Subsequently, the Transformer #OTHEREFR wrapped these vertices to fit the object silhouette better."], "text_after_citation": ["It employed a differentiable rendering loss to ensure that masks rendered from the predicted points agreed with the ground-truth masks.", "BoundaryFormer #OTHEREFR , on the other hand, applied a differentiable rasterization method to generate masks from polygons, achieving stunning results that are almost comparable to its mask-based counterparts on standard benchmarks.", "PolarMask #OTHEREFR and its followups #OTHEREFR adopted a set of rays in the polar coordinate system to represent object contours, which enables an efficient calculation of Intersection-over-Union.", "Furthermore, E2EC #OTHEREFR took the vertices and rays to define contours simultaneously.", "However, the deep learning-based methods mentioned above require expensive ground-truth masks or polygons, which hinders their practical applicability and extension."], "citing_paper_content": {"title": "Boxsnake: Polygonal Instance Segmentation With Box Supervision", "abstract": "Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new endto-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarsegrained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weaklysupervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset. The source code will be available at https://github.com/Yangr116/BoxSnake."}, "cited_paper_content": {"title": "Fast Interactive Object Annotation With Curve-Gcn", "abstract": "Manually labeling objects by tracing their boundaries is a laborious process. In Polygon-RNN++, the authors proposed Polygon-RNN that produces polygonal annotations in a recurrent manner using a CNN-RNN architecture, allowing interactive correction via humans-in-the-loop. We propose a new framework that alleviates the sequential nature of Polygon-RNN, by predicting all vertices simultaneously using a Graph Convolutional Network (GCN). Our model is trained end-to-end, and runs in real time. It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. We show that Curve-GCN outperforms all existing approaches in automatic mode, including the powerful DeepLab, and is significantly more efficient in interactive mode than Polygon-RNN++. Our model runs at 29.3ms in automatic, and 2.6ms in interactive mode, making it 10x and 100x faster than Polygon-RNN++."}, "keywords": ["graph convolutional network"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For instance, Polygon RNN [9, 1] employed a CNN-RNN architecture to sequentially trace object boundaries in a given image patch. Two-stage Deep Snake #OTHEREFR created initial octagon contours using a detector and then iteratively deformed them through a circular convolution network. Based on that, DANCE #OTHEREFR utilized a segmentwise matching scheme and attentive contour deformation to facilitate learning. PolyTransform #OTHEREFR generated masks for each object using an off-the-shelf mask-based segmentation pipeline and converted the resulting mask contours into a set of vertices. Subsequently, the Transformer #OTHEREFR wrapped these vertices to fit the object silhouette better. Curve-GCN, proposed by the authors in [REFR], predicts all vertices simultaneously using a Graph Convolutional Network (GCN) and supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "PolyTransform generated masks for each object using an off-the-shelf mask-based segmentation pipeline and converted the resulting mask contours into a set of vertices, while Curve-GCN #REFR predicts all vertices simultaneously using a Graph Convolutional Network, significantly improving labeling efficiency for both line-based and curved objects.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "PolyTransform generated masks for each object using an off-the-shelf mask-based segmentation pipeline and converted the resulting mask contours into a set of vertices, while Curve-GCN #REFR predicts all vertices simultaneously using a graph convolutional network, significantly improving labeling efficiency for both line-based and curved objects.", "given_citation_intent": "method", "given_keywords": ["graph convolutional network"]}]}
{"citing_id": "2304.03147v1", "cited_id": "1606.00061", "section_title": "Results And Analysis.", "citation": "Ultimately, based on the results in Table 4 , we conclude that HieCoAtt #REFR is the most robust VQA model.", "text_before_citation": ["Because the plots are monotonously decreasing in accuracy, or, equivalently, monotonously increasing in accuracy decrement, the ranking is effective.", "In this figure, \"First top 3\" represents the first partition, \"Second top 3\" represents the second partition and so on. models are more robust than non-attention-based ones.", "However, when we examine MU and MUA in Table 4 ( 2 ), the non-attention-based model (MU) is more robust than the attention-based model (MUA).", "It is worth noting that the only difference between MU and MUA is the attention mechanism.", "Meanwhile, in Table 4 ( 1 ), MUA is more robust than MU, indicating that the diversity of BQ candidates affects the robustness of attention-based VQA models in some cases."], "text_after_citation": ["The HieCoAtt model employs a co-attention mechanism that repeatedly exploits the text and image information to guide Fig. 7 . Visual Question Answering by Basic Questions (VQABQ) pipeline.", "Note that in Module 1 all of the training and validation questions are only encoded by Skip-Thought Question Encoder once for generating the Basic Question Matrix.", "That is, the next input of Skip-Thought Question Encoder is only a new main question.", "Module 2 is a VQA model which we want to test, and it is the HieCoAtt VQA model in our case.", "Regarding the input question of the HieCoAtt model, it is the direct concatenation of a given main question with the corresponding selected basic questions based on the Threshold-based Criterion. \"\u2295\" denotes the direct concatenation of basic questions."], "citing_paper_content": {"title": "Improving Visual Question Answering Models Through Robustness Analysis And In-Context Learning With A Chain Of Basic Questions", "abstract": "Deep neural networks have been critical in the task of Visual Question Answering (VQA), with research traditionally focused on improving model accuracy. Recently, however, there has been a trend towards evaluating the robustness of these models against adversarial attacks. This involves assessing the accuracy of VQA models under increasing levels of noise in the input, which can target either the image or the proposed query question, dubbed the main question. However, there is currently a lack of proper analysis of this aspect of VQA. This work proposes a new method that utilizes semantically related questions, referred to as basic questions, acting as noise to evaluate the robustness of VQA models. It is hypothesized that as the similarity of a basic question to the main question decreases, the level of noise increases. To generate a reasonable noise level for a given main question, a pool of basic questions is ranked based on their similarity to the main question, and this ranking problem is cast as a optimization problem. Additionally, this work proposes a novel robustness measure, , and two basic question datasets to standardize the analysis of VQA model robustness. The experimental results demonstrate that the proposed evaluation method effectively analyzes the robustness of VQA models. Moreover, the experiments show that in-context learning with a chain of basic questions can enhance model accuracy."}, "cited_paper_content": {"title": "Hierarchical Question-Image Co-Attention For Visual Question Answering", "abstract": "A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN). Our model improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset. By using ResNet, the performance is further improved to 62.1% for VQA and 65.4% for COCO-QA."}, "keywords": ["robust VQA model"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention, and present a novel co-attention model for VQA that jointly reasons about image and question attention #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The attention mechanism has been shown to improve the accuracy of Visual Question Answering (VQA) models, but the diversity of basic question (BQ) candidates can also affect their robustness in some cases, as demonstrated in Table 4 (1) of the manuscript. The proposed co-attention model for VQA in #REFR jointly reasons about image and question attention, and improves the state-of-the-art on the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA dataset.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The attention mechanism in VQA models affects the robustness of the model, as shown by the results in Table 4 (1) and (2) of the manuscript. Specifically, the attention-based model is less robust than the non-attention-based model in some cases, and the diversity of basic question candidates affects the robustness of attention-based VQA models. These findings are consistent with those in the hierarchical co-attention model proposed in #REFR, which improves the state-of-the-art on the VQA dataset and the COCO-QA dataset.", "given_citation_intent": "result", "given_keywords": ["robust VQA model"]}]}
{"citing_id": "2304.10643v1", "cited_id": "1703.09370", "section_title": "Performance Metrics", "citation": "As demonstrated in #REFR , activities in the Opportunity dataset were less structured and closer to real-life scenarios, hence is the most difficult dataset among the three, which resulted in the lowest performance of M S on D S .", "text_before_citation": ["The scores were then averaged across the classes to obtain the final precision, recall and F1 scores.", "For each of the three dataset, we evaluated the performance metrics with three experiments:", "\u2022 Table 1 shows the accuracy, precision, recall and F1 score for each of these three models for all the three datasets.", "The supervised training of M S on D S yields the highest scores for the PAMAP2 dataset.", "Lower performance on the MHEALTH dataset could be partially attributed to the smaller volume of training data and fewer channels of data being used (as stated in Section 5 we only used three accelerometer channels for MHEALTH dataset as opposed to nine IMU channels for the other two datasets)."], "text_after_citation": ["The same performance trend across the three datasets is also observed in the other two experiments, M S on D ST and M T on D ST .", "When comparing the model performance on the target domain, the target domain model M T trained on D ST performs significantly better in all the three datasets than the source domain model being directly applied to the target domain i.e., M S on D T .", "This shows that the embedding extractor E T , when trained on D ST , is able to learn similar discriminative information as in the source domain embedding e S for constructing the target domain embeddings e T .", "Thus, fine-tuning an existing activity classification model, as part of our unsupervised method, could be potentially sufficient for performing the classification task at a new body location.", "However, as the performance of M T on D ST across the three datasets are in the same order as of M S on D S , the potential performance increase from such fine-tuning could be partially limited by the performance of the original supervised model from the source location. Fig."], "citing_paper_content": {"title": "Activity Classification Using Unsupervised Domain Transfer From Body Worn Sensors", "abstract": "Activity classification has become a vital feature of wearable health tracking devices. As innovation in this field grows, wearable devices worn on different parts of the body are emerging. To perform activity classification on a new body location, labeled data corresponding to the new locations are generally required, but this is expensive to acquire. In this work, we present an innovative method to leverage an existing activity classifier, trained on Inertial Measurement Unit (IMU) data from a reference body location (the source domain), in order to perform activity classification on a new body location (the target domain) in an unsupervised way, i.e. without the need for classification labels at the new location. Specifically, given an IMU embedding model trained to perform activity classification at the source domain, we train an embedding model to perform activity classification at the target domain by replicating the embeddings at the source domain. This is achieved using simultaneous IMU measurements at the source and target domains. The replicated embeddings at the target domain are used by a classification model that has previously been trained on the source domain to perform activity classification at the target domain. We have evaluated the proposed methods on three activity classification datasets PAMAP2, MHealth, and Opportunity, yielding high F1 scores of 67.19%, 70.40% and 68.34%, respectively when the source domain is the wrist and the target domain is the torso."}, "cited_paper_content": {"title": "Ensembles Of Deep Lstm Learners For Activity Recognition Using Wearables", "abstract": "Recently, deep learning (DL) methods have been introduced very successfully into human activity recognition (HAR) scenarios in ubiquitous and wearable computing. Especially the prospect of overcoming the need for manual feature design combined with superior classification capabilities render deep neural networks very attractive for real-life HAR applications. Even though DL-based approaches now outperform the state-of-the-art in a number of recognition tasks, still substantial challenges remain. Most prominently, issues with real-life datasets, typically including imbalanced datasets and problematic data quality, still limit the effectiveness of activity recognition using wearables. In this paper we tackle such challenges through Ensembles of deep Long Short Term Memory (LSTM) networks. LSTM networks currently represent the state-of-the-art with superior classification performance on relevant HAR benchmark datasets. We have developed modified training procedures for LSTM networks and combine sets of diverse LSTM learners into classifier collectives. We demonstrate that Ensembles of deep LSTM learners outperform individual LSTM networks and thus push the state-of-the-art in human activity recognition using wearables. Through an extensive experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition capabilities of our approach and its potential for real-life applications of human activity recognition."}, "keywords": ["difficult dataset"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The supervised training of M S on D S yields the highest scores for the PAMAP2 dataset (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors demonstrated that Ensembles of deep LSTM learners outperform individual LSTM networks in human activity recognition using wearables, as shown in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors tackled the challenges of difficult datasets in human activity recognition using wearables by proposing ensembles of deep LSTM learners, which outperformed individual LSTM networks and pushed the state-of-the-art in this field (#REFR).", "given_citation_intent": "result", "given_keywords": ["difficult dataset"]}]}
{"citing_id": "2303.02186v1", "cited_id": "1606.03203", "section_title": "Non-Matching Assumptions", "citation": "Namely, RESIT leans a causal structure assuming noise model structural equations, while Lattimore et al. #REFR actually assumes a non-parametric causal graph.", "text_before_citation": ["In our example in fig.", "5 , we have not addressed the fact that the used structure learner actually yields a graph on a different parametric level than what is assumed by the reasoning task."], "text_after_citation": ["Luckily, because of the composition of both the structural and parametric scale, each stricter assumption is subsumed by a lower level assumption.", "As such, from left to right, we can always relax the assumptions (such as allowing non-parametric reasoning based on a noise-model causal structure), but not the other way around.", "If, for example, our structure learner yielded a plausible but non-parametric causal structure, we are not guaranteed optimal regret from Lattimore et al. #OTHEREFR .", "On our map, that would become clear as we would move from a less strict assumption (plausible causality) to a strict assumption (full causality)."], "citing_paper_content": {"title": "Causal Deep Learning", "abstract": "Causality has the potential to truly transform the way we solve a large number of realworld problems. Yet, so far, its potential remains largely unlocked since most work so far requires strict assumptions which do not hold true in practice. To address this challenge and make progress in solving real-world problems, we propose a new way of thinking about causality-we call this causal deep learning. The framework which we propose for causal deep learning spans three dimensions: (1) a structural dimension, which allows incomplete causal knowledge rather than assuming either full or no causal knowledge; (2) a parametric dimension, which encompasses parametric forms which are typically ignored; and finally, (3) a temporal dimension, which explicitly allows for situations which capture exposure times or temporal structure. Together, these dimensions allow us to make progress on a variety of real-world problems by leveraging (sometimes incomplete) causal knowledge and/or combining diverse causal deep learning methods. This new framework also enables researchers to compare systematically across existing works as well as identify promising research areas which can lead to real-world impact."}, "cited_paper_content": {"title": "Causal Bandits: Learning Good Interventions Via Causal Inference", "abstract": "We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information."}, "keywords": ["causal structure"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In our example in fig. 5, we have not addressed the fact that the used structure learner actually yields a graph on a different parametric level than what is assumed by the reasoning task, which could be improved by using the causal feedback proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of incomplete causal knowledge, the authors propose a new framework for causal deep learning that spans three dimensions: a structural dimension, a parametric dimension, and a temporal dimension, which allows for situations that capture exposure times or temporal structure, as well as leveraging incomplete causal knowledge and combining diverse causal deep learning methods #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of incomplete causal knowledge in our reasoning task, we can leverage the approach proposed in the reference paper #REFR, which combines multi-arm bandits and causal inference to model a novel type of bandit feedback that exploits the causal structure.", "given_citation_intent": "background", "given_keywords": ["causal structure"]}]}
{"citing_id": "2304.03507v1", "cited_id": "1910.12933", "section_title": "Iv. Experiments", "citation": "However, it is argued in works such as #REFR that for certain datasets such as Airport and Disease, one should consider embedding nodes in hyperbolic spaces and perform feature aggregation in the tangent spaces of hyperbolic spaces.", "text_before_citation": ["In each row, best performers are highlighted in blue and red for our approach and benchmarks, respectively.", "An underlined entry means no noticeable performance improvement over the base model is observed.", "In general, we see that our proposed regularized models improve upon their respective base models with significant performance gain in many cases.", "Moreover, our method can match up with or even outperform many benchmarks.", "2) Hyperbolic models: Base models considered in Section IV-A1 generate embedding of nodes in Euclidean spaces."], "text_after_citation": ["Such a consideration is plausible as certain graphs are inherently hyperbolic (measured by \u03b4-hyperbolicity, see #OTHEREFR ).", "In this subsection for Airport and Disease datasets, we use hyperbolic versions of their Euclidean counterparts HGCN #OTHEREFR , and HGAT #OTHEREFR as base models.", "We also consider the interactive model GIL that combines both Euclidean and hyperbolic approaches #OTHEREFR . The comparison results are shown in Table II .", "Again, we see a general improvement by using the proposed regularization, which yields performance comparable with benchmarks.", "3) Inductive learning models: In contrast with transductive learning, inductive learning requires one to deal with unseen data outside the training set."], "citing_paper_content": {"title": "Distributional Signals For Node Classification In Graph Neural Networks", "abstract": "In graph neural networks (GNNs), both node features and labels are examples of graph signals, a key notion in graph signal processing (GSP). While it is common in GSP to impose signal smoothness constraints in learning and estimation tasks, it is unclear how this can be done for discrete node labels. We bridge this gap by introducing the concept of distributional graph signals. In our framework, we work with the distributions of node labels instead of their values and propose notions of smoothness and non-uniformity of such distributional graph signals. We then propose a general regularization method for GNNs that allows us to encode distributional smoothness and non-uniformity of the model output in semi-supervised node classification tasks. Numerical experiments demonstrate that our method can significantly improve the performance of most base GNN models in different problem settings."}, "cited_paper_content": {"title": "Hyperbolic Graph Convolutional Neural Networks", "abstract": "Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCNs operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset."}, "keywords": ["nodes", "hyperbolic spaces"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Base models considered in Section IV-A1 generate embedding of nodes in Euclidean spaces. To address the issue of large distortion in real-world graphs with scale-free or hierarchical structure, the authors propose Hyperbolic Graph Convolutional Neural Network (HGCN) that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of embedding real-world graphs with scale-free or hierarchical structure, recent research has explored the use of hyperbolic geometry, which enables embeddings with much smaller distortion than Euclidean space. One such approach is the Hyperbolic Graph Convolutional Neural Network (HGCN) proposed by Li et al. in their paper \"#REFR\". #REFR presents the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To embed nodes in hyperbolic spaces with smaller distortion, recent research has proposed Hyperbolic Graph Convolutional Neural Network (HGCN) that leverages both the expressiveness of GCNs and hyperbolic geometry, and derives GCNs operations in the hyperboloid model of hyperbolic space to map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer, which can be a promising approach for improving the performance of GNNs in tasks involving #KEYWORD(nodes) in #KEYWORD(hyperbolic spaces) such as ours. #REFR", "given_citation_intent": "background", "given_keywords": ["nodes", "hyperbolic spaces"]}]}
{"citing_id": "2303.15127v1", "cited_id": "1706.06083", "section_title": "Related Work", "citation": "Effective methods to gain adversarial robustness usually involve adversarial training #REFR , which leverages adversarial examples to train models.", "text_before_citation": ["Adversarial examples and adversarial training.", "Adversarial examples deceive machine learning models by adding adversarial perturbations, often imperceptible to human, to source images, leading to incorrect classification results #OTHEREFR .", "White-box adversarial attacks #OTHEREFR maximize the loss of a source image with gradient descent on the defending model to add adversarial perturbations onto an image to maximize its loss on the model."], "text_after_citation": ["Adversarial training algorithms thus solve the min-max problem of minimizing the loss function for most adversarial examples within a perturbation budget, typically bounded in p .", "Recent years have thus observed an arms race between adversarial attack strategies and defense mechanisms #OTHEREFR .", "Data poisoning.", "Data poisoning attacks manipulate the training of a deep learning model by injecting malicious and poisoned examples into its training set #OTHEREFR .", "Data poisoning methods #OTHEREFR achieve their malicious objectives by stealthily replacing a portion of training data, and successful attacks can be triggered with specially-crafted prescribed inputs."], "citing_paper_content": {"title": "Learning The Unlearnable: Adversarial Augmentations Suppress Unlearnable Example Attacks", "abstract": "Unlearnable example attacks are data poisoning techniques that can be used to safeguard public data against unauthorized training of deep learning models. These methods add stealthy perturbations to the original image, thereby making it difficult for deep learning models to learn from these training data effectively. Current research suggests that adversarial training can, to a certain degree, mitigate the impact of unlearnable example attacks, while common data augmentation methods are not effective against such poisons. Adversarial training, however, demands considerable computational resources and can result in non-trivial accuracy loss. In this paper, we introduce the UEraser method, which outperforms current defenses against different types of state-of-the-art unlearnable example attacks through a combination of effective data augmentation policies and loss-maximizing adversarial augmentations. In stark contrast to the current SOTA adversarial training methods, UEraser uses adversarial augmentations, which extends beyond the confines of p perturbation budget assumed by current unlearning attacks and defenses. It also helps to improve the model's generalization ability, thus protecting against accuracy loss. UEraser wipes out the unlearning effect with error-maximizing data augmentations, thus restoring trained model accuracies. Interestingly, UEraser-Lite, a fast variant without adversarial augmentations, is also highly effective in preserving clean accuracies. On challenging unlearnable CIFAR-10, CIFAR-100, SVHN, and ImageNet-subset datasets produced with various attacks, it achieves results that are comparable to those obtained during clean training. We also demonstrate the efficacy of UEraser against possible adaptive attacks. Our code is open source and available to the deep learning community 1 ."}, "cited_paper_content": {"title": "Towards Deep Learning Models Resistant To Adversarial Attacks", "abstract": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models."}, "keywords": ["adversarial training", "adversarial robustness"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, the authors of this manuscript study the adversarial robustness of neural networks through the lens of robust optimization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the vulnerability of deep learning models to adversarial attacks, the authors propose a robust optimization approach that provides a concrete security guarantee against any adversary, resulting in significantly improved resistance to a wide range of adversarial attacks #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adversarial training is a reliable method for improving the adversarial robustness of neural networks, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["adversarial training", "adversarial robustness"]}]}
{"citing_id": "2303.06273v1", "cited_id": "1906.02243", "section_title": "Discussion", "citation": "For instance, training a BERT-base model without hyperparameter tuning, which is 1590 times smaller than ChatGPT, requires a CO2 emission of 650kg, which is comparable to flying from New York to San Francisco for one passenger #REFR .", "text_before_citation": ["Question: Do both sentences mean the same thing? Answer:\" PERTURBED OUTPUT: \"No, the two sentences do not mean the same thing.", "The first sentence says that he asked for six pizzas and soda, while the second sentence says that he asked for six pepperoni pizzas and two six-packs of soft drinks.", "The type of pizza and the specific drinks requested are different in the two sentences.\" TASK: SNLI, Semantic-BECEL ORIGINAL QUESTION: \"An enthusiastic boy pushes a shopping card containing a smiling boy. Question: A boy is pushing a shopping cart.", "True, False or Neither? Answer:\" ORIGINAL OUTPUT: \"True\" PERTURBED QUESTION: \"An enthusiastic boy pushes a shopping card containing a smiling boy. Question: A young man is pushing a shopping cart.", "True, False or Neither? Answer:\" PERTURBED OUTPUT: \"Neither (the description does not mention the age or gender of the person pushing the shopping cart, only that the person inside the cart is a smiling boy.) \" et al., 2021)."], "text_after_citation": ["A simple expectation of CO2 emission for re-training ChatGPT is 1033t, while a human is responsible for 5t CO2 emission per year.", "Therefore, it is desirable to enlarge our viewpoint beyond LLMs to implement sustainable remedies that can fundamentally solve the inconsistency problem, particularly in a modern society facing the global climate crisis."], "citing_paper_content": {"title": "Consistency Analysis Of Chatgpt", "abstract": "ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate Chat-GPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that Chat-GPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk-sensitive areas."}, "cited_paper_content": {"title": "Energy And Policy Considerations For Deep Learning In Nlp", "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."}, "keywords": ["ChatGPT", "BERT-base model"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this paper we bring attention to the financial and environmental costs of training large neural network models for NLP, and propose actionable recommendations to reduce costs and improve equity in NLP research and practice #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent progress in hardware and methodology for training neural networks has led to the development of large networks trained on abundant data, which require substantial energy consumption and are costly to train and develop financially and environmentally (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent progress in deep learning has led to the development of large neural network models that require substantial energy consumption and are costly to train and develop, both financially and environmentally (Chakraborty et al., 2021). #REFR", "given_citation_intent": "background", "given_keywords": ["ChatGPT", "BERT-base model"]}]}
{"citing_id": "2304.10334v1", "cited_id": "1805.02724", "section_title": "Conclusions And Open Questions", "citation": "We introduced least fixed formulae that use recursion on second-order function symbols and provided logical characterizations of SpanL and TotP, answering an open question of #REFR . Furthermore, we determined logics that capture SpanPSPACE and FPSPACE.", "text_before_citation": ["Inspired by the two-step semantics developed in the context of weighted logics, we introduced two-step semantics that enriches the existing framework of quantitative logics, i.e. logics for expressing counting problems."], "text_after_citation": ["Some of the quantitative logics we introduced in this work naturally capture classes of functions that count different valid outputs of space-restricted transducers.", "The logic that captures TotP over finite ordered structures was defined in a more complicated way that is related to the properties of TotP problems: recursion of the logic expresses self-reducibility and the restricted form of the recursion captures the easy-decision property.", "It would be interesting to investigate whether TotP is captured by a simpler, more elegant logic.", "The two-step semantics that we propose in this work is noteworthy for reasons beyond its primary objective.", "It can be generalized to map formulae to elements of any structure S equipped with operations \u222a and \u2022, instead of solely sets of strings. Conversely, it can also be specialized."], "citing_paper_content": {"title": "Counting Computations With Formulae: Logical Characterisations Of Counting Complexity Classes", "abstract": "We present quantitative logics with two-step semantics based on the framework of quantitative logics introduced by Arenas et al. (2020) and the two-step semantics defined in the context of weighted logics by Gastin & Monmege (2018). We show that some of the fragments of our logics augmented with a least fixed point operator capture interesting classes of counting problems. Specifically, we answer an open question in the area of descriptive complexity of counting problems by providing logical characterizations of two subclasses of #P, namely SpanL and TotP, that play a significant role in the study of approximable counting problems. Moreover, we define logics that capture FPSPACE and SpanPSPACE, which are counting versions of PSPACE."}, "cited_paper_content": {"title": "Descriptive Complexity For Counting Complexity Classes", "abstract": "Descriptive Complexity has been very successful in characterizing complexity classes of decision problems in terms of the properties definable in some logics. However, descriptive complexity for counting complexity classes, such as FP and #P, has not been systematically studied, and it is not as developed as its decision counterpart. In this paper, we propose a framework based on Weighted Logics to address this issue. Specifically, by focusing on the natural numbers we obtain a logic called Quantitative Second Order Logics (QSO), and show how some of its fragments can be used to capture fundamental counting complexity classes such as FP, #P and FPSPACE, among others. We also use QSO to define a hierarchy inside #P, identifying counting complexity classes with good closure and approximation properties, and which admit natural complete problems. Finally, we add recursion to QSO, and show how this extension naturally captures lower counting complexity classes such as #L."}, "keywords": ["logical characterizations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Inspired by the two-step semantics developed in the context of weighted logics, we introduced two-step semantics that enriches the existing framework of quantitative logics, i.e. logics for expressing counting problems. This framework has been successfully used to capture fundamental counting complexity classes such as FP, #P, and FPSPACE, among others, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of descriptive complexity for counting complexity classes, the authors proposed a framework based on Weighted Logics, which was used to capture fundamental counting complexity classes such as FP, #P, and FPSPACE, among others, as well as to define a hierarchy inside #P, identifying counting complexity classes with good closure and approximation properties, and which admit natural complete problems (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide logical characterizations of counting complexity classes, the authors proposed a framework based on Weighted Logics, which was inspired by the two-step semantics developed in the context of weighted logics #REFR.", "given_citation_intent": "background", "given_keywords": ["logical characterizations"]}]}
{"citing_id": "2305.00956v1", "cited_id": "2001.00611", "section_title": "I. Introduction", "citation": "However, these works focus on channel models such as binary input additive white Gaussian noise (BIAWGN) that do not match the ET-QKD channel #REFR .", "text_before_citation": ["Hence, baseline NB-LDPC codes with large field sizes are not favorable in QKD applications requiring low latency, such as in #OTHEREFR , #OTHEREFR .", "In addition to the above latency vs.", "key rate trade-off, the LDPC codes used previously in the IR step of ET-QKD protocols have not fully utilized the properties of the ET-QKD channel.", "For example, #OTHEREFR used a standard LDPC ensemble without optimization.", "Similarly, spatially-coupled (SC) LDPC codes, irregular repeat accumulate (IRA) codes, SC-IRA codes, and multi-edge-type (MET) codes have been discussed for the continuous-variable (CV) QKD #OTHEREFR , #OTHEREFR ."], "text_after_citation": ["A unique property of the ET-QKD problem considered in this paper is that the key rate of the system is closely dependent on both the rate of the code and the frame error rate (FER) performance. Fig.", "2 shows the FER and key rates obtained by a random LDPC code for different values of rate.", "From this graph, we see that increasing the code rate can improve the key rate even at the cost of higher FER, a phenomenon we see in both binary and non-binary LDPC codes.", "Additionally, the maximum in the key rate occurs for a relatively large value of FER (\u223c 5%).", "While the conventional code design approach is to minimize the FER to a very small value for a given rate, in this case, the goal is to jointly optimize both the rate and the FER to achieve the largest key rate."], "citing_paper_content": {"title": "Non-Binary Ldpc Code Design For Energy-Time Entanglement Quantum Key Distribution", "abstract": "In energy-time entanglement Quantum Key Distribution (QKD), two users extract a shared secret key from the arrival times (discretized as symbols) of entangled photon pairs. In prior work, Zhou et al. proposed a multi-level coding (MLC) scheme that splits the observed symbols into bit layers and utilizes binary Low-Density Parity-Check (LDPC) codes for reconciliation of the symbols. While binary LDPC codes offer low latency for key generation, splitting the symbols into bits results in a loss of key generation rate due to error propagation. Additionally, existing LDPC codes do not fully utilize the properties of the QKD channel to optimize the key rates. In this paper, we mitigate the above issues by first generalizing the MLC scheme to a nonbinary(NB) MLC scheme that has layers with non-binary symbols and utilizes NB-LDPC codes. We show the NB-MLC scheme offers flexibility in system design. Additionally, we show that the NB-MLC scheme with a small symbol size per layer offers the best trade-off between latency and key rate. We then propose a framework to jointly optimize the rate and degree profile of the NB-LDPC codes that is tailored towards the QKD channel resulting in higher key rates than prior work."}, "cited_paper_content": {"title": "Efficient Information Reconciliation For Energy-Time Entanglement Quantum Key Distribution", "abstract": "Graph based codes such as low density parity check (LDPC) codes have been shown promising for the information reconciliation phase in quantum key distribution (QKD). However, existing graph coding schemes have not fully utilized the properties of the QKD channel. In this work, we first investigate the channel statistics for discrete variable (DV) QKD based on energy-time entangled photons. We then establish a so-called balanced modulation scheme that is promising for this channel. Based on the modulation, we propose a joint local-global graph coding scheme that is expected to achieve good error-correction performance."}, "keywords": ["ET-QKD channel"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For example, previous work on the information reconciliation step of energy-time entanglement quantum key distribution (ET-QKD) protocols used standard LDPC ensembles without optimization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Existing LDPC codes have not fully utilized the properties of the QKD channel, which can result in suboptimal key rates, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Existing LDPC codes have not fully utilized the properties of the ET-QKD channel, but the proposed joint local-global graph coding scheme in #REFR is expected to achieve good error-correction performance for this channel.", "given_citation_intent": "background", "given_keywords": ["ET-QKD channel"]}]}
{"citing_id": "2304.07781v1", "cited_id": "1705.00648", "section_title": "Additional Experiments", "citation": "Table 3 presents the results obtained by the different machine and deep learning algorithms on the LIAR dataset #REFR .", "text_before_citation": ["For our experiments, we used the dataset as it was initially released, with 6 labels #OTHEREFR (Table 3) , and by balancing the dataset's labels (Table 4) as proposed in #OTHEREFR .", "To balance the labels, we created binary labels, i.e., all the texts that are not labeled with true are considered false.", "Using the same experimental configurations as presented in Section 4.2, we obtained results that are aligned with our original observations on the proposed dataset.", "Further, we obtained results similar to state-of-the-art results for the multi-label dataset, e.g., Wang #OTHEREFR and Alhindi et al. #OTHEREFR obtained an accuracy of \u223c20%.", "For the binary classification, we obtained results that go beyond the the state of the art, e.g.,Upadhayay and Behzadan #OTHEREFR obtains an accuracy of 70% while we obtain an accuracy of 83.99% with the LSTM model that employs the document embeddings constructed with GLOVE."], "text_after_citation": ["The dataset contains approximately 12.8K human annotated short statements collected using POLITIFACT.COM's API.", "In this set of experiments, we used all the 6 labels of LIAR, i.e., pants-fire, false, barely-true, half-true, mostlytrue, and true, to build our classification models.", "The dataset is highly imbalanced, as there are more news articles labeled with true than news articles labeled with the other five classes combined.", "Due to this high degree of imbalance, the models performed poorly.", "We observe that the best-performing models employ document embedding constructed with BART."], "citing_paper_content": {"title": "It'S All In The Embedding! Fake News Detection Using Document Embeddings", "abstract": "With the current shift in the mass media landscape from journalistic rigor to social media, personalized social media is becoming the new norm. Although the digitalization progress of the media brings many advantages, it also increases the risk of spreading disinformation, misinformation, and malformation through the use of fake news. The emergence of this harmful phenomenon has managed to polarize society and manipulate public opinion on particular topics, e.g., elections, vaccinations, etc. Such information propagated on social media can distort public perceptions and generate social unrest while lacking the rigor of traditional journalism. Natural Language Processing and Machine Learning techniques are essential for developing efficient tools that can detect fake news. Models that use the context of textual data are essential for resolving the fake news detection problem, as they manage to encode linguistic features within the vector representation of words. In this paper, we propose a new approach that uses document embeddings to build multiple models that accurately label news articles as reliable or fake. We also present a benchmark on different architectures that detect fake news using binary or multi-labeled classification. We evaluated the models on five large news corpora using accuracy, precision, and recall. We obtained better results than more complex state-of-the-art Deep Neural Network models. We observe that the most important factor for obtaining high accuracy is the document encoding, not the classification model's complexity."}, "cited_paper_content": {"title": "\"Liar, Liar Pants On Fire\": A New Benchmark Dataset For Fake News Detection", "abstract": "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present liar: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model."}, "keywords": ["LIAR dataset"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To balance the labels, we created binary labels, i.e., all the texts that are not labeled with true are considered false. Using the same experimental configurations as presented in Section 4.2, we obtained results that are aligned with our original observations on the proposed dataset. Further, we obtained results similar to state-of-the-art results for the multi-label dataset, e.g., Wang et al. (2018) and Alhindi et al. (2018) obtained an accuracy of \u223c20%. For the binary classification, we obtained results that go beyond the state of the art, e.g., Upadhayay and Behzadan (2019) obtains an accuracy of 70% while we obtain an accuracy of 83.99% with the LSTM model that employs the document embeddings constructed with GLOVE. Our approach is similar to the one proposed by Wang et al. (2018) who used the same dataset to investigate automatic fake news detection based on surface-level linguistic patterns. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To balance the labels, the authors created binary labels, and obtained an accuracy of 83.99% with the LSTM model that employs the document embeddings constructed with GLOVE, which outperforms the state-of-the-art results for binary classification, as reported in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To balance the LIAR dataset's labels, the authors created binary labels, and obtained results that go beyond the state of the art, e.g., Upadhayay and Behzadan (2019) obtains an accuracy of 70% while the proposed LSTM model that employs the document embeddings constructed with GLOVE obtains an accuracy of 83.99% #REFR.", "given_citation_intent": "method", "given_keywords": ["LIAR dataset"]}]}
{"citing_id": "2305.00873v2", "cited_id": "1602.05629", "section_title": "Introduction", "citation": "However, the model performance degradation is still significant compared with FL methods without considering privacy, such as FedAvg #REFR .", "text_before_citation": ["adopted for ensuring strong client-level DP.", "However, this method includes two operations: clipping the l 2 norm of local updates to a sensitivity threshold C and adding random noise proportional to the model size, whose standard deviation (STD) is also decided by C.", "These steps may cause severe performance degradation #OTHEREFR , #OTHEREFR , especially on large-scale complex model #OTHEREFR , such as ResNet-18 #OTHEREFR , or with heterogeneous data.", "The reasons behind this issue are two-fold: (i) The useful information is dropped due to the clipping operation, especially with small C values, which is contained in the local updates; (ii) The model inconsistency among local models is exacerbated as the addition of random noise severely damages local updates and leads to large variances between local models, especially with large C values #OTHEREFR .", "Existing works try to overcome these issues via restricting the norm of local update #OTHEREFR and leveraging local update sparsification technique #OTHEREFR , #OTHEREFR to reduce the adverse impacts of clipping and adding random noise."], "text_after_citation": [], "citing_paper_content": {"title": "Towards The Flatter Landscape And Better Generalization In Federated Learning Under Client-Level Differential Privacy", "abstract": "To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. To further reduce the magnitude of random noise while achieving better performance, we propose DP-FedSAM-top k by adopting the local update sparsification technique. From the theoretical perspective, we present the convergence analysis to investigate how our algorithms mitigate the performance degradation induced by DP. Meanwhile, we give rigorous privacy guarantees with R\u00e9nyi DP, the sensitivity analysis of local updates, and generalization analysis. At last, we empirically confirm that our algorithms achieve state-of-the-art (SOTA) performance compared with existing SOTA baselines in DPFL."}, "cited_paper_content": {"title": "Communication-Efficient Learning Of Deep Networks From Decentralized Data", "abstract": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."}, "keywords": ["privacy"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Existing works try to overcome these issues via restricting the norm of local update #REFR and leveraging local update sparsification technique #OTHEREFR , #OTHEREFR to reduce the adverse impacts of clipping and adding random noise.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Existing works try to overcome the issues of performance degradation caused by clipping and adding random noise via restricting the norm of local update and leveraging local update sparsification technique. However, these methods may not be effective for large-scale complex models, such as ResNet-18, or with heterogeneous data. In contrast, Federated Learning (FL) with iterative model averaging, as proposed in #REFR, shows robustness to the unbalanced and non-IID data distributions that are a defining characteristic of this setting, and reduces the required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To address this issue, the authors propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP and integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, resulting in small norm of local updates and robustness to DP noise, thereby improving the performance #REFR.", "given_citation_intent": "result", "given_keywords": ["privacy"]}]}
{"citing_id": "2304.12053v1", "cited_id": "1603.09382", "section_title": "Implementation Details", "citation": "Additionally, a drop path #REFR rate of 0.1 is employed to prevent overfitting, which randomly drops entire paths (i.e., sequences of layers) in the model during training.", "text_before_citation": ["While during testing, we only resize images to 224 \u00d7 224.", "For our detector, we use a ResNet-50 #OTHEREFR pretrained on Ima-geNet #OTHEREFR .", "The model is trained using the AdamW optimizer #OTHEREFR .", "The learning rate was equal to 10 \u22123 , and a step scheduler with 5 epochs was used.", "Weight decay is also applied with a factor of 5 \u2022 10 \u22125 ."], "text_after_citation": ["All training and evaluation processes were carried out on a server with one NVIDIA GeForce RTX 3060 GPU."], "citing_paper_content": {"title": "Improving Synthetically Generated Image Detection In Cross-Concept Settings", "abstract": "New advancements for the detection of synthetic images are critical for fighting disinformation, as the capabilities of generative AI models continuously evolve and can lead to hyper-realistic synthetic imagery at unprecedented scale and speed. In this paper, we focus on the challenge of generalizing across different concept classes, e.g., when training a detector on human faces and testing on synthetic animal images-highlighting the ineffectiveness of existing approaches that randomly sample generated images to train their models. By contrast, we propose an approach based on the premise that the robustness of the detector can be enhanced by training it on realistic synthetic images that are selected based on their quality scores according to a probabilistic quality estimation model. We demonstrate the effectiveness of the proposed approach by conducting experiments with generated images from two seminal architectures, StyleGAN2 and Latent Diffusion, and using three different concepts for each, so as to measure the cross-concept generalization ability. Our results show that our quality-based sampling method leads to higher detection performance for nearly all concepts, improving the overall effectiveness of the synthetic image detectors."}, "cited_paper_content": {"title": "Deep Networks With Stochastic Depth", "abstract": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10)."}, "keywords": ["overfitting", "layers"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of training very deep networks, the authors of #REFR propose stochastic depth, a training procedure that randomly drops a subset of layers during training and bypasses them with the identity function, which complements the recent success of residual networks and reduces training time substantially while improving the test error significantly on almost all data sets.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenges of training very deep networks, the proposed stochastic depth method enables the training of short networks and the use of deep networks at test time by randomly dropping a subset of layers for each mini-batch and bypassing them with the identity function #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the overfitting issue, we adopt the stochastic depth method proposed in the Deep Networks With Stochastic Depth paper #REFR, which randomly drops a subset of layers during training and bypasses them with the identity function, enabling the training of short networks and the use of deep networks at test time.", "given_citation_intent": "method", "given_keywords": ["overfitting", "layers"]}]}
{"citing_id": "2304.12825v1", "cited_id": "1802.04364", "section_title": "2D Topology Prior Encoding", "citation": "To encode the 2D topology in R 2 into a prior distribution, we adopt the junction tree encoder architecture from JT-VAE #REFR .", "text_before_citation": [], "text_after_citation": ["The whole procedure is illustrated in Figure 3 and detailed next."], "citing_paper_content": {"title": "Graphvf: Controllable Protein-Specific 3D Molecule Generation With Variational Flow Fang Sun", "abstract": "Designing molecules that bind to specific target proteins is a fundamental task in drug discovery. Recent models leverage geometric constraints to generate ligand molecules that bind cohesively with specific protein pockets. However, these models cannot effectively generate 3D molecules with 2D skeletal curtailments and property constraints, which are pivotal to drug potency and development. To tackle this challenge, we propose GraphVF, a variational flow-based framework that combines 2D topology and 3D geometry, for controllable generation of binding 3D molecules. Empirically, our method achieves state-of-the-art binding affinity and realistic sub-structural layouts for protein-specific generation. In particular, GraphVF represents the first controllable geometry-aware, protein-specific molecule generation method, which can generate binding 3D molecules with tailored sub-structures and physio-chemical properties. Our code is available at https://github.com/Franco-Solis/GraphVF-code."}, "cited_paper_content": {"title": "Junction Tree Variational Autoencoder For Molecular Graph Generation", "abstract": "We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin."}, "keywords": ["2D topology", "junction tree encoder"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To generate molecular graphs, Sun et al. propose GraphVF, a variational flow-based framework that combines 2D topology and 3D geometry, and generates binding 3D molecules with tailored sub-structures and physio-chemical properties (#REFR). Their approach outperforms previous state-of-the-art baselines by a significant margin, and represents the first controllable geometry-aware, protein-specific molecule generation method.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate molecular graphs while maintaining chemical validity, Sun et al. propose a junction tree variational autoencoder that generates a tree-structured scaffold over chemical substructures and combines them into a molecule with a graph message passing network #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To generate 3D molecules with 2D skeletal curtailments and property constraints, GraphVF leverages a variational flow-based framework that combines 2D topology and 3D geometry, as well as a junction tree encoder for generating a tree-structured scaffold over chemical substructures, as proposed in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["2D topology", "junction tree encoder"]}]}
{"citing_id": "2305.01032v1", "cited_id": "1704.03647", "section_title": "Vi. Numerical Results", "citation": "By comparing the results with one of the most comprehensive papers on component-based algorithms #REFR , we see that our group-based DiCA returns more accurate solutions in much smaller number of iterations.", "text_before_citation": ["For Algorithm 2, we need to set four parameters \u03c1 v , \u03c1 \u03b8 , \u03c1 p and \u03c1 q .", "For simplicity and based on numerical results, we control all of them with a single scalar \u03c1 as", "EQUATION", "Table III shows the results of running the DiCA algorithm for some problem instances.", "The values for \u03c1 are chosen as the one that yields the minimum number of iterations among some multiples of 50."], "text_after_citation": ["Some of the common cases reported in #OTHEREFR are given in Table IV for comparison.", "Figure 3 shows the plots of the minimum residual of the regions versus the number of iterations for different values of \u03c1.", "As can be seen, \u03c1 can significantly change the number of iterations.", "As also reported in #OTHEREFR , the oscillations in the progress of the residual slow down the convergence of the distributed algorithms. Figure 4 shows the number of iterations versus \u03c1.", "For all the tested problems, DiCA does not converge if \u03c1 is smaller than a certain limit, for example, in Figure 3 for the case300 problem, when \u03c1 = 200."], "citing_paper_content": {"title": "Distributed Optimization For Power Systems With Radial Partitioning", "abstract": "This paper proposes group-based distributed optimization algorithms on top of intelligent partitioning for the optimal power flow (OPF) problem. Radial partitioning of the graph of a network is introduced as a systematic way to split a large-scale problem into more tractable sub-problems, which can potentially be solved efficiently with methods such as convex relaxations. The simple implementation of a Distributed Consensus Algorithm (DiCA) with very few parameters makes it viable for different parameter selection methods, which are crucial for the fast convergence of the distributed algorithms. The DiCA algorithm returns more accurate solutions to the tested problems with fewer iterations than component-based algorithms. Our numerical results show the performance of the algorithms for different power network instances and the effect of parameter selection. A software package DiCARP is created, which is implemented in Python using the Pyomo optimization package."}, "cited_paper_content": {"title": "A Component-Based Dual Decomposition Method For The Opf Problem", "abstract": "This paper proposes a component-based dual decomposition of the nonconvex AC optimal power flow (OPF) problem, where the modified dual function is solved in a distributed fashion. The main contribution of this work is that is demonstrates that a distributed method with carefully tuned parameters can converge to globally optimal solutions despite the inherent nonconvexity of the problem and the absence of theoretical guarantees of convergence. This paper is the first to conduct extensive numerical analysis resulting in the identification and tabulation of the algorithmic parameter settings that are crucial for the convergence of the method on 72 AC OPF test instances. Moreover, this work provides a deeper insight into the geometry of the modified Lagrange dual function of the OPF problem and highlights the conditions that make this function differentiable. This numerical demonstration of convergence coupled with the scalability and the privacy preserving nature of the proposed method makes it well suited for smart grid applications such as multi-period OPF with demand response (DR) and security constrained unit commitment (SCUC) with contingency constraints and multiple transmission system operators (TSOs)."}, "keywords": ["group-based DiCA", "component-based algorithms"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For Algorithm 2, we need to set four parameters \u03c1 v , \u03c1 \u03b8 , \u03c1 p and \u03c1 q . For simplicity and based on numerical results, we control all of them with a single scalar \u03c1 as EQUATION Table III shows the results of running the DiCA algorithm for some problem instances. The values for \u03c1 are chosen as the one that yields the minimum number of iterations among some multiples of 50. The authors of this manuscript proposed a component-based dual decomposition method for the OPF problem, which is solved in a distributed fashion. They conducted extensive numerical analysis resulting in the identification and tabulation of the algorithmic parameter settings that are crucial for the convergence of the method on 72 AC OPF test instances. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "According to the numerical results presented in Table III, the proposed DiCA algorithm outperforms component-based algorithms in terms of accuracy and iteration count, and can converge to globally optimal solutions with carefully tuned parameters, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The numerical results show that the group-based DiCA algorithm outperforms component-based algorithms in terms of accuracy and convergence speed for solving the optimal power flow problem, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": ["group-based DiCA", "component-based algorithms"]}]}
{"citing_id": "2303.14939v1", "cited_id": "1707.06766", "section_title": "Related Work", "citation": "Among the different prediction tasks in the Predictive Process Monitoring state-of-the-art, an important group of papers focuses, as this work, on predicting outcomes #REFR .", "text_before_citation": ["To the best of our knowledge, no other works exist on the accuracy improvement exploiting frequent explanations, except for our previous work #OTHEREFR .", "We hence first position our work with respect to the works in Predictive Process Monitoring, the works focused on explanations in Predictive Process Monitoring, and the works improving the predictive model's performance employing (user) feedback; we finally address the specific comparison with our previous work #OTHEREFR ."], "text_after_citation": ["For instance, several approaches deal with predicting the fulfilment (or the violation) of a boolean predicate in a running case #OTHEREFR , #OTHEREFR .", "Initial approaches and encodings for outcome-oriented Predictive Process Monitoring have been enhanced in terms of performance by introducing pre-processing steps based on clustering and bucketing #OTHEREFR , #OTHEREFR , as well as in terms of prediction accuracy by introducing different types of encodings #OTHEREFR , #OTHEREFR .", "Moreover, more recently, deep learning approaches have also been investigated for predicting outcomes #OTHEREFR , #OTHEREFR .", "Differently from these works, the focus of this paper is not on proposing a specific outcome prediction method, but rather on explaining why predictive models are wrong and on leveraging these explanations for eventually improving the performance of the predictive model.", "A number of works have recently focused on providing explanations in Predictive Process Monitoring #OTHEREFR ."], "citing_paper_content": {"title": "Explain, Adapt And Retrain: How To Improve The Accuracy Of A Ppm Classifier Through Different Explanation Styles", "abstract": "Recent papers have introduced a novel approach to explain why a Predictive Process Monitoring (PPM) model for outcome-oriented predictions provides wrong predictions. Moreover, they have shown how to exploit the explanations, obtained using state-of-the art post-hoc explainers, to identify the most common features that induce a predictor to make mistakes in a semi-automated way, and, in turn, to reduce the impact of those features and increase the accuracy of the predictive model. This work starts from the assumption that frequent control flow patterns in event logs may represent important features that characterize, and therefore explain, a certain prediction. Therefore, in this paper, we (i) employ a novel encoding able to leverage DECLARE constraints in Predictive Process Monitoring and compare the effectiveness of this encoding with Predictive Process Monitoring state-of-the art encodings, in particular for the task of outcome-oriented predictions; (ii) introduce a completely automated pipeline for the identification of the most common features inducing a predictor to make mistakes; and (iii) show the effectiveness of the proposed pipeline in increasing the accuracy of the predictive model by validating it on different real-life datasets."}, "cited_paper_content": {"title": "Outcome-Oriented Predictive Process Monitoring: Review And Benchmark", "abstract": "Predictive business process monitoring refers to the act of making predictions about the future state of ongoing cases of a business process, based on their incomplete execution traces and logs of historical (completed) traces. Motivated by the increasingly pervasive availability of fine-grained event data about business process executions, the problem of predictive process monitoring has received substantial attention in the past years. In particular, a considerable number of methods have been put forward to address the problem of outcome-oriented predictive process monitoring, which refers to classifying each ongoing case of a process according to a given set of possible outcomes - e.g. Will the customer complain or not? Will an order be delivered, cancelled or withdrawn? Unfortunately, different authors have used different datasets, experimental settings, evaluation measures and baselines to assess their proposals, resulting in poor comparability and an unclear picture of the relative merits and applicability of different methods. To address this gap, this article presents a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and a comparative experimental evaluation of eleven representative methods using a benchmark covering twelve predictive process monitoring tasks based on four real-life event logs."}, "keywords": ["Predictive Process Monitoring"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To the best of our knowledge, no other works exist on the accuracy improvement exploiting frequent explanations, except for our previous work #OTHEREFR. We hence first position our work with respect to the works in Predictive Process Monitoring, the works focused on explanations in Predictive Process Monitoring, and the works improving the predictive model's performance employing (user) feedback; we finally address the specific comparison with our previous work #OTHEREFR. To provide a systematic review and taxonomy of outcome-oriented predictive process monitoring methods, and to address the poor comparability and unclear picture of the relative merits and applicability of different methods, the authors of #REFR have proposed a comparative experimental evaluation of eleven representative methods using a benchmark covering twelve predictive process monitoring tasks based on four real-life event logs.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide a comprehensive overview of outcome-oriented predictive process monitoring methods, the authors cite a systematic review and benchmark study in which eleven representative methods were evaluated on a benchmark covering twelve predictive process monitoring tasks based on four real-life event logs #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide a comprehensive overview of the state-of-the-art in Predictive Process Monitoring, the authors conducted a systematic review and benchmark of outcome-oriented predictive process monitoring methods, which can serve as a valuable reference for future research in this area #REFR.", "given_citation_intent": "background", "given_keywords": ["Predictive Process Monitoring"]}]}
{"citing_id": "2304.04275v1", "cited_id": "1810.11975", "section_title": "Sparse Self-Attention", "citation": "Next, a thresholding step is done, where the probabilities lower than a dynamic threshold \u03c4 are truncated to zero, while redistributing the remaining probabilities. For more details refer to the paper #REFR .", "text_before_citation": ["The key idea here is to turn these non-consequential pairwise connections to zero attention weights, while bumping the attention score of the important ones, i.e., a sparse attention distribution.", "In particular, Sparsegen-lin activation projects the attention scores a \u2208 R n onto a probability simplex p \u2208 R n , along with a regularization coefficient \u03bb < 1:", "EQUATION", "with, \u2206 n\u22121 = {p \u2208 R n | p = 1, p > 0} enforcing constraints of probabilities summing to one and greater than non-zero.", "Note, the L2 norm with negative \u03bb regularization acts to actually assign larger probability values in p, as the objective is to minimize the cost function above."], "text_after_citation": ["We can use this Sparse Self-Attention by replacing the softmax with Sparsegen function:", "EQUATION", "Note, sparse (self-)attention and its variants has been used in NLP works with impressive results #OTHEREFR as well as time series forecasting #OTHEREFR ."], "citing_paper_content": {"title": "Filling Out The Missing Gaps: Time Series Imputation With Semi-Supervised Learning", "abstract": "Missing data in time series is a challenging issue affecting time series analysis. Missing data occurs due to problems like data drops or sensor malfunctioning. Imputation methods are used to fill in these values, with quality of imputation having a significant impact on downstream tasks like classification. In this work, we propose a semi-supervised imputation method, ST-Impute, that uses both unlabeled data along with downstream task's labeled data. ST-Impute is based on sparse self-attention and trains on tasks that mimic the imputation process. Our results indicate that the proposed method outperforms the existing supervised and unsupervised time series imputation methods measured on the imputation quality as well as on the downstream tasks ingesting imputed time series."}, "cited_paper_content": {"title": "On Controllable Sparse Alternatives To Softmax", "abstract": "Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization."}, "keywords": ["details"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed Sparsegen-lin activation provides a control over the degree of desired sparsity in attention weights, which can be achieved by projecting the attention scores onto a probability simplex with a regularization coefficient \u03bb < 1 #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a unified framework that encompasses various probability mapping functions, including sparsemax, and introduce two novel sparse formulations, sparsegen-lin and sparsehourglass, that provide control over the degree of desired sparsity, which could be useful for imputation methods like ST-Impute. #REFR", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors propose a unified framework that encompasses various probability mapping functions, including sparsemax, and two novel sparse formulations, sparsegen-lin and sparsehourglass, that provide control over the degree of desired sparsity, offering important #REFR details for implementing sparse attention distributions.", "given_citation_intent": "method", "given_keywords": ["details"]}]}
{"citing_id": "2304.13722v1", "cited_id": "2004.00049", "section_title": "G.1 Experiments On Openimages500K", "citation": "Note that (Chai et al., 2021) can only generate one deterministic output given the intput collage (first row), whereas #REFR generates images with little diversity (second row).", "text_before_citation": ["When training with random crops, the boxes are obtained by sampling from the ground-truth box distribution, which means that the bounding box distribution at inference time is in-distribution, what could explain the lower image FID when compared to Mask-RCNN or OLN.", "However, the latter methods obtain a better object FID, which could be the result of the bounding boxes being more likely to contain actual objects than the random crops baseline.", "Moreover, in Figure 17 , we show generated images using out-of-distribution object combinations in a collage and observe that training with Mask-RCNN and OLN bounding boxes results in better object generations than when training with random crops, as already seen quantitatively in Table 12 , as well as visually comparable image quality and diversity to the model trained with ground-truth bounding boxes.", "Overall, these experiments showcase that M&Ms can be trained without ground-truth bounding boxes to generate reasonable scenes, see Figure 17 , at the expense of worse image and object FID metrics for the in-distribution setting, compared to the model trained with ground-truth boxes.", "Sample obtained with (Chai et al., 2021) Collage Samples obtained with in-domain image editing #OTHEREFR Samples obtained with M&Ms (a) Qualitative comparison with image editing methods (Chai et al., 2021; #OTHEREFR that admit a collage as input."], "text_after_citation": ["In contrast, M&Ms offers diverse outputs given the same collage (third row).", "Note that neither of the image editing methods supports moving nor resizing the collage elements.", "Sample obtained with (Chai et al., 2021) Collage Samples obtained with in-domain image editing #OTHEREFR Samples obtained with M&Ms (a) Qualitative comparison with image editing methods (Chai et al., 2021; #OTHEREFR that admit a collage as input.", "Comparisons are made with collages that include the novel class cacti.", "Note that (Chai et al., 2021 ) can only generate one deterministic output given the intput collage (first row), whereas #OTHEREFR generates images with little diversity (second row)."], "citing_paper_content": {"title": "Controllable Image Generation Via Collage Representations", "abstract": "Recent advances in conditional generative image models have enabled impressive results. On the one hand, text-based conditional models have achieved remarkable generation quality, by leveraging large-scale datasets of image-text pairs. To enable fine-grained controllability, however, text-based models require long prompts, whose details may be ignored by the model. On the other hand, layout-based conditional models have also witnessed significant advances. These models rely on bounding boxes or segmentation maps for precise spatial conditioning in combination with coarse semantic labels. The semantic labels, however, cannot be used to express detailed appearance characteristics. In this paper, we approach fine-grained scene controllability through image collages which allow a rich visual description of the desired scene as well as the appearance and location of the objects therein, without the need of class nor attribute labels. We introduce \"mixing and matching scenes\" (M&Ms), an approach that consists of an adversarially trained generative image model which is conditioned on appearance features and spatial positions of the different elements in a collage, and integrates these into a coherent image. We train our model on the OpenImages (OI) dataset and evaluate it on collages derived from OI and MS-COCO datasets. Our experiments on the OI dataset show that M&Ms outperforms baselines in terms of fine-grained scene controllability while being very competitive in terms of image quality and sample diversity. On the MS-COCO dataset, we highlight the generalization ability of our model by outperforming DALL-E in terms of the zero-shot FID metric, despite using two magnitudes fewer parameters and data. Collage based generative models have the potential to advance content creation in an efficient and effective way as they are intuitive to use and yield high quality generations."}, "cited_paper_content": {"title": "In-Domain Gan Inversion For Real Image Editing", "abstract": "Recent work has shown that a variety of controllable semantics emerges in the latent space of the Generative Adversarial Networks (GANs) when being trained to synthesize images. However, it is difficult to use these learned semantics for real image editing. A common practice of feeding a real image to a trained GAN generator is to invert it back to a latent code. However, we find that existing inversion methods typically focus on reconstructing the target image by pixel values yet fail to land the inverted code in the semantic domain of the original latent space. As a result, the reconstructed image cannot well support semantic editing through varying the latent code. To solve this problem, we propose an in-domain GAN inversion approach, which not only faithfully reconstructs the input image but also ensures the inverted code to be semantically meaningful for editing. We first learn a novel domain-guided encoder to project any given image to the native latent space of GANs. We then propose a domain-regularized optimization by involving the encoder as a regularizer to fine-tune the code produced by the encoder, which better recovers the target image. Extensive experiments suggest that our inversion method achieves satisfying real image reconstruction and more importantly facilitates various image editing tasks, such as image interpolation and semantic manipulation, significantly outperforming start-of-the-arts."}, "keywords": ["intput collage", "images"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Moreover, in Figure 17, we show generated images using out-of-distribution object combinations in a collage and observe that training with Mask-RCNN and OLN bounding boxes results in better object generations than when training with random crops, as already seen quantitatively in Table 12, as well as visually comparable image quality and diversity to the model trained with ground-truth bounding boxes. Overall, these experiments showcase that M&Ms can be trained without ground-truth bounding boxes to generate reasonable scenes, see Figure 17, at the expense of worse image and object FID metrics for the in-distribution setting, compared to the model trained with ground-truth boxes. Sample obtained with (Chai et al., 2021) Collage Samples obtained with in-domain image editing #REFR that admit a collage as input.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In-domain GAN inversion has been proposed to ensure the inverted code to be semantically meaningful for editing, which significantly outperforms start-of-the-arts in various image editing tasks, as shown in (Chen et al., 2020). #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In-domain image editing methods, such as the one proposed by #REFR, have shown to significantly facilitate various image editing tasks, including semantic manipulation, by ensuring the inverted code to be semantically meaningful for editing, which could be particularly useful when dealing with input collages and generating high-quality images.", "given_citation_intent": "background", "given_keywords": ["intput collage", "images"]}]}
{"citing_id": "2303.07911v1", "cited_id": "1207.5726", "section_title": "Appendix H: First Benchmarking Sdp 1", "citation": "In this appendix, we detail the derivation of our first benchmarking SDP 1 , based on the SDP for fidelity #REFR . Let and be bipartite states.", "text_before_citation": [], "text_after_citation": ["The SDP for the root fidelity \u221a ( , ), which makes use of Uhlmann's theorem #OTHEREFR , is as follows:", "EQUATION", "where L (H ) is the set of all linear operators acting on the Hilbert space H .", "Note that there is no semidefinite constraint that directly corresponds to optimizing over the set of separable states #OTHEREFR .", "Instead, we can constrain to have a positive partial transpose (PPT) #OTHEREFR and be -extendible #OTHEREFR , since all separable states satisfy these constraints."], "citing_paper_content": {"title": "Quantum Steering Algorithm For Estimating Fidelity Of Separability", "abstract": "Quantifying entanglement is an important task by which the resourcefulness of a state can be measured. Here we develop a quantum algorithm that tests for and quantifies the separability of a general bipartite state, by making use of the quantum steering effect. Our first separability test consists of a distributed quantum computation involving two parties: a computationally limited client, who prepares a purification of the state of interest, and a computationally unbounded server, who tries to steer the reduced systems to a probabilistic ensemble of pure product states. To design a practical algorithm, we replace the role of the server by a combination of parameterized unitary circuits and classical optimization techniques to perform the necessary computation. The result is a variational quantum steering algorithm (VQSA), which is our second separability test that is better suited for the capabilities of quantum computers available today. This VQSA has an additional interpretation as a distributed variational quantum algorithm (VQA) that can be executed over a quantum network, in which each node is equipped with classical and quantum computers capable of executing VQA. We then simulate our VQSA on noisy quantum simulators and find favorable convergence properties on the examples tested. We also develop semidefinite programs, executable on classical computers, that benchmark the results obtained from our VQSA. Our findings here thus provide a meaningful connection between steering, entanglement, quantum algorithms, and quantum computational complexity theory. They also demonstrate the value of a parameterized mid-circuit measurement in a VQSA and represent a first-of-its-kind application for a distributed VQA. Finally, the whole framework generalizes to the case of multipartite states and entanglement."}, "cited_paper_content": {"title": "Simpler Semidefinite Programs For Completely Bounded Norms", "abstract": "The completely bounded trace and spectral norms, for finite-dimensional spaces, are known to be efficiently expressible by semidefinite programs (J. Watrous, Theory of Computing 5: 11, 2009). This paper presents two new, and arguably much simpler, semidefinite programming formulations of these norms."}, "keywords": ["bipartite states", "first benchmarking SDP"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To benchmark the results obtained from our VQSA, we developed semidefinite programs, executable on classical computers, which are based on the efficient expressibility of completely bounded trace and spectral norms by semidefinite programs as presented in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The completely bounded trace and spectral norms, which are efficiently expressible by semidefinite programs for finite-dimensional spaces, have been recently formulated in two new and simpler ways #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To benchmark the results obtained from our VQSA on bipartite states, we develop semidefinite programs that are executable on classical computers, which are based on the efficient expressibility of completely bounded norms by semidefinite programs as shown in #REFR. Additionally, this represents the first benchmarking SDP for a distributed VQA.", "given_citation_intent": "background", "given_keywords": ["bipartite states", "first benchmarking SDP"]}]}
{"citing_id": "2304.01955v1", "cited_id": "1803.00418", "section_title": "Dynamic Modeling Of Pipe Flow", "citation": "To solve for dynamics of mass flows and pressures across the system we use the staggered-grid approach of #REFR which is an explicit, conservative, second order, finite difference scheme, stable given a CFL condition is satisfied.", "text_before_citation": ["EQUATION", "\u03c6", "EQUATION", "Where S ij is the cross-section of the pipe.", "Initial conditions for density and mass-flux in the system are constructed based on actual operational data."], "text_after_citation": ["We remind that, as of now, the Israel system does not contain compressors."], "citing_paper_content": {"title": "Control Of Line Pack In Natural Gas System: Balancing Limited Resources Under Uncertainty", "abstract": "We build and experiment with a realistic but reduced natural gas model of Israel. The system is unusual because (a) it is controlled from a limited number of points which are at, or close to, the gas extraction sites offshore of Israel's Mediterranean coast; (b) control specifies average flux at inlet, not pressure; (c) there are no inland compressors to regulate pressure; (d) power system is the main consumer of gas (70% of Israel's power is generated at gas-fired power plants distributed across the country). Nature of the system suggests that a special attention should be given to understanding dynamics driven by fast transients in gas consumption meeting intraday variations in the electricity demand, and accounting for increasing role of uncertain renewable generation (mainly solar). Based on all of the above we pose and resolve a sequence of dynamic and control challenges, such as: How to time ramping up-and down-injection of gas to guarantee a healthy intra-day line-pack which meets both pressure constraints and gas-extraction patterns? We report simulation results and utilize monotonicity properties of the natural gas flows which render robustness of our conclusions to the uncertainties of the edge withdrawals of gas."}, "cited_paper_content": {"title": "An Explicit Staggered-Grid Method For Numerical Simulation Of Large-Scale Natural Gas Pipeline Networks", "abstract": "We present an explicit second order staggered finite difference (FD) discretization scheme for forward simulation of natural gas transport in pipeline networks. By construction, this discretization approach guarantees that the conservation of mass condition is satisfied exactly. The mathematical model is formulated in terms of density, pressure, and mass flux variables, and as a result permits the use of a general equation of state to define the relation between the gas density and pressure for a given temperature. In a single pipe, the model represents the dynamics of the density by propagation of a non-linear wave according to a variable wave speed. We derive compatibility conditions for linking domain boundary values to enable efficient, explicit simulation of gas flows propagating through a network with pressure changes created by gas compressors. We compare our staggered grid method with an explicit operator splitting method and a lumped element scheme, and perform numerical experiments to validate the convergence order of the new discretization approach. In addition, we perform several computations to investigate the influence of non-ideal equation of state models and temperature effects on pipeline simulations with boundary conditions on various time and space scales."}, "keywords": ["staggered-grid approach", "finite difference scheme"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The explicit second order staggered finite difference discretization scheme for forward simulation of natural gas transport in pipeline networks presented in #REFR guarantees the conservation of mass condition and allows the use of a general equation of state to define the relation between the gas density and pressure for a given temperature, which could be useful for constructing initial conditions for density and mass-flux in the system based on actual operational data.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct initial conditions for density and mass-flux in the system, the authors utilized actual operational data and the explicit second order staggered finite difference discretization scheme presented in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The initial conditions for density and mass-flux in the system are constructed based on actual operational data using a second order staggered finite difference discretization scheme, which guarantees the conservation of mass condition is satisfied exactly, as presented in #REFR.", "given_citation_intent": "method", "given_keywords": ["staggered-grid approach", "finite difference scheme"]}]}
{"citing_id": "2303.07014v1", "cited_id": "1806.03589", "section_title": "Iii. Method", "citation": "In order to force the training generator to produce more realistic outputs, we employ a global discriminator and three local discriminators against the generator. We use SN-PatchGAN #REFR as our global discriminator.", "text_before_citation": ["The generator adopts an encoder-decoder architecture with skip connections #OTHEREFR , as illustrated in Fig. 3(a) .", "Specifically, the encoder is composed of several successive gated convolutional layers #OTHEREFR with stride 2.", "The decoder is composed of several Half-AdaIN with upsampling operations.", "In addition, two CWSI are inserted into the decoder at the resolution of 64 \u00d7 64 and 128 \u00d7 128.", "For the inpainting task, we generally believe that the encoder can transfer the available information from the known pixels to missing pixels by gradually increasing the receptive field, and the decoder is responsible for the reconstruction of details #OTHEREFR , so we only place the control modules in the decoder."], "text_after_citation": ["The local discriminators are focused on specific sub-regions, including two eyes and mouths, which helps the generator synthesize high-frequency textures in these regions. The discriminators are omitted in Fig. 3 ."], "citing_paper_content": {"title": "Reference-Guided Large-Scale Face Inpainting With Identity And Texture Control", "abstract": "Face inpainting aims at plausibly predicting missing pixels of face images within a corrupted region. Most existing methods rely on generative models learning a face image distribution from a big dataset, which produces uncontrollable results, especially with large-scale missing regions. To introduce strong control for face inpainting, we propose a novel reference-guided face inpainting method that fills the large-scale missing region with identity and texture control guided by a reference face image. However, generating high-quality results under imposing two control signals is challenging. To tackle such difficulty, we propose a dual control one-stage framework that decouples the reference image into two levels for flexible control: High-level identity information and low-level texture information, where the identity information figures out the shape of the face and the texture information depicts the component-aware texture. To synthesize high-quality results, we design two novel modules referred to as Half-AdaIN and Component-Wise Style Injector (CWSI) to inject the two kinds of control information into the inpainting processing. Our method produces realistic results with identity and texture control faithful to reference images. To the best of our knowledge, it is the first work to concurrently apply identity and component-level controls in face inpainting to promise more precise and controllable results."}, "cited_paper_content": {"title": "Free-Form Image Inpainting With Gated Convolution", "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: \\url{https://github.com/JiahuiYu/generative_inpainting}."}, "keywords": ["training generator", "SN-PatchGAN"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed Half-AdaIN and Component-Wise Style Injector (CWSI) modules are designed to inject control information into the inpainting process #REFR, which enables the generator to produce realistic results with faithful identity and texture control guided by a reference face image.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To synthesize high-quality results with identity and texture control, the proposed method employs two novel modules, Half-AdaIN and Component-Wise Style Injector (CWSI), which inject the two kinds of control information into the inpainting processing #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To stabilize the training generator, the authors adopted SN-PatchGAN #REFR, which applies spectral-normalized discriminator on dense image patches, as a patch-based GAN loss during the training process #OTHEREFR.", "given_citation_intent": "method", "given_keywords": ["training generator", "SN-PatchGAN"]}]}
{"citing_id": "2304.06767v1", "cited_id": "1707.06347", "section_title": "Related Work", "citation": "The main idea is learning a reward function to reflect human preferences with human annotations and optimize language models by RL methods like proximal policy optimization (PPO) #REFR .", "text_before_citation": ["Alignment of Generative Models.", "Alignment #OTHEREFR is first proposed to build agents that behave in accordance with the human's intention.", "By communicating with human, agents can get accurate supervised signals #OTHEREFR by applying several scalable reward learning methods #OTHEREFR 20] .", "Alignment benefits many recent generative foundation models, like InstructGPT #OTHEREFR , Claude #OTHEREFR and Sparrow #OTHEREFR , in achieving better performance.", "In language foundation model training #OTHEREFR , alignment is often achieved by Reinforcement Learning from Human Feedback (RLHF)."], "text_after_citation": ["By incorporating supervised finetuning (SFT), InstructGPT #OTHEREFR successfully achieved alignment for GPT-3 #OTHEREFR .", "Besides, Claude #OTHEREFR and Sparrow #OTHEREFR stressed aligning language foundation models from helpful, honest, and harmless(HHH) human feedbacks.", "In visual generative models, several works #OTHEREFR studied aligning them with human feedbacks.", "Models are expected to understand specific visual control signals like colors, counts, and backgrounds #OTHEREFR more accurately after alignment.", "It is still challenging to achieve tradeoffs between aligning human preferences and generating high-fidelity images."], "citing_paper_content": {"title": "Raft: Reward Ranked Finetuning For Generative Foundation Model Alignment", "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a humanfeedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models more effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently assembles a streaming dataset. This dataset serves as the basis for aligning the generative model and can be employed under both offline and online settings. Notably, the sample generation process within RAFT is gradient-free, rendering it compatible with black-box generators. Through extensive experiments, we demonstrate that our proposed algorithm exhibits strong performance in the context of both large language models and diffusion models."}, "cited_paper_content": {"title": "Proximal Policy Optimization Algorithms", "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."}, "keywords": ["proximal policy optimization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In language foundation model training, alignment is often achieved by Reinforcement Learning from Human Feedback (RLHF) #REFR. Prior research has primarily employed RLHF as a means of addressing the problem of implicit biases in generative foundation models.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve better performance in Reinforcement Learning from Human Feedback (RLHF), the authors propose a novel objective function that enables multiple epochs of minibatch updates, which is called proximal policy optimization (PPO) #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In language foundation model training, alignment is often achieved by Reinforcement Learning from Human Feedback (RLHF), utilizing several scalable reward learning methods #REFR, such as proximal policy optimization.", "given_citation_intent": "method", "given_keywords": ["proximal policy optimization"]}]}
{"citing_id": "2303.06223v1", "cited_id": "1911.03064", "section_title": "Large Language Models", "citation": "Counterfactual fairness #REFR examines how perturbing the demographic signals of existing test examples can change the performance of the model (e.g.", "text_before_citation": ["Current LLM evaluation mechanisms include quantitative metrics measuring notions of accuracy (how similar are the generated outputs to the expected outputs), robustness (how resilient is the model to transformations of the input), calibration (how meaningful are the generated probabilities in respect to uncertainty), efficiency (what are the energy, carbon, and time costs for training and inference) and more #OTHEREFR .", "Some also go beyond singular trainingtime loss objectives and implement reinforcement learning using human feedback in the loop #OTHEREFR .", "A variety of potential harms and failure modes for LLMs have also been identified.", "There are substantial environmental costs associated with the volume of computational power required for training and inference #OTHEREFR .", "There are also concerns of LLMs propagating unfairness or bias."], "text_after_citation": ["\"He worked at the local hospital\" versus \"She worked at the local hospital\").", "Fairness can also be evaluated via performance disparities between demographic subgroups.", "There are also other concerning forms of biases such as stereotypical associations, erasure, and over-representation in the semantics of its output #OTHEREFR . Finally, LLMs have been shown to produce toxic outputs.", "Toxicity in this context refers to hateful, violent, or offensive text #OTHEREFR , and has been shown to result even when the text prompt input is not itself toxic #OTHEREFR .", "LLMs often suffer from factual errors-they can \"hallucinate\" information #OTHEREFR by providing very confident-sounding but entirely false responses."], "citing_paper_content": {"title": "Who'S Thinking? A Push For Human-Centered Evaluation Of Llms Using The Xai Playbook", "abstract": "Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs-complete with their cognitive biases and quirksshould be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendenciesagain, complete with their cognitive biases and quirks-should rest front and center when evaluating deployed LLMs. We outline three developed focus areas of human-centered evaluation of XAI: mental models, use case utility, and cognitive engagement, and we highlight the importance of exploring each of these concepts for LLMs. Our goal is to jumpstart human-centered LLM evaluation. CCS CONCEPTS \u2022 Human-centered computing \u2192 User models; HCI theory, concepts and models; \u2022 Computing methodologies \u2192 Philosophical/theoretical foundations of artificial intelligence; Artificial intelligence."}, "cited_paper_content": {"title": "Reducing Sentiment Bias In Language Models Via Counterfactual Evaluation", "abstract": "Recent improvements in large-scale language models have driven progress on automatic generation of syntactically and semantically consistent text for many real-world applications. Many of these advances leverage the availability of large corpora. While training on such corpora encourages the model to understand long-range dependencies in text, it can also result in the models internalizing the social biases present in the corpora. This paper aims to quantify and reduce biases exhibited by language models. Given a conditioning context (e.g. a writing prompt) and a language model, we analyze if (and how) the sentiment of the generated text is affected by changes in values of sensitive attributes (e.g. country names, occupations, genders, etc.) in the conditioning context, a.k.a. counterfactual evaluation. We quantify these biases by adapting individual and group fairness metrics from the fair machine learning literature. Extensive evaluation on two different corpora (news articles and Wikipedia) shows that state-of-the-art Transformer-based language models exhibit biases learned from data. We propose embedding-similarity and sentiment-similarity regularization methods that improve both individual and group fairness metrics without sacrificing perplexity and semantic similarity---a positive step toward development and deployment of fairer language models for real-world applications."}, "keywords": ["model", "Counterfactual fairness"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "There are also concerns of LLMs propagating unfairness or bias, as shown by recent work on reducing sentiment bias in language models via counterfactual evaluation #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent studies have shown that state-of-the-art Transformer-based language models can internalize social biases present in the corpora, which can result in biased generated text, and the proposed counterfactual evaluation method in #REFR can help reduce such biases.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent studies have shown that large language models (LLMs) can internalize social biases present in the corpora used for training, which can result in biased generated text; to address this issue, researchers have proposed methods for quantifying and reducing biases exhibited by language models, such as Counterfactual fairness, as discussed in #REFR.", "given_citation_intent": "background", "given_keywords": ["model", "Counterfactual fairness"]}]}
{"citing_id": "2304.02147v1", "cited_id": "1811.11742", "section_title": "Method", "citation": "Following #REFR , we predict the 3D pose for the central frame from any such sequence, i.e.p i/2 \u2208 R J\u00d73 .", "text_before_citation": ["In the following subsections, we present an overview of our solution methodology for estimating 3D poses from a sequence of 2D poses, then we describe in our global network architecture, and lastly we present our dynamic multi-headed convolutional self-attention mechanism.", "The overall architecture of our methodology is described in Figure 1 .", "Given a sequence of 2D poses P = {Pi} T i=1 \u2282 R J\u00d72 where T represents the number of frames in the sequence and J is the number of joints in the skeleton.", "We seek to reconstruct the 3D poses in the root relative camera reference frame (i.e.", "the camera reference frame where the root joint sits at the origin)."], "text_after_citation": ["Our network contains two Dynamic ConvFormer blocks, one with spatial attention and the other with temporal attention.", "More specifically, we leverage a spatial attention mechanism to extract frame-wise inter-joint dependencies by analyzing sections of joints that are related.", "The temporal attention mechanism extracts global inter-frame relationships by analyzing correlations between the temporal profiles of joints.", "In contrast to #OTHEREFR , which queries latent pose representations for individual frames and then computes attention with respect to the temporal axis, our temporal joints profile mechanism fuses temporal information at the querying level prior to computing self-attention with respect to the temporal axis."], "citing_paper_content": {"title": "Convformer: Parameter Reduction In Transformer Models For 3D Human Pose Estimation By Leveraging Dynamic Multi-Headed Convolutional Attention", "abstract": "Recently, fully-transformer architectures have replaced the defacto convolutional architecture for the 3D human pose estimation task. In this paper we propose ConvFormer , a novel convolutional transformer that leverages a new dynamic multi-headed convolutional self-attention mechanism for monocular 3D human pose estimation. We designed a spatial and temporal convolutional transformer to comprehensively model human joint relations within individual frames and globally across the motion sequence. Moreover, we introduce a novel notion of temporal joints profile for our temporal ConvFormer that fuses complete temporal information immediately for a local neighborhood of joint features. We have quantitatively and qualitatively validated our method on three common benchmark datasets: Human3.6M, MPI-INF-3DHP, and HumanEva. Extensive experiments have been conducted to identify the optimal hyper-parameter set. These experiments demonstrated that we achieved a significant parameter reduction relative to prior transformer models while attaining State-of-the-Art (SOTA) or near SOTA on all three datasets. Additionally, we achieved SOTA for Protocol III on H36M for both GT and CPN detection inputs. Finally, we obtained SOTA on all three metrics for the MPI-INF-3DHP dataset and for all three subjects on HumanEva under Protocol II."}, "cited_paper_content": {"title": "3D Human Pose Estimation In Video With Temporal Convolutions And Semi-Supervised Training", "abstract": "In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D"}, "keywords": ["3D"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To estimate 3D poses from a sequence of 2D poses, the authors propose a methodology that leverages a dynamic multi-headed convolutional self-attention mechanism in their ConvFormer architecture #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate 3D poses from a sequence of 2D poses, the authors proposed a fully convolutional model based on dilated temporal convolutions over 2D keypoints, which outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M in the supervised setting, as well as shows significant improvements on HumanEva-I, according to #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate 3D poses from a sequence of 2D poses, the authors proposed a fully convolutional model based on dilated temporal convolutions over 2D keypoints, which outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, as reported in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": ["3D"]}]}
{"citing_id": "2303.02411v1", "cited_id": "1906.00067", "section_title": "Introduction", "citation": "The first image of Figure 1 requires knowledge about human culture and history #REFR , to combine with visual information: the object in the image is a church, and people usually go to the church on Sundays.", "text_before_citation": ["For example, masking out words from image captions enforces learning how to fill them based on visual cues; reversely, image regions can be masked out, with language guiding their reconstruction.", "Task-specific fine-tuning steps upon this basic understanding of vision and language, by refining the neural weights of the trained model to adapt to each specific task at a time, upon which the final evaluation is performed.", "Despite the rich VL knowledge acquired during this process, current transformer-based VL models #OTHEREFR lack generalization to several concepts and scenarios that require commonsense knowledge, or knowledge of abstract entities, facts and real-world events.", "Of course, this is somehow expected, since neither pre-training nor fine-tuning VL datasets contain or demand perceiving concepts beyond visual descriptions.", "Figure 1 presents some examples of this claim: questions (Q) about the image (I ) require some knowledge beyond the visual domain, so that the correct answer (A) can be inferred. #OTHEREFR ."], "text_after_citation": ["The second image #OTHEREFR requires one more reasoning step, since it is not only required to detect that this is a postage stamp containing the photo of a person (visual information), but also who this person is.", "Knowledge about named entities recognizes this person as Alexander Hamilton.", "Further factual knowledge provides that Alexander Hamilton was born in todays Saint Kitts and Nevis and Saint Kitts and Nevis is in North America.", "The combination of these two facts derives the final answer Alexander Hamilton was born in North America.", "The third image #OTHEREFR requires the visual extraction of the two people present in it."], "citing_paper_content": {"title": "The Contribution Of Knowledge In Visiolinguistic Learning: A Survey On Tasks And Challenges", "abstract": "Recent advancements in visiolinguistic (VL) learning have allowed the development of multiple models and techniques that offer several impressive implementations, able to currently resolve a variety of tasks that require the collaboration of vision and language. Current datasets used for VL pre-training only contain a limited amount of visual and linguistic knowledge, thus significantly limiting the generalization capabilities of many VL models. External knowledge sources such as knowledge graphs (KGs) and Large Language Models (LLMs) are able to cover such generalization gaps by filling in missing knowledge, resulting in the emergence of hybrid architectures. In the current survey, we analyze tasks that have benefited from such hybrid approaches. Moreover, we categorize existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches."}, "cited_paper_content": {"title": "Ok-Vqa: A Visual Question Answering Benchmark Requiring External Knowledge", "abstract": "Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain."}, "keywords": ["visual information"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of limited generalization capabilities of current transformer-based VL models, external knowledge sources such as knowledge graphs and Large Language Models have been utilized to fill in missing knowledge gaps, resulting in the emergence of hybrid architectures (Figure 1). As demonstrated in the OK-VQA benchmark, such external knowledge resources are crucial for knowledge-based visual question answering tasks that require reasoning beyond what is in the image #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of current transformer-based VL models in handling concepts beyond visual descriptions, external knowledge sources such as knowledge graphs and Large Language Models have been utilized to fill in missing knowledge gaps, as demonstrated in the OK-VQA benchmark dataset #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the limitations of current transformer-based VL models in perceiving concepts beyond visual descriptions, Ok-Vqa dataset has been introduced, which requires external knowledge to answer questions that go beyond visual information #REFR.", "given_citation_intent": "background", "given_keywords": ["visual information"]}]}
{"citing_id": "2304.07638v1", "cited_id": "1701.06686", "section_title": "Outlook", "citation": "This would extend an analogous result for causal effects by Richardson et al. from Ref. #REFR (see Rems.", "text_before_citation": ["Counterfactuals. An obvious next step, pointed out in Sec.", "8.2.3, is to prove completeness of the identification algorithm id-cf for counterfactuals.", "This would establish necessity of the diagrammatic identifiability criterion from Cor.", "91 and complete the diagrammatic translation of the results by Shpitser et al. from Ref. #OTHEREFR .", "Once completeness is established, it is natural to check whether the string diagrammatic treatment of counterfactuals actually allows for an easy argument to the effect that there is no loss of generality in studying the identifiability of counterfactuals based on ADMGs as latent projections."], "text_after_citation": ["81 and 93) and fill a gap that seems to currently exist in the conventional literature.", "Beyond these, it would be interesting to study whether our approach might naturally extend to cover socalled nested counterfactuals #OTHEREFR by allowing for connections across worlds other than just through the shared background variables U .", "Finally, the generalisation of the notion of a counterfactual in Sec.", "8.3, which involves the conditioning with fuzzy facts and parallel worlds defined by interventions more general than do-interventions, deserves further exploration.", "In particular, one might study how the criteria and methods for the identification of counterfactuals would change in this case."], "citing_paper_content": {"title": "Causal Models In String Diagrams", "abstract": "The framework of causal models, pioneered by Pearl and his collaborators, as well as Spirtes, Glymour and Scheines, provides a principled approach to causal reasoning which is applied today across many scientific domains. Here we present the framework of causal models in the language of string diagrams, interpreted formally using category theory. A class of string diagrams, called network diagrams, are in 1-to-1 correspondence with directed acyclic graphs. A causal model is given by interpreting the components of such a diagram in terms of stochastic maps, functions, or more general channels in a symmetric monoidal category with a 'copy-discard' structure (cd-category). This represents a model as a single mathematical object that can be reasoned with intuitively and yet rigorously. Building on earlier works, most notably by Fong and Jacobs, Kissinger and Zanasi, as well as Fritz and Klingler, this work presents diagrammatic definitions of causal models and functional causal models in a cd-category, which generalise causal Bayesian networks and structural causal models, respectively. We formalise the most general kind of interventions on a model, including but beyond atomic ones described by do-interventions, and present the natural notion of an open causal model, as a causal model 'with inputs'. To apply these to causal reasoning, we also give an approach to conditioning based on a normalisation box, which allows causal inference calculations to be done fully diagrammatically. We use these to define counterfactuals in this setup, and to treat the problems of the identifiability of both causal effects and counterfactuals fully diagrammatically. The benefits of such a presentation of causal models lie both in foundational questions in causal reasoning, and in particular in their clarificatory role and pedagogical value. In fact this manuscript aims to be accessible to different communities, including causal model practitioners as well as researchers in applied category theory. For illustration of the key ideas many examples from the causal model literature are discussed in the diagrammatic language. Overall, we argue and demonstrate that causal reasoning according to the causal model framework is most naturally and intuitively done as diagrammatic reasoning."}, "cited_paper_content": {"title": "Nested Markov Properties For Acyclic Directed Mixed Graphs", "abstract": "Directed acyclic graph (DAG) models may be characterized in at least four different ways: via a factorization, the d-separation criterion, the moralization criterion, and the local Markov property. As pointed out by Robins (1986, 1999), Verma and Pearl (1990), and Tian and Pearl (2002b), marginals of DAG models also imply equality constraints that are not conditional independences. The well-known `Verma constraint' is an example. Constraints of this type were used for testing edges (Shpitser et al., 2009), and an efficient marginalization scheme via variable elimination (Shpitser et al., 2011). ::: We show that equality constraints like the `Verma constraint' can be viewed as conditional independences in kernel objects obtained from joint distributions via a fixing operation that generalizes conditioning and marginalization. We use these constraints to define, via Markov properties and a factorization, a graphical model associated with acyclic directed mixed graphs (ADMGs). We show that marginal distributions of DAG models lie in this model, prove that a characterization of these constraints given in (Tian and Pearl, 2002b) gives an alternative definition of the model, and finally show that the fixing operation we used to define the model can be used to give a particularly simple characterization of identifiable causal effects in hidden variable graphical causal models."}, "keywords": ["causal effects"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Once completeness is established, it is natural to check whether the string diagrammatic treatment of counterfactuals actually allows for an easy argument to the effect that there is no loss of generality in studying the identifiability of counterfactuals based on ADMGs as latent projections (Tian and Pearl, 2002b).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The identification algorithm for counterfactuals can be completed by proving its completeness, as suggested in Sec. 8.2.3, which would establish the necessity of the diagrammatic identifiability criterion from Cor. 91 and complete the diagrammatic translation of the results by Shpitser et al. from Ref. #OTHEREFR. This algorithm is based on the constraints of the 'Verma type' that were used for testing edges and an efficient marginalization scheme via variable elimination, as described in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The identification algorithm for counterfactuals is completed by proving its completeness, as pointed out in Sec. 8.2.3, and this would establish the necessity of the diagrammatic identifiability criterion from Cor. 91, which is a result presented in #REFR.", "given_citation_intent": "result", "given_keywords": ["causal effects"]}]}
{"citing_id": "2304.10726v1", "cited_id": "1910.10601", "section_title": "Related Work", "citation": "Moreover, empirical evaluation of 9 static analysis tools #REFR classified 93% of contracts as vulnerable, thus indicating a considerable number of false positives.", "text_before_citation": ["Although such tools are very impressive, and indeed we ourselves use Slither, this reliance on expert rules can make these tools difficult to maintain and update.", "We are unaware of any detection tool that detects all known vulnerabilities; or that is easily extendable for future bugs without human developers carefully crafting subtle expert rules and/or hardcoding additional features.", "Most smart contract vulnerability analyzers use symbolic execution to reason about all execution paths of a program.", "However, symbolic execution can suffer from \"path explosion\" when the size and complexity of the code increases, leading to significant time and space requirements.", "Practical limits on time and space can lead to difficulties analyzing smart contracts at scale."], "text_after_citation": ["In addition, only a few vulnerabilities were detected simultaneously that got consensus from four or more tools.", "Fuzzing is a dynamic analysis technique, that has the advantage of scaling well to larger programs.", "Contractfuzzer #OTHEREFR , and Echidna #OTHEREFR are two notable examples applied to smart contracts.", "Rather than relying on a fixed set of pre-defined bug oracles to detect vulnerabilities, fuzzing technique uses sophisticated grammar-based fuzzing campaigns based on a contract API to falsify user-defined predicates or Solidity assertions.", "However, generating meaningful inputs for fuzzing typically requires annotating the source code of a contract."], "citing_paper_content": {"title": "Smart Learning To Find Dumb Contracts", "abstract": "We introduce Deep Learning Vulnerability Analyzer (DLVA), a vulnerability detection tool for Ethereum smart contracts based on powerful deep learning techniques for sequential data adapted for bytecode. We train DLVA to judge bytecode even though the supervising oracle, Slither, can only judge source code. DLVA's training algorithm is general: we \"extend\" a source code analysis to bytecode without any manual feature engineering, predefined patterns, or expert rules. DLVA's training algorithm is also robust: it overcame a 1.25% error rate mislabeled contracts, and-the student surpassing the teacher-found vulnerable contracts that Slither mislabeled. In addition to extending a source code analyzer to bytecode, DLVA is much faster than conventional tools for smart contract vulnerability detection based on formal methods: DLVA checks contracts for 29 vulnerabilities in 0.2 seconds, a speedup of 10-500x+ compared to traditional tools. DLVA has three key components. Smart Contract to Vector (SC2V) uses neural networks to map arbitrary smart contract bytecode to an high-dimensional floating-point vector. Sibling Detector (SD) classifies contracts when a target contract's vector is Euclidian-close to a labeled contract's vector in a training set; although only able to judge 55.7% of the contracts in our test set, it has an average accuracy of 97.4% with a false positive rate of only 0.1%. Lastly, Core Classifier (CC) uses neural networks to infer vulnerable contracts regardless of vector distance. DLVA has an overall accuracy of 96.6% with an associated false positive rate of only 3.7%. We benchmark DLVA's CC with 10 \"off-the-shelf\" machine learning techniques and show that the CC is more accurate, reducing the average size of the error set by 36.5%. We also benchmark DLVA against well-known alternatives: Oyente, Mythril, Osiris, Smartcheck, Slither, and SoliAudit. DLVA enjoyed meaningfully higher true positive rates while producing comparable false positive rates. Moreover, it did so despite using dramatically less analysis time. Notably, DLVA outperformed state-of-the-art alternatives without using any painstakingly-crafted expert rules or predefined patterns. Table 1: Summary of DLVA results on all datasets analysed (TPR: detection rate; FPR: false alarm rate). Analysed data Test set Acc TPR FPR EthereumSC large [2] 22,634 87.7% 87.3% 11.9% EthereumSC small [3] 1,381 97.6% 95.4% 2.3% SolidiFI benchmark [5] 444 98.6% 100% 1.9% Elysium benchmark [1] 900 99.5% 99.3% 2.2% Reentrancy benchmark [4] 473 99.4% 94.3% 0.0% Average 25,832 96.6% 95.3% 3.7%"}, "cited_paper_content": {"title": "Empirical Review Of Automated Analysis Tools On 47,587 Ethereum Smart Contracts", "abstract": "Over the last few years, there has been substantial research on automated analysis, testing, and debugging of Ethereum smart contracts. However, it is not trivial to compare and reproduce that research. To address this, we present an empirical evaluation of 9 state-of-the-art automated analysis tools using two new datasets: i) a dataset of 69 annotated vulnerable smart contracts that can be used to evaluate the precision of analysis tools; and ii) a dataset with all the smart contracts in the Ethereum Blockchain that have Solidity source code available on Etherscan (a total of 47,518 contracts). The datasets are part of SmartBugs, a new extendable execution framework that we created to facilitate the integration and comparison between multiple analysis tools and the analysis of Ethereum smart contracts. We used SmartBugs to execute the 9 automated analysis tools on the two datasets. In total, we ran 428,337 analyses that took approximately 564 days and 3 hours, being the largest experimental setup to date both in the number of tools and in execution time. We found that only 42% of the vulnerabilities from our annotated dataset are detected by all the tools, with the tool Mythril having the higher accuracy (27%). When considering the largest dataset, we observed that 97% of contracts are tagged as vulnerable, thus suggesting a considerable number of false positives. Indeed, only a small number of vulnerabilities (and of only two categories) were detected simultaneously by four or more tools."}, "keywords": ["false positives", "9 static analysis"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Most smart contract vulnerability analyzers use symbolic execution to reason about all execution paths of a program, which can suffer from \"path explosion\" when the size and complexity of the code increases, leading to significant time and space requirements. To address this, the authors of a recent study evaluated 9 state-of-the-art automated analysis tools using two new datasets, including a dataset of 69 annotated vulnerable smart contracts, and found that only 42% of the vulnerabilities from the annotated dataset are detected by all the tools, with the tool Mythril having the higher accuracy (27%) (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Automated analysis tools have been extensively researched in the past few years, and an empirical evaluation of nine state-of-the-art tools was conducted on a dataset of 69 annotated vulnerable smart contracts and a dataset with all the smart contracts in the Ethereum Blockchain, which have Solidity source code available on Etherscan, using SmartBugs, a new extendable execution framework that facilitates the integration and comparison between multiple analysis tools and the analysis of Ethereum smart contracts. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Automated analysis tools for smart contracts can suffer from a considerable number of false positives, with only a small number of vulnerabilities being detected simultaneously by four or more tools, as shown in a recent study that evaluated 9 static analysis tools on a dataset of 69 annotated vulnerable smart contracts and a dataset with all the smart contracts in the Ethereum Blockchain, comprising a total of 47,518 contracts #REFR.", "given_citation_intent": "background", "given_keywords": ["false positives", "9 static analysis"]}]}
{"citing_id": "2303.06872v3", "cited_id": "1812.07035", "section_title": "B. Camera-Lidar Fusion For Relocalization With Multi-Head Self-Attention", "citation": "To take into account the continuity of the rotation angle #REFR , we present the rotation q as [cos \u03b8, sin \u03b8] T rather than \u03b8.", "text_before_citation": ["T and the orientation q = [cos \u03b8, sin \u03b8]", "T from the output of the MHSA module.", "It consists of a position branch and an orientation branch as in #OTHEREFR , #OTHEREFR . Each branch is composed of consecutive MLPs.", "In #OTHEREFR , a leaky ReLU activation function was used after each MLP except for the last one in its regression head, but we replace it with the ReLU activation function in our network.", "Different from most of the previous studies for end-to-end relocalization, both the position and the orientation are two-dimensional under the assumption that typical serving robots move on planar space."], "text_after_citation": ["To our best knowledge, this work is the first study addressing the end-to-end relocalization for a serving robot based on the camera-2D LiDAR fusion in two-dimensional planar space."], "citing_paper_content": {"title": "Fusionloc: Camera-2D Lidar Fusion Using Multi-Head Self-Attention For End-To-End Serving Robot Relocalization", "abstract": "As technology advances in autonomous mobile robots, mobile service robots have been actively used more and more for various purposes. Especially, serving robots have been not surprising products anymore since the COVID-19 pandemic. One of the practical problems in operating serving a robot is that it often fails to estimate its pose on a map that it moves around. Whenever the failure happens, servers should bring the serving robot to its initial location and reboot it manually. In this paper, we focus on end-to-end relocalization of serving robots to address the problem. It is to predict robot pose directly from only the onboard sensor data using neural networks. In particular, we propose a deep neural network architecture for the relocalization based on camera-2D LiDAR sensor fusion. We call the proposed method FusionLoc. In the proposed method, the multi-head selfattention complements different types of information captured by the two sensors to regress the robot pose. Our experiments on a dataset collected by a commercial serving robot demonstrate that FusionLoc can provide better performances than previous endto-end relocalization methods taking only a single image or a 2D LiDAR point cloud as well as a straightforward fusion method concatenating their features."}, "cited_paper_content": {"title": "On The Continuity Of Rotation Representations In Neural Networks", "abstract": "In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses."}, "keywords": ["rotation"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the problem of end-to-end relocalization of serving robots, the proposed method FusionLoc uses a deep neural network architecture based on camera-2D LiDAR sensor fusion, which complements different types of information captured by the two sensors to regress the robot pose using a multi-head self-attention mechanism (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of end-to-end relocalization of serving robots, the proposed FusionLoc method uses multi-head self-attention to complement different types of information captured by the camera and 2D LiDAR sensors, and regresses the robot pose using a position and an orientation branch as in #OTHEREFR, which presents a definition of a continuous representation for rotations that can be helpful for training deep neural networks.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the problem of end-to-end relocalization of serving robots, the proposed FusionLoc method uses multi-head self-attention to complement different types of information captured by the camera and 2D LiDAR sensors, and regresses the robot pose, as in the continuous rotation representations proposed in #REFR.", "given_citation_intent": "method", "given_keywords": ["rotation"]}]}
{"citing_id": "2305.00127v1", "cited_id": "1507.06527", "section_title": "B. Designing The Framework", "citation": "There are a few popular DRL algorithms for solving POMDP such as Deep Recurrent Q-Learning (DRQN) #REFR and RDPG.", "text_before_citation": ["As we focus on energy management within one day with T time steps, our task corresponds to a finite horizon POMDP model."], "text_after_citation": ["The basic ideas of DRQN and RDPG are to add recurrency to DQN and DDPG algorithms, respectively, by replacing the first fully-connected layer with a recurrent long short-term memory (LSTM) layer.", "However, the above DRL algorithms are developed for an infinite horizon setting, while the value function, i.e., critic, and the policy, i.e., actor, are normally dependent on the time steps for the finite horizon case.", "Therefore, we propose a novel DRL algorithm named HAFH-RDPG, which adopts a framework that is a combination of dynamic programming and DRL, where RDPG with fixed target is embedded within the framework of finite-horizon value iteration.", "In HAFH-RDPG, the finite-horizon value iteration starts from the time step T , and uses backward induction to iteratively derive the value function and optimal policy for each time step t \u2208 {T, T \u2212 1, ..., 1}, until it reaches the first time step t = 1.", "In each time step, the RDPG algorithm is used to solve a simple one-step POMDP where the target actor and critic networks, i.e., \u03bb t and \u00b5 t are fixed to be the trained actor and critic networks of the next time step, i.e., \u03bb t+1 and \u00b5 t+1 , which greatly increases stability and performance."], "citing_paper_content": {"title": "Optimal Scheduling In Iot-Driven Smart Isolated Microgrids Based On Deep Reinforcement Learning", "abstract": "In this paper, we investigate the scheduling issue of diesel generators (DGs) in an Internet of Things (IoT)-Driven isolated microgrid (MG) by deep reinforcement learning (DRL). The renewable energy is fully exploited under the uncertainty of renewable generation and load demand. The DRL agent learns an optimal policy from history renewable and load data of previous days, where the policy can generate real-time decisions based on observations of past renewable and load data of previous hours collected by connected sensors. The goal is to reduce operating cost on the premise of ensuring supply-demand balance. In specific, a novel finite-horizon partial observable Markov decision process (POMDP) model is conceived considering the spinning reserve. In order to overcome the challenge of discrete-continuous hybrid action space due to the binary DG switching decision and continuous energy dispatch (ED) decision, a DRL algorithm, namely the hybrid action finite-horizon RDPG (HAFH-RDPG), is proposed. HAFH-RDPG seamlessly integrates two classical DRL algorithms, i.e., deep Q-network (DQN) and recurrent deterministic policy gradient (RDPG), based on a finite-horizon dynamic programming (DP) framework. Extensive experiments are performed with real-world data in an IoT-driven MG to evaluate the capability of the proposed algorithm in handling the uncertainty due to inter-hour and inter-day power fluctuation and to compare its performance with those of the benchmark algorithms."}, "cited_paper_content": {"title": "Deep Recurrent Q-Learning For Partially Observable Mdps", "abstract": "Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting \\textit{Deep Recurrent Q-Network} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes."}, "keywords": ["Deep Recurrent Q-Learning"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "As we focus on energy management within one day with T time steps, our task corresponds to a finite horizon POMDP model. Recurrent neural networks have been shown to be effective in partially observable environments, and the Deep Recurrent Q-Network (DRQN) proposed by Hausknecht and Stone (#REFR) successfully integrates information through time and replicates the performance of Deep Q-Networks on standard Atari games and partially observed equivalents featuring flickering game screens.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of partial observability, the authors propose a Deep Recurrent Q-Network (DRQN) that integrates information through time and successfully replicates the performance of Deep Q-Network (DQN) on standard Atari games, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of partial observability, the authors propose a Deep Recurrent Q-Network (DRQN) that integrates information through time and successfully replicates the performance of Deep Q-Network (DQN) on standard Atari games, as shown in #REFR.", "given_citation_intent": "method", "given_keywords": ["Deep Recurrent Q-Learning"]}]}
{"citing_id": "2303.11100v1", "cited_id": "1512.02325", "section_title": "Iii. Mthars Methodology", "citation": "In this section, we provide details of the problem definition and proposed MTHARS method inspired by SSD #REFR that exploited in the computer vision field.", "text_before_citation": [], "text_after_citation": ["There are four main components: multiscale window generation and matching, non-maximum suppression, model architecture, and model training."], "citing_paper_content": {"title": "A Multi-Task Deep Learning Approach For Sensor-Based Human Activity Recognition And Segmentation", "abstract": "Sensor-based human activity segmentation and recognition are two important and challenging problems in many real-world applications and they have drawn increasing attention from the deep learning community in recent years. Most of the existing deep learning works were designed based on pre-segmented sensor streams and they have treated activity segmentation and recognition as two separate tasks. In practice, performing data stream segmentation is very challenging. We believe that both activity segmentation and recognition may convey unique information which can complement each other to improve the performance of the two tasks. In this paper, we firstly proposes a new multitask deep neural network to solve the two tasks simultaneously. The proposed neural network adopts selective convolution and features multiscale windows to segment activities of long or short time durations. First, multiple windows of different scales are generated to center on each unit of the feature sequence. Then, the model is trained to predict, for each window, the activity class and the offset to the true activity boundaries. Finally, overlapping windows are filtered out by non-maximum suppression, and adjacent windows of the same activity are concatenated to complete the segmentation task. Extensive experiments were conducted on eight popular benchmarking datasets, and the results show that our proposed method outperforms the state-of-the-art methods both for activity recognition and segmentation."}, "cited_paper_content": {"title": "Ssd: Single Shot Multibox Detector", "abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\(300 \\times 300\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\(512 \\times 512\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd."}, "keywords": ["computer vision field"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed multitask deep neural network adopts selective convolution and multiscale windows to segment activities of long or short time durations and achieves state-of-the-art performance on eight benchmarking datasets for activity recognition and segmentation (#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To detect objects in images, the authors propose a method named SSD that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location, and generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape, which eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed multitask deep neural network in this paper adopts selective convolution and multiscale windows to segment activities of long or short time durations in the computer vision field, and it achieves state-of-the-art performance for both activity recognition and segmentation tasks compared to existing methods. #REFR", "given_citation_intent": "method", "given_keywords": ["computer vision field"]}]}
{"citing_id": "2303.07080v1", "cited_id": "1704.04861", "section_title": "Introduction", "citation": "Efficient structural design is a challenge in research, with introduction of the separated convolution #REFR proposed as an effective technique.", "text_before_citation": ["Since the introduction of AlexNet #OTHEREFR , there has been an exponential increase in the number of exceptional convolutional neural networks proposed, resulting in promising outcomes for a variety of visual tasks #OTHEREFR .", "Despite the remarkable results, deploying CNN models on embedded or mobile devices proves challenging as it poses an immense burden on computation and memory storage.", "To address this issue, a significant amount of research has been dedicated to reducing associated costs, thereby making CNN models more practical for real-world applications.", "Broadly speaking, this line of research can be categorized into three distinct areas: efficient structure design, network pruning, and network quantization."], "text_after_citation": ["This method factorizes the standard convolution into a depthwise and pointwise convolution, reducing computation.", "Successful examples of its use in efficient networks include MobileNets #OTHEREFR and ShuffleNets #OTHEREFR .", "These networks are widely used on resource-constrained devices and have shown promise in practical applications.", "Besides that, various pruning strategies #OTHEREFR have also been proposed to reduce both the computational and storage burdens.", "However, these methods often incur accuracy degradation, making them less attractive for practical applications."], "citing_paper_content": {"title": "Bag Of Tricks With Quantized Convolutional Neural Networks For Image Classification", "abstract": "Deep neural networks have been proven effective in a wide range of tasks. However, their high computational and memory costs make them impractical to deploy on resourceconstrained devices. To address this issue, quantization schemes have been proposed to reduce the memory footprint and improve inference speed. While numerous quantization methods have been proposed, they lack systematic analysis for their effectiveness. To bridge this gap, we collect and improve existing quantization methods and propose a gold guideline for post-training quantization. We evaluate the effectiveness of our proposed method with two popular models, ResNet50 and MobileNetV2, on the ImageNet dataset. By following our guidelines, no accuracy degradation occurs even after directly quantizing the model to 8-bits without additional training. A quantization-aware training based on the guidelines can further improve the accuracy in lower-bits quantization. Moreover, we have integrated a multi-stage fine-tuning strategy that works harmoniously with existing pruning techniques to reduce cost even further. Remarkably, our results reveal that a quantized MobileNetV2 with 30% sparsity actually surpasses the performance of the equivalent full-precision model, underscoring the effectiveness and resilience of our proposed scheme."}, "cited_paper_content": {"title": "Mobilenets: Efficient Convolutional Neural Networks For Mobile Vision Applications", "abstract": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."}, "keywords": ["separated convolution"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of deploying CNN models on resource-constrained devices, researchers have proposed various methods such as efficient structure design, network pruning, and network quantization #REFR. The reference paper presents MobileNets, a class of efficient models based on depth-wise separable convolutions, which can be used to build lightweight deep neural networks for mobile and embedded vision applications.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To efficiently reduce the computational and memory costs of deploying CNN models on resource-constrained devices, the authors propose a quantization scheme that reduces the memory footprint and improves inference speed, which is demonstrated to be effective on popular models such as ResNet50 and MobileNetV2 #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To build light weight deep neural networks, MobileNets utilize depth-wise #REFR separable convolutions, which are highly effective in reducing computational and memory costs, making them practical for deployment on resource-constrained devices.", "given_citation_intent": "method", "given_keywords": ["separated convolution"]}]}
{"citing_id": "2304.02878v1", "cited_id": "1905.11968", "section_title": "=1", "citation": "Instead, #REFR generalizes ideas from NCBC and proposes an algorithm that selects the functional Steiner point of the work function.", "text_before_citation": ["where OPT is the offline optimal total path length.", "There are many works that combine the Steiner point algorithm for NCBC with existing control methods to perform learning-based online control for LTI systems, e.g., #OTHEREFR , #OTHEREFR , #OTHEREFR .", "2) General CBC: For general CBC problems, we can no longer take advantage of the nested property of the convex bodies.", "One may consider naively applying NCBC algorithms when the convex bodies happen to be nested and restarting the NCBC algorithm when they are not.", "However, due to the myopic nature of NCBC algorithms, which try to remain deep inside of each convex set, they no longer guarantee a competitive ratio when used this way."], "text_after_citation": [], "citing_paper_content": {"title": "Online Stabilization Of Unknown Linear Time-Varying Systems", "abstract": "This paper studies the problem of online stabilization of an unknown discrete-time linear time-varying (LTV) system under bounded non-stochastic (potentially adversarial) disturbances. We propose a novel algorithm based on convex body chasing (CBC). Under the assumption of infrequently changing or slowly drifting dynamics, the algorithm guarantees bounded-input-bounded-output stability in the closed loop. Our approach avoids system identification and applies, with minimal disturbance assumptions, to a variety of LTV systems of practical importance. We demonstrate the algorithm numerically on examples of LTV systems including Markov linear jump systems with finitely many jumps."}, "cited_paper_content": {"title": "Chasing Convex Bodies Optimally", "abstract": "In the chasing convex bodies problem, an online player receives a request sequence of $N$ convex sets $K_1,\\dots, K_N$ contained in a normed space $\\mathbb R^d$. The player starts at $x_0\\in \\mathbb R^d$, and after observing each $K_n$ picks a new point $x_n\\in K_n$. At each step the player pays a movement cost of $||x_n-x_{n-1}||$. The player aims to maintain a constant competitive ratio against the minimum cost possible in hindsight, i.e. knowing all requests in advance. The existence of a finite competitive ratio for convex body chasing was first conjectured in 1991 by Friedman and Linial. This conjecture was recently resolved with an exponential $2^{O(d)}$ upper bound on the competitive ratio. ::: In this paper, we drastically improve the exponential upper bound. We give an algorithm achieving competitive ratio $d$ for arbitrary normed spaces, which is exactly tight for $\\ell^{\\infty}$. In Euclidean space, our algorithm achieves nearly optimal competitive ratio $O(\\sqrt{d\\log N})$, compared to a lower bound of $\\sqrt{d}$. Our approach extends another recent work which chases nested convex bodies using the classical Steiner point of a convex body. We define the functional Steiner point of a convex function and apply it to the work function to obtain our algorithm."}, "keywords": ["functional Steiner point"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We propose a novel algorithm based on convex body chasing (CBC) #REFR to address the problem of online stabilization of an unknown discrete-time linear time-varying (LTV) system under bounded non-stochastic disturbances.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Convex body chasing has been used in various control methods for LTI systems, and the recent work by #REFR provides an algorithm achieving a competitive ratio of $d$ for arbitrary normed spaces, which is exactly tight for $\\ell^{\\infty}$.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve a competitive ratio of d for arbitrary normed spaces, the authors propose an algorithm based on the functional Steiner point of a convex function, which extends the work of chasing nested convex bodies using the classical Steiner point algorithm, as presented in #REFR.", "given_citation_intent": "background", "given_keywords": ["functional Steiner point"]}]}
{"citing_id": "2303.13434v1", "cited_id": "1702.08811", "section_title": "Related Work", "citation": "In addition, the central moment discrepancy (CMD) loss #REFR and maximum density divergence (MDD) loss [16] are also proposed to align the feature distributions.", "text_before_citation": ["Unsupervised Domain Adaptation.", "The prevailing UDA methods focus on domain alignment and learning discriminative domain-invariant features via metric learning, domain adversarial training, and optimal transport.", "Firstly, the metric learning-based methods aim to reduce the domain discrepancy by learning the domain-invariant feature representations using various metrics.", "For instance, some methods [14, 25, 26, #OTHEREFR use the maximum mean discrepancy (MMD) loss to measure the divergence between different domains."], "text_after_citation": ["Secondly, the domain adversarial training methods learn the domaininvariant representations to encourage samples from different domains to be non-discriminative with respect to the domain labels via an adversarial loss [13, #OTHEREFR .", "The third type of approach aims to minimize the cost transported from the source to the target distribution by finding an optimal coupling cost to mitigate the domain shift [6, 7] .", "Unfortunately, these methods are not robust enough for the noisy pseudo target labels for accurate domain alignment.", "Different from these mainstream UDA methods and [2] , we interpret the process of UDA as a min-max CE game and find the Nash Equilibria for domain alignment with an intermediate domain and a pure ViT-based solution. Mixup.", "It is an effective data augmentation technique to prevent models from over-fitting by linearly interpolating two input data."], "citing_paper_content": {"title": "Patch-Mix Transformer For Unsupervised Domain Adaptation: A Game Perspective", "abstract": "Endeavors have been recently made to leverage the vision transformer (ViT) for the challenging unsupervised domain adaptation (UDA) task. They typically adopt the cross-attention in ViT for direct domain alignment. However, as the performance of cross-attention highly relies on the quality of pseudo labels for targeted samples, it becomes less effective when the domain gap becomes large. We solve this problem from a game theory's perspective with the proposed model dubbed as PMTrans, which bridges source and target domains with an intermediate domain. Specifically, we propose a novel ViT-based module called PatchMix that effectively builds up the intermediate domain, i.e., probability distribution, by learning to sample patches from both domains based on the game-theoretical models. This way, it learns to mix the patches from the source and target domains to maximize the cross entropy (CE), while exploiting two semi-supervised mixup losses in the feature and label spaces to minimize it. As such, we interpret the process of UDA as a min-max CE game with three players, including the feature extractor, classifier, and PatchMix, to find the Nash Equilibria. Moreover, we leverage attention maps from ViT to re-weight the label of each patch by its importance, making it possible to obtain more domain-discriminative feature representations. We conduct extensive experiments on four benchmark datasets, and the results show that PM-Trans significantly surpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home, +1.4% on Office-31, and +17.7% on DomainNet, respectively. https: //vlis2022.github.io/cvpr23/PMTrans"}, "cited_paper_content": {"title": "Central Moment Discrepancy (Cmd) For Domain-Invariant Representation Learning", "abstract": "The learning of domain-invariant representations in the context of domain adaptation with neural networks is considered. We propose a new regularization method that minimizes the discrepancy between domain-specific latent feature representations directly in the hidden activation space. Although some standard distribution matching approaches exist that can be interpreted as the matching of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit order-wise matching of higher order moments has not been considered before. We propose to match the higher order central moments of probability distributions by means of order-wise moment differences. Our model does not require computationally expensive distance and kernel matrix computations. We utilize the equivalent representation of probability distributions by moment sequences to define a new distance function, called Central Moment Discrepancy (CMD). We prove that CMD is a metric on the set of probability distributions on a compact interval. We further prove that convergence of probability distributions on compact intervals w.r.t. the new metric implies convergence in distribution of the respective random variables. We test our approach on two different benchmark data sets for object recognition (Office) and sentiment analysis of product reviews (Amazon reviews). CMD achieves a new state-of-the-art performance on most domain adaptation tasks of Office and outperforms networks trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity analysis shows that the new approach is stable w.r.t. parameter changes in a certain interval. The source code of the experiments is publicly available."}, "keywords": ["feature distributions"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The metric learning-based methods aim to reduce the domain discrepancy by learning the domain-invariant feature representations using various metrics, such as Maximum Mean Discrepancy (MMD) loss used in some methods [14, 25, 26, #REFR].", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Some methods use the maximum mean discrepancy (MMD) loss to measure the divergence between different domains, but the proposed Central Moment Discrepancy (CMD) method in #REFR matches the higher order central moments of probability distributions by means of order-wise moment differences, achieving a new state-of-the-art performance on most domain adaptation tasks.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Some methods use the maximum mean discrepancy (MMD) loss to measure the divergence between different domains, but the proposed method in this manuscript matches the higher order central moments of probability distributions by means of order-wise moment differences, which is more efficient and effective for aligning feature distributions. #REFR", "given_citation_intent": "method", "given_keywords": ["feature distributions"]}]}
{"citing_id": "2304.05216v1", "cited_id": "1902.00751", "section_title": "Accelerating The Fine-Tuning Process", "citation": "For example, Houlsby et al.l #REFR design some adapters with two orders of magnitude fewer parameters to fine-tune compared to full models and achieve similar performance with fine-tuning all parameters of the pre-trained mode.", "text_before_citation": ["There are many studies on accelerating fine-tuning process #OTHEREFR . These studies can be roughly categorized into two categories.", "The first is to use the knowledge distillation technique to compress large-scale pre-trained language models #OTHEREFR .", "For example, Jiao et al.", "#OTHEREFR propose TinyBERT to distill BERT and only use about 28% parameter for natural language understanding.", "The second is the adapter-based fine-tuning approach #OTHEREFR , where adapters are new trainable modules added between layers of pre-trained models."], "text_after_citation": ["In addition, there are some studies on efficient neural network training from scratch with layer freezing #OTHEREFR . For example, Wang et al.", "#OTHEREFR leverage the knowledge from a reference model to accurately evaluate individual layers' training plasticity, freeze the converged ones and unfreeze the frozen layers to continue training.", "Our study could motivate researchers to come up with more efficient fine-tuning approaches."], "citing_paper_content": {"title": "Towards Efficient Fine-Tuning Of Pre-Trained Code Models: An Experimental Study And Beyond", "abstract": "Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. (3) Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. The extensive experimental results on five various downstream tasks demonstrate that training parameters and the corresponding time cost are greatly reduced, while performances are similar or better. CCS CONCEPTS \u2022 Software and its engineering \u2192 Software development techniques; Reusability."}, "cited_paper_content": {"title": "Parameter-Efficient Transfer Learning For Nlp", "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."}, "keywords": ["fine-tuning"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The second is the adapter-based fine-tuning approach, where adapters are new trainable modules added between layers of pre-trained models. As an alternative, the authors propose transfer with adapter modules, which yield a compact and extensible model, and attain near state-of-the-art performance on diverse text classification tasks with only a few parameters added per task #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Adapter-based fine-tuning approach has been proposed as an alternative to fine-tuning large pre-trained models, where adapters are new trainable modules added between layers of pre-trained models #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Fine-tuning large pre-trained models is a computationally expensive process, but an alternative approach is to use adapter modules that add only a few trainable parameters per task, as proposed by Houlsby et al. in their paper on parameter-efficient transfer learning for NLP #REFR.", "given_citation_intent": "background", "given_keywords": ["fine-tuning"]}]}
{"citing_id": "2304.09214v1", "cited_id": "1911.08251", "section_title": "Experimental Setup", "citation": "The architectures are inspired from the work of #REFR and are presented in a generic fashion in Table 2 .", "text_before_citation": ["In this work, we use again the implementation provided by the authors of E(2)-CNNs, who re-implement HNets in their own framework, for convenience.", "\u2022 Regarding our B-CNNs, four setups are considered to achieve SO(2) or O(2), with or without scale invariance (denoted by the presence or not of \"+\" in our tables and figures), with the computation of k max as described by Equation (4.7).", "Another setup for SO(2) invariance with a stronger cutoff frequency, that corresponds to half the initial k max , is also considered.", "This last setup is motivated by the empirical observation that it often leads to better performances.", "\u2022 Finally, a vanilla CNN with the same architecture than for the other methods, as well as a ResNet-18 #OTHEREFR are also trained for reference."], "text_after_citation": ["Note that the size of the filters is larger than conventional sizes in CNNs.", "The reason why it is preferable to increase the size of the filters in those cases is explained in Section 6.3.", "The same template architecture is used for all the methods (except for the ResNet-18 architecture that is kept unmodified) and data sets. Nonetheless, minor modifications are sometimes performed.", "Firstly, the number of filters in each convolutional layer should be adapted from one method to another, in order to keep the same number of trainable parameters.", "To do so, a parameter \u03bb is introduced to manually scale the number of filters and guarantee the same number of trainable parameters for all the methods."], "citing_paper_content": {"title": "So(2) And O(2) Equivariance In Image Recognition With Bessel-Convolutional Neural Networks", "abstract": "For many years, it has been shown how much exploiting equivariances can be beneficial when solving image analysis tasks. For example, the superiority of convolutional neural networks (CNNs) compared to dense networks mainly comes from an elegant exploitation of the translation equivariance. Patterns can appear at arbitrary positions and convolutions take this into account to achieve translation invariant operations through weight sharing. Nevertheless, images often involve other symmetries that can also be exploited. It is the case of rotations and reflections that have drawn particular attention and led to the development of multiple equivariant CNN architectures. Among all these methods, Bessel-convolutional neural networks (B-CNNs) exploit a particular decomposition based on Bessel functions to modify the key operation between images and filters and make it by design equivariant to all the continuous set of planar rotations. In this work, the mathematical developments of B-CNNs are presented along with several improvements, including the incorporation of reflection and multi-scale equivariances. Extensive study is carried out to assess the performances of B-CNNs compared to other methods. Finally, we emphasize the theoretical advantages of B-CNNs by giving more insights and in-depth mathematical details."}, "cited_paper_content": {"title": "General $E(2)$-Equivariant Steerable Cnns", "abstract": "The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of $E(2)$-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group $E(2)$ and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. $E(2)$-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions."}, "keywords": ["architectures", "generic fashion"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In this work, we use again the implementation provided by the authors of E(2)-CNNs, who re-implement HNets in their own framework, for convenience. Regarding our B-CNNs, four setups are considered to achieve SO(2) or O(2), with or without scale invariance (denoted by the presence or not of \"+\" in our tables and figures), with the computation of k max as described by Equation (4.7). Another setup for SO(2) invariance with a stronger cutoff frequency, that corresponds to half the initial k max, is also considered. This last setup is motivated by the empirical observation that it often leads to better performances. Finally, a vanilla CNN with the same architecture than for the other methods, as well as a ResNet-18 #REFR are also trained for reference.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve SO(2) or O(2) equivariance, the authors use the implementation provided by the authors of E(2)-CNNs, who re-implement HNets in their own framework, for convenience #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To implement a wide range of previously proposed and entirely new equivariant network architectures in a generic fashion, the authors of this work used the implementation provided by the authors of E(2)-CNNs, who re-implement HNets in their own framework, for convenience #REFR.", "given_citation_intent": "method", "given_keywords": ["architectures", "generic fashion"]}]}
{"citing_id": "2303.15742v2", "cited_id": "1908.09216", "section_title": "Experiments", "citation": "We compare our method with the existing dynamic pose estimation networks, i.e., DKD #REFR and Skip-Convolution [12] .", "text_before_citation": ["A trace of models' accuracy and delay for online videobased action recognition (left) and pose estimation (right).", "In the areas masked by the gray color, we can observe a rapid increase of the baseline model's delay (see the second row), which corresponds to the high-load status of the system.", "joints.", "Here, we consider three (i.e., m = 3) resolution candidates: [128, 192, 256] and follow the work of #OTHEREFR to use 3 deconvolutional layers to build the task head for pose estimation.", "Moreover, we replace LSTM with a ConvL-STM #OTHEREFR to accommodate for 2D feature maps."], "text_after_citation": ["Similarly, we re-implement DKD and Skip-Convolutions as there is no publicly available code. Experiment settings.", "To build dynamic systems with varying system status, we design multiple background processes (i.e., matrix calculators, video compressors, and large deep learning models) to occupy various amount of computational resources.", "We can then generate dynamic system load trajectories using these processes to simulate dynamic system load environments.", "During model training, we randomly generate dynamic system trajectories in each iteration to simulate various dynamic environments to train our SAN.", "During testing, for fair comparisons, we first generate 3 dynamic trajectories unseen in training, and then run all the evaluation experiments on these 3 fixed trajectories and report the average result."], "citing_paper_content": {"title": "System-Status-Aware Adaptive Network For Online Streaming Video Understanding", "abstract": "Recent years have witnessed great progress in deep neural networks for real-time applications. However, most existing works do not explicitly consider the general case where the device's state and the available resources fluctuate over time, and none of them investigate or address the impact of varying computational resources for online video understanding tasks. This paper proposes a Systemstatus-aware Adaptive Network (SAN) that considers the device's real-time state to provide high-quality predictions with low delay. Usage of our agent's policy improves efficiency and robustness to fluctuations of the system status. On two widely used video understanding tasks, SAN obtains state-of-the-art performance while constantly keeping processing delays low. Moreover, training such an agent on various types of hardware configurations is not easy as the labeled training data might not be available, or can be computationally prohibitive. To address this challenging problem, we propose a Meta Self-supervised Adaptation (MSA) method that adapts the agent's policy to new hardware configurations at test-time, allowing for easy deployment of the model onto other unseen hardware platforms."}, "cited_paper_content": {"title": "Dynamic Kernel Distillation For Efficient Pose Estimation In Videos", "abstract": "Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy."}, "keywords": ["estimation networks"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For pose estimation in videos, recent works have proposed various methods to reduce the computational cost. One such method is Dynamic Kernel Distillation (DKD) proposed by #REFR, which distills pose kernels via leveraging temporal cues from the previous frame and simplifies body joint localization into a matching procedure between the pose kernels and the current frame, resulting in a 10x flops reduction and 2x speedup over the previous best model.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the high computational cost of video-based human pose estimation, the authors propose a novel Dynamic Kernel Distillation (DKD) model that significantly lifts the efficiency by distilling pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the high computational cost of existing video-based human pose estimation methods, the authors propose a novel Dynamic Kernel Distillation (DKD) model that facilitates small networks for estimating human poses in videos, as demonstrated in #REFR, which could be a promising solution for efficient pose estimation networks.", "given_citation_intent": "method", "given_keywords": ["estimation networks"]}]}
{"citing_id": "2304.14484v1", "cited_id": "1612.00496", "section_title": "Vi. Conclusion", "citation": "In this project, we have successfully replicated the results from the original paper #REFR in terms of extracting the 3D bounding boxes from a single view for three different dataset categories.", "text_before_citation": [], "text_after_citation": ["We then extended the 3D bounding box accuracies, using the light-weight MobileNet-v2 and EfficientNet-v2 feature extractors, and later a similar performance boost is also observed with our modified multibin architecture.", "As a result, we conclude that MobileNet-v2 and EfficientNet-v2 outperform VGG-19 architecture in 3D bounding box estimation, from a single view of an RGB image."], "citing_paper_content": {"title": "Oricon3D: Effective 3D Object Detection Using Orientation And Confidence", "abstract": "We introduce a technique for detecting 3D objects and estimating their position from a single image. Our method is built on top of a similar state-of-the-art technique [1], but with improved accuracy. The approach followed in this research first estimates common 3D properties of an object using a Deep Convolutional Neural Network (DCNN), contrary to other frameworks that only leverage centre-point predictions. We then combine these estimates with geometric constraints provided by a 2D bounding box to produce a complete 3D bounding box. The first output of our network estimates the 3D object orientation using a discrete-continuous loss [1]. The second output predicts the 3D object dimensions with minimal variance. Here we also present our extensions by augmenting lightweight feature extractors and a customized multibin architecture. By combining these estimates with the geometric constraints of the 2D bounding box, we can accurately (or comparatively) determine the 3D object pose better than our baseline [1] on the KITTI 3D detection benchmark [2]."}, "cited_paper_content": {"title": "3D Bounding Box Estimation Using Deep Learning And Geometry", "abstract": "We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26]."}, "keywords": ["3D bounding boxes"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We combine estimates of common 3D object properties obtained using a deep convolutional neural network with geometric constraints provided by a 2D bounding box to produce a complete 3D bounding box, as proposed in the reference paper #REFR. The first output of our network estimates the 3D object orientation using a discrete-continuous loss, which has been shown to outperform the L2 loss.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24], as demonstrated on the challenging KITTI object detection benchmark [2] for 3D orientation estimation and the accuracy of the obtained 3D bounding boxes. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [4] and sub-category detection [23][24], achieving state-of-the-art results for 3D bounding boxes on the KITTI object detection benchmark [2] compared to the previous method [1]. #REFR", "given_citation_intent": "result", "given_keywords": ["3D bounding boxes"]}]}
{"citing_id": "2303.12255v1", "cited_id": "1612.00796", "section_title": "Background And Related Work", "citation": "When the model is updated to learn the current task, we observe catastrophic forgetting #REFR in the model due to deviation from the optimal parameters for the past learned tasks.", "text_before_citation": ["The reconstruction prior used in VAEs is local low-variance Bournourli distribution.", "This requirement can be relaxed to more global distributions seen in GAN [7] , or distributed throughout the full model as in diffusion models #OTHEREFR .", "Our method employs ideas of (i) and (iii).", "When used on a unimodal Gaussian VAE, it is a GMVAE with 2 d components, with deterministic assignment conditioned on the approximate posterior, and unimodal Gaussian reparameterization to distributed the mass in the latent embedding.", "Continual Learning aims to learn a set of sequentially arriving tasks using a shared model."], "text_after_citation": ["CL algorithms employ three primary strategies to mitigate catastrophic forgetting.", "One approach involves regularizing a fixed shared model, such that different information pathways are used to learn each task's weights #OTHEREFR .", "The aim is to identify critical model parameters that encode the learned knowledge of each task and consolidate these parameters while updating the model to learn new tasks.", "The downside is that the learning capacity of the model is compromised as more weights are consolidated.", "Another approach relies on model expansion #OTHEREFR , which involves adding new weights to a base model and customizing the network to learn new tasks via these additional weights."], "citing_paper_content": {"title": "Encoding Binary Concepts In The Latent Space Of Generative Models For Enhancing Data Representation", "abstract": "Binary concepts 1 are empirically used by humans to generalize efficiently. And they are based on Bernoulli distribution which is the building block of information. These concepts span both low-level and high-level features such as \"large vs small\" and \"a neuron is active or inactive\". Binary concepts are ubiquitous features and can be used to transfer knowledge to improve model generalization. We propose a novel binarized regularization to facilitate learning of binary concepts to improve the quality of data generation in autoencoders. We introduce a binarizing hyperparameter r in data generation process to disentangle the latent space symmetrically. We demonstrate that this method can be applied easily to existing variational autoencoder (VAE) variants to encourage symmetric disentanglement, improve reconstruction quality, and prevent posterior collapse without computation overhead. We also demonstrate that this method can boost existing models to learn more transferable representations and generate more representative samples for the input distribution which can alleviate catastrophic forgetting using generative replay under continual learning settings."}, "cited_paper_content": {"title": "Overcoming Catastrophic Forgetting In Neural Networks", "abstract": "The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially."}, "keywords": ["past learned tasks", "catastrophic forgetting"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Continual Learning aims to learn a set of sequentially arriving tasks using a shared model. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks, which has been shown to overcome catastrophic forgetting in neural networks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To overcome the issue of catastrophic forgetting in neural networks, the authors of this manuscript cite the reference paper \"Overcoming Catastrophic Forgetting In Neural Networks\" which proposes a method to selectively slow down learning on the weights important for old tasks to maintain expertise on them for a long time. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Catastrophic forgetting, which has been widely thought to be an inevitable feature of connectionist models, can be overcome by selectively slowing down learning on the weights important for past learned tasks, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["past learned tasks", "catastrophic forgetting"]}]}
{"citing_id": "2304.10828v1", "cited_id": "2002.07738", "section_title": "Introduction", "citation": "Consequently, albeit being defined on the full input space and over a fairness similarity metric, because of its worst-case nature, IF has been linked to adversarial robustness #REFR .", "text_before_citation": ["Several calls for caution have, however, recently been raised about their deployment in tasks where fairness is of concern #OTHEREFR .", "In fact, Neural Networks (NNs) have been found to reinforce negative biases from sensitive datasets #OTHEREFR , discriminating against individuals on the basis of attributes such as gender or race.", "To address this, research efforts have been directed at both measuring the fairness of NNs, their de-biased training, as well as defining precise notions of fairness.", "Given a BNN f and a similarity metric between individuals d f air , which encodes a task-dependent notion of similarity #OTHEREFR , Individual Fairness (IF) enforces that all pairs of similar individuals in the input space get treated similarly by f #OTHEREFR .", "As opposed to the statistical nature of group fairness #OTHEREFR , IF aims at computing worst-case bias measures over a model input space."], "text_after_citation": ["As it has recently been show that Bayesian Neural Networks (BNNs) have a tendency to be less fragile to adversarial attacks than their frequentist counter-parts #OTHEREFR , it is natural to wonder whether approximate Bayesian inference may also have a positive impact over the IF of a neural network.", "However, to the best of our knowledge, no work has been conducted along these lines of inquire.", "In this paper, we investigate the IF of BNNs and empirically evaluate it on various benchmarks.", "While exact computations of IF in BNNs is infeasible due to their non-convexity, we exploit the relationship between IF and adversarial robustness #OTHEREFR to develop a framework for the adaptation of adversarial attack methods for IF.", "In particular, we explicitly instantiate Fair-FGSM and Fair-PGD as extensions of their corresponding adversarial attacks #OTHEREFR by employing gradient steps modifications and projections specific to d f air metrics commonly used in the fairness literature."], "citing_paper_content": {"title": "Individual Fairness In Bayesian Neural Networks", "abstract": "We study Individual Fairness (IF) for Bayesian neural networks (BNNs). Specifically, we consider the-\u03b4-individual fairness notion, which requires that, for any pair of input points that are-similar according to a given similarity metrics, the output of the BNN is within a given tolerance \u03b4 > 0. We leverage bounds on statistical sampling over the input space and the relationship between adversarial robustness and individual fairness to derive a framework for the systematic estimation of-\u03b4-IF, designing Fair-FGSM and Fair-PGD as global, fairness-aware extensions to gradient-based attacks for BNNs. We empirically study IF of a variety of approximately inferred BNNs with different architectures on fairness benchmarks, and compare against deterministic models learnt using frequentist techniques. Interestingly, we find that BNNs trained by means of approximate Bayesian inference consistently tend to be markedly more individually fair than their deterministic counterparts."}, "cited_paper_content": {"title": "Individual Fairness Revisited: Transferring Techniques From Adversarial Robustness", "abstract": "We turn the definition of individual fairness on its head---rather than ascertaining the fairness of a model given a predetermined metric, we find a metric for a given model that satisfies individual fairness. This can facilitate the discussion on the fairness of a model, addressing the issue that it may be difficult to specify a priori a suitable metric. Our contributions are twofold: First, we introduce the definition of a minimal metric and characterize the behavior of models in terms of minimal metrics. Second, for more complicated models, we apply the mechanism of randomized smoothing from adversarial robustness to make them individually fair under a given weighted $L^p$ metric. Our experiments show that adapting the minimal metrics of linear models to more complicated neural networks can lead to meaningful and interpretable fairness guarantees at little cost to utility."}, "keywords": ["fairness similarity metric", "adversarial robustness"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of fairness in neural networks, researchers have proposed the notion of Individual Fairness (IF), which enforces that all pairs of similar individuals in the input space get treated similarly by the model. In their work, the authors of #REFR turn the definition of individual fairness on its head by finding a metric for a given model that satisfies individual fairness, which can facilitate the discussion on the fairness of a model, addressing the issue that it may be difficult to specify a priori a suitable metric.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of fairness in neural networks, recent research has focused on defining precise notions of fairness, such as Individual Fairness (IF), which enforces that all pairs of similar individuals in the input space get treated similarly by the model, and techniques such as randomized smoothing from adversarial robustness have been applied to make models individually fair under a given weighted $L^p$ metric #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of fairness in neural networks, research efforts have been directed at defining precise notions of fairness, such as Individual Fairness (IF), which enforces that all pairs of similar individuals in the input space get treated similarly by the model, given a BNN and a similarity metric between individuals #OTHEREFR. The reference paper provides a method to make models individually fair under a given weighted $L^p$ metric by applying the mechanism of randomized smoothing from adversarial robustness #REFR.", "given_citation_intent": "background", "given_keywords": ["fairness similarity metric", "adversarial robustness"]}]}
{"citing_id": "2304.02693v1", "cited_id": "1711.09856", "section_title": "5.2.2.", "citation": "Overall, HRNet is the most robust against the PGD and CR-PGD attacks, in that it has the smallest PixAcc drop when increases. On the other hand, PSPNet is the most vulnerable. We note that #REFR has similar observations. Running time comparison.", "text_before_citation": ["This is because neighboring pixels with small certified radii can easily affect each other, which naturally forms the groups in the certified radius map (also see Figure 1 (i)).", "Figure 5 verifies this intuition, where we test 10 random testing images in Pascal VOC.", "We can see that a majority of the perturbations in CR-PGD are assigned to the pixels with relatively smaller certified radii, in order to wrongly predict more pixels.", "In contrast, most of the perturbations in PGD are assigned to the pixels with relatively larger certified radii.", "As wrongly predicting these pixels requires a larger perturbation, PGD misclassifies much fewer pixels than our CR-PGD. Different models have different robustness."], "text_after_citation": ["Over all testing images in the three datasets, the average time of CR-PGD is 4.0 seconds, while that of PGD is 3.6 seconds. The overhead of CR-PGD over PGD is 11%."], "citing_paper_content": {"title": "A Certified Radius-Guided Attack Framework To Image Segmentation Models", "abstract": "Image segmentation is an important problem in many safety-critical applications such as medical imaging and autonomous driving. Recent studies show that modern image segmentation models are vulnerable to adversarial perturbations, while existing attack methods mainly follow the idea of attacking image classification models. We argue that image segmentation and classification have inherent differences, and design an attack framework specially for image segmentation models. Our goal is to thoroughly explore the vulnerabilities of modern segmentation models, i.e., aiming to misclassify as many pixels as possible under a perturbation budget in both white-box and black-box settings. Our attack framework is inspired by certified radius, which was originally used by defenders to defend against adversarial perturbations to classification models. We are the first, from the attacker perspective, to leverage the properties of certified radius and propose a certified radius guided attack framework against image segmentation models. Specifically, we first adapt randomized smoothing, the state-of-theart certification method for classification models, to derive the pixel's certified radius. A larger certified radius of a pixel means the pixel is theoretically more robust to adversarial perturbations. This observation inspires us to focus more on disrupting pixels with relatively smaller certified radii. Accordingly, we design a pixel-wise certified radius guided loss, when plugged into any existing white-box attack, yields our certified radius-guided white-box attack. Next, we propose the first black-box attack to image segmentation models via bandit. A key challenge is no gradient information is available. To address it, we design a novel gradient estimator, based on bandit feedback, which is queryefficient and provably unbiased and stable. We use this gradient estimator to design a projected bandit gradient descent (PBGD) attack. We further use pixels' certified radii and design a certified radius-guided PBGD (CR-PBGD) attack. We prove our PBGD and CR-PBGD attacks can achieve asymptotically optimal attack performance with an optimal rate. We evaluate our certified-radius guided white-box and black-box attacks on multiple modern image segmentation models and datasets. Our results validate the effectiveness of our certified radius-guided attack framework."}, "cited_paper_content": {"title": "On The Robustness Of Semantic Segmentation Models To Adversarial Attacks", "abstract": "Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness."}, "keywords": ["CR-PGD attacks"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Different models have different robustness, and recent studies have shown that modern image segmentation models are vulnerable to adversarial perturbations. To address this issue, #REFR present a rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets, and analyze the effect of different network architectures, model capacity, and multiscale processing.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' findings that \"many observations made on the task of classification do not always transfer to this more complex task\" align with our observations that image segmentation and classification have inherent differences, as discussed in our attack framework (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of the manuscript demonstrate the effectiveness of their certified radius-guided white-box and black-box attacks against modern image segmentation models, showing that their CR-PGD attacks assign perturbations to pixels with relatively smaller certified radii, leading to more wrongly predicted pixels compared to PGD attacks (#REFR).", "given_citation_intent": "result", "given_keywords": ["CR-PGD attacks"]}]}
{"citing_id": "2303.14771v1", "cited_id": "1908.04742", "section_title": "Evaluations On Task-Incremental Setting", "citation": "We evaluate Split-CIFAR100, Split-MiniImagenet, and Ima-geNet32 using the protocol from #REFR with 100 training epochs training per task.", "text_before_citation": [], "text_after_citation": ["We report the mean and standard error over 3 runs.", "Split-CIFAR100 and Split-MiniImageNet We consider Split-CIFAR100 and Split-MiniImageNet with 20 tasks of 5 classes each.", "The results can be found in Figure 2 , for Split-CIFAR100 and Split-MiniImageNet, using different buffer sizes for ER.", "In this setting, we can observe that our Figure 4 . Class-incremental accuracy on 20-Task Split-CIFAR100(left) and Split-MiniImageNet(right).", "We observe that PRD outperforms not only other replay-free baselines but also ER, M=5, and is on par with ER, M=20, without storing any data."], "citing_paper_content": {"title": "Prototype-Sample Relation Distillation: Towards Replay-Free Continual Learning", "abstract": "In Continual learning (CL) balancing effective adaptation while combating catastrophic forgetting is a central challenge. Many of the recent best-performing methods utilize various forms of prior task data, e.g. a replay buffer, to tackle the catastrophic forgetting problem. Having access to previous task data can be restrictive in many real-world scenarios, for example when task data is sensitive or proprietary. To overcome the necessity of using previous tasks data, in this work, we start with strong representation learning methods that have been shown to be less prone to forgetting. We propose a holistic approach to jointly learn the representation and class prototypes while maintaining the relevance of old class prototypes and their embedded similarities. Specifically, samples are mapped to an embedding space where the representations are learned using a supervised contrastive loss. Class prototypes are evolved continually in the same latent space, enabling learning and prediction at any point. To continually adapt the prototypes without keeping any prior task data, we propose a novel distillation loss that constrains class prototypes to maintain relative similarities as compared to new task data. This method yields state-of-the-art performance in the task-incremental setting where we are able to outperform other methods that both use no data as well as approaches relying on large amounts of data. Our method is also shown to provide strong performance in the class-incremental setting without using any stored data points."}, "cited_paper_content": {"title": "Online Continual Learning With Maximally Interfered Retrieval", "abstract": "Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or \"single-pass through the data\" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work, we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally_Interfered_Retrieval."}, "keywords": ["100 training epochs"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In their work on replay-free continual learning, the authors propose a novel approach to jointly learn representation and class prototypes, which yields state-of-the-art performance in task-incremental and class-incremental settings without using any stored data points. To tackle the catastrophic forgetting problem, many recent best-performing methods utilize various forms of prior task data, such as a replay buffer. However, this can be restrictive in real-world scenarios where task data is sensitive or proprietary. To overcome this limitation, the authors start with strong representation learning methods that have been shown to be less prone to forgetting. They propose a controlled sampling of memories for replay, where the samples which are most interfered are retrieved, and show consistent gains in performance and greatly reduced forgetting. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the suboptimal random selection of samples from the replay memory or from a generative model, the authors propose a controlled sampling of memories for replay by retrieving the samples which are most interfered, as presented in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of suboptimal random selection of samples from a replay memory or a generative model, the authors of this manuscript propose a controlled sampling of memories for replay based on the most interfered samples, as presented in the reference paper #REFR, which consistently improves performance and reduces forgetting, even after 100 training epochs.", "given_citation_intent": "method", "given_keywords": ["100 training epochs"]}]}
{"citing_id": "2304.03191v1", "cited_id": "1504.05477", "section_title": "Sharper Krylov Subspace Algorithms", "citation": "We do this by explicity analyzing the Chebyshev polynomial (as opposed to a polynomial approximation to a threshold function in #REFR ) and demonstrate that the output of Algorithm 2.2 is at least as good as outputting the aformentioned vector v (see Lemma 7.7 for details).", "text_before_citation": ["A (1 + \u03b5) relative-error solution to the above cost corresponds to an additive \u03b5 2/3 error.", "The standard analysis of Krylov iteration #OTHEREFR , states that after q = log(n)/ \u221a \u03b6 iterations, for any 0 < \u03b6 < 1, the algorithm outputs a vector v such that v A Av A 2 op \u2212 \u03b6\u03c3 2 2 . By Pythagorean theorem,", "A I \u2212 vv 2 F = A 2 op \u2212 Av 2 2 min u 2 =1 A I \u2212 uu + \u03c3 2 2 \u03b6.", "Since \u03c3 2 2 1, it suffices to set \u03b6 = \u03b5 2/3 and thus q = O(log(n)/\u03b5 1/3 ) iterations suffice.", "We strengthen this analysis by showing that a significantly lower dimensional Krylov subspace (corresponding to O log(1/\u03b5)/\u03b5 1/3 iterations) spans a vector v such that A I \u2212 vv 2 F min u 2 =1 A I \u2212 uu + \u03b5 2/3 ."], "text_after_citation": ["In the complementary case, we deviate significantly from any prior analysis of Krylov iteration, including #OTHEREFR .", "Here, we know that the number of singular values in the range [1/2, 1 \u2212 \u03b5] is at most O(\u03b5 \u22121/3 ).", "We therefore construct an entirely different polynomial, which is no longer based on Chebyshev polynomials.", "This polynomial is designed to explicitly zero out all singular values in the range [1/2, 1 \u2212 \u03b5].", "We note that the degree of this polynomial, p 0 , is only O(\u03b5 \u22121/3 ), and it allows us to remove the contribution of all medium sized singular values, similar to starting with a larger block size."], "citing_paper_content": {"title": "Krylov Methods Are (Nearly) Optimal For Low-Rank Approximation", "abstract": "We consider the problem of rank-1 low-rank approximation (LRA) in the matrix-vector product model under various Schatten norms: min u 2 =1 A(I \u2212 uu) S p , where M S p denotes the p norm of the singular values of M. Given \u03b5 > 0, our goal is to output a unit vector v such that A I \u2212 vv S p (1 + \u03b5) min u 2 =1 A I \u2212 uu S p. Our main result shows that Krylov methods (nearly) achieve the information-theoretically optimal 1 number of matrix-vector products for Spectral (p = \u221e), Frobenius (p = 2) and Nuclear (p = 1) LRA. In particular, for Spectral LRA, we show that any algorithm requires \u2126 log(n)/\u03b5 1/2 matrix-vector products, exactly matching the upper bound obtained by Krylov methods [MM15]. Our lower bound addresses Open Question 1 in [Woo14], providing evidence for the lack of progress on algorithms for Spectral LRA and resolves Open Question 1.2 in [BCW22]. Next, we show that for any fixed constant p, i.e. 1 p = O(1), there is an upper bound of O log(1/\u03b5)/\u03b5 1/3 matrix-vector products, implying that the complexity does not grow as a function of input size. This improves the O log(n/\u03b5)/\u03b5 1/3 bound recently obtained in [BCW22], and matches their \u2126 1/\u03b5 1/3 lower bound, to a log(1/\u03b5) factor."}, "cited_paper_content": {"title": "Randomized Block Krylov Methods For Stronger And Faster Approximate Singular Value Decomposition", "abstract": "Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko, Martinsson, and Tropp, randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for any matrix, independently of singular value gaps. After $\\tilde{O}(1/\\epsilon)$ iterations, it gives a low-rank approximation within $(1+\\epsilon)$ of optimal for spectral norm error. ::: We give the first provable runtime improvement on Simultaneous Iteration: a simple randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just $\\tilde{O}(1/\\sqrt{\\epsilon})$ iterations and performs substantially better experimentally. Despite their long history, our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice. ::: Furthermore, while it is a simple accuracy benchmark, even $(1+\\epsilon)$ error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and a minor modification of Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods. ::: Finally, we give insight beyond the worst case, justifying why both algorithms can run much faster in practice than predicted. We clarify how simple techniques can take advantage of common matrix properties to significantly improve runtime."}, "keywords": ["polynomial approximation", "Algorithm"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We strengthen this analysis by showing that a significantly lower dimensional Krylov subspace (corresponding to O(log(1/\u03b5)/\u03b5^(1/3)) iterations) spans a vector v such that A(I - vv)^2_F <= min_u||A(I - uu)||^2_F + \u03b5^(2/3) (1+#REFR).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The standard analysis of Krylov iteration states that after q = log(n)/ \u221a \u03b6 iterations, for any 0 < \u03b6 < 1, the algorithm outputs a vector v such that v A Av A 2 op \u2212 \u03b6\u03c3 2 2, and this method has been shown to be more accurate than simpler sketching algorithms #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The standard analysis of Krylov iteration states that after a polynomial number of iterations, the algorithm outputs a vector v such that A I \u2212 vv 2 F min u 2 =1 A I \u2212 uu + \u03b5 2/3, and this result has been improved by the Algorithm proposed in #REFR for polynomial approximation.", "given_citation_intent": "background", "given_keywords": ["polynomial approximation", "Algorithm"]}]}
{"citing_id": "2304.14299v1", "cited_id": "1905.03244", "section_title": "Related Work", "citation": "To relax this heavy reliance on the parameter space, some approaches directly regress 3D positions of mesh vertices instead of predicting the model's parameters. Among these approaches, Kolotouros et al. #REFR and Hongsuk et al.", "text_before_citation": ["The fitted mesh is then used as a supervisory signal to train a feed-forward network with a mesh convolutional decoder. Spurr et al.", "#OTHEREFR introduce biomechanical constraints to guide the network to predict feasible hand poses with weakly-annotated real-world data. Chen et al.", "#OTHEREFR use 2D joints extracted from an off-the-shelf 2D pose estimator as a supervisory signal to train a modelbased autoencoder to estimate 3D hand pose and shape.", "However, similar to the model based approaches, they do not exploit correlation between joints and mesh vertices, yet our proposed AMVUR model addresses this issue and improves the feature representation of joints and vertices.", "Model-free Methods: Although hand parametric models such as MANO serve as a strong structural prior to support 3D hand reconstruction, help to handle severe occlusions and help to accommodate weakly-annotated data, approaches that rely on this can easily get stuck in the model's parameter space, resulting in a non-minimal representation problem #OTHEREFR ."], "text_after_citation": ["#OTHEREFR combine an image-based CNN and a GraphCNN to estimate human mesh coordinates directly. Lin et al.", "#OTHEREFR argue that GraphCNN can only capture the local interactions between neighboring vertices of the triangle mesh, so they use a self-attention mechanism to capture global interactions between the vertices and joints. Most recently, Hampali et al.", "#OTHEREFR first extract joint features by localizing them on CNN feature maps, then take these features and their spatial encodings as the input to a transformer model for 3D hand pose estimation.", "However, spatial encoding is ambiguous to describe joints' 3D locations, especially for overlapping 3D joints in 2D images.", "Different from the above approaches, in AMVUR, a cross-attention module is proposed to learn the correlation between joints and mesh vertices, followed by a self-attention module to learn the correlation between different vertices."], "citing_paper_content": {"title": "A Probabilistic Attention Model With Occlusion-Aware Texture Regression For 3D Hand Reconstruction From A Single Rgb Image", "abstract": "Recently, deep learning based approaches have shown promising results in 3D hand reconstruction from a single RGB image. These approaches can be roughly divided into model-based approaches, which are heavily dependent on the model's parameter space, and model-free approaches, which require large numbers of 3D ground truths to reduce depth ambiguity and struggle in weakly-supervised scenarios. To overcome these issues, we propose a novel probabilistic model to achieve the robustness of model-based approaches and reduced dependence on the model's parameter space of model-free approaches. The proposed probabilistic model incorporates a model-based network as a prior-net to estimate the prior probability distribution of joints and vertices. An Attention-based Mesh Vertices Uncertainty Regression (AMVUR) model is proposed to capture dependencies among vertices and the correlation between joints and mesh vertices to improve their feature representation. We further propose a learning based occlusionaware Hand Texture Regression model to achieve highfidelity texture reconstruction. We demonstrate the flexibility of the proposed probabilistic model to be trained in both supervised and weakly-supervised scenarios. The experimental results demonstrate our probabilistic model's stateof-the-art accuracy in 3D hand and texture reconstruction from a single image in both training schemes, including in the presence of severe occlusions."}, "cited_paper_content": {"title": "Convolutional Mesh Regression For Single-Image Human Shape Reconstruction", "abstract": "This paper addresses the problem of 3D human pose and shape estimation from a single image. Previous approaches consider a parametric model of the human body, SMPL, and attempt to regress the model parameters that give rise to a mesh consistent with image evidence. This parameter regression has been a very challenging task, with model-based approaches underperforming compared to nonparametric solutions in terms of pose estimation. In our work, we propose to relax this heavy reliance on the model's parameter space. We still retain the topology of the SMPL template mesh, but instead of predicting model parameters, we directly regress the 3D location of the mesh vertices. This is a heavy task for a typical network, but our key insight is that the regression becomes significantly easier using a Graph-CNN. This architecture allows us to explicitly encode the template mesh structure within the network and leverage the spatial locality the mesh has to offer. Image-based features are attached to the mesh vertices and the Graph-CNN is responsible to process them on the mesh structure, while the regression target for each vertex is its 3D location. Having recovered the complete 3D geometry of the mesh, if we still require a specific model parametrization, this can be reliably regressed from the vertices locations. We demonstrate the flexibility and the effectiveness of our proposed graph-based mesh regression by attaching different types of features on the mesh vertices. In all cases, we outperform the comparable baselines relying on model parameter regression, while we also achieve state-of-the-art results among model-based pose estimation approaches."}, "keywords": ["mesh vertices"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Chen et al. #REFR propose to use 2D joints extracted from an off-the-shelf 2D pose estimator as a supervisory signal to train a model-based autoencoder to estimate 3D hand pose and shape, but they do not exploit correlation between joints and mesh vertices, yet our proposed AMVUR model addresses this issue and improves the feature representation of joints and vertices.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of heavy reliance on the model's parameter space in model-based approaches for 3D human pose and shape estimation, the authors propose a graph-based mesh regression method that directly regresses the 3D location of the mesh vertices, which outperforms comparable baselines relying on model parameter regression, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Chen et al. use a model-based autoencoder to estimate 3D hand pose and shape, but they do not exploit the correlation between joints and #REFR mesh vertices, which is addressed in our proposed AMVUR model to improve the feature representation of joints and vertices.", "given_citation_intent": "method", "given_keywords": ["mesh vertices"]}]}
{"citing_id": "2303.17600v1", "cited_id": "1910.10897", "section_title": "The Stretch Pick-And-Place Benchmark", "citation": "With perfect execution, success can generally be achieved within 50 steps, thisis similar to other short-horizon, continuousspace, manipulation tasks #REFR . See Appendix. B.1 and Table.", "text_before_citation": ["We also include proprioceptive sensors corresponding to the agent's arm position.", "The arm of a Stretch RE1 agent uses a telescoping mechanism to move forward and back, may move up and down, and the gripper has one rotational degree of freedom allowing for changes in yaw, see Figures 2 and 3 for 3rd person views of the stretch robot.", "The robotic arm of the Stretch RE1 robot is orthogonal to the agent's forward and backward movement and so, to move the arm laterally, the agent must move its body in the forward and backward direction.", "To highlight the study of irreversible transitions in our benchmark, and as to not add additional complexity to STRETCH-P&P, we restrict the robot body to not rotate in training, although the wrist of the agent may do so.", "The maximum rotation for the wrist is 2 \u2022 per step, and horizontal/vertical arm movement is limited to 5cm per step."], "text_after_citation": ["B.1 for further details regarding the observation and action spaces."], "citing_paper_content": {"title": "When Learning Is Out Of Reach, Reset: Generalization In Autonomous Visuomotor Reinforcement Learning", "abstract": "Figure 1: Episodic, Reset-Free, and Reset-Minimizing RL. In standard (i.e. episodic) reinforcement learning (RL) agents have their environments reset after every success or failure, an expensive operation in the real world. In Reset-Free RL (RF-RL), researchers have designed \"reset games\" which allow for learning so long as special care is taken to avoid irreversible transitions (e.g. an apple falling out of reach). We consider Reset-Minimizing RL (RM-RL) where in realistic and dynamic environments agents may request human interventions but should minimize these requests."}, "cited_paper_content": {"title": "Meta-World: A Benchmark And Evaluation For Multi-Task And Meta Reinforcement Learning", "abstract": "Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods."}, "keywords": ["short-horizon, continuousspace, manipulation", "tasks"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To highlight the study of irreversible transitions in our benchmark, and as to not add additional complexity to STRETCH-P&P, we restrict the robot body to not rotate in training, although the wrist of the agent may do so (Meta-World: A Benchmark And Evaluation For Multi-Task And Meta Reinforcement Learning).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The benchmark proposed in this study for meta-reinforcement learning and multi-task learning, consisting of 50 distinct robotic manipulation tasks, provides a broad task distribution that enables generalization to new behaviors, as opposed to the narrow task distributions used in current research, as noted in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The benchmark proposed in this paper evaluates meta-reinforcement learning and multi-task learning algorithms on 50 distinct robotic manipulation #KEYWORDS(manipulation, tasks) tasks, and the authors found that while each task and its variations can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks, which highlights the difficulty of generalizing to #KEYWORDS(short-horizon, continuous-space) short-horizon and continuous-space tasks. #REFR", "given_citation_intent": "result", "given_keywords": ["short-horizon, continuousspace, manipulation", "tasks"]}]}
{"citing_id": "2303.12710v2", "cited_id": "1705.04058", "section_title": "Introduction", "citation": "Artistic style transfer, as an efficient way to create a new painting by combining the content of a natural images and the style of an existing painting image, is a major research topic in computer graphics and computer vision #REFR .", "text_before_citation": ["If a picture is worth a thousand words, then an artwork may tell the whole story.", "The art style depicts the visual appearance of an artwork and characterizes how the artist expresses a theme and shows his/her creativity.", "The features that identify an artwork, such as the artist's use of stroke, color and composition, determine the style."], "text_after_citation": ["The main challenges of arbitrary style transfer are extracting styles from artistic images and mapping a specific realistic image into an artistic one in a controllable way.", "The core problem for style extraction is to find an effective representation of styles since it is in general hard to give explicit definitions across different styles.", "To build a reasonable style feature space, it is necessary to explore the relationship and distribution of styles in order to capture both individual and holistic characteristics.", "For the mapping process, several generative mechanisms are adopted to address different issues, such as auto-encoder #OTHEREFR , neural flow model #OTHEREFR and visual transformer #OTHEREFR .", "In contrast to the goal of those methods, we propose to improve arbitrary style transfer via a unified framework that offers the guidance of proper artistic style representation and works for various generative backbones."], "citing_paper_content": {"title": "A Unified Arbitrary Style Transfer Framework Via Adaptive Contrastive Learning", "abstract": "Flow-based backbone [An et al. 2021] in UCAST ViT-based backbone [Deng et al. 2022] in UCAST CNN-based backbone [Huang and Belongie 2017] in UCAST (b) Aquarelle (d) Aquarelle (k) Impressionism (g) Line Art (h) Ink and Wash (i) Impressionism (j) Ink and Wash"}, "cited_paper_content": {"title": "Neural Style Transfer: A Review", "abstract": "The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at this https URL."}, "keywords": ["Artistic style transfer"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The art style of an artwork is determined by its unique features such as the artist's use of stroke, color, and composition. Neural Style Transfer (NST) is a powerful technique that uses Convolutional Neural Networks (CNNs) to render a content image in different styles, as demonstrated by Gatys et al. in their seminal work. Since then, NST has become a trending topic in both academic literature and industrial applications, with a variety of approaches proposed to improve or extend the original algorithm. #REFR provides a comprehensive overview of the current progress towards NST, including a taxonomy of current algorithms, evaluation methods, and comparison results.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The style of an artwork is determined by the artist's use of stroke, color, and composition, which are the features that identify an artwork and characterize its style. Neural Style Transfer (NST) is a process that uses Convolutional Neural Networks (CNNs) to render a content image in different styles, and it has become a trending topic in both academic literature and industrial applications [Gatys et al.]. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The identification of artistic style transfer features, such as stroke, color, and composition, has enabled the development of various approaches to render a content image in different styles, including the popular Neural Style Transfer (NST) algorithm [#REFR].", "given_citation_intent": "background", "given_keywords": ["Artistic style transfer"]}]}
{"citing_id": "2303.09930v1", "cited_id": "1905.02249", "section_title": "Ii. Related Work", "citation": "The proposed framework addresses this issue by simultaneously training and optimizing OOD detection, and a MixMatch #REFR based semi-SL in a multitask learning framework.", "text_before_citation": ["3) ReMixMatch: ReMixMatch #OTHEREFR enhances the Mix-Match #OTHEREFR framework by introducing the principles of distribution alignment and augmentation anchoring.", "The notion of distribution alignment replaces the sharpening step of the MixMatch #OTHEREFR framework in an attempt to match model aggregated class prediction to that of the marginal distribution of the given ground truth.", "Further, ReMixMatch #OTHEREFR introduces another principle of augmentation alignment in place of the consistency regularization step of MixMatch #OTHEREFR to encourage each output to be close to the prediction for a weakly-augmented version of the same input.", "By incorporating these changes, ReMixMatch #OTHEREFR has been reported to be data efficient compared to MixMatch #OTHEREFR frameworks.", "4) Multi-task Curriculum learning (MTL) Framework Guided Semi-SL: Unlike previous methods that focused on developing strong semi-SL models, the goal of MTL #OTHEREFR guided semi-SL is to explicitly address the issues associated with the presence of open-set samples in semi-SL."], "text_after_citation": ["The paper proposes a novel OOD detector based on the model's capacity to identify noisy labeled training data.", "The OOD detector ensures that the subsequent semi-SL framework, based on MixMatch #OTHEREFR , is only trained with the inliers samples by selecting an appropriate threshold on the OOD score and filtering out the outlier samples from the unlabelled data.", "The method significantly produced superior results in settings where samples from a different distribution contaminates the unlabelled data.", "Along with these frameworks, a few studies have also reexamined the original MixMatch #OTHEREFR algorithm itself.", "One such study found that the performance degradation caused by open-set samples in unlabelled data is primarily due to the Pseudo-Labelling (PL) task of MixMatch."], "citing_paper_content": {"title": "Robust Semi-Supervised Learning For Histopathology Images Through Self-Supervision Guided Out-Of-Distribution Scoring", "abstract": "Semi-supervised learning (semi-SL) is a promising alternative to supervised learning for medical image analysis when obtaining good quality supervision for medical imaging is difficult. However, semi-SL assumes that the underlying distribution of unaudited data matches that of the few labeled samples, which is often violated in practical settings, particularly in medical images. The presence of out-of-distribution (OOD) samples in the unlabeled training pool of semi-SL is inevitable and can reduce the efficiency of the algorithm. Common preprocessing methods to filter out outlier samples may not be suitable for medical images that involve a wide range of anatomical structures and rare morphologies. In this paper, we propose a novel pipeline for addressing open-set supervised learning challenges in digital histology images. Our pipeline efficiently estimates an OOD score for each unlabelled data point based on self-supervised learning to calibrate the knowledge needed for a subsequent semi-SL framework. The outlier score derived from the OOD detector is used to modulate sample selection for the subsequent semi-SL stage, ensuring that samples conforming to the distribution of the few labeled samples are more frequently exposed to the subsequent semi-SL framework. Our framework is compatible with any semi-SL framework, and we base our experiments on the popular Mixmatch semi-SL framework. We conduct extensive studies on two digital pathology datasets, Kather colorectal histology dataset and a dataset derived from TCGA-BRCA whole slide images, and establish the effectiveness of our method by comparing with popular methods and frameworks in semi-SL algorithms through various experiments. Index Terms-Semi Supervised learning, open-set, label-noise, mixmatch I. INTRODUCTION Medical image analysis requires large volumes of supervised data to train deep learning models effectively, but obtaining good quality supervision for medical imaging is inherently difficult due to the associated labor, expertise, and time required [1]-[3]. In such scenarios, semi-supervised learning (semi-SL) offers an efficient alternative, especially when there are only a few labeled samples but plenty of unlabeled or unaudited data. Semi-SL algorithms can leverage the vast pool of unaudited training data by extracting discriminative information from the structure of unlabeled data that complements the knowledge gained from a small number of supervisory data samples. However, semi-SL assumes that the underlying distribution of the unaudited data matches that of the few labeled samples [4], [5]."}, "cited_paper_content": {"title": "Mixmatch: A Holistic Approach To Semi-Supervised Learning", "abstract": "Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success."}, "keywords": ["OOD detection", "MixMatch"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "ReMixMatch enhances the Mix-Match framework by introducing the principles of distribution alignment and augmentation anchoring. The notion of distribution alignment replaces the sharpening step of the MixMatch framework in an attempt to match model aggregated class prediction to that of the marginal distribution of the given ground truth. Further, ReMixMatch introduces another principle of augmentation alignment in place of the consistency regularization step of MixMatch to encourage each output to be close to the prediction for a weakly-augmented version of the same input. By incorporating these changes, ReMixMatch has been reported to be data efficient compared to MixMatch frameworks (cite: Mixmatch: A Holistic Approach To Semi-Supervised Learning).", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "MixMatch #REFR is a powerful algorithm for semi-supervised learning that has been shown to obtain state-of-the-art results by a large margin across many datasets and labeled data amounts.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "MixMatch #REFR is a popular semi-supervised learning framework that has been shown to achieve state-of-the-art results on various datasets, and it has been used as a baseline in recent works on OOD detection.", "given_citation_intent": "method", "given_keywords": ["OOD detection", "MixMatch"]}]}
{"citing_id": "2303.09681v1", "cited_id": "1706.03762", "section_title": "Spiking Spatiotemporal Transformer", "citation": "Attention scores of inner product used in the original transformer #REFR is not well-defined in our spiking transformer.", "text_before_citation": ["(b) Architecture of Spiking Spatiotemporal Attention that aims to address the one-way flow of information over time in SNNs, thus compensating for missing pose information, particularly for the early time steps.", "Normalized Hamming similarity is proposed for the attention score, which has been shown to be equivalent to inner product similarity between real valued vectors used in original transformer.", "definition mostly follows #OTHEREFR as", "PE(pos, 2i) = 1 T sin(pos/10000 2i/C k ), PE(pos, 2i + 1) = 1 T cos(pos/10000 2i/C k ),", "where pos represents the position in the sequence, while 2i or 2i + 1 denotes the position of C k channel."], "text_after_citation": ["If the spike key is a zero vector, s k j = 0, then the attention score will always be zero for any spike query s q i , that is 0 \u2022 s q j = 0.", "This means the inner product used in #OTHEREFR is not able to accurately determine the similarity between two spike vectors."], "citing_paper_content": {"title": "Event-Based Human Pose Tracking By Spiking Spatiotemporal Transformer", "abstract": "Event camera, as an emerging biologically-inspired vision sensor for capturing motion dynamics, presents new potential for 3D human pose tracking, or video-based 3D human pose estimation. However, existing works in pose tracking either require the presence of additional gray-scale images to establish a solid starting pose, or ignore the temporal dependencies all together by collapsing segments of event streams to form static image frames. Meanwhile, although the effectiveness of Artificial Neural Networks (ANNs, a.k.a. dense deep learning) has been showcased in many event-based tasks, the use of ANNs tends to neglect the fact that compared to the dense frame-based image sequences, the occurrence of events from an event camera is spatiotemporally much sparser. Motivated by the above mentioned issues, we present in this paper a dedicated end-to-end sparse deep learning approach for event-based pose tracking: 1) to our knowledge this is the first time that 3D human pose tracking is obtained from events only, thus eliminating the need of accessing to any frame-based images as part of input; 2) our approach is based entirely upon the framework of Spiking Neural Networks (SNNs), which consists of Spike-Element-Wise (SEW) ResNet and our proposed spiking spatiotemporal transformer; 3) a large-scale synthetic dataset is constructed that features a broad and diverse set of annotated 3D human motions, as well as longer hours of event stream data, named SynEventHPD. Empirical experiments demonstrate the superiority of our approach in both performance and efficiency measures. For example, with comparable performance to the state-of-the-art ANNs counterparts, our approach achieves a computation reduction of 20% in FLOPS. Our implementation is made available at https://github.com/JimmyZou/HumanPoseTracking SNN and dataset will be released upon paper acceptance."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["spiking transformer", "Attention scores"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed Spiking Spatiotemporal Attention compensates for missing pose information in Spiking Neural Networks by using a normalized Hamming similarity for the attention score, which has been shown to be equivalent to the inner product similarity between real-valued vectors used in the original Transformer #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Spiking Spatiotemporal Attention compensates for missing pose information by addressing the one-way flow of information over time in SNNs, and is based on the attention mechanism proposed in the #REFR paper.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed Spiking Spatiotemporal Attention architecture compensates for missing pose information in SNNs by using normalized Hamming similarity for the attention score, which has been shown to be equivalent to inner product similarity between real valued vectors used in the original transformer #REFR.", "given_citation_intent": "background", "given_keywords": ["spiking transformer", "Attention scores"]}]}
{"citing_id": "2305.00772v1", "cited_id": "1801.09736", "section_title": "Neumann Boundary Conditions P(U)|", "citation": "For the rectangular elements, the approximation of the singular function follows closely the proof in #REFR , and we present it below for the convenience of the reader.", "text_before_citation": ["Note that the approximation error for the smooth term is of higher order.", "By summing over all rectangles \u0393 j of the mesh of the screen and all components, we conclude that", "p(u)\u2212\u03a0 x \u03a0 t p(u) r,\u2212 1 2 ,\u0393, * h\u03b2 Re \u03bd * \u2212\u03b5 if \u2206t \u2264 min{h 1 , h 2 }.", "(a), cone singularity: To discuss the approximation of p(u) in the cone geometry, for simplicity, we let \u0393 be the squareR = [0, 1] 2 .", "Figure 4 shows how to reduce the mesh on the cone to this case by an affine map, with the exception of a small number of triangular elements."], "text_after_citation": ["For the additional triangular elements in Figure 4 (b) with linear basis functions, the crucial observation is that their angles are independent of h, leading to a shape-regular mesh.", "In particular, the quotient \u03c1 of the radii of the smallest circumscribed to the largest inscribed circle remains bounded and the expected interpolation inequalities hold: For the linear interpolant p of a function f determined by the vertices of a triangle T of circumscribed radius \u2264 h, one has", "f \u2212 p H s (T ) \u2264 C 0 h 2\u2212s f H 2 (T ) .", "Here, s \u2208 [0, 1] and the constant C 0 only depends on \u03c1 and s.", "The respective proofs for the regular part \u03c6 \u03c6 \u03c6 0 and the singular function r \u03bb\u22121 b i in this way directly apply to the arising triangles."], "citing_paper_content": {"title": "Higher-Order Time Domain Boundary Elements For Elastodynamicsgraded Meshes And Hp Versions", "abstract": "The solution to the elastodynamic equation in the exterior of a polyhedral domain or a screen exhibits singular behavior from the corners and edges. The detailed expansion of the singularities implies quasi-optimal estimates for piecewise polynomial approximations of the Dirichlet trace of the solution and the traction. The results are applied to hp and graded versions of the time domain boundary element method for the weakly singular and the hypersingular integral equations. Numerical examples confirm the theoretical results for the Dirichlet and Neumann problems for screens and for polygonal domains in 2d. They exhibit the expected quasi-optimal convergence rates and the singular behavior of the solutions."}, "cited_paper_content": {"title": "Boundary Elements With Mesh Refinements For The Wave Equation", "abstract": "The solution of the wave equation in a polyhedral domain in $\\mathbb{R}^3$ admits an asymptotic singular expansion in a neighborhood of the corners and edges. In this article we formulate boundary and screen problems for the wave equation as equivalent boundary integral equations in time domain, study the regularity properties of their solutions and the numerical approximation. Guided by the theory for elliptic equations, graded meshes are shown to recover the optimal approximation rates known for smooth solutions. Numerical experiments illustrate the theory for screen problems. In particular, we discuss the Dirichlet and Neumann problems, as well as the Dirichlet-to-Neumann operator and applications to the sound emission of tires."}, "keywords": ["rectangular elements"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The graded meshes used in this study recover the optimal approximation rates for smooth solutions, as shown in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To understand the regularity properties of solutions and numerical approximation for boundary and screen problems of the wave equation, the authors cite the paper on \"Boundary Elements With Mesh Refinements For The Wave Equation\" (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The use of graded meshes to recover optimal approximation rates for smooth solutions in boundary element methods has been well established in the literature (Boundary Elements With Mesh Refinements For The Wave Equation). #REFR", "given_citation_intent": "background", "given_keywords": ["rectangular elements"]}]}
{"citing_id": "2304.14154v1", "cited_id": "1802.04799", "section_title": "Introduction", "citation": "The achieved performance is comparable to the traditionally designed TVM compiler #REFR for deep learning.", "text_before_citation": ["To control the application of rewrite rules, strategy languages, such as Stratego #OTHEREFR ] have been proposed. #OTHEREFR provides a recent overview of the field.", "These strategy languages enable strategic rewriting by providing combinators to compose rewrite rules into larger strategies.", "Stratego is an integral part of the Spoofax language workbench by Kats and #OTHEREFR designed to declaratively specify languages and tailored IDEs to work with them.", "The Stratego strategy language is used here to write interpreters and compilers purely using compositions of rewrites. #OTHEREFR", "[ , 2020 describes the ELEVATE strategy language and how it is used to encode and control the application of traditional compiler optimizations, such as loop-tiling, compositionally."], "text_after_citation": ["In fact, ELEVATE shows how to rethink the design of \"user-schedulable languages\" as strategy languages, as highlighted by #OTHEREFR .", "Schedules allow experts precise control over what compiler optimizations to apply, an idea popularized by the domain-specific compiler Halide #OTHEREFR", "2013] and now widely adopted in other optimizing compilers, such as TVM.", "Closely related to strategy languages are tactic languages in automatic theorem proving that allow to control the arrangement of individual proof steps.", "These ideas go all the way back to #OTHEREFR"], "citing_paper_content": {"title": "Traced Types For Safe Strategic Rewriting", "abstract": "Strategy languages enable programmers to compose rewrite rules into strategies and control their application. This is useful in programming languages, e.g., for describing program transformations compositionally, but also in automated theorem proving, where related ideas have been studies with tactics languages. Clearly, not all compositions of rewrites are correct, but how can we assist programmers in writing correct strategies? In this paper, we present a static type system for strategy languages. We combine a structural type system capturing how rewrite strategies transform the shape of the rewritten syntax with a novel tracing system that keeps track of all possible legal strategy execution paths. Our type system raises warnings when parts of a composition are guaranteed to fail at runtime, and errors when no legal execution for a strategy is possible. We present a formalization of our strategy language and novel tracing type system, and formally prove its type soundness. We present formal results, showing that ill-traced strategies are guaranteed to fail at runtime and that well-traced strategy executions \"can't go wrong\", meaning that they are guaranteed to have a possible successful execution path."}, "cited_paper_content": {"title": "Tvm: An Automated End-To-End Optimizing Compiler For Deep Learning", "abstract": "There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies."}, "keywords": ["compiler", "deep learning"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To automate the optimization of low-level programs to hardware characteristics, TVM employs a novel, learning-based cost modeling method for rapid exploration of code optimizations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "TVM, an automated end-to-end optimizing compiler for deep learning, has been shown to deliver competitive performance across hardware back-ends, including low-power CPUs, mobile GPUs, and server-class GPUs, as well as new accelerators such as FPGA-based generic deep learning accelerators. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve performance portability of deep learning workloads across diverse hardware back-ends, TVM, an automated end-to-end optimizing #REFR compiler, exposes graph-level and operator-level optimizations, and employs a novel, learning-based cost modeling method for rapid exploration of code optimizations, as demonstrated by the experimental results that show its competitive performance across hardware back-ends, including low-power CPU, mobile GPU, and server-class GPUs.", "given_citation_intent": "result", "given_keywords": ["compiler", "deep learning"]}]}
{"citing_id": "2303.02948v1", "cited_id": "1802.06739", "section_title": "B. Afl-Based Anomaly Detection Model", "citation": "Adding noise to the gradients of the Wasserstein distance is more efficient than adding noise to the final parameters directly with respect to preserving privacy #REFR .", "text_before_citation": ["Although AFL has distinct privacy advantages like FL, current research shows that sensitive information can still be inferred by using shared parameters during the learning process #OTHEREFR .", "To address this issue, we propose a differentially private WGAN-GP model, where differential privacy is achieved in WGAN-GP by adding carefully designed noise to gradients during the learning process. Definition 1.", "A randomized function F is considered as ( , \u03b4)-differentially private if the following inequality is satisfied for any two databases X and X differing in a single point and for any output subset S #OTHEREFR :", "EQUATION", "where F (X) and F (X ) are the outputs of the function F for inputs X and X , respectively."], "text_after_citation": ["The gradients \u2206w n of discriminator parameter w n after adding noise can be expressed as follows:", "EQUATION", "where \u03c3 n represents the noise scale, and c g is the bound on the gradients of the Wasserstein distance.", "According to Lemma 1 in #OTHEREFR , given the sampling rate p = m M (where m is the batch size and M is the total number of training data used in each discriminator iteration), the number of discriminator iterations N d between two generator iterations, and privacy violation \u03b4, then for any positive , the discriminator parameter guarantees ( , \u03b4)-differential privacy with respect to all data used in the generator iteration if we choose", "EQUATION"], "citing_paper_content": {"title": "A Vhetnet-Enabled Asynchronous Federated Learning-Based Anomaly Detection Framework For Ubiquitous Iot", "abstract": "Anomaly detection for the Internet of Things (IoT) is a major intelligent service required by many fields, including intrusion detection, state monitoring, device-activity analysis, and security supervision. However, the heterogeneous distribution of data and resource-constrained end nodes in ubiquitous IoT systems present challenges for existing anomaly detection models. Due to the advantages of flexible deployment and multidimensional resources, high altitude platform stations (HAPSs) and unmanned aerial vehicles (UAVs), which are important components of vertical heterogeneous networks (VHetNets), have significant potential for sensing, computing, storage, and communication applications in ubiquitous IoT systems. In this paper, we propose a novel VHetNet-enabled asynchronous federated learning (AFL) framework to enable decentralized UAVs to collaboratively train a global anomaly detection model based on their local sensory data from ubiquitous IoT devices. In the VHetNet-enabled AFL framework, a HAPS operates as a central aerial server, and the local models trained in UAVs are uploaded to the HAPS for global aggregation due to its wide coverage and strong storage and computation capabilities. We also introduce a UAV selection strategy into the AFL framework to prevent UAVs with low local model quality and large energy consumption from affecting the learning efficiency and detection accuracy of the global model. To ensure the security of transmissions between UAVs and the HAPS via wireless links, we add designed noise to local model parameters in UAVs to achieve differential privacy during the information exchange process. Moreover, we propose a compound-action actor-critic (CA2C)-based joint device association, UAV selection, and UAV trajectory planning algorithm to further enhance the overall federated execution efficiency and detection model accuracy under the UAV energy constraints. Extensive experimental evaluation on a real-world dataset demonstrates that the proposed algorithm can achieve high detection accuracy with short federated execution time and low energy consumption. Index Terms-Anomaly detection, ubiquitous Internet of Things (IoT), vertical heterogeneous network (VHetNet), asynchronous federated learning (AFL), differential privacy."}, "cited_paper_content": {"title": "Differentially Private Generative Adversarial Network", "abstract": "Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level."}, "keywords": ["privacy"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of sensitive data privacy in federated learning, the authors propose a differentially private GAN model, where carefully designed noise is added to gradients during the learning procedure #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of concentration of distribution in GANs, which can divulge critical patient information, the authors propose a differentially private GAN (DPGAN) model, where they achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the privacy of transmissions between UAVs and the HAPS via wireless links, the proposed VHetNet-enabled AFL framework adds designed noise to local model parameters in UAVs to achieve differential privacy during the information exchange process #REFR.", "given_citation_intent": "background", "given_keywords": ["privacy"]}]}
{"citing_id": "2303.15410v1", "cited_id": "1906.03950", "section_title": "Proposed Method", "citation": "The key idea is to let them use separate batch normalization (BN) layers while sharing all the other parameters of the network; following the domain-specific batch normalization #REFR we call such a set of separate BNs lighting-condition specific BN (LSBN).", "text_before_citation": ["As our target model deals with extremely low-light images, it suffers from the significantly low quality of inputs.", "To alleviate this, we propose a new method for learning the target model using the paired well-lit images as privileged information #OTHEREFR , additional high-quality input data accessible only in training.", "Our method introduces another model called teacher that takes the privileged information as input and provides rich supervision to the target model called student.", "This method for learning using privileged information (LUPI) allows the student to simulate the internal behavior of the teacher as well as learn to predict human poses.", "To further exploit the privileged information, we design a single concise architecture that integrates the teacher and the student."], "text_after_citation": ["This design choice allows the student to enjoy the strong representation learned using the well-lit images (i.e., the privileged information) while capturing specific characteristics of low-light images through the separate BN parameters.", "Our model architecture and LUPI strategy are depicted in Fig. 5 .", "Note that, before being fed to the student, low-light images are scaled automatically by adjusting their average pixel intensity value to a predefined constant.", "On the other hand, the teacher takes as input well-lit images as-is.", "Both of the teacher and the student are trained by a common pose estimation loss:"], "citing_paper_content": {"title": "Human Pose Estimation In Extremely Low-Light Conditions", "abstract": "We study human pose estimation in extremely low-light images. This task is challenging due to the difficulty of collecting real low-light images with accurate labels, and severely corrupted inputs that degrade prediction quality significantly. To address the first issue, we develop a dedicated camera system and build a new dataset of real lowlight images with accurate pose labels. Thanks to our camera system, each low-light image in our dataset is coupled with an aligned well-lit image, which enables accurate pose labeling and is used as privileged information during training. We also propose a new model and a new training strategy that fully exploit the privileged information to learn representation insensitive to lighting conditions. Our method demonstrates outstanding performance on real extremely low-light images, and extensive analyses validate that both of our model and dataset contribute to the success."}, "cited_paper_content": {"title": "Domain-Specific Batch Normalization For Unsupervised Domain Adaptation", "abstract": "We propose a novel unsupervised domain adaptation framework based on domain-specific batch normalization in deep neural networks. We aim to adapt to both domains by specializing batch normalization layers in convolutional neural networks while allowing them to share all other model parameters, which is realized by a two-stage algorithm. In the first stage, we estimate pseudo-labels for the examples in the target domain using an external unsupervised domain adaptation algorithm---for example, MSTN or CPUA---integrating the proposed domain-specific batch normalization. The second stage learns the final models using a multi-task classification loss for the source and target domains. Note that the two domains have separate batch normalization layers in both stages. Our framework can be easily incorporated into the domain adaptation techniques based on deep neural networks with batch normalization layers. We also present that our approach can be extended to the problem with multiple source domains. The proposed algorithm is evaluated on multiple benchmark datasets and achieves the state-of-the-art accuracy in the standard setting and the multi-source domain adaption scenario."}, "keywords": ["lighting-condition specific BN", "domain-specific batch normalization"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To specialize batch normalization layers in convolutional neural networks for unsupervised domain adaptation, the proposed domain-specific batch normalization algorithm in #REFR can be integrated with external unsupervised domain adaptation algorithms such as MSTN or CPUA to estimate pseudo-labels for the target domain, as demonstrated in previous work.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the issue of low-quality inputs in extremely low-light images, we propose a new method for learning the target model using paired well-lit images as privileged information #REFR, which allows the student to simulate the internal behavior of the teacher and learn to predict human poses.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of learning in low-light conditions, we propose a new method that utilizes paired well-lit images as privileged information and introduces a teacher-student model architecture #REFR, which is inspired by the domain-specific batch normalization approach proposed in the reference paper.", "given_citation_intent": "method", "given_keywords": ["lighting-condition specific BN", "domain-specific batch normalization"]}]}
{"citing_id": "2303.01559v1", "cited_id": "1710.09412", "section_title": "Ood Detection", "citation": "In addition, besides AdaptiveMix loss, we can use mixingbased cross-entropy loss in the learning objective of image classification following augmentation #REFR , since we use Mixup to generate hard samples (See supplementary material for more details on classification).", "text_before_citation": ["Benchmark #OTHEREFR Table 12 +3.5% F1 \u2191 class-aware separation, we then introduce the orthogonal constraint to initialize W, which is defined as:", "EQUATION"], "text_after_citation": [], "citing_paper_content": {"title": "Improving Gan Training Via Feature Space Shrinkage", "abstract": "Due to the outstanding capability for data generation, Generative Adversarial Networks (GANs) have attracted considerable attention in unsupervised learning. However, training GANs is difficult, since the training distribution is dynamic for the discriminator, leading to unstable image representation. In this paper, we address the problem of training GANs from a novel perspective, i.e., robust image classification. Motivated by studies on robust image representation, we propose a simple yet effective module, namely AdaptiveMix, for GANs, which shrinks the regions of training data in the image representation space of the discriminator. Considering it is intractable to directly bound feature space, we propose to construct hard samples and narrow down the feature distance between hard and easy samples. The hard samples are constructed by mixing a pair of training images. We evaluate the effectiveness of our AdaptiveMix with widely-used and state-of-the-art GAN architectures. The evaluation results demonstrate that our AdaptiveMix can facilitate the training of GANs and effectively improve the image quality of generated samples. We also show that our AdaptiveMix can be further applied to image classification and Out-Of-Distribution (OOD) detection tasks, by equipping it with state-of-theart methods. Extensive experiments on seven publicly available datasets show that our method effectively boosts the performance of baselines. The code is publicly available at https : / / github. com / WentianZhang-ML/AdaptiveMix."}, "cited_paper_content": {"title": "Mixup: Beyond Empirical Risk Minimization", "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks."}, "keywords": ["image classification", "mixingbased cross-entropy loss"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To improve the training of Generative Adversarial Networks (GANs), the authors propose AdaptiveMix, a module that shrinks the regions of training data in the image representation space of the discriminator by mixing a pair of training images, as demonstrated in the reference paper Mixup: Beyond Empirical Risk Minimization #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the generalization of state-of-the-art neural network architectures, mixup, a simple learning principle that trains a neural network on convex combinations of pairs of examples and their labels, was proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve the performance of image classification, the authors propose AdaptiveMix, a module that shrinks the regions of training data in the image representation space of the discriminator by mixing a pair of training images, which is inspired by the mixup method proposed in the Mixup: Beyond Empirical Risk Minimization paper. #REFR", "given_citation_intent": "method", "given_keywords": ["image classification", "mixingbased cross-entropy loss"]}]}
{"citing_id": "2304.00801v3", "cited_id": "1806.04618", "section_title": "Related Work", "citation": "Relevant experimental work on noisy segmentation include #REFR where the authors considered three different label noise models and observed that increased noise level caused worse performance.", "text_before_citation": ["This is especially important for this work because it closely relates to the studied problems.", "In binary classification, optimal threshold based classifiers for the F 1 metric was discussed in #OTHEREFR and further elaborated by others.", "Because of the close relationship between the Dice metric in segmentation and the F 1 metric in binary classification, this idea was later taken to the segmentation context and used to give a characterization of the optimal segmentations with respect to Dice #OTHEREFR .", "Furthermore, sharp bounds of the volume of these solutions were provided.", "Similar inspiration from work in binary classification [2, 3] made authors propose that calibration can be a property of importance for explaining the good performance obtained by soft-Dice when evaluation is done by Dice #OTHEREFR ."], "text_after_citation": ["A similar experimental study also including experiments on biased noise was later described in #OTHEREFR .", "Finally, experimental work on soft labels with other loss functions than soft-Dice include #OTHEREFR .", "For recent general reviews on work on noisy segmentation and imperfect data, see #OTHEREFR and #OTHEREFR ."], "citing_paper_content": {"title": "Noisy Image Segmentation With Soft-Dice", "abstract": "This paper presents a study on the soft-Dice loss, one of the most popular loss functions in medical image segmentation, for situations where noise is present in target labels. In particular, the set of optimal solutions are characterized and sharp bounds on the volume bias of these solutions are provided. It is further shown that a sequence of soft segmentations converging to optimal soft-Dice also converges to optimal Dice when converted to hard segmentations using thresholding. This is an important result because soft-Dice is often used as a proxy for maximizing the Dice metric. Finally, experiments confirming the theoretical results are provided."}, "cited_paper_content": {"title": "Imperfect Segmentation Labels: How Much Do They Matter?", "abstract": "Labeled datasets for semantic segmentation are imperfect, especially in medical imaging where borders are often subtle or ill-defined. Little work has been done to analyze the effect that label errors have on the performance of segmentation methodologies. Here we present a large-scale study of model performance in the presence of varying types and degrees of error in training data. We trained U-Net, SegNet, and FCN32 several times for liver segmentation with 10 different modes of ground-truth perturbation. Our results show that for each architecture, performance steadily declines with boundary-localized errors, however, U-Net was significantly more robust to jagged boundary errors than the other architectures. We also found that each architecture was very robust to non-boundary-localized errors, suggesting that boundary-localized errors are fundamentally different and more challenging problem than random label errors in a classification setting."}, "keywords": ["noisy segmentation"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "In the context of analyzing the effect of label errors on segmentation methodologies, the authors of #REFR found that boundary-localized errors are more challenging than random label errors in a classification setting.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' study on the effect of label errors on segmentation performance is highly relevant to our work, as it highlights the challenges of imperfect segmentation labels in medical imaging #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The reference paper #REFR presents a large-scale study of model performance in the presence of varying types and degrees of error in training data for semantic segmentation, which is highly relevant to the problem of noisy segmentation addressed in this work.", "given_citation_intent": "background", "given_keywords": ["noisy segmentation"]}]}
{"citing_id": "2303.14961v1", "cited_id": "1802.03471", "section_title": "Background", "citation": "This robustness verification method (Cohen et al., 2019; #REFR computes the 2 -norm certificates around an input sample x by counting which class is most likely to be returned when x is perturbed by isotropic Gaussian noise.", "text_before_citation": ["Even though an adversariallytrained network is resilient to attacks created during training, it can still be susceptible to unseen new attacks.", "To overcome this problem, certified defenses formally guarantee the stability of the prediction in a neighbourhood of the input.", "In other words, a neural network f is certifiably robust for the input x \u2208 R d , if the prediction for all perturbed versions", "x remains unchanged such that x \u2212 x p \u2264 , where \u2022 p is the p -norm around x of size > 0.", "Randomized Smoothing."], "text_after_citation": ["Formally, given a soft classifier F , randomized smoothing considers a smooth version of F defined as:", "EQUATION", "where \u03c3 > 0 represents the standard deviation.", "As previously, we define the hard version of G(x) as g(x) = arg max y\u2208Y G(x) y .", "Contrary to other formal verification methods, randomized smoothing does not make any assumptions regarding the model's properties, allowing certification to be scaled to larger and more complex networks. Cohen et al."], "citing_paper_content": {"title": "Diffusion Denoised Smoothing For Certified And Adversarial Robust Out-Of-Distribution Detection", "abstract": "As the use of machine learning continues to expand, the importance of ensuring its safety cannot be overstated. A key concern in this regard is the ability to identify whether a given sample is from the training distribution, or is an \"Out-Of-Distribution\" (OOD) sample. In addition, adversaries can manipulate OOD samples in ways that lead a classifier to make a confident prediction. In this study, we present a novel approach for certifying the robustness of OOD detection within a 2-norm around the input, regardless of network architecture and without the need for specific components or additional training. Further, we improve current techniques for detecting adversarial attacks on OOD samples, while providing high levels of certified and adversarial robustness on in-distribution samples. The average of all OOD detection metrics on CIFAR10/100 shows an increase of \u223c 13%/5% relative to previous approaches."}, "cited_paper_content": {"title": "Certified Robustness To Adversarial Examples With Differential Privacy", "abstract": "Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google\u2019s Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense."}, "keywords": ["robustness verification method"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To achieve certified robustness against adversarial examples, recent works have introduced a set of defenses that provide guarantees of robustness to norm-bounded attacks. One such defense, PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy. It provides a rigorous, generic, and flexible foundation for defense, and is the first certified defense that both scales to large networks and datasets. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To achieve certified robustness against adversarial examples, PixelDP, a defense mechanism based on differential privacy, is proposed in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure the robustness of the proposed method against adversarial attacks, the authors employ the PixelDP #REFR, a certified robustness verification method based on differential privacy.", "given_citation_intent": "method", "given_keywords": ["robustness verification method"]}]}
{"citing_id": "2304.04484v1", "cited_id": "1606.04171", "section_title": "I. Introduction", "citation": "As an early prototype system of global connectivity, narrowband IoT #REFR has enjoyed significant success in the long-term evolution of terrestrial networks (TNs), which has motivated considerable interest in moving from human-type communication to machine-type communication (MTC) .", "text_before_citation": ["As an indispensable component of the space-air-ground integrated network, low earth orbit (LEO) satellites have received extensive attention in the research of beyond fifth generation (B5G) and sixth generation (6G) mobile communication systems #OTHEREFR - #OTHEREFR .", "Extensive efforts have been devoted to the construction of satellite constellations over the past few decades, for example, the Iridium system in the 1990s and Starlink LEO constellation projects more recently #OTHEREFR .", "With the evolution of space and communication technologies, satellite communication (SatCom) has extended from its original narrowband voice service to broadband multimedia service, which also brings more opportunities to the ubiquitous space-air-ground integrated connectivity."], "text_after_citation": ["Given the explosive growth of data traffic, advanced MTC applications will be more data-intensive, and the demands of advanced IoT-enabled applications will shift from low-rate short packet transmission to more rigorous low-latency, broadband, and reliable information interaction such as industrial Internet, smart cities, intelligent transportation, industrial metaverse, holographic communications, and so on #OTHEREFR .", "On the other hand, remote and disaster areas also still face challenges in accessing the network due to the high cost of terrestrial infrastructure.", "Therefore, it is also of interest to promote broadband LEO satellite-enabled non-terrestrial networks (NTNs) #OTHEREFR as a part of the communication infrastructure.", "Due to the massive number of potential MTC user terminals (UTs) and the long propagation delay between ground and LEO satellites, it is inefficient to coordinate the channel resources for uplink access through traditional handshaking protocols.", "Grant free-random access (GF-RA) is a compelling paradigm for massive access in MTC since it allows UTs to directly transmit their respective preamble and payload data to base stations (BSs) without the aforehand handshaking process."], "citing_paper_content": {"title": "Quasi-Synchronous Random Access For Massive Mimo-Based Leo Satellite Constellations", "abstract": "Low earth orbit (LEO) satellite constellation-enabled communication networks are expected to be an important part of many Internet of Things (IoT) deployments due to their unique advantage of providing seamless global coverage. In this paper, we investigate the random access problem in massive multiple-input multiple-output-based LEO satellite systems, where the multi-satellite cooperative processing mechanism is considered. Specifically, at edge satellite nodes, we conceive a training sequence padded multi-carrier system to overcome the issue of imperfect synchronization, where the training sequence is utilized to detect the devices' activity and estimate their channels. Considering the inherent sparsity of terrestrial-satellite links and the sporadic traffic feature of IoT terminals, we utilize the orthogonal approximate message passing-multiple measurement vector algorithm to estimate the delay coefficients and user terminal activity. To further utilize the structure of the receive array, a two-dimensional estimation of signal parameters via rotational invariance technique is performed for enhancing channel estimation. Finally, at the central server node, we propose a majority voting scheme to enhance activity detection by aggregating backhaul information from multiple satellites. Moreover, multi-satellite cooperative linear data detection and multi-satellite cooperative Bayesian dequantization data detection are proposed to cope with perfect and quantized backhaul, respectively. Simulation results verify the effectiveness of our proposed schemes in terms of channel estimation, activity detection, and data detection for quasi-synchronous random access in satellite systems. Index Terms Internet of Things (IoT), low earth orbit (LEO) satellite, massive multiple-input multiple-output (mMIMO), random access"}, "cited_paper_content": {"title": "A Primer On 3Gpp Narrowband Internet Of Things", "abstract": "Narrowband Internet of Things (NB-IoT) is a new cellular technology introduced in 3GPP Release 13 for providing wide-area coverage for IoT. This article provides an overview of the air interface of NB-IoT. We describe how NB-IoT addresses key IoT requirements such as deployment flexibility, low device complexity, long battery lifetime, support of massive numbers of devices in a cell, and significant coverage extension beyond existing cellular technologies. We also share the various design rationales during the standardization of NB-IoT in Release 13 and point out several open areas for future evolution of NB-IoT."}, "keywords": ["IoT"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "With the evolution of space and communication technologies, satellite communication (SatCom) has extended from its original narrowband voice service to broadband multimedia service, which also brings more opportunities to the ubiquitous space-air-ground integrated connectivity. To provide wide-area coverage for IoT, Narrowband Internet of Things (NB-IoT) has been introduced in 3GPP Release 13 #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Satellite communication has extended from its original narrowband voice service to broadband multimedia service, which also brings more opportunities to the ubiquitous space-air-ground integrated connectivity. As a new cellular technology introduced in 3GPP Release 13 for providing wide-area coverage for IoT, Narrowband Internet of Things (NB-IoT) addresses key IoT requirements such as deployment flexibility, low device complexity, long battery lifetime, support of massive numbers of devices in a cell, and significant coverage extension beyond existing cellular technologies #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The introduction of Narrowband Internet of Things (NB-IoT) in 3GPP Release 13 has provided a new cellular technology for addressing key IoT requirements such as deployment flexibility, low device complexity, long battery lifetime, support of massive numbers of devices in a cell, and significant coverage extension beyond existing cellular technologies #REFR.", "given_citation_intent": "background", "given_keywords": ["IoT"]}]}
{"citing_id": "2304.05884v1", "cited_id": "1807.05520", "section_title": "Related Work", "citation": "As a representative work, DeepCluster #REFR ) adopts a standard k-means for clustering, but it contains degenerate solutions.", "text_before_citation": ["Instance and Cluster Discrimination.", "Instance discrimination #OTHEREFR Radford et al., 2021) is realized with a contrastive loss which targets at pulling closer samples from the same instance while pushing away samples from different instances.", "Despite the impressive performance, instance-wise contrastive learning can not capture the semantic information from the training data because it is trained to ignore the similarity between different instances.", "Cluster discrimination #OTHEREFR is processed with iterative steps: the clustering step to assign pseudo class labels for each sample, and then the classification step to map each sample to its assigned label.", "Since one cluster has more than one instance, learning representations with clusters will gather similar instances together, which can explore potential semantic structures in data."], "text_after_citation": ["To this end, recent research work #OTHEREFR focuses on improving the label assignment during clustering but employs a standard cross-entropy loss during discrimination.", "In this paper, we only employ one step of off-line clustering but design a robust classifier to achieve good feature representation when training on the automatically clustered large-scale data.", "Image Retrieval.", "Image retrieval task typically relies on fine-tuning pre-trained visual models #OTHEREFR and can be divided into two learning categories: supervised and unsupervised metric learning.", "For supervised metric learning, pair-wise loss #OTHEREFR and cross-entropy loss #OTHEREFR are extensively studied and recent bench-marking results #OTHEREFR indicate that the margin-based softmax loss (e.g., ArcFace #OTHEREFR ) can achieve state-of-the-art performance."], "citing_paper_content": {"title": "Unicom: Universal And Compact Representation Learning For Image Retrieval", "abstract": "Modern image retrieval methods typically rely on fine-tuning pre-trained encoders to extract image-level descriptors. However, the most widely used models are pre-trained on ImageNet-1K with limited classes. The pre-trained feature representation is therefore not universal enough to generalize well to the diverse open-world classes. In this paper, we first cluster the large-scale LAION 400M dataset into one million pseudo classes based on the joint textual and visual features extracted by the CLIP model. Due to the confusion of label granularity, the automatically clustered dataset inevitably contains heavy inter-class conflict. To alleviate such conflict, we randomly select partial inter-class prototypes to construct the margin-based softmax loss. To further enhance the low-dimensional feature representation, we randomly select partial feature dimensions when calculating the similarities between embeddings and class-wise prototypes. The dual random partial selections are with respect to the class dimension and the feature dimension of the prototype matrix, making the classification conflict-robust and the feature embedding compact. Our method significantly outperforms state-of-the-art unsupervised and supervised image retrieval approaches on multiple benchmarks. The code and pre-trained models are released to facilitate future research https://github.com/deepglint/unicom."}, "cited_paper_content": {"title": "Deep Clustering For Unsupervised Learning Of Visual Features", "abstract": "Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks."}, "keywords": ["DeepCluster"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Cluster discrimination is processed with iterative steps: the clustering step to assign pseudo class labels for each sample, and then the classification step to map each sample to its assigned label. Since one cluster has more than one instance, learning representations with clusters will gather similar instances together, which can explore potential semantic structures in data. Deep clustering methods, such as DeepCluster, jointly learn the parameters of a neural network and the cluster assignments of the resulting features, and have been shown to outperform the current state of the art on all the standard benchmarks #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To jointly learn the parameters of a neural network and the cluster assignments of the resulting features, DeepCluster, a clustering method that uses k-means algorithm, was proposed in the reference paper #REFR, which could be a useful method to apply in the clustering step of cluster discrimination as described in the local context.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To jointly learn the parameters of a neural network and the cluster assignments of the resulting features, DeepCluster is a clustering method that can be applied to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M #REFR.", "given_citation_intent": "method", "given_keywords": ["DeepCluster"]}]}
{"citing_id": "2304.04886v1", "cited_id": "1911.08632", "section_title": "Introduction", "citation": "In fact, for the effectively acyclic case, #REFR only provides a sufficient condition that a given footprint yields a framepreserving update but it gives no algorithm for computing such a footprint.", "text_before_citation": ["This way, one can focus on the least flow, which is guaranteed to exist if one applies standard fixed point theorems, imposing only mild assumptions on the edge functions. However, cancellativity is inherently incompatible with standard domain-theoretic prerequisites.", "For instance, the only ordered cancellative commutative monoid that is a directed cpo is the trivial one: M 0 = {0}.", "Similarly, M 0 is the only such monoid that has a greatest element.", "For cases where unique flows are desired, #OTHEREFR imposes additional requirements on the edge functions (nil-potent) or the graph structure (effectively acyclic). The former is quite restrictive in terms of expressivity.", "The latter again complicates the computation of frame-preserving updates: one now has to ensure that no cycles are introduced when the updated graph h 2 is composed with its frame h 1 ."], "text_after_citation": ["Contributions.", "In this paper, we propose a new meta theory of flows based on flow monoids that form \u03c9-cpos (but need not be cancellative).", "The cpo requirement yields the desired least fixed point semantics.", "The differences in the requirements on the flow monoid necessitate a new notion of flow graph composition.", "In particular, for a least fixed point semantics of flows, h = h 1 * h 2 is only defined if the flows of h 1 and h 2 do not vanish."], "citing_paper_content": {"title": "Make Flows Small Again: Revisiting The Flow Framework", "abstract": "We present a new flow framework for separation logic reasoning about programs that manipulate general graphs. The framework overcomes problems in earlier developments: it is based on standard fixed point theory, guarantees least flows, rules out vanishing flows, and has an easy to understand notion of footprint as needed for soundness of the frame rule. In addition, we present algorithms for automating the frame rule, which we evaluate on graph updates extracted from linearizability proofs for concurrent data structures. The evaluation demonstrates that our algorithms help to automate key aspects of these proofs that have previously relied on user guidance or heuristics."}, "cited_paper_content": {"title": "Local Reasoning For Global Graph Properties", "abstract": "Separation logics are widely used for verifying programs that manipulate complex heap-based data structures. These logics build on so-called separation algebras, which allow expressing properties of heap regions such that modifications to a region do not invalidate properties stated about the remainder of the heap. This concept is key to enabling modular reasoning and also extends to concurrency. While heaps are naturally related to mathematical graphs, many ubiquitous graph properties are non-local in character, such as reachability between nodes, path lengths, acyclicity and other structural invariants, as well as data invariants which combine with these notions. Reasoning modularly about such graph properties remains notoriously difficult, since a local modification can have side-effects on a global property that cannot be easily confined to a small region. ::: In this paper, we address the question: What separation algebra can be used to avoid proof arguments reverting back to tedious global reasoning in such cases? To this end, we consider a general class of global graph properties expressed as fixpoints of algebraic equations over graphs. We present mathematical foundations for reasoning about this class of properties, imposing minimal requirements on the underlying theory that allow us to define a suitable separation algebra. Building on this theory we develop a general proof technique for modular reasoning about global graph properties over program heaps, in a way which can be integrated with existing separation logics. To demonstrate our approach, we present local proofs for two challenging examples: a priority inheritance protocol and the non-blocking concurrent Harris list."}, "keywords": ["effectively acyclic case"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the challenge of modular reasoning about non-local graph properties, a suitable separation algebra is proposed in #REFR, which is based on a general class of global graph properties expressed as fixpoints of algebraic equations over graphs.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' approach for modular reasoning about global graph properties over program heaps, which can be integrated with existing separation logics, is based on a general class of global graph properties expressed as fixpoints of algebraic equations over graphs, and a suitable separation algebra defined by minimal requirements on the underlying theory, as presented in #OTHEREFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "For the effectively acyclic case, ensuring that no cycles are introduced when the updated graph is composed with its frame can complicate the computation of frame-preserving updates. To address this issue, #REFR presents a general proof technique for modular reasoning about global graph properties over program heaps, which can be integrated with existing separation logics.", "given_citation_intent": "background", "given_keywords": ["effectively acyclic case"]}]}
{"citing_id": "2305.00983v1", "cited_id": "1911.03462", "section_title": "Semantic Segmentation", "citation": "Thereby, the performance on the previously-known classes is similar to the baseline even without including a distillation loss #REFR .", "text_before_citation": ["The quantitative results of our semantic segmentation method, reported in Tab.", "2, demonstrate, that the empty classes are \"filled\" with the novel concepts human and car."], "text_after_citation": ["For the car class, our method outperforms the baseline with respect to IoU (+2.87 pp), precision (+0.55 pp) and recall (+3.06 pp).", "We lose performance in terms of IoU for the human class due to a higher tendency for false positives.", "However, the false negative rate is significantly reduced, which is indicated by an increase in the recall value of 26.89 pp. The improved recall score is also visible in Fig.", "9 , showing two examples from the Cityscapes validation dataset.", "In the top row, several pedestrians are crossing the street, which are mostly segmented by our DNN, whereas the baseline DNN mostly misses the persons in the center as well as all heads."], "citing_paper_content": {"title": "Detecting Novelties With Empty Classes", "abstract": "For open world applications, deep neural networks (DNNs) need to be aware of previously unseen data and adaptable to evolving environments. Furthermore, it is desirable to detect and learn novel classes which are not included in the DNNs underlying set of semantic classes in an unsupervised fashion. The method proposed in this article builds upon anomaly detection to retrieve out-of-distribution (OoD) data as candidates for new classes. We thereafter extend the DNN by k empty classes and fine-tune it on the OoD data samples. To this end, we introduce two loss functions, which 1) entice the DNN to assign OoD samples to the empty classes and 2) to minimize the inner-class feature distances between them. Thus, instead of ground truth which contains labels for the different novel classes, the DNN obtains a single OoD label together with a distance matrix, which is computed in advance. We perform several experiments for image classification and semantic segmentation, which demonstrate that a DNN can extend its own semantic space by multiple classes without having access to ground truth."}, "cited_paper_content": {"title": "Knowledge Distillation For Incremental Learning In Semantic Segmentation", "abstract": "Although deep learning architectures have shown remarkable results in scene understanding problems, they exhibit a critical drop of overall performance due to catastrophic forgetting when they are required to incrementally learn to recognize new classes without forgetting the old ones. This phenomenon impacts on the deployment of artificial intelligence in real world scenarios where systems need to learn new and different representations over time. Current approaches for incremental learning deal only with the image classification and object detection tasks. In this work we formally introduce the incremental learning problem for semantic segmentation. To avoid catastrophic forgetting we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We developed three main methodologies of knowledge distillation working on both the output layers and the internal feature representations. Furthermore, differently from other recent frameworks, we do not store any image belonging to the previous training stages while only the last model is used to preserve high accuracy on previously learned classes. Extensive results were conducted on the Pascal VOC2012 dataset and show the effectiveness of the proposed approaches in different incremental learning scenarios."}, "keywords": ["previously-known classes"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The quantitative results of our semantic segmentation method, reported in Tab. 2, demonstrate, that the empty classes are \"filled\" with the novel concepts human and car, as proposed in the incremental learning method of knowledge distillation for semantic segmentation by #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors' proposed approach of distilling knowledge from the previous model to retain information about previously learned classes while updating the current model to learn new ones, as described in \"Knowledge Distillation For Incremental Learning In Semantic Segmentation\" #REFR, effectively avoids catastrophic forgetting, as demonstrated by the quantitative results reported in Tab. 2.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in this article addresses the issue of catastrophic forgetting in incremental learning by distilling the knowledge of the previous model to retain the information about #REFR(previously-known classes), while updating the current model to learn the new ones, as demonstrated in the results on the Pascal VOC2012 dataset.", "given_citation_intent": "result", "given_keywords": ["previously-known classes"]}]}
{"citing_id": "2304.04027v1", "cited_id": "2003.08934", "section_title": "Introduction", "citation": "The generation module is inspired by NeRF #REFR that predicts the density of the position but with the input image feature condition.", "text_before_citation": ["We made these simulated PX using a ray-based method and sample rays based on the principles of PX.", "Those rays are rendered as a synthesized image pixel using the Beer-Lambert law with CBCT data.", "Still, there exists a domain gap between real-world PX and simulated ones.", "We utilized CycleGAN-based translation module that supports unpaired image-to-image translation, as obtaining paired PX and CBCT datasets is challenging.", "We propose a new loss function that enhances the existing CycleGAN model to generate a more plausible synthesized image from the real-world image."], "text_after_citation": ["Lastly, we add an encoder-decoder-based refinement module to enhance the 3D reconstruction quality.", "The main contributions of our paper are as follows: #OTHEREFR We introduce a novel architecture that can process real-world PX to the 3D oral structure without any prior data such as a dental arch, #OTHEREFR We propose a new synthesizing method that eliminates the need for matching CBCT and PX datasets during training, and a loss function that reduces the gap between the simulated image and real-world image, #OTHEREFR Results show that our model can generate high-quality oral structure and much more robust to the real-world image than other state-of-the-art models."], "citing_paper_content": {"title": "Nebla: Neural Beer-Lambert For 3D Reconstruction Of Oral Structures From Panoramic Radiographs", "abstract": "Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Conebeam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input image without any prior information such as a dental arch. Our ray-based generation approach makes it possible to reverse the process of generating PX from oral structure in order to reconstruct CBCT data. Lastly, the refinement module enhances the quality of the 3D output. Results show that our approach works better for simulated and real-world images compared to other state-of-the-art methods."}, "cited_paper_content": {"title": "Nerf: Representing Scenes As Neural Radiance Fields For View Synthesis", "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons."}, "keywords": ["generation module", "density"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To estimate the 3D oral structure from real-world panoramic radiographs, the authors utilized a ray-based method and rendered synthesized images using the Beer-Lambert law with CBCT data. To address the domain gap between real-world and simulated PX, they employed a CycleGAN-based translation module that supports unpaired image-to-image translation. The proposed method achieved state-of-the-art results for 3D reconstruction of oral structures. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate the 3D oral structure from panoramic radiographs, the authors utilized a ray-based method and rendered the rays using the Beer-Lambert law with CBCT data, and further improved the quality of the synthesized image using a CycleGAN-based translation module, which is a state-of-the-art method for unpaired image-to-image translation #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To estimate the 3D oral structure from panoramic radiographs, the authors utilized a ray-based generation approach that makes it possible to reverse the process of generating PX from oral structure, and for this, they used a fully-connected deep network that represents a scene using a single continuous 5D coordinate and its output is the volume #REFR density and view-dependent emitted radiance at that spatial location, which is optimized using a set of images with known camera poses in the generation module.", "given_citation_intent": "method", "given_keywords": ["generation module", "density"]}]}
{"citing_id": "2304.08592v1", "cited_id": "1904.01906", "section_title": "A.1. Experimental Setup In Section 3", "citation": "For the experiments in Section 3, we utilize a widelyused model architecture in STR, denoted as TRBA #REFR .", "text_before_citation": [], "text_after_citation": ["Note that we use the same model architecture for Section 3 experiments for a fair comparison.", "According to the previous work #OTHEREFR , the four stages derived from the STR models are as follows:", "\u2022 Transformation.", "The thin-plate spline (TPS) transformation, a variant of a spatial transformation network #OTHEREFR , normalizes the perspective or curved text image into a horizontal text image.", "\u2022 Feature Extraction."], "citing_paper_content": {"title": "Improving Scene Text Recognition For Character-Level Long-Tailed Distribution", "abstract": "Figure 1: (a) We visualize the character-level distributions of WikiSynth of Korean (Kr) and Chinese (Cn). We categorize the characters according to the number of training samples: many, medium, and few. We also show misclassified images of tail characters predicted wrongly as head characters. (b) Our approach outperforms the baseline model when evaluated with character-level (char) F1 score, a newly proposed evaluation metric, which measures the performance at the character level. The higher score, the better. This result shows that our method enhances the performance on few characters significantly."}, "cited_paper_content": {"title": "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset And Model Analysis", "abstract": "Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules."}, "keywords": ["STR"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The authors' proposed approach for improving scene text recognition by enhancing the performance on few characters significantly outperforms the baseline model when evaluated with character-level F1 score, as shown in Figure 1(b) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the inconsistencies in training and evaluation datasets and to analyze the module-wise contributions to performance, the authors proposed a unified four-stage STR framework that most existing STR models fit into, as well as a comprehensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations, as described in the reference paper #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To address the inconsistencies in training and evaluation datasets in scene text recognition (STR) models, a recent study proposed a unified four-stage STR framework that allows for extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations, as well as module-wise analyses of their contributions to performance in terms of accuracy, speed, and memory demand under one consistent set of training and evaluation datasets #REFR.", "given_citation_intent": "method", "given_keywords": ["STR"]}]}
{"citing_id": "2303.02572v1", "cited_id": "1411.1736", "section_title": "\u03a0-Structure", "citation": "As in #REFR , this can be constructed using the locally cartesian closed structure of C q .", "text_before_citation": ["Lr( p A ) C\u00b5C\u03bd * (p E A ) (5.16)", "Thus, the assumption of pre-\u03a0-structure tells us that L r ( p A ) is type-exponentiable, and hence the pushforward of B along it is represented by a type.", "What remains is to construct an appropriate local universe to make this strictly stable.", "Let V \u03a0(A,B) be the universal object equipped with maps", "\u03c0 A \u2236 V \u03a0(A,B) \u2192 C \u03c9 (V A ) \u03c0 B \u2236 \u03c0 * A (C \u03c9 (V A E A )) \u2192 V B ."], "text_after_citation": ["Now because we have pre-\u03a0-structure, the map \u03a0(A,B) ), i.e. we have a distributivity pullback", "\u03c0 * A (C \u03c9 (p E A )) is type- exponentiable.", "Thus, the pushforward of E B [\u03c0 B ] \u2208 Ty q (\u03c0 * A (C \u03c9 (V A E A ))) along it is represented by a type E \u03a0(A,B) \u2208 Ty q (V", "\u2022 \u03c0 * A (C \u03c9 (V A E A )) E B [\u03c0 B ] \u03c0 * A (C \u03c9 (V A E A )) V \u03a0(A,B) E \u03a0(A,B) V \u03a0(A,B)", "Now the bottom map in (5.16) together with \u231cB\u231d \u03a0(A,B) ."], "citing_paper_content": {"title": "Semantics Of Multimodal Adjoint Type Theory", "abstract": "We show that contrary to appearances, Multimodal Type Theory (MTT) over a 2-category M can be interpreted in any M-shaped diagram of categories having, and functors preserving, M-sized limits, without the need for extra left adjoints. This is achieved by a construction called \"co-dextrification\" that co-freely adds left adjoints to any such diagram, which can then be used to interpret the \"context lock\" functors of MTT. Furthermore, if any of the functors in the diagram have right adjoints, these can also be internalized in type theory as negative modalities in the style of FitchTT. We introduce the name Multimodal Adjoint Type Theory (MATT) for the resulting combined general modal type theory. In particular, we can interpret MATT in any finite diagram of toposes and geometric morphisms, with positive modalities for inverse image functors and negative modalities for direct image functors."}, "cited_paper_content": {"title": "The Local Universes Model: An Overlooked Coherence Construction For Dependent Type Theories", "abstract": "We present a new coherence theorem for comprehension categories, providing strict models of dependent type theory with all standard constructors, including dependent products, dependent sums, identity types, and other inductive types. Precisely, we take as input a\"weak model\": a comprehension category, equipped with structure corresponding to the desired logical constructions. We assume throughout that the base category is close to locally Cartesian closed: specifically, that products and certain exponentials exist. Beyond this, we require only that the logical structure should be *weakly stable* --- a pure existence statement, not involving any specific choice of structure, weaker than standard categorical Beck--Chevalley conditions, and holding in the now standard homotopy-theoretic models of type theory. Given such a comprehension category, we construct an equivalent split one, whose logical structure is strictly stable under reindexing. This yields an interpretation of type theory with the chosen constructors. The model is adapted from Voevodsky's use of universes for coherence, and at the level of fibrations is a classical construction of Giraud. It may be viewed in terms of local universes or delayed substitutions."}, "keywords": ["structure"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To construct an appropriate local universe to make this strictly stable, the authors rely on the model presented in the reference paper #REFR, which provides strict models of dependent type theory with all standard constructors, including dependent products, dependent sums, identity types, and other inductive types.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct an appropriate local universe to make this strictly stable, the authors of this manuscript use a new coherence theorem for comprehension categories, which provides strict models of dependent type theory with all standard constructors, including dependent products, dependent sums, identity types, and other inductive types #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To construct an appropriate local universe to make this strictly stable, the authors of this manuscript use a new coherence theorem for comprehension categories, which provides strict models of dependent type theory with all standard constructors, including dependent products, dependent sums, identity types, and other inductive types, and takes as input a \"weak model\" - a comprehension category, equipped with structure corresponding to the desired logical constructions, and constructs an equivalent split one, whose logical structure is strictly stable under reindexing #REFR. The specified keyword in this citation sentence is \"structure\".", "given_citation_intent": "method", "given_keywords": ["structure"]}]}
{"citing_id": "2303.11759v1", "cited_id": "1608.06993", "section_title": "5.", "citation": "DenseNets #REFR : Introduced in 2018, the DenseNet architecture contributes to techniques aimed at information preservation in deep neural networks.", "text_before_citation": [], "text_after_citation": ["Coming at the back of successes (and challenges) with preceding architectures, notably ResNets and InceptionNets, DenseNets take advantage of feature re-use as a way to preserve information flow.", "What this means is, as opposed to combining features of each layer with those of preceding layers via addition (as is the case in ResNets), DenseNets use concatenation technique, such that the output feature Figure 4 : A residual connection between a layer and successive layers map of each layer is concatenated with those of preceding layers, the output of which is sent to subsequent layers.", "Compared to ResNets, the number of parameters is considerably smaller and it has the added advantage of improved flow of information and gradients through the network, thus making it easier to train.", "Compared to InceptionNets which also concatenate feature maps, making the network wider, the DenseNets are simpler and more efficient.", "For this task, we train DenseNet-121 architecture on our data."], "citing_paper_content": {"title": "Simulating Malaria Detection In Laboratories Using Deep Learning", "abstract": "Malaria is usually diagnosed by a microbiologist by examining a small sample of blood smear. Reducing mortality from malaria infection is possible if it is diagnosed early and followed with appropriate treatment. While the WHO has set audacious goals of reducing malaria incidence and mortality rates by 90% in 2030 and eliminating malaria in 35 countries by that time [1], it still remains a difficult challenge. Computer-assisted diagnostics are on the rise these days as they can be used effectively as a primary test in the absence of or providing assistance to a physician or pathologist. The purpose of this paper is to describe an approach to detecting, localizing and counting parasitic cells in blood sample images towards easing the burden on healthcare workers."}, "cited_paper_content": {"title": "Densely Connected Convolutional Networks", "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL ."}, "keywords": ["deep neural networks", "DenseNet architecture"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Computer-assisted diagnostics are on the rise these days as they can be used effectively as a primary test in the absence of or providing assistance to a physician or pathologist. The purpose of this paper is to describe an approach to detecting, localizing and counting parasitic cells in blood sample images towards easing the burden on healthcare workers. To achieve this, the authors propose a Dense Convolutional Network (DenseNet) that connects each layer to every other layer in a feed-forward fashion, which substantially reduces the number of parameters and obtains significant improvements over the state-of-the-art on most of the benchmark tasks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Deep neural networks have shown remarkable performance in various computer vision tasks, including object recognition, due to their ability to learn hierarchical representations of visual data; for instance, the DenseNet architecture proposed in [Densely Connected Convolutional Networks] connects each layer to every other layer in a feed-forward fashion, which substantially reduces the number of parameters and strengthens feature propagation. #REFR", "given_citation_intent": "background", "given_keywords": ["deep neural networks", "DenseNet architecture"]}]}
{"citing_id": "2304.13575v1", "cited_id": "1510.09129", "section_title": "Main Lines Of The Computation", "citation": "A bit later, I proved that in the heptagrid, the tessellation {7, 3} of the hyperbolic plane, there is a weakly universal cellular automaton with three states which is rotation invariant and which is truly planar, #REFR .", "text_before_citation": ["The first paper about a universal cellular automaton in the pentagrid, the tessellation {5, 4} of the hyperbolic plane, was #OTHEREFR .", "This cellular automaton was also rotation invariant, at each step of the computation, the set of non quiescent states had infinitely many cycles: we shall say that it is a truly planar cellular automaton. That automaton had 22 states.", "That result was improved by a cellular automaton with 9 states in #OTHEREFR .", "Recently, it was improved with 5 states, see #OTHEREFR ."], "text_after_citation": ["Later, I improved the result down to two states but the rules are no more rotation invariant, see #OTHEREFR .", "Paper #OTHEREFR constructs three cellular automata which are strongly universal and rotation invariant: one in the pentagrid, one in the heptagrid, one in the tessellation {5, 3, 4} of the hyperbolic 3D-space.", "By strongly universal we mean that the initial configuration is finite, i.e. it lies within a large enough circle.", "In the present paper, we borrow ideas from #OTHEREFR and from the previous paper, #OTHEREFR .", "To #OTHEREFR , we borrow the idea of implementing a two register structure which is finite at each time of the computation."], "citing_paper_content": {"title": "A Strongly Universal Cellular Automaton On The Heptagrid With Seven States, New Proof", "abstract": "In this paper, we prove that there is a strongly universal cellular automaton on the heptagrid with seven states which is rotation invariant. This improves a previous paper of the author with the same number of states. Here, the structures are simpler and the number of rules is much less."}, "cited_paper_content": {"title": "A Weakly Universal Cellular Automaton On The Heptagrid With Three States", "abstract": "In this paper, we prove that there is a weakly universal cellular automaton on the pentagrid with three states which is rotation invariant and which uses \\`a la Moore neighbourhood. Moreover, at each step of the computation, the set of non quiescent states has always infinitely many cycles."}, "keywords": ["heptagrid", "weakly universal cellular"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The first paper about a universal cellular automaton in the pentagrid, the tessellation {5, 4} of the hyperbolic plane, was improved with a weakly universal cellular automaton on the pentagrid with three states, which is rotation invariant and uses \\`a la Moore neighbourhood. At each step of the computation, the set of non quiescent states has always infinitely many cycles. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors of the current paper build upon previous work on cellular automata in the pentagrid, including a rotation-invariant weakly universal cellular automaton with three states that was proven to have infinitely many cycles in the set of non-quiescent states at each step of computation #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "In the heptagrid, a weakly universal cellular automaton with three states was proven to exist and be rotation invariant, as shown in #REFR. This result builds upon the previous work on universal cellular automata in the pentagrid and provides further insight into the behavior of cellular automata on non-Euclidean grids.", "given_citation_intent": "background", "given_keywords": ["heptagrid", "weakly universal cellular"]}]}
{"citing_id": "2303.13743v1", "cited_id": "1812.04948", "section_title": "Experiments And Results", "citation": "The qualitative results for TEGLO trained on FFHQ #REFR data for single-view 3D reconstruc- tion on samples from CelebA-HQ are in Fig.(9) .", "text_before_citation": ["In Fig.(10) , we show qualitative results including the texture image (t O ) for complex appearance and geometry such as multi-view consistent eyeglasses, 3D make-up and hair.", "Compared with Fig.(24) in #OTHEREFR , we show improved multi-view consistent results for eyeglasses in row-1, and 3D make-up in row-2.", "Compared with Fig.(25) in #OTHEREFR , we show multi-view consistent representations for beard that the baseline method #OTHEREFR was unable to model.", "Single-view 3D reconstruction.", "It is the task of representing an in-the-wild or out-of-distribution image using a trained network."], "text_after_citation": ["Previous work such as AUVNet #OTHEREFR require additional training of a ResNet-18 #OTHEREFR for the image encoder and IM-Net #OTHEREFR for the shape decoder followed by ray marching to obtain the mesh to represent the image while methods such as EG3D #OTHEREFR require PTI (Pivotal Tuning Inversion #OTHEREFR ) fine-tuning to represent the image.", "For single-view textured 3D representation in TEGLO, we simply invert the image into the latent and do not require any fine-tuning.", "Reconstructing single-view images at arbitrary resolutions while preserving 3D consistency is very desirable for many applications.", "However, EG3D #OTHEREFR has a limitation in performing this task because its generator is conditioned on the camera intrinsic and extrinsic parameters, leading to a \"baked-in\" training image resolution.", "As TEGLO does not condition on the camera, it enables single-view 3D reconstruction and novel view synthesis at arbitrary resolutions without requiring re-training for different resolutions."], "citing_paper_content": {"title": "Teglo: High Fidelity Canonical Texture Mapping From Single-View Images", "abstract": "Recent work in Neural Fields (NFs) learn 3D representations from class-specific single view image collections. However, they are unable to reconstruct the input data preserving high-frequency details. Further, these methods do not disentangle appearance from geometry and hence are not suitable for tasks such as texture transfer and editing. In this work, we propose TEGLO (Textured EG3D-GLO) for learning 3D representations from single view in-the-wild image collections for a given class of objects. We accomplish this by training a conditional Neural Radiance Field (NeRF) without any explicit 3D supervision. We equip our method with editing capabilities by creating a dense correspondence mapping to a 2D canonical space. We demonstrate that such mapping enables texture transfer and texture editing without requiring meshes with shared topology. Our key insight is that by mapping the input image pixels onto the texture space we can achieve near perfect reconstruction (\u2265 74 dB PSNR at 1024 2 resolution). Our formulation allows for high quality 3D consistent novel view synthesis with high-frequency details at megapixel image resolution."}, "cited_paper_content": {"title": "A Style-Based Generator Architecture For Generative Adversarial Networks", "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces."}, "keywords": ["single-view 3D reconstruc-", "samples"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Single-view 3D reconstruction. It is the task of representing an in-the-wild or out-of-distribution image using a trained network. For this purpose, the authors propose TEGLO (Textured EG3D-GLO) which learns 3D representations from single view in-the-wild image collections for a given class of objects. The proposed method achieves near-perfect reconstruction by mapping the input image pixels onto the texture space. The authors also demonstrate that such mapping enables texture transfer and editing without requiring meshes with shared topology. The proposed method outperforms the baseline method in terms of multi-view consistent results for eyeglasses, 3D make-up, and beard representations. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Compared with the baseline method, the proposed TEGLO achieves near-perfect reconstruction of high-frequency details, such as hair and freckles, as shown in Fig.(10), demonstrating superior performance in texture transfer and editing tasks (#REFR).", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Compared with the single-view 3D reconstruction method proposed in the reference paper #REFR, our approach achieves near perfect reconstruction of high-frequency details in the texture space, as demonstrated by the high-quality samples in Fig.(10).", "given_citation_intent": "result", "given_keywords": ["single-view 3D reconstruc-", "samples"]}]}
{"citing_id": "2305.02247v1", "cited_id": "1509.01240", "section_title": "On-Average Stability", "citation": "In contrast to prior work #REFR that considers only uniformity with respect to the data-set, we extend the generalization error upper bounds for a general class of gradient-based algorithms (A(\u03b7 t , T )).", "text_before_citation": ["A key part of our analysis is the derivation of upper bounds by considering on-average algorithmic stability, namely 1", "n n i=1 A R (S) \u2212 A R (S (i) )", ".", "Through on-average stability we show upper bounds for the generalization error by using a unified analysis among all algorithms A R within A(\u03b7 t , T ).", "Specifically, we develop a uniform stability analysis for all data-sets S and selection rules R."], "text_after_citation": ["In line with standard prior work, for convex and nonconvex losses we assume that the loss is uniformly Lipschitz.", "For strongly-convex losses, the Lipschitz assumption does not hold and we instead consider a relaxed path-boundedness assumption on the loss gradients, introduced later to avoid clutter (see Section 5)."], "citing_paper_content": {"title": "Select Without Fear: Almost All Mini-Batch Schedules Generalize Optimally", "abstract": "We establish matching upper and lower generalization error bounds for mini-batch Gradient Descent (GD) training with either deterministic or stochastic, data-independent, but otherwise arbitrary batch selection rules. We consider smooth Lipschitz-convex/nonconvex/stronglyconvex loss functions, and show that classical upper bounds for Stochastic GD (SGD) also hold verbatim for such arbitrary nonadaptive batch schedules, including all deterministic ones. Further, for convex and strongly-convex losses we prove matching lower bounds directly on the generalization error uniform over the aforementioned class of batch schedules, showing that all such batch schedules generalize optimally. Lastly, for smooth (non-Lipschitz) nonconvex losses, we show that full-batch (deterministic) GD is essentially optimal, among all possible batch schedules within the considered class, including all stochastic ones."}, "cited_paper_content": {"title": "Train Faster, Generalize Better: Stability Of Stochastic Gradient Descent", "abstract": "We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. ::: Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit."}, "keywords": ["gradient-based algorithms"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "A key part of our analysis is the derivation of upper bounds by considering on-average algorithmic stability, namely 1 n n i=1 A R (S) \u2212 A R (S (i) ) . Through on-average stability we show upper bounds for the generalization error by using a unified analysis among all algorithms A R within A(\u03b7 t , T ). Specifically, we develop a uniform stability analysis for all data-sets S and selection rules R. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Through on-average stability, the authors show upper bounds for the generalization error by using a unified analysis among all algorithms A_R within A(\u03b7_t, T), and derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions, as demonstrated in #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The on-average stability analysis used in our work to derive upper bounds for the generalization error of gradient-based algorithms is based on the algorithmic stability of stochastic gradient methods, as shown in #REFR.", "given_citation_intent": "result", "given_keywords": ["gradient-based algorithms"]}]}
{"citing_id": "2303.09849v1", "cited_id": "2003.07833", "section_title": "Ablation Study", "citation": "Table 3 demonstrates that (1) without stage 1 (epoch=0), our method performs even worse than TF-VAEGAN #REFR , and (2) including stage 1 in training has a positive effect, and the more epochs we pretrain at stage 1, the better results we finally achieve.", "text_before_citation": ["t-SNE Visualization.", "To further verify our method, we conduct t-SNE #OTHEREFR visualization for synthesized unseen features of our method and TF-VAEGAN #OTHEREFR on CUB.", "As shown in Figure 2 , compared to TF-VAEGAN #OTHEREFR , the synthesized features of our method are more compact and more distinct, especially in the top-left corner of the figure. Importance of stage 1.", "We take experiments to verify the importance of pretraining at stage 1.", "Table 3 shows the conventional ZSL results of our method of CUB dataset with different training epochs in stage 1."], "text_after_citation": [], "citing_paper_content": {"title": "Exploiting Semantic Attributes For Transductive Zero-Shot Learning", "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes by generalizing the relation between visual features and semantic attributes learned from the seen classes. A recent paradigm called transductive zero-shot learning further leverages unlabeled unseen data during training and has obtained impressive results. These methods always synthesize unseen features from attributes through a generative adversarial network to mitigate the bias towards seen classes. However, they neglect the semantic information in the unlabeled unseen data and thus fail to generate high-fidelity attribute-consistent unseen features. To address this issue, we present a novel transductive ZSL method that produces semantic attributes of the unseen data and imposes them on the generative process. In particular, we first train an attribute decoder that learns the mapping from visual features to semantic attributes. Then, from the attribute decoder, we obtain pseudo-attributes of unlabeled data and integrate them into the generative model, which helps capture the detailed differences within unseen classes so as to synthesize more discriminative features. Experiments on five standard benchmarks show that our method yields state-of-the-art results for zero-shot learning."}, "cited_paper_content": {"title": "Latent Embedding Feedback And Discriminative Features For Zero-Shot Classification", "abstract": "Zero-shot learning strives to classify unseen categories for which no data is available during training. In the generalized variant, the test samples can further belong to seen or unseen categories. The state-of-the-art relies on Generative Adversarial Networks that synthesize unseen class features by leveraging class-specific semantic embeddings. During training, they generate semantically consistent features, but discard this constraint during feature synthesis and classification. We propose to enforce semantic consistency at all stages of (generalized) zero-shot learning: training, feature synthesis and classification. We further introduce a feedback loop, from a semantic embedding decoder, that iteratively refines the generated features during both the training and feature synthesis stages. The synthesized features together with their corresponding latent embeddings from the decoder are transformed into discriminative features and utilized during classification to reduce ambiguities among categories. Experiments on (generalized) zero-shot learning for object and action classification reveal the benefit of semantic consistency and iterative feedback for GAN-based networks, outperforming existing methods on six zero-shot learning benchmarks."}, "keywords": ["TF-VAEGAN", "training"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To address the issue of generating high-fidelity attribute-consistent unseen features in transductive zero-shot learning, the authors propose a method that produces semantic attributes of the unseen data and imposes them on the generative process, as described in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our method outperforms existing methods on six zero-shot learning benchmarks, as demonstrated in the reference paper #REFR.", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Our method outperforms TF-VAEGAN #REFR on CUB dataset, as shown in Figure 2 and Table 3, where the synthesized features of our method are more compact and distinct, especially in the top-left corner of the figure, and our method achieves better conventional ZSL results with different training epochs in stage 1.", "given_citation_intent": "result", "given_keywords": ["TF-VAEGAN", "training"]}]}
{"citing_id": "2304.12424v1", "cited_id": "1911.08748", "section_title": "I. Introduction", "citation": "Matching the pathology of new patients with already diagnosed and curated cases offers pathologists a new approach to improve diagnostic accuracy through visual inspection of similar cases and a computational majority vote for consensus building #REFR .", "text_before_citation": ["The emergence of digital pathology (DP) has opened new horizons for histopathology #OTHEREFR .", "Machine Learning (ML) algorithms are able to operate on digitized slides to assist pathologists with different tasks.", "Whereas ML-involving classification and segmentation methods have obvious benefits for image analysis, image search represents a fundamental shift in computational pathology #OTHEREFR ."], "text_after_citation": ["Pathologists examine tissue slides under a microscope on a regular basis and write diagnostic and prognostic reports based on their visual inspections.", "The use of immunologic research methodologies in histopathology has resulted in a significant improvement in neoplasm microscopic diagnosis #OTHEREFR .", "Immunohistochemistry (IHC) has become a formidable tool at the pathologist's disposal, despite the fact that histological analysis of hematoxylin and eosin (H&E) stained tissue sections remain at the core of the discipline #OTHEREFR - #OTHEREFR .", "IHC is a technique for detecting specific antigens (proteins) in tissue slices using labeled antibodies that bind with the antigen #OTHEREFR .", "The purpose of staining is to draw attention to the area of interest while also providing contrast against the 'background'."], "citing_paper_content": {"title": "Immunohistochemistry Biomarkers-Guided Image Search For Histopathology", "abstract": "Medical practitioners use a number of diagnostic tests to make a reliable diagnosis. Traditionally, Haematoxylin and Eosin (H&E) stained glass slides have been used for cancer diagnosis and tumor detection. However, recently a variety of immunohistochemistry (IHC) stained slides can be requested by pathologists to examine and confirm diagnoses for determining the subtype of a tumor when this is difficult using H&E slides only. Deep learning (DL) has received a lot of interest recently for image search engines to extract features from tissue regions, which may or may not be the target region for diagnosis. This approach generally fails to capture high-level patterns corresponding to the malignant or abnormal content of histopathology images. In this work, we are proposing a targeted image search approach, inspired by the pathologists' workflow, which may use information from multiple IHC biomarker images when available. These IHC images could be aligned, filtered, and merged together to generate a composite biomarker image (CBI) that could eventually be used to generate an attention map to guide the search engine for localized search. In our experiments, we observed that an IHC-guided image search engine can retrieve relevant data more accurately than a conventional (i.e., H&E-only) search engine without IHC guidance. Moreover, such engines are also able to accurately conclude the subtypes through majority votes."}, "cited_paper_content": {"title": "Yottixel -- An Image Search Engine For Large Archives Of Histopathology Whole Slide Images", "abstract": "With the emergence of digital pathology, searching for similar images in large archives has gained considerable attention. Image retrieval can provide pathologists with unprecedented access to the evidence embodied in already diagnosed and treated cases from the past. This paper proposes a search engine specialized for digital pathology, called Yottixel, a portmanteau for \"one yotta pixel,\" alluding to the big-data nature of histopathology images. The most impressive characteristic of Yottixel is its ability to represent whole slide images (WSIs) in a compact manner. Yottixel can perform millions of searches in real-time with a high search accuracy and low storage profile. Yottixel uses an intelligent indexing algorithm capable of representing WSIs with a mosaic of patches by converting them into a small number of methodically extracted barcodes, called \"Bunch of Barcodes\" (BoB), the most prominent performance enabler of Yottixel. The performance of the prototype platform is qualitatively tested using 300 WSIs from the University of Pittsburgh Medical Center (UPMC) and 2,020 WSIs from The Cancer Genome Atlas Program (TCGA) provided by the National Cancer Institute. Both datasets amount to more than 4,000,000 patches of 1000x1000 pixels. We report three sets of experiments that show that Yottixel can accurately retrieve organs and malignancies, and its semantic ordering shows good agreement with the subjective evaluation of human observers."}, "keywords": ["pathology"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed search engine specialized for digital pathology, Yottixel, can represent whole slide images in a compact manner and perform millions of searches in real-time with high accuracy and low storage profile #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To represent whole slide images (WSIs) in a compact manner, Yottixel uses an intelligent indexing algorithm capable of converting them into a small number of methodically extracted barcodes, called \"Bunch of Barcodes\" (BoB), which is the most prominent performance enabler of Yottixel #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Yottixel's intelligent indexing algorithm, capable of representing whole slide images with a mosaic of patches, has shown impressive performance in real-time searches with high accuracy and low storage profile, making it a valuable tool for digital pathology. #REFR", "given_citation_intent": "method", "given_keywords": ["pathology"]}]}
{"citing_id": "2303.14829v1", "cited_id": "1907.11692", "section_title": "Experimental Results And Evaluations", "citation": "We again use pre-trained roBERTa #REFR for text embedding which gives an embedding of size 768 for 'determinant + subject', 'verb', 'auxiliary verb', 'determinant + object', and the whole caption.", "text_before_citation": ["We use the same train, validation, and test split as the existing methods. Metrics.", "For evaluation with the existing methods, we use the following set of video captioning metrics: BLEU@4 #OTHEREFR , METEOR #OTHEREFR , ROUGE-L #OTHEREFR and CIDEr #OTHEREFR .", "CIDEr #OTHEREFR is studied to be robust in the condition where the semantic meaning of the caption remains intact #OTHEREFR .", "Also, we are the first to use GPT-2 #OTHEREFR pre-trained model for measuring the Grammatical correctness Score (GS) of the captions generated by our model in comparison to state-of-the-art which we implement for the GS metric, demonstrating improved performance (see Section 4.2). Implementation.", "For text, we have used spaCy 2 with roBERTa #OTHEREFR , a version of BERT #OTHEREFR , to extract POS components along with nouns from the groundtruth captions."], "text_after_citation": ["Following the existing methods, we use InceptionRes-NetV2 #OTHEREFR to extract the spatial features and C3D #OTHEREFR to extract the temporal features.", "These features are projected to 512 sizes before being input into the network.", "We train for epochs 25 and use a learning rate of 0.00015, batch size 16, ADAM optimizer #OTHEREFR , 16 samples per video as well as a hidden state size of 512 for the Caption block. Our model has 76M parameters and 0.045s inference time.", "Apart from that, we use Yolov7 #OTHEREFR for extracting object features for the noun anchor.", "The whole implementation is performed using one NVIDIA GeForce RTX 3090 and PyTorch."], "citing_paper_content": {"title": "Sem-Pos: Grammatically And Semantically Correct Video Captioning", "abstract": "Generating grammatically and semantically correct captions in video captioning is a challenging task. The captions generated from the existing methods are either word-byword that do not align with grammatical structure or miss key information from the input videos. To address these issues, we introduce a novel globallocal fusion network, with a Global-Local Fusion Block (GLFB) that encodes and fuses features from different parts of speech (POS) components with visual-spatial features. We use novel combinations of different POS components-'determinant + subject', 'auxiliary verb', 'verb', and 'determinant + object' for supervision of the POS blocks-Det + Subject, Aux Verb, Verb, and Det + Object respectively. The novel global-local fusion network together with POS blocks helps align the visual features with language description to generate grammatically and semantically correct captions. Extensive qualitative and quantitative experiments on benchmark MSVD and MSRVTT datasets demonstrate that the proposed approach generates more grammatically and semantically correct captions compared to the existing methods, achieving the new state-of-the-art. Ablations on the POS blocks and the GLFB demonstrate the impact of the contributions on the proposed method."}, "cited_paper_content": {"title": "Roberta: A Robustly Optimized Bert Pretraining Approach", "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."}, "keywords": ["whole caption", "embedding"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "For measuring the Grammatical correctness Score (GS) of the captions generated by our model, we used roBERTa, a version of BERT, to extract POS components along with nouns from the groundtruth captions, and we are the first to use GPT-2 pre-trained model for this purpose, achieving improved performance compared to the state-of-the-art #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To measure the Grammatical correctness Score (GS) of the captions, we used roBERTa, a version of BERT, to extract POS components from the groundtruth captions, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To ensure grammatical and semantic correctness of the whole caption, the authors used roBERTa #REFR to extract POS components and nouns from the groundtruth captions.", "given_citation_intent": "method", "given_keywords": ["whole caption", "embedding"]}]}
{"citing_id": "2304.03571v1", "cited_id": "1511.07289", "section_title": "\u0392-Vae Implementation Details", "citation": "The chosen activation function is the exponential linear unit (ELU) #REFR for all layers except the last, where the activation is linear.", "text_before_citation": ["During training, the distributions are sampled to generate inputs to the decoder network, while only the \u00b5 values are used to encode the time series used later by the predictor.", "The decoder model is designed as an almost symmetric network to the encoder.", "Latent-space samples are fed into a fully-connected layer, and its output is reshaped to the same shape as the last convolutional layer in the encoder.", "Six transposed convolution layers are then used to increase the spatial dimension with decreasing filters.", "A final transposed convolution layer with two filters produces the two output channels."], "text_after_citation": [], "citing_paper_content": {"title": "\u0392-Variational Autoencoders And Transformers For Reduced-Order Modelling Of Fluid Flows", "abstract": "Variational autoencoder (VAE) architectures have the potential to develop reduced-order models (ROMs) for chaotic fluid flows. We propose a method for learning compact and near-orthogonal ROMs using a combination of a \u03b2-VAE and a transformer, tested on numerical data from a twodimensional viscous flow in both periodic and chaotic regimes. The \u03b2-VAE is trained to learn a compact latent representation of the flow velocity, and the transformer is trained to predict the temporal dynamics in latent space. Using the \u03b2-VAE to learn disentangled representations in latentspace, we obtain a more interpretable flow model with features that resemble those observed in the proper orthogonal decomposition, but with a more efficient representation. Using Poincar\u00e9 maps, the results show that our method can capture the underlying dynamics of the flow outperforming other prediction models. The proposed method has potential applications in other fields such as weather forecasting, structural dynamics or biomedical engineering."}, "cited_paper_content": {"title": "Fast And Accurate Deep Network Learning By Exponential Linear Units (Elus)", "abstract": "We introduce the\"exponential linear unit\"(ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."}, "keywords": ["chosen activation function", "exponential linear unit"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "During training, the distributions are sampled to generate inputs to the decoder network, while only the \u00b5 values are used to encode the time series used later by the predictor. The decoder model is designed as an almost symmetric network to the encoder. Latent-space samples are fed into a fully-connected layer, and its output is reshaped to the same shape as the last convolutional layer in the encoder. Six transposed convolution layers are then used to increase the spatial dimension with decreasing filters. A final transposed convolution layer with two filters produces the two output channels. ELUs can be used as activation functions in the convolutional layers to speed up learning and improve generalization performance compared to ReLUs and LReLUs, as demonstrated in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To learn a compact latent representation of the flow velocity, the authors used a \u03b2-VAE method combined with a transformer, as described in #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The authors used the exponential linear unit (ELU) as the chosen activation function in their deep neural network, which has been shown to improve learning characteristics compared to other units with different activation functions, as demonstrated in #REFR.", "given_citation_intent": "method", "given_keywords": ["chosen activation function", "exponential linear unit"]}]}
{"citing_id": "2304.09965v1", "cited_id": "1902.07288", "section_title": "(3)", "citation": "To this end, we don't need to include the case where m = 0 in the boundary condition in #REFR .", "text_before_citation": ["It is worth pointing out that when an honest miner mines a block and then the length of the authentic branch grows to L, the length of the counterfeit branch cannot be the same as that of the authentic branch.", "This is because if so, then before the honest miner mines a block, the counterfeit branch has already grown to L blocks, and hence, the counterfeit branch has already been longer than the authentic branch which implies that we have already reached the boundary condition in (2) before the honest miner mines a block."], "text_after_citation": ["By employing (1), #OTHEREFR and 3, the pursuit of the closed-form expression for \u00c8 L (m, n) can be cast as a two-sided boundary hitting problem for a two-dimensional random walk with two possible moving directions, which is illustrated in Fig. 4 .", "To be specific, let s T [m, n] T + t=T t=1 \u03b4 t denote the position of the random walk in two-dimensional space after the T -th step (i.e., after T blocks have been mined in the blockchain), where for any t, \u03b4 t is a random vector \u03b4 t = \u03b4 #OTHEREFR [\u22121, 0]", "T with probability I, \u03b4 #OTHEREFR", "EQUATION", "Let"], "citing_paper_content": {"title": "Vulnerability Of Finitely-Long Blockchains In Securing Data", "abstract": "Recently, blockchain has been applied in various fields to secure data exchanges and storage in decentralized systems. In a blockchain application where the task of the application which makes use of the data stored in a blockchain has to be accomplished by a time instant, the employed blockchain is essentially finitely-long. In this paper, we consider a general finitely-long blockchain model which is generalized from most existing works on finitely-long blockchain applications, and take the first step towards characterizing the vulnerability of finitely-long blockchains in securing data against double-spending attacks. For the first time, we develop a general closed-form expression for the probability of success in launching a double-spending attack on a finitely-long blockchain. This probability essentially characterizes the vulnerability of finitely-long blockchains. Then, we prove that the probability of success in launching a doublespending attack on a finitely-long blockchain is no greater than that on an infinitely-long blockchain, which implies that finitelylong blockchains are less vulnerable to double-spending attacks than infinitely-long blockchains. Moreover, we show that unlike infinitely-long blockchains which can be surely paralyzed by a 51% attack, finitely-long blockchains are more resistant to 51% attacks. Index Terms-Finitely-long blockchain, double-spending attack, proof-of-work, 51% attack."}, "cited_paper_content": {"title": "Secure Distributed Dynamic State Estimation In Wide-Area Smart Grids", "abstract": "Smart grid is a large complex network with a myriad of vulnerabilities, usually operated in adversarial settings and regulated based on estimated system states. In this study, we propose a novel highly secure distributed dynamic state estimation mechanism for wide-area (multi-area) smart grids, composed of geographically separated subregions, each supervised by a local control center. We first propose a distributed state estimator assuming regular system operation that achieves near-optimal performance based on the local Kalman filters and with the exchange of necessary information between local centers. To enhance the security, we further propose to 1) protect the network database and the network communication channels against attacks and data manipulations via a blockchain (BC)-based system design, where the BC operates on the peer-to-peer network of local centers, 2) locally detect the measurement anomalies in real-time to eliminate their effects on the state estimation process, and 3) detect misbehaving (hacked/faulty) local centers in real-time via a distributed trust management scheme over the network. We provide theoretical guarantees regarding the false alarm rates of the proposed detection schemes, where the false alarms can be easily controlled. Numerical studies illustrate that the proposed mechanism offers reliable state estimation under regular system operation, timely and accurate detection of anomalies, and good state recovery performance in case of anomalies."}, "keywords": ["case"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The proposed mechanism in #REFR offers reliable state estimation under regular system operation, timely and accurate detection of anomalies, and good state recovery performance in case of anomalies.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This vulnerability of finitely-long blockchains against double-spending attacks can be addressed by using a blockchain-based system design, as proposed in #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "This vulnerability of finitely-long blockchains has been addressed in recent studies, where a blockchain-based system design has been proposed to protect the network database and communication channels against attacks and data manipulations, as well as to detect misbehaving local centers in real-time via a distributed trust management scheme over the network #REFR.", "given_citation_intent": "background", "given_keywords": ["case"]}]}
{"citing_id": "2303.10622v2", "cited_id": "2002.09006", "section_title": "Orthogonal Multiplicity.", "citation": "Indeed, Harase #REFR obtained Tausworthe generators over F 2 with t-value two or three for s = 3, but they were not optimal with respect to the t-value.", "text_before_citation": ["#OTHEREFR ), have the t-value zero for s = 3 only if the period length is exactly three.", "Their proof was specialized for the case F 2 ; for example, they used the property", "L m \u2229 U m = {I} in [16, Proof of Theorem 1],", "where L m and U m denote a set of non-singular m \u00d7 m lower-triangular and upper-triangular matrices, respectively.", "This is false in the fields F b except for F 2 ."], "text_after_citation": ["Thus, we conducted a search over F b , whose restrictions are looser than those over F 2 ."], "citing_paper_content": {"title": "A Generalization Of Short-Period Tausworthe Generators And Its Application To Markov Chain Quasi-Monte Carlo", "abstract": "A one-dimensional sequence u 0 , u 1 , u 2 ,. .. \u2208 [0, 1) is said to be completely uniformly distributed (CUD) if overlapping s-blocks (u i , u i+1 ,. .. , u i+s\u22121), i = 0, 1, 2,. . ., are uniformly distributed for every dimension s \u2265 1. This concept naturally arises in Markov chain quasi-Monte Carlo (QMC). However, the definition of CUD sequences is not constructive, and thus there remains the problem of how to implement the Markov chain QMC algorithm in practice. Harase (2021) focused on the t-value, which is a measure of uniformity widely used in the study of QMC, and implemented short-period Tausworthe generators (i.e., linear feedback shift register generators) over the twoelement field F 2 that approximate CUD sequences by running for the entire period. In this paper, we generalize a search algorithm over F 2 to that over arbitrary finite fields F b with b elements and conduct a search for Tausworthe generators over F b with t-values zero (i.e., optimal) for dimension s = 3 and small for s \u2265 4, especially in the case where b = 3, 4, and 5. We provide a parameter table of Tausworthe generators over F 4 , and report a comparison between our new generators over F 4 and existing generators over F 2 in numerical examples using Markov chain QMC."}, "cited_paper_content": {"title": "A Table Of Short-Period Tausworthe Generators For Markov Chain Quasi-Monte Carlo", "abstract": "We consider the problem of estimating expectations by using Markov chain Monte Carlo methods and improving the accuracy by replacing IID uniform random points with quasi-Monte Carlo (QMC) points. Recently, it has been shown that Markov chain QMC remains consistent when the driving sequences are completely uniformly distributed (CUD). However, the definition of CUD sequences is not constructive, so an implementation method using short-period Tausworthe generators (i.e., linear feedback shift register generators over the two-element field) that approximate CUD sequences has been proposed. In this paper, we conduct an exhaustive search of short-period Tausworthe generators for Markov chain QMC in terms of the $t$-value, which is a criterion of uniformity widely used in the study of QMC methods. We provide a parameter table of Tausworthe generators and show the effectiveness in a numerical example using Gibbs sampling."}, "keywords": ["Tausworthe generators"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Harase (2021) proposed an implementation method for completely uniformly distributed (CUD) sequences using short-period Tausworthe generators, which approximate CUD sequences and have been shown to be effective in Markov chain quasi-Monte Carlo (QMC) methods. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The implementation of short-period Tausworthe generators proposed by Harase (2021) that approximate completely uniformly distributed (CUD) sequences by running for the entire period is effective in improving the accuracy of Markov chain Monte Carlo methods by replacing IID uniform random points with quasi-Monte Carlo (QMC) points, as shown in their exhaustive search of short-period Tausworthe generators for Markov chain QMC in terms of the t-value, which is a criterion of uniformity widely used in the study of QMC methods. #REFR", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Harase (2021) proposed an implementation method for Markov chain quasi-Monte Carlo using short-period #REFR Tausworthe generators that approximate completely uniformly distributed sequences, which has been shown to remain consistent, and conducted an exhaustive search of Tausworthe generators for Markov chain quasi-Monte Carlo in terms of the t-value.", "given_citation_intent": "background", "given_keywords": ["Tausworthe generators"]}]}
{"citing_id": "2304.02539v1", "cited_id": "1906.02530", "section_title": "Datasets:", "citation": "In the literature, there exist many further evaluation scores, particularly for assessing probability calibration #REFR .", "text_before_citation": ["Moreover, NLL is a proper scoring rule #OTHEREFR ) such that the best score corresponds to a perfect prediction.", "Brier score (BS, \u2193), proposed by Brier (1950), is another proper scoring rule, which measures the squared error between predicted probability vectors and one-hot encoded target vectors:", "EQUATION", "AP-BS (X, y, Z,p \u03b8,\u03c9", "EQUATION"], "text_after_citation": ["As a comprehensive evaluation of probabilities is beyond this article's scope, we focus on proper scoring rules inducing calibration measures.", "Accordingly, we have omitted other evaluation scores, such as the expected calibration error (Naeini et al., 2015) being a non-proper scoring rule.", "Multi-annotator supervised learning techniques: By default, we train MaDL via the weighted loss function in Eq.", "25 using the hyperparameter values from Section 4 and the most general architecture depicted by Fig. 3 .", "In addition to ablations as part of analyzing the three RQs, we present a detailed ablation study on the hyperparameters of MaDL in Appendix A."], "citing_paper_content": {"title": "Multi-Annotator Deep Learning: A Probabilistic Framework For Classification", "abstract": "Solving complex classification tasks using deep neural networks typically requires large amounts of annotated data. However, corresponding class labels are noisy when provided by error-prone annotators, e.g., crowd workers. Training standard deep neural networks leads to subpar performances in such multi-annotator supervised learning settings. We address this issue by presenting a probabilistic training framework named multi-annotator deep learning (MaDL). A ground truth and an annotator performance model are jointly trained in an end-to-end learning approach. The ground truth model learns to predict instances' true class labels, while the annotator performance model infers probabilistic estimates of annotators' performances. A modular network architecture enables us to make varying assumptions regarding annotators' performances, e.g., an optional class or instance dependency. Further, we learn annotator embeddings to estimate annotators' densities within a latent space as proxies of their potentially correlated annotations. Together with a weighted loss function, we improve the learning from correlated annotation patterns. In a comprehensive evaluation, we examine three research questions about multi-annotator supervised learning. Our findings indicate MaDL's state-of-the-art performance and robustness against many correlated, spamming annotators."}, "cited_paper_content": {"title": "Can You Trust Your Model'S Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift", "abstract": "Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks."}, "keywords": ["probability calibration"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Moreover, NLL is a proper scoring rule #OTHEREFR ) such that the best score corresponds to a perfect prediction. Brier score (BS, \u2193), proposed by Brier (1950), is another proper scoring rule, which measures the squared error between predicted probability vectors and one-hot encoded target vectors: EQUATION AP-BS (X, y, Z,p \u03b8,\u03c9 EQUATION. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. #REFR presents a large-scale benchmark of existing state-of-the-art methods on classification problems and investigates the effect of dataset shift on accuracy and calibration.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Quantifying predictive uncertainty is critical in real-world settings, especially when input distributions are shifted from the training distribution, and many probabilistic deep learning methods have been proposed for this purpose, as discussed in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Probability calibration is critical for quantifying uncertainty in machine learning models, especially in real-world settings where input distributions are shifted from the training distribution, and well-calibrated uncertainty estimates convey information about when a model's output should be trusted (#REFR).", "given_citation_intent": "background", "given_keywords": ["probability calibration"]}]}
{"citing_id": "2303.16109v1", "cited_id": "1706.03762", "section_title": "I. Introduction", "citation": "The framework also employs state-of-the-art transformer neural networks #REFR augmented by manoeuvre-specific heads to predict multiple trajectories conditioned on the predicted manoeuvre vectors.", "text_before_citation": ["To address the aforementioned limitations, we propose a novel Multimodal Manoeuvre and Trajectory Prediction (MMnTP) framework.", "Firstly, we propose a bespoke formulation of manoeuvre prediction based on a vector representation of manoeuvres.", "This representation includes a sequence of manoeuvre types and transition times between those during the prediction window.", "To increase the plausibility of the predictions, constraints are introduced on the manoeuvre types and the number of allowed manoeuvre changes within the prediction horizon.", "We then propose a multimodal discriminative manoeuvre prediction model using this new formulation."], "text_after_citation": ["To train the model, a novel multimodal manoeuvre prediction loss function and a mode selection method are proposed based on the predicted types and timings of manoeuvres.", "The proposed framework is evaluated in highway driving scenarios using two public trajectory datasets, namely NGSIM #OTHEREFR and highD #OTHEREFR . Our contributions can be summarised as follows:", "\u2022 A bespoke formulation of manoeuvre prediction, which allows estimating a sequence of manoeuvre types and transition times between them.", "\u2022 A novel transformer-based model to predict multimodal manoeuvres and their corresponding trajectories.", "\u2022 A tailored multimodal training method using a new multimodal manoeuvre loss function and mode selection method."], "citing_paper_content": {"title": "Multimodal Manoeuvre And Trajectory Prediction For Autonomous Vehicles Using Transformer Networks", "abstract": "Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users, including vehicles, is critical for the safe and efficient operation of autonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to the uncertain future behaviour of vehicles, multiple future behaviour modes are often plausible for a vehicle in a given driving scene. Therefore, multimodal prediction can provide richer information than single-mode prediction enabling AVs to perform a better risk assessment. To this end, we propose a novel multimodal prediction framework that can predict multiple plausible behaviour modes and their likelihoods. The proposed framework includes a bespoke problem formulation for manoeuvre prediction, a novel transformer-based prediction model, and a tailored training method for multimodal manoeuvre and trajectory prediction. The performance of the framework is evaluated using two public benchmark highway driving datasets, namely NGSIM and highD. The results show that the proposed framework outperforms the state-of-theart multimodal methods in the literature in terms of prediction error and is capable of predicting plausible manoeuvre and trajectory modes."}, "cited_paper_content": {"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."}, "keywords": ["predicted manoeuvre vectors"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To increase the plausibility of the predictions, constraints are introduced on the manoeuvre types and the number of allowed manoeuvre changes within the prediction horizon. We then propose a multimodal discriminative manoeuvre prediction model using this new formulation, building upon the attention mechanism-based Transformer architecture proposed in #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To predict multiple plausible behaviour modes and their likelihoods, the proposed framework includes a novel transformer-based prediction model#REFR, which has been shown to be superior in quality and more parallelizable than complex recurrent or convolutional neural networks.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To predict plausible manoeuvre vectors, the proposed Multimodal Manoeuvre and Trajectory Prediction (MMnTP) framework employs a novel transformer-based prediction model, as introduced in the reference paper \"Attention Is All You Need\" (#REFR).", "given_citation_intent": "method", "given_keywords": ["predicted manoeuvre vectors"]}]}
{"citing_id": "2304.05497v1", "cited_id": "0911.0460", "section_title": "Related Work", "citation": "In contrast, we show that combining one specialized expert with the generic knowledge base model with simple ensemble methods such as averaging or linear stacking #REFR is generally more efficient than ensembling multiple specialized experts.", "text_before_citation": ["A follow-up line of thought extracts such information from a pretrained classifier #OTHEREFR , or even learns the optimal taxonomy jointly with the image representations #OTHEREFR .", "Such models have been shown to improve the efficiency/accuracy trade-off in classification tasks.", "However, this class-based routing is a limiting assumption, and per-sample routing has been shown to outperform hierarchical classification models when correctly parametrized #OTHEREFR .", "Finally, MoE can be seen as an ensembling technique whose weights are learned by the gate.", "While it is common to assume each sample is routed to a unique expert to maximize efficiency, some works #OTHEREFR have considered combining several experts to boost accuracy."], "text_after_citation": [], "citing_paper_content": {"title": "Revisiting Single-Gated Mixtures Of Experts", "abstract": "Mixture of Experts (MoE) are rising in popularity as a means to train extremely large-scale models, yet allowing for a reasonable computational cost at inference time. Recent state-of-the-art approaches usually assume a large number of experts, and require training all experts jointly, which often lead to training instabilities such as the router collapsing. In contrast, in this work, we propose to revisit simple single-gate MoE, which allows for more practical training. Key to our work are (i) a base model branch acting both as an early-exit and an ensembling regularization scheme, (ii) a simple and efficient asynchronous training pipeline without router collapse issues, and finally (iii) an automatic per-sample clustering-based initialization. We show experimentally that the proposed model obtains efficiency-to-accuracy trade-offs comparable with other more complex MoE, and outperforms non-mixture baselines. This showcases the merits of even a simple single-gate MoE, and motivates further exploration in this area."}, "cited_paper_content": {"title": "Feature-Weighted Linear Stacking", "abstract": "Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset."}, "keywords": ["multiple specialized experts", "simple ensemble methods"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Finally, MoE can be seen as an ensembling technique whose weights are learned by the gate. While it is common to assume each sample is routed to a unique expert to maximize efficiency, some works have considered combining several experts to boost accuracy. To improve accuracy while retaining the virtues of linear regression, Feature-Weighted Linear Stacking (FWLS) has been proposed #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To improve accuracy while retaining the virtues of linear regression, Feature-Weighted Linear Stacking (FWLS) has been proposed as a linear technique that incorporates meta-features, which was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Simple ensemble methods, such as Feature-Weighted Linear Stacking (FWLS), have been shown to improve accuracy by blending the predictions of #REFR multiple specialized experts, while retaining the virtues of linear regression regarding speed, stability, and interpretability.", "given_citation_intent": "method", "given_keywords": ["multiple specialized experts", "simple ensemble methods"]}]}
{"citing_id": "2303.08566v1", "cited_id": "1906.10771", "section_title": "Algorithm 1 Computing Task-Specific Parameter Sensitivities", "citation": "Note that although our criterion draws inspiration from pruning work #REFR , it is distinct from it.", "text_before_citation": ["(1) n \u2248 g 2 n ,", "where is the learning rate.", "Since is same for all parameters, we can eliminate it when comparing the sensitivity with the other parameters and finally get", "EQUATION", "Therefore, the sensitivity of a parameter can be efficiently measured by its potential to reduce the loss on the target domain."], "text_after_citation": ["#OTHEREFR measures the parameter importance by the squared change in loss when removing them, i.e., (E(D t , w) \u2212 E(D t , w | w n = 0)) 2 and finally derives the parameter importance by (g n w n ) 2 , which is different from our formulations in Eqs. #OTHEREFR and #OTHEREFR .", "In practice, we accumulate S from a total number of C training samples ahead of fine-tuning to generate accurate sensitivity as shown in Algorithm 1, where C is a predefined hyper-parameter.", "In Section 4.3, we show that employing only 400 training samples is sufficient for getting reasonable parameter sensitivity, which requires only 5.5 seconds with a single GPU for any VTAB-1k dataset with ViT-B/16 backbone #OTHEREFR ."], "citing_paper_content": {"title": "Sensitivity-Aware Visual Parameter-Efficient Tuning", "abstract": "Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of sensitive parameters exceeds a pre-defined threshold by utilizing any of the existing structured tuning methods, e.g., LoRA [27] or Adapter [26], to replace directly tuning the selected sensitive parameters (unstructured tuning) under the budget. Extensive experiments on a wide range of downstream recognition tasks show that our SPT is complementary to the existing VPET methods and largely boosts their performance, e.g., SPT improves Adapter with supervised pre-trained ViT-B/16 backbone by 4.2% and 1.4% mean Top-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks, respectively. Source code is at https://github.com/ ziplab/SPT."}, "cited_paper_content": {"title": "Importance Estimation For Neural Network Pruning", "abstract": "Structural pruning of neural network parameters reduces computational, energy, and memory transfer costs during inference. We propose a novel method that estimates the contribution of a neuron (filter) to the final loss and iteratively removes those with smaller scores. We describe two variations of our method using the first and second-order Taylor expansions to approximate a filter's contribution. Both methods scale consistently across any network layer without requiring per-layer sensitivity analysis and can be applied to any kind of layer, including skip connections. For modern networks trained on ImageNet, we measured experimentally a high (>93%) correlation between the contribution computed by our methods and a reliable estimate of the true importance. Pruning with the proposed methods led to an improvement over state-of-the-art in terms of accuracy, FLOPs, and parameter reduction. On ResNet-101, we achieve a 40% FLOPS reduction by removing 30% of the parameters, with a loss of 0.02% in the top-1 accuracy on ImageNet."}, "keywords": ["pruning work"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "The sensitivity of a parameter can be efficiently measured by its potential to reduce the loss on the target domain, as proposed in the reference paper #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method in #REFR estimates the contribution of a neuron to the final loss and removes those with smaller scores, which leads to an improvement over state-of-the-art in terms of accuracy, FLOPs, and parameter reduction.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The proposed method for estimating the contribution of a neuron to the final loss and iteratively removing those with smaller scores, as described in #REFR, has shown to be effective in reducing computational, energy, and memory transfer costs during inference, which is a crucial aspect of pruning work.", "given_citation_intent": "background", "given_keywords": ["pruning work"]}]}
{"citing_id": "2303.03075v1", "cited_id": "1905.09604", "section_title": "Instances Of The Redistribution Mechanism Framework", "citation": "The largest known set of diffusion auction mechanisms with the above properties is Critical Diffusion Mechanism (CDM) #REFR .", "text_before_citation": ["In our network-based redistribution mechanism framework, if we require the output mechanism to be IC and IR, then the input diffusion auction mechanism should also be IC, IR and non-deficit."], "text_after_citation": ["Especially, the first diffusion auction mechanism, Incentive Diffusion Mechanism (IDM) #OTHEREFR is also a member in CDM, which has the highest efficiency.", "In this section, we input IDM and another mechanism in CDM called Threshold Neighbourhood Mechanism (TNM) [12] into our framework to see the outcomes.", "For convenience, we briefly introduce the idea of the IDM and TNM with our notations.", "Both IDM and TNM first find the agent with the highest valuation and their critical ancestors.", "Then the mechanisms check these agents from the sponsor to the agent with the highest valuation."], "citing_paper_content": {"title": "A Redistribution Framework For Diffusion Auctions", "abstract": "Redistribution mechanism design aims to redistribute the revenue collected by a truthful auction back to its participants without affecting the truthfulness. We study redistribution mechanisms for diffusion auctions, which is a new trend in mechanism design [19]. The key property of a diffusion auction is that the existing participants are incentivized to invite new participants to join the auctions. Hence, when we design redistributions, we also need to maintain this incentive. Existing redistribution mechanisms in the traditional setting are targeted at modifying the payment design of a truthful mechanism, such as the Vickrey auction. In this paper, we do not focus on one specific mechanism. Instead, we propose a general framework to redistribute the revenue back for all truthful diffusion auctions for selling a single item. The framework treats the original truthful diffusion auction as a black box, and it does not affect its truthfulness. The framework can also distribute back almost all the revenue."}, "cited_paper_content": {"title": "Diffusion And Auction On Graphs", "abstract": "Auction is the common paradigm for resource allocation which is a fundamental problem in human society. Existing research indicates that the two primary objectives, the seller's revenue and the allocation efficiency, are generally conflicting in auction design. For the first time, we expand the domain of the classic auction to a social graph and formally identify a new class of auction mechanisms on graphs. All mechanisms in this class are incentive-compatible and also promote all buyers to diffuse the auction information to others, whereby both the seller's revenue and the allocation efficiency are significantly improved comparing with the Vickrey auction. It is found that the recently proposed information diffusion mechanism is an extreme case with the lowest revenue in this new class. Our work could potentially inspire a new perspective for the efficient and optimal auction design and could be applied into the prevalent online social and economic networks. \u00a9 2019 International Joint Conferences on Artificial Intelligence. All rights reserved."}, "keywords": ["diffusion auction mechanisms"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our proposed general framework for redistributing the revenue back for all truthful diffusion auctions for selling a single item does not affect the truthfulness of the original auction mechanism, and can distribute back almost all the revenue. This framework is applicable to all truthful diffusion auctions that are IC, IR, and non-deficit, and can potentially inspire a new perspective for efficient and optimal auction design in prevalent online social and economic networks. #REFR", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Diffusion auctions on graphs have been shown to significantly improve both the seller's revenue and the allocation efficiency, and a recent study has identified a new class of auction mechanisms on graphs that are incentive-compatible and promote information diffusion among buyers, which could potentially inspire a new perspective for efficient and optimal auction design in online social and economic networks (#REFR).", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Diffusion auction mechanisms have been identified as a new class of auction mechanisms on graphs that are incentive-compatible and promote information diffusion, leading to significant improvements in both the seller's revenue and the allocation efficiency compared to the Vickrey auction, as demonstrated in the reference paper #REFR.", "given_citation_intent": "background", "given_keywords": ["diffusion auction mechanisms"]}]}
{"citing_id": "2303.13559v1", "cited_id": "1904.04100", "section_title": "Librispeech Comparisons", "citation": "We only compare with w2v-U since it has achieved significantly better results than former GAN or HMM based unsupervised methods #REFR .", "text_before_citation": ["Our model performs better than w2v-U-L under three configurations: with or without ST and 4-gram or transformer language models.", "The absolute improvements ranges from 0.2% to 1.7%, and the relative improvements ranges from 5.1% to 12.8%.", "These results reflect that injecting instance noises of various intensities and appending diffusion timestep-dependent discriminators during adversarial training are effective.", "Table 2 reports phoneme error rates (PER) under TIMIT's matched and unmatched training data setups.", "The same 4gram LM is used for these four models."], "text_after_citation": ["Under both matched and unmatched setups, our diffusion-GAN enhanced w2v-U models outperforms w2v-U, with absolution improvements from 0.3% to 1.4% and relative improvements from 2.7% to 7.9%."], "citing_paper_content": {"title": "Enhancing Unsupervised Speech Recognition With Diffusion Gans", "abstract": "We enhance the vanilla adversarial training method for unsupervised Automatic Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance noises of various intensities to the generator's output and unlabeled reference text which are sampled from pretrained phoneme language models with a length constraint, (2) asks diffusion timestep-dependent discriminators to separate them, and (3) back-propagates the gradients to update the generator. Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our enhancement strategies work effectively."}, "cited_paper_content": {"title": "Completely Unsupervised Phoneme Recognition By A Generative Adversarial Network Harmonized With Iteratively Refined Hidden Markov Models", "abstract": "Producing a large annotated speech corpus for training ASR systems remains difficult for more than 95% of languages all over the world which are low-resourced, but collecting a relatively big unlabeled data set for such languages is more achievable. This is why some initial effort have been reported on completely unsupervised speech recognition learned from unlabeled data only, although with relatively high error rates. In this paper, we develop a Generative Adversarial Network (GAN) to achieve this purpose, in which a Generator and a Discriminator learn from each other iteratively to improve the performance. We further use a set of Hidden Markov Models (HMMs) iteratively refined from the machine generated labels to work in harmony with the GAN. The initial experiments on TIMIT data set achieve an phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art."}, "keywords": ["former GAN", "based unsupervised methods"], "citation_intent": "result", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Our model achieves better performance than the previous state-of-the-art in completely unsupervised speech recognition by using a Generative Adversarial Network (GAN) harmonized with iteratively refined Hidden Markov Models (HMMs) #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The initial experiments on TIMIT dataset achieve an impressive phone error rate of 33.1%, which is 8.5% lower than the previous state-of-the-art, and can serve as a benchmark for future unsupervised speech recognition studies. #REFR", "given_citation_intent": "result", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The former GAN-based unsupervised methods achieved a phone error rate of 33.1% on TIMIT dataset, which is significantly higher than the results reported in this manuscript. #REFR", "given_citation_intent": "result", "given_keywords": ["former GAN", "based unsupervised methods"]}]}
{"citing_id": "2303.02160v1", "cited_id": "1603.04467", "section_title": "A.2 Training Details", "citation": "We trained all agents using Tensorflow 2.3 #REFR and the OpenAI Baselines PPO2 implementation [17] with a distributed sampler.", "text_before_citation": ["We provide important details about our training setup here.", "We train all three agents with PPO #OTHEREFR , a popular deep reinforcement learning algorithm.", "We choose PPO for training our agents for a few reasons.", "This algorithm is commonly used because it is found to be empirically robust and effective in a wide range of tasks #OTHEREFR .", "We train each of the three agent architectures for 15 hours, the equivalent of 10 million training timesteps, on at least 3 different random seeds."], "text_after_citation": ["For a full list of training hyperparameters used in all agent versions, please refer to Table 6 .", "We found this set to perform best on preliminary experiments.", "To effectively train agents in a complex video game setting, we use a distributed approach leveraging an in-house sample collection framework and Azure cloud resources.", "Training samples are collected from a scaleset of 20 low priority GPU virtual machines (Azure NV6), each running 3 video game instances.", "The samples are then sent to one training head node, a CPU-only Azure E32s memory-optimized virtual machine."], "citing_paper_content": {"title": "Navigates Like Me: Understanding How People Evaluate Human-Like Ai In Video Games", "abstract": "We aim to understand how people assess human likeness in navigation produced by people and artificially intelligent (AI) agents in a video game. To this end, we propose a novel AI agent with the goal of generating more human-like behavior. We collect hundreds of crowd-sourced assessments comparing the human-likeness of navigation behavior generated by our agent and baseline AI agents with human-generated behavior. Our proposed agent passes a Turing Test, while the baseline agents do not. By passing a Turing Test, we mean that human judges could not quantitatively distinguish between videos of a person and an AI agent navigating. To understand what people believe constitutes human-like navigation, we extensively analyze the justifications of these assessments. This work provides insights into the characteristics that people consider human-like in the context of goal-directed video game navigation, which is a key step for further improving human interactions with AI agents. CCS CONCEPTS \u2022 Applied computing \u2192 Computer games; \u2022 Computing methodologies \u2192 Reinforcement learning; \u2022 Human-centered computing \u2192 Empirical studies in HCI."}, "cited_paper_content": {"title": "Tensorflow: Large-Scale Machine Learning On Heterogeneous Distributed Systems", "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."}, "keywords": ["Tensorflow"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "We provide important details about our training setup here. We train all three agents with PPO, a popular deep reinforcement learning algorithm. We choose PPO for training our agents for a few reasons. This algorithm is commonly used because it is found to be empirically robust and effective in a wide range of tasks (Tensorflow: Large-Scale Machine Learning On Heterogeneous Distributed Systems). We train each of the three agent architectures for 15 hours, the equivalent of 10 million training timesteps, on at least 3 different random seeds.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We train our agents using PPO, a popular deep reinforcement learning algorithm that has been found to be empirically robust and effective in a wide range of tasks #OTHEREFR, as described in the reference paper \"Tensorflow: Large-Scale Machine Learning On Heterogeneous Distributed Systems\".", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "We train our agents using PPO, a popular deep reinforcement learning algorithm that is empirically robust and effective in a wide range of tasks, including those involving #KEYWORD(TensorFlow) #REFR.", "given_citation_intent": "method", "given_keywords": ["Tensorflow"]}]}
{"citing_id": "2305.02029v1", "cited_id": "1905.09598", "section_title": "Topic Modelling", "citation": "More sophisticated methods tend to be based on more complex algorithms such as neural networks, such as the work from #REFR which explores the use of self-organising maps (SOMs) to reduce dimensionality within the data and create an interpretable 2D map of topics.", "text_before_citation": ["Topic modelling provides an unsupervised approach to topic allocation in texts, with a broad range of complexities.", "It will be explored in sections four and five of this paper Simple approaches such as Latent Semantic Indexing (LSI) #OTHEREFR involve the vectorisation of texts within a corpus and grouping together based on co-sine similarity (an effective measure used to compare similarity of vectors #OTHEREFR ).", "Such methods are quick to implement and require little resources but come with the large disadvantage that the nature of the topic groupings remains unknown, making the results very difficult to interpret."], "text_after_citation": ["But whilst having the advantage of interpretability these methods are often computationally expensive and can tie up valuable resources within an organisation.", "Latent Dirichlet Allocation (LDA) #OTHEREFR provides a middle ground between overly simply non-interpretable and overly-complex resource heavy topic modelling techniques, and is one of the most commonly used methods in the field #OTHEREFR .", "LDA involves creating a latent layer of topics within a dataset where words that are likely to be found near each other within texts are grouped.", "Each text within a corpus is then evaluated for a percentage match with each of the topics in the latent layer to allow allocation.", "One of the drawbacks with LDA is that as a statistical approach, interpretation of the topics is still required to achieve sensible results, just because words are statistically found near each other does not necessarily mean they will be considered related by a human observer."], "citing_paper_content": {"title": "Natural Language Processing On Customer Note Data", "abstract": "Automatic analysis of customer data for businesses is an area that is of interest to companies. Business to business data is studied rarely in academia due to the sensitive nature of such information. Applying natural language processing can speed up the analysis of prohibitively large sets of data. This paper addresses this subject and applies sentiment analysis, topic modelling and keyword extraction to a B2B data set. We show that accurate sentiment can be extracted from the notes automatically and the notes can be sorted by relevance into different topics. We see that without clear separation topics can lack relevance to a business context."}, "cited_paper_content": {"title": "Cuda-Self-Organizing Feature Map Based Visual Sentiment Analysis Of Bank Customer Complaints For Analytical Crm", "abstract": "With the widespread use of social media, companies now have access to a wealth of customer feedback data which has valuable applications to Customer Relationship Management (CRM). Analyzing customer grievances data, is paramount as their speedy non-redressal would lead to customer churn resulting in lower profitability. In this paper, we propose a descriptive analytics framework using Self-organizing feature map (SOM), for Visual Sentiment Analysis of customer complaints. The network learns the inherent grouping of the complaints automatically which can then be visualized too using various techniques. Analytical Customer Relationship Management (ACRM) executives can draw useful business insights from the maps and take timely remedial action. We also propose a high-performance version of the algorithm CUDASOM (CUDA based Self Organizing feature Map) implemented using NVIDIA parallel computing platform, CUDA, which speeds up the processing of high-dimensional text data and generates fast results. The efficacy of the proposed model has been demonstrated on the customer complaints data regarding the products and services of four leading Indian banks. CUDASOM achieved an average speed up of 44 times. Our approach can expand research into intelligent grievance redressal system to provide rapid solutions to the complaining customers."}, "keywords": ["topics", "self-organising maps"], "citation_intent": "method", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "Topic modelling provides an unsupervised approach to topic allocation in texts, with a broad range of complexities. It will be explored in sections four and five of this paper. Simple approaches such as Latent Semantic Indexing (LSI) #REFR involve the vectorisation of texts within a corpus and grouping together based on co-sine similarity (an effective measure used to compare similarity of vectors). Such methods are quick to implement and require little resources but come with the large disadvantage that the nature of the topic groupings remains unknown, making the results very difficult to interpret. The proposed model in #REFR uses Self-organizing feature map (SOM) for Visual Sentiment Analysis of customer complaints, which can automatically learn the inherent grouping of the complaints and provide useful business insights to Analytical Customer Relationship Management (ACRM) executives.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "The paper proposes a high-performance version of the Self-organizing feature map (SOM) algorithm, CUDASOM, which is implemented using NVIDIA parallel computing platform, CUDA, and achieves an average speed up of 44 times on high-dimensional text data #REFR.", "given_citation_intent": "method", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "Self-organizing feature maps (SOM) provide an effective unsupervised approach to topic allocation in texts, as demonstrated in the reference paper #REFR, which proposes a descriptive analytics framework for visual sentiment analysis of customer complaints using SOM.", "given_citation_intent": "method", "given_keywords": ["topics", "self-organising maps"]}]}
{"citing_id": "2303.01675v1", "cited_id": "1802.04799", "section_title": "Deep Learning Compilers", "citation": "TVM #REFR is a compiler that exposes graph-level and operatorlevel optimizations to provide performance portability for DL workloads across diverse hardware backends.", "text_before_citation": ["HLO #OTHEREFR is an single assignment-based intermediate representation(IR) for tensor computations in XLA.", "MLIR #OTHEREFR is a reusable and extensible compiler infrastructure that standardizes the static single Assignment-based IR data structures and provides a declarative system to define IR dialects.", "Gshard #OTHEREFR and GSPMD #OTHEREFR introduce collective communication primitives on HLO IR and provides convenient APIs for sharding large models.", "Relay #OTHEREFR presents a compiler framework to unify and generalize IR in existing frameworks."], "text_after_citation": [], "citing_paper_content": {"title": "Ada-Grouper: Accelerating Pipeline Parallelism In Preempted Network By Adaptive Group-Scheduling For Micro-Batches", "abstract": "Pipeline parallelism has been demonstrated to be a remarkable approach to improve throughput for training deep neural networks with billions of parameters over heterogeneous clusters. The 1F1B scheduling plan is a widely adopted strategy for memory and performance optimization, which interchanges the forward and backward stage computations of different micro-batches. On the other hand, a common issue in using the 1F1B scheduling is that stage computation is delayed due to the data transfer when network resources are preempted by other tasks, even with the minimum communication between stages. The exclusive access of these network resources cannot be guaranteed in cloud offerings. We present a general scheduling technique to accommodate pipeline parallelism to preempted network environments at the expense of a certain amount of memory pressure. The core concept is to extend 1F1B schedule scheme to kFkB, which groups k micro-batches, and alternately executes k forward and backward computations. We propose Ada-Grouper, an adaptive kFkB scheduler which regularly adjusts the number of group members k to maintain an optimal balance between communication and computation efficiency correspond to changes in a changing network environment under the memory limit. Experimental results demonstrate that our design maintains stable performance for pipeline parallelism, yielding a performance increase of up from 4% to 30%, compared with 1F1B in preempted network scenarios."}, "cited_paper_content": {"title": "Tvm: An Automated End-To-End Optimizing Compiler For Deep Learning", "abstract": "There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies."}, "keywords": ["DL workloads", "operatorlevel optimizations"], "citation_intent": "background", "generated_citations": [{"model": "gpt-3.5-turbo-0301", "text": "To provide performance portability to deep learning workloads across diverse hardware back-ends, TVM proposes a compiler that exposes graph-level and operator-level optimizations, solves optimization challenges specific to deep learning, and automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations #REFR.", "given_citation_intent": null, "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To bring machine learning to a wide diversity of hardware devices, there is an increasing need for automated end-to-end optimizing compilers such as TVM #REFR, which provides performance portability to deep learning workloads across diverse hardware back-ends, including mobile phones, embedded devices, and accelerators.", "given_citation_intent": "background", "given_keywords": null}, {"model": "gpt-3.5-turbo-0301", "text": "To provide performance portability for DL workloads across diverse hardware back-ends, TVM proposes graph-level and #OTHEREFR operator-level optimizations, which automate optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. #REFR", "given_citation_intent": "background", "given_keywords": ["DL workloads", "operatorlevel optimizations"]}]}
